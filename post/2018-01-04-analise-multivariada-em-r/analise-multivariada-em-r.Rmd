---
title: Análise Multivariada com R
author: Fellipe Gomes
date: '2018-01-04'
slug: []
categories:
  - Estatistica
  - R
  - Teoria
  - Analise Mutivariada
  - Nao supervisionado
  - Clustering
tags:
  - Estatistica
  - gomesfellipe
  - R Markdown
  - R
  - Teoria
  - pca
  - kmeans
  - clustering
  - analise multivariada
description: ''
featured: 'multivariada-fatorial-cluster-R.png'
featuredalt: 'Pic 9'
featuredpath: 'date'
linktitle: ''
type: "post"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, error = F, warning = F)
```

# Análise Multivariada   

[Imagem do Wikpedia](https://commons.wikimedia.org/wiki/File:Multivariate_Gaussian.png) 

Esse é o primeiro post do ano e como no ano de 2017 falou-se tanto das maravilhas computacionais desta onda do Big Data e em contra partida, [identificamos que deste 2004 a popularidade pelo termo "estatística" vem diminuindo  como mostrei em uma breve pesquisa neste post sobre a API do googletrends](https://gomesfellipe.github.io/post/2017-12-12-google-trends-e-r/google-trends-e-r/) sinto que existe uma necessidade de se ampliar também a divulgação dos métodos estatísticos pois o aprofundamento na teoria é fundamental (é muito fácil achar resultados sem fundamento apenas "apertando botão"), como as ferramentas da estatística multivariada que muitas vezes servem como soluções para essas grandes quantidades de dados 

Diversas vezes nos deparamos com bases de dados que envolvem além de muitas observações, muitas variáveis, especialmente nas análises de fenômenos ou processos sociais, psicológicos, educacionais e econômicos bem como na área da química, biologia, geologia, marketing, medicina, medicina veterinária, dentre muitas outras.

Com as ferramentas estatísticas da análise multivariada somos capazes de identificar muitos elementos que podem ter relevância na análise dos dados, dois exemplos de ferramentas importantes são as que permitem encontrar fatores que não são diretamente observáveis com base em um conjunto de variáveis observáveis e as que permitem agrupar conjuntos de dados que possuem características semelhantes com algorítimos computacionais (chamados de aprendizados não-supervisionados ou semi-supervisionados em machine learning) e a partir dai estudar as novas classificações.

Neste post será apresentado algumas soluções para o caso em que existe a necessidade de avaliar um grande conjunto de dados com muitas variáveis e não temos muitas informações a respeito. 

# Análise Fatorial

Na análise fatorial buscamos fatores que explicam parte da variância total dos dados, os fatores são as somas das variâncias originais.

```{r,echo=F}
dados=read.csv2("Dados.csv")
library(dplyr)
```


## Objetivo da análise fatorial:

  * Procura identificar fatores que não são diretamente observáveis, com base em um conjunto de variáveis observáveis.

  * Explicar a correlação ou covariância, entre um conjunto de variáveis, em termos de um número limitado de variáveis não-observáveis, chamadas de fatores ou variáveis latentes.
  
  * Em casos nos quais se tem um número grande de variáveis medidas e correlacionadas entre si, seria possível identificar-se um número menor de variáveis alternativas, não correlacionadas e que de algum modo sumarizassem as informações principais das variáveis originais.
  
  * A partir do momento em que os fatores são identificados, seus valores numéricos, chamados de escores, podem ser obtidos para cada elemento amostral. Conseqüentemente, estes escores podem ser utilizados em outras análises que envolvam outras técnicas estatísticas, como análise de regressão ou análise de variância, por exemplo.
  
## Etapas para realização

  * Computação da matriz de correlações para as variáveis originais;
  
  * Extração de fatores
  
  * Rotação dos fatores para tonar a interpretação mais fácil;
  
  * Cálculo dos escores dos fatores
  
### Matriz de Correlação:

  * Teste de Bartlett - a hipótese nula da matriz de correlação ser uma matriz identidade ( $| R | = 1$ ), isto é, avalia se os componentes fora da diagonal principal são zero. O resultado significativo indica que existem algumas relações entre as variáveis.

No R:

```{r}
Bartlett.sphericity.test <- function(x)
{
  method <- "Teste de esfericidade de Bartlett"
  data.name <- deparse(substitute(x))
  x <- subset(x, complete.cases(x)) # Omitindo valores faltantes
  n <- nrow(x)
  p <- ncol(x)
  chisq <- (1-n+(2*p+5)/6)*log(det(cor(x)))
  df <- p*(p-1)/2
  p.value <- pchisq(chisq, df, lower.tail=FALSE)
  names(chisq) <- "X-squared"
  names(df) <- "df"
  return(structure(list(statistic=chisq, parameter=df, p.value=p.value,
                        method=method, data.name=data.name), class="htest"))
}
Bartlett.sphericity.test(dados)
```


  * Teste KMO (Kaiser-Meyer-Olkin) - avalia a adequação do tamanho amostra. Varia entre 0 e 1, onde: zero indica inadequado para análise fatorial, aceitável se for maior que 0.5, recomendado acima de 0.8.
  
No R:

```{r}
kmo = function(x)
{
  x = subset(x, complete.cases(x))
  r = cor(x)
  r2 = r^2 
  i = solve(r) 
  d = diag(i) 
  p2 = (-i/sqrt(outer(d, d)))^2 
  diag(r2) <- diag(p2) <- 0 
  KMO = sum(r2)/(sum(r2)+sum(p2))
  MSA = colSums(r2)/(colSums(r2)+colSums(p2))
  return(list(KMO=KMO, MSA=MSA))
}

kmo(dados)
```


#### Tipos de correlação:

Nem sempre é possível utilizar a correlação de pearson, porém, existem diversas outras maneiras de se saber qual a correlação dos dados. Podemos utilizar correlações como de Spearman, Policórica, etc.. Já fiz um post onde explico os [diferentes tipos de relações entre os tipos de variáveis](https://gomesfellipe.github.io/post/tipos-de-relacoes-entre-variaveis/) e os [tipos de correlações](https://gomesfellipe.github.io/post/tipos-de-correlacoes/) possíveis para avaliar a relação dessas variáveis.
  
Aqui um outro exemplo de como utilizar a correlação parcial

```{r}
partial.cor <- function (x)
{
R <- cor(x)
RI <- solve(R)
D <- 1/sqrt(diag(RI))
Rp <- -RI * (D %o% D)
diag(Rp) <- 0
rownames(Rp) <- colnames(Rp) <- colnames(x)
Rp
}
mat_anti_imagem <- -partial.cor(dados[,1:10])
mat_anti_imagem
```

  
### Extração de fatores via componentes principais

  * Determinado o número de fatores necessários para representar os dados
  
  * Também é determinado o método que será utilizado, o mais utilizado é a análise de componentes principais

#### Estimação do número de fatores m

  * Estimação do número de fatores m

  * Para a estimação de m, bastará extrair-se os autovalores da matriz de correlação amostral.

  * Observa-se quais autovalores são os mais importantes em termos de grandeza numérica.
  
  * os autovalores refletem a importância do fator se o número de fatores for igual ao número de variáveis então a soma dos autovetores é igual a soma das variâncias (pois cada variância será igual a 1).
  
  * Portanto a razão $ \lambda / 2 var $ indica proporção da variabilidade total explicada pelo fator
  
**Critérios:**

  1. A análise da proporção da variância total relacionada com cada autovalor ($\lambda_i$). Permanecem aqueles autovalores que maiores proporções da variância total e, portanto, o valor de m será igual ao número de autovalores retidos;
  
  2. A comparação do valor numérico de ($\lambda_i$) com o valor 1. O valor de m será igual ao número de autovalores maiores ou iguais a 1.
  
  3. Observação do gráfico scree-plot, que dispõe os valores de ($\lambda_i$) ordenados em ordem decrescente. Por este critério, procura-se no gráfico um "ponto de salto", que estaria representando um decréscimo de importância em relação à variância total. O valor de m seria então igual ao número de autovalores anteriores ao "ponto de salto".

  
### Análise de componentes Principais:

  * Fatores são obtidos através da decomposição espectral da matriz de correlações, resultado em cargas fatoriais que indicam o quanto cada variável está associada a cada fator e os autovalores associados a cada um dos fatores envolvidos

  * São formadas combinações lineares das variáveis observadas.

  * O primeiro componente principal consiste na combinação que responde pela maior quantidade de variância na amostra.

  * O segundo componente responde pela segunda maior variância na amostra e não é correlacionado com o primeiro componente.

  * Sucessivos componentes explicam progressivamente menores porções de variância total da amostra e todos são não correlacionados uns aos outros.
  
No R a análise de componentes principais pode ser realizada com as funções nativas `prcomp()` e a visualização pode ser realizada com a função `biplot` nativa do R ou com a função `autoplot()` do pacote `ggfortify` apresentado em um posto que eu [comento sobre ajustes de modelos lineares](https://gomesfellipe.github.io/post/2017-12-26-diagnostico-de-modelos/diagnostico-de-modelos/). 

Neste exemplo utilizaremos [as funções de código aberto encontrei nesse github](https://github.com/vqv/ggbiplot/blob/master/R/ggscreeplot.r) que permite elaborar o gráfico baseado em funções do `ggplot`, além disso também carregaremos o pacote deste Github. Veja:

```{r,echo=F}
# 
#  ggscreeplot.r
#
# link: https://github.com/vqv/ggbiplot/blob/master/R/ggscreeplot.r
#  Copyright 2011 Vincent Q. Vu.
# 
#  This program is free software; you can redistribute it and/or
#  modify it under the terms of the GNU General Public License
#  as published by the Free Software Foundation; either version 2
#  of the License, or (at your option) any later version.
#  
#  This program is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#  
#  You should have received a copy of the GNU General Public License
#  along with this program; if not, write to the Free Software
#  Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
# 

#' Screeplot for Principal Components
#'
#' @param pcobj          an object returned by prcomp() or princomp()
#' @param type           the type of scree plot.  'pev' corresponds proportion of explained variance, i.e. the eigenvalues divided by the trace. 'cev' corresponds to the cumulative proportion of explained variance, i.e. the partial sum of the first k eigenvalues divided by the trace.
#' @export
#' @examples
#'   data(wine)
#'   wine.pca <- prcomp(wine, scale. = TRUE)
#'   print(ggscreeplot(wine.pca))
#'

ggscreeplot <- function(pcobj, type = c('pev', 'cev')) 
{
  type <- match.arg(type)
  d <- pcobj$sdev^2
  yvar <- switch(type, 
                 pev = d / sum(d), 
                 cev = cumsum(d) / sum(d))

  yvar.lab <- switch(type,
                     pev = 'proportion of explained variance',
                     cev = 'cumulative proportion of explained variance')

  df <- data.frame(PC = 1:length(d), yvar = yvar)

  ggplot(data = df, aes(x = PC, y = yvar)) + 
    xlab('principal component number') + ylab(yvar.lab) +
    geom_point() + geom_path()
}
```


```{r,warning=F}
library(ggplot2)
library(ggfortify)
library(ggbiplot)
#Componentes principais:
acpcor=prcomp(dados, scale = TRUE)
summary(acpcor)

ggbiplot(acpcor, obs.scale = 1, var.scale = 1,
   ellipse = TRUE, circle = TRUE) +
  scale_color_discrete(name = '') +
  theme(legend.direction = 'horizontal', legend.position = 'top')


# autoplot(acpcor, label = TRUE, label.size = 1,
#          loadings = TRUE, loadings.label = TRUE, loadings.label.size  = 3)
```


Para a observação do gráfico scree-plot podemos utilizar os comandos a seguir (com funções nativas do R ou mesmo com funções personalizadas como a que eu acabei de comentar [disponivel nesse github](https://github.com/vqv/ggbiplot/blob/master/R/ggscreeplot.r)

```{r}
#Com Funcao nativa do R:
# plot(1:ncol(dados), acpcor$sdev^2, type = "b", xlab = "Componente",
#      ylab = "Variância", pch = 20, cex.axis = 1.3, cex.lab = 1.3)

#Ou funcao personalizada com ggplot2:
ggscreeplot(acpcor)
```

#### Rotação 

  * Algumas variáveis são mais correlacionadas com alguns fatores do que outras.
  
  * Em alguns casos, a interpretação dos fatores originais pode não ser tarefa muito fácil devido à aparição de coeficientes de grandeza numérica similar, e não desprezível, em vários fatores diferentes.

  * O propósito da rotação é obter uma estrutura simples.

  * Em uma estrutura simples, cada fator tem carga alta somente para algumas variáveis, tornando mais fácil a sua identificação.
  
  * Tipos: Varimax, Quartimax, Equamax
  
Aplicando a Varimax:

```{r}
k <- 6 #6 fatores selecionados
carfat = acpcor$rotation[, 1:k] %*% diag(acpcor$sdev[1:k])
carfatr = varimax(carfat)
```
  
#### Comunalidade
  
  * Índices atribuídos a variável original que expressam em % o quanto da variabilidade de cada variável é explicada pelo modelo

  * Designa-se por comunalidade ($h^{2}_i$)a proporção da variância de cada variável explicada pelos fatores comuns.

  * As comunalidades variam entre 0 e 1, sendo 0 quando os fatores comuns não explicam nenhuma variância da variável e 1 quando explicam toda a sua variância.

  * Quando o valor das comunalidades é menor que 0,6 deve-se
pensar em: aumentar a amostra, eliminar as variáveis.

### Interpretar o modelo

  * Feito pelas cargas fatoriais que são os parâmetros do modelo
  
  * Fatores expressam as covariâncias entre cada fator e as variáveis originais
  
  * Varimax ajuda a interpretar o modelo
  
  * Rotações ortogonais (para dependente) ; Rotações oblíquas (para independentes)
  

# Clusters

Técnica estatística multivariada que tem como objetivo organizar um conjunto de objetos em um determinado nº de subconjuntos mutuamente exclusivos (clusters), de tal forma que os objetos em um mesmo cluster sejam semelhantes entre si,porém diferentes dos objetos nos outros clusters
  
Etapas para análise de clusters, que são comuns em qualquer análise (KDD):

  * Seleção dos objetos a serem agrupados
  * Definir conjunto de atributos que caracterizam os objetos
  * Medida de dissimilaridade
  * Seleção de um algoritmo de agregação
  * Definição do número de clusters
  * Interpretação e validação dos clusters
  
Critérios para a seleção:

  * Selecionar variáveis diferentes entre si
  * Variáveis padronizadas (padronização mais comum é a Z-score)

Existem algumas abordagens para a utilização das técnicas de análises de clusters, as diferenças entre os métodos  hierárquicos e os não hierárquicos são as seguintes:

Métodos Hierárquicos são preferidos quando:

  * Serão analisadas varias alternativas de agrupamento.
  * O tamanho da amostra é moderado ( de 300 a 1000 objetos )
  
Métodos não-hierárquicos são preferidos quando:

  * O número de grupos é conhecido.
  * Presença dos outliers, desde que os métodos não-hierárquicos são
menos influenciados por outliers.
  * Há um grande nº de objetos a serem agrupados.

## Método hierárquico de agrupamento

É realizado em dois passos, o primeiro deles calcula-se a matriz de similaridade com o uso da função *dist()* (existem diversos tipos de distâncias que podem ser utilizadas aqui), o método utilizado será o de **Ward** (também poderíamos escolher o método da menor distância, maior distância ou a distância média).

Vantagens:
  
  * Rápidos e exigem menos tempo de processamento.
  * Apresentam resultados para diferentes níveis de agregação.

Desvantagens:

  * Alocação de um objeto em um cluster é irrevogável
  * Impacto substancial dos outliers ( apesar do Ward ser o menos susceptível)
  * Não apropriados para analisar uma amostra muito extensa, pois a medida que o tamanho da amostra aumenta, a necessidade de armazenamento das distâncias cresce drasticamente

Para bases grandes é melhor não usar este método pois precisa da matriz de distâncias.

Dentre os métodos, a menor distância pode ser ruim em muitas situações, pois coloca muitos objetos no mesmo cluster.

Geralmente utiliza-se o dendograma para a visualização dos clusters.

```{r,warning=F}
#Construindo a matriz de similaridade:
matriz_similaridade = dist(iris[,-5],             #Conjunto de dados utilizados
                           "euclidean"            #medida de distância utilizada
                           )

#Construindo o agrupamento hierárquico aglomerativo:
agrupamento = hclust(matriz_similaridade,     #Matriz de similaridade calculada
                     "ward.D"                 #Método de agrupamento
                     )
#Converte hclust em dendrograma e plot:
hcd <- as.dendrogram(agrupamento)

library(ggdendro)
# Tipo pode ser "rectangle" ou "triangle"
dend_data <- dendro_data(hcd, type = "rectangle")
# o que esta contido em dend_data:
names(dend_data)
```

```{r}
plot(agrupamento,xlab="Matriz de similaridade",main = "Dendograma", cex = 0.3)
#Construindo representacao de grupos - geração de vetores:
grupos = cutree(agrupamento,             #Variável calculada em hclust
                3                        #Quantidade de grupos desejados
                )

#Construindo o dendograma:
rect.hclust(agrupamento, k=3, border="red")

```

Existem diversas outras maneiras de se visualizar dendogramas, veja a seguir um outro exemplo utilizando o pacote `ape`:

```{r,warning=F}
library(ape)
plot(as.phylo(agrupamento), type = "unrooted", cex = 0.6,
     no.margin = TRUE)
```


Para mais informações de métodos de plot de dendogramas,talvez [essa página](http://www.sthda.com/english/wiki/beautiful-dendrogram-visualizations-in-r-5-must-known-methods-unsupervised-machine-learning) possa ser útil.

## Método não hierárquico de agrupamento K-means

Esta é uma das mais populares abordagens de agrupamento de dados por partição. A partir de uma escolha inicial para os centroides, o algoritmo procede verificando quais exemplares são mais similares a quais centroides.

Vantagens:
  
  * Tendem a maximizar a dispersão entre os centros de gravidade dos clusters (mantem os clusters bem separados)
  * Simplicidade de cálculo, calcula somente as distâncias entre os objetos e os centros de gravidade dos clusters

Desvantagens:

  * Depende dos conjuntos de sementes iniciais, principalmente se a seleção das sementes é aleatória
  * Não há garantias de um agrupamento ótimo dos objetos


```{r}
#Construindo o agrupamento por particionamento:
c = kmeans(iris[,-5],          #Conjunto de dados utilizados
                 2,            #Número de  grupos a ser descoberto
                 iter.max=5    #Número máximo de iterações permitido no algorítmo
                 )



```

Para efeito de visualização, podemos utilizar a seguinte função que encontra dois fatores principais a partir da análise fatorial e às utiliza como eixos

```{r,warning=F}
plot_kmeans = function(df, clusters, runs) {
  suppressMessages(library(psych))
  suppressMessages(library(ggplot2))
  
  #cluster
  tmp_k = kmeans(df, centers = clusters, nstart = 100)
  
  #factor
  tmp_f = fa(df, 2, rotate = "none")
  
  #collect data
  tmp_d = data.frame(matrix(ncol=0, nrow=nrow(df)))
  tmp_d$cluster = as.factor(tmp_k$cluster)
  tmp_d$fact_1 = as.numeric(tmp_f$scores[, 1])
  tmp_d$fact_2 = as.numeric(tmp_f$scores[, 2])
  tmp_d$label = rownames(df)
  
  #plot
  g = ggplot(tmp_d, aes(fact_1, fact_2, color = cluster)) + geom_point() + geom_text(aes(label = label), size = 3, vjust = 1, color = "black")
  return(g)
}
plot_kmeans(iris[,-5], 3)
```


### Análise exploratória dos clusters

Não vou me estender nessa parte, mas é bom esclarecer que após encontrar os clusters e de extrema importância realizar a análise exploratória deles para entender os comportamentos dos grupos identificados.

```{r}
#Conferindo os grupos formados:
c$cluster%>%
  table()

c$cluster%>%
  table()%>%
  barplot(main="Frequências dos clusters", names.arg=c("Cluster 1", "Cluster 2"))

```



## Medidas de validação e estabilidade

#### pseudo-F

O número adequado de clusters (k) deve ser maximizar o pseudo-F:

$$
pseudo-F = \dfrac{  \dfrac{BSS}{k-1} } { \dfrac{WSS}{N-k}} =\dfrac{\textrm{Quadrado médio entre clusters}}{\textrm{Quadrado médio dentro dos clusters}}
$$

#### library(clvalid)

Este pacote faz os cálculos das medidas que avaliam se os clusters são compactos, bem separados e estáveis.

Vejamos os tipos de medidas:

**Medidas de validação**:

1. conectividade: relativa ao grau de vizinhança entre objetos em um mesmo cluster, varia
entre 0 e infinito e quanto menor melhor.
2. silhueta: homogeneidade interna, assume valores entre -1 e 1 e quanto mais próximo de 1
melhor.
3. índice de Dunn: quantifica a separação entre os agrupamentos, assume valores entre 0 e 1 e
quanto maior melhor.

**Medidas de estabilidade**:

1. APN - average proportion of non-overlap: proporção média de observações não
classificadas no mesmo cluster nos casos com dados completos e incompletos. Assume valor
no intervalo [0,1], próximos de 0 indicam agrupamentos consistentes.
2. AD - average distance: distância média entre observações classificadas no mesmo cluster
nos casos com dados completos e incompletos. Assume valores não negativos, sendo
preferíveis valores próximos de zero.
3. ADM - average distance between means: distância média entre os centroides quando as
observações estão em um mesmo cluster. Assume valores não negativos, sendo preferíveis
valores próximos de zero.
4. FOM - figure of merit: medida do erro cometido ao usar os centroides como estimativas das
observações na coluna removida. Assume valores não negativos, sendo preferíveis valores
próximos de zero.


```{r,warning=F}
library(clValid)

#Medidas de validação:
valida=clValid(iris[1:4],3,clMethods=c("hierarchical","kmeans"),validation="internal")
summary(valida)

#Medidas de estabilidade;
valida=clValid(iris[1:4],3,clMethods=c("hierarchical","kmeans"),validation="stability")
summary(valida)
```

#### Gráfico da silhueta:

```{r}
library(cluster)
#Construindo a matriz de similaridade:
matriz_similaridade = dist(iris[,-5],             #Conjunto de dados utilizados
                           "euclidean"            #medida de distância utilizada
                           )

#Construindo o agrupamento hierárquico aglomerativo:
agrupamento = hclust(matriz_similaridade,     #Matriz de similaridade calculada
                     "ward.D"                 #Método de agrupamento
                     )

silhueta =silhouette(cutree(agrupamento,k=3),dist(iris[,-5]))
plot(silhueta,main="")
summary(silhueta)
```

### Muitas opções

Como podemos observar, a análise de agrupamentos é um método exploratório. É útil para organizar conjuntos de dados que contam com características semelhantes.

É uma das principais técnicas da mineração de dados e já conta com grande variedade de algoritmos.


# Referência

[I Johnson e Wichern (2007). Applied Multivariate Statistical Analysis, 6th
Edition. Prentice-Hal](https://www1.udel.edu/oiss/pdf/617.pdf)
