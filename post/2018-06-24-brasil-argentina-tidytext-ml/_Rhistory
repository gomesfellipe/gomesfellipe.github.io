LETTERS,letters,1:10,
"hat","trick","bc","de","tem","twitte","fez",
'pra',"vai","ta","so","ja","rt")
stop_words = data_frame(word = c(stopwords::stopwords("pt"), excludewords))
tidy_books <- original_books %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
# Chunk 7
#Palavras mais faladas:
tidy_books %>%
count(word, sort = TRUE)
#Apos a limpeza, caso precise voltar as frases:
original_books = tidy_books%>%
group_by(book,line)%>%
summarise(text=paste(word,collapse = " "))
# Chunk 8
suppressMessages(library(ggplot2))
tidy_books %>%
count(word, sort = TRUE) %>%
filter(n > 400) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = I("yellow"), colour = I("green"))) +
geom_col(position="dodge") +
xlab(NULL) +
labs(title = "Freq")+
coord_flip()+ theme(
panel.background = element_rect(fill = "#74acdf",
colour = "lightblue",
size = 0.5, linetype = "solid"),
panel.grid.major = element_line(size = 0.5, linetype = 'solid',
colour = "white"),
panel.grid.minor = element_line(size = 0.25, linetype = 'solid',
colour = "white")
)
# Chunk 9
#Criando nuvem de palavra:
library(wordcloud)
par(mfrow=c(1,2))
tidy_books %>%
filter(book=="br")%>%
count(word) %>%
with(wordcloud(word, n, max.words = 100,random.order = F,min.freq = 15,random.color = F,colors = c("#009b3a", "#fedf00","#002776"),scale = c(2,1),rot.per = 0.05))
tidy_books %>%
filter(book=="arg")%>%
count(word) %>%
with(wordcloud(word, n, max.words = 100,min.freq = 15,random.order = F,random.color = F,colors = c("#75ade0", "#ffffff","#f6b506"),scale = c(2,1),rot.per = 0.05))
par(mfrow=c(1,1))
# Chunk 10
# Analise de sentimentos:
library(lexiconPT)
sentiment = data.frame(word = sentiLex_lem_PT02$term ,
polarity = sentiLex_lem_PT02$polarity) %>%
mutate(sentiment = if_else(polarity>0,"positive",if_else(polarity<0,"negative","neutro")),
word = as.character(word)) %>%
as_tibble()
suppressMessages(library(tidyr))
book_sentiment <- tidy_books %>%
inner_join(sentiment) %>%
count(book,word, index = line , sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative) %T>%
print
# Chunk 11
book_sentiment%>%
count(sentiment,book)%>%
arrange(book) %>%
ggplot(aes(x = factor(sentiment),y = n,fill=book))+
geom_bar(stat="identity",position="dodge")+
facet_wrap(~book) +
theme_bw()+
scale_fill_manual(values=c("#75ade0", "#009b3a"))
# Chunk 12
# Nuvem de comparação:
suppressMessages(library(reshape2))
tidy_books %>%
filter(book=="br")%>%
inner_join(sentiment) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("red", "gray80","green"),
max.words = 200)
# Chunk 13
tidy_books %>%
filter(book=="arg")%>%
inner_join(sentiment) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("red", "gray80","green"),
max.words = 200)
# Chunk 14
# Proporção de palavras negativas:
bingnegative <- sentiment %>%
filter(sentiment == "negative")
bingpositive <- sentiment %>%
filter(sentiment == "positive")
wordcounts <- tidy_books %>%
group_by(book, line) %>%
summarize(words = n())
# Chunk 15
tidy_books %>%
semi_join(bingnegative) %>%
group_by(book, line) %>%
summarize(negativewords = n()) %>%
left_join(wordcounts, by = c("book", "line")) %>%
mutate(ratio = negativewords/words) %>%
top_n(5) %>%
ungroup() %>% arrange(desc(ratio)) %>% filter(book=="br")
# Chunk 16
base %>%
filter(book=="br",line==2580) %>% mutate(text = as.character(text))%>% select(text) %>% c()
base %>%
filter(book=="arg",line==572) %>% mutate(text = as.character(text))%>% select(text) %>% c()
# Chunk 17
tidy_books %>%
semi_join(bingpositive) %>%
group_by(book, line) %>%
summarize(positivewords = n()) %>%
left_join(wordcounts, by = c("book", "line")) %>%
mutate(ratio = positivewords/words) %>%
top_n(5) %>%
ungroup() %>% arrange(desc(ratio))
# Chunk 18
base %>%
filter(book=="br",line==2374) %>% mutate(text = as.character(text))%>% select(text) %>% c()
base %>%
filter(book=="arg",line==2120) %>% mutate(text = as.character(text))%>% select(text) %>% c()
# Chunk 19
# Obtendo numero de palavras
book_words <- original_books %>%
unnest_tokens(word, text) %>%
count(book, word, sort = TRUE) %>%
ungroup()%>%
anti_join(stop_words)
total_words <- book_words %>%
group_by(book) %>%
summarize(total = sum(n))
book_words <- left_join(book_words, total_words)
# tf-idf:
book_words <- book_words %>%
bind_tf_idf(word, book, n)
book_words %>%
arrange(desc(tf_idf))
# Chunk 20
book_words %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
group_by(book) %>%
top_n(15) %>%
ungroup %>%
ggplot(aes(word, tf_idf, fill = book)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~book, ncol = 2, scales = "free") +
coord_flip()+
theme_bw()+
scale_fill_manual(values=c("#75ade0", "#009b3a"))
# Chunk 21
# Bi grams
book_bigrams <- original_books %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
book_bigrams %>%
count(bigram, sort = TRUE)
# Chunk 22
bigrams_separated <- book_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
# new bigram counts:
bigram_counts <- bigrams_filtered %>%
count(word1, word2, sort = TRUE)
bigram_counts
# Chunk 23
bigrams_united <- bigrams_filtered %>%
unite(bigram, word1, word2, sep = " ")
bigrams_united
# Chunk 24
#bi grams com tf idf
bigram_tf_idf <- bigrams_united %>%
count(book, bigram) %>%
bind_tf_idf(bigram, book, n) %>%
arrange(desc(tf_idf))
bigram_tf_idf
# Chunk 25
bigrams_separated %>%
filter(word1 == "nao") %>%
count(word1, word2, sort = TRUE)
not_words <- bigrams_separated %>%
filter(word1 == "nao") %>%
inner_join(sentiment, by = c(word2 = "word")) %>%
count(word2, sentiment, sort = TRUE) %>%
ungroup()
not_words
# Chunk 26
not_words %>%
mutate(sentiment=ifelse(sentiment=="positive",1,ifelse(sentiment=="negative",-1,0)))%>%
mutate(contribution = n * sentiment) %>%
arrange(desc(abs(contribution))) %>%
head(20) %>%
mutate(word2 = reorder(word2, contribution)) %>%
ggplot(aes(word2, n * sentiment, fill = n * sentiment > 0)) +
geom_col() +
xlab("Words preceded by \"not\"") +
ylab("Sentiment score * number of occurrences") +
coord_flip()+
theme_bw()
# Chunk 27
# Ref: https://cfss.uchicago.edu/text_classification.html
# https://www.r-bloggers.com/illustrated-guide-to-roc-and-auc/
devtools::source_url("https://raw.githubusercontent.com/gomesfellipe/functions/master/plot_pred_type_distribution.R")
base <- base %>%
mutate(length = str_length(text),
ncap = str_count(text, "[A-Z]"),
ncap_len = ncap / length,
nexcl = str_count(text, fixed("!")),
nquest = str_count(text, fixed("?")),
npunct = str_count(text, "[[:punct:]]"),
nword = str_count(text, "\\w+"),
nsymb = str_count(text, "&|@|#|\\$|%|\\*|\\^"),
nsmile = str_count(text, "((?::|;|=)(?:-)?(?:\\)|D|P))"),
text = clean_tweets(text) %>% enc2native() %>% rm_accent())%>%
unnest_tokens(word, text) %>%
anti_join(stop_words)%>%
group_by(book,line,length, ncap, ncap_len, nexcl, nquest, npunct, nword, nsymb, nsmile)%>%
summarise(text=paste(word,collapse = " ")) %>%
select(text,everything())%T>%
print()
# Chunk 28
suppressMessages(library(tm))       #Pacote de para text mining
corpus <- Corpus(VectorSource(base$text))
#Criando a matrix de termos:
book_dtm = DocumentTermMatrix(corpus, control = list(minWordLength=2,minDocFreq=3)) %>%
weightTfIdf(normalize = T) %>%    # Transformação tf-idf com pacote tm
removeSparseTerms( sparse = .95)  # obtendo matriz esparsa com pacote tm
#Transformando em matrix, permitindo a manipulacao:
matrix = as.matrix(book_dtm)
dim(matrix)
# Chunk 29
#Criando a base de dados:
full=data.frame(cbind(
base[,"book"],
matrix,
base[,-c(1:3)]
))
# Chunk 30
set.seed(825)
particao = sample(1:2,nrow(full), replace = T,prob = c(0.7,0.3))
train = full[particao==1,]
test = full[particao==2,]
suppressMessages(library(caret))
# Chunk 31: knn
# knn -------
set.seed(825)
antes = Sys.time()
book_knn <- train(book ~.,
data=train,
method = "knn",
trControl = trainControl(method = "cv",number = 10), # validacao cruzada
preProc = c("center", "scale"))
time_knn <- Sys.time() - antes
Sys.time() - antes
plot(book_knn)
previsao  = predict(book_knn, test)
confusionMatrix(previsao, test$book)
df = cbind(fit = if_else(previsao=="br",1,0), class = if_else(test$book=="br",1,0)) %>% as.data.frame()
plot_pred_type_distribution(df,0.5)
# Chunk 32: rf
set.seed(824)
# Random Forest
antes = Sys.time()
book_rf <- train(book ~.,
data=train,
method = "rf",trace=F,
ntree = 200,
trControl = trainControl(method = "cv",number = 10))
time_rf <- Sys.time() - antes
Sys.time() - antes
suppressMessages(library(randomForest))
varImpPlot(book_rf$finalModel)
previsao  = predict(book_rf, test)
confusionMatrix(previsao, test$book)
# https://www.r-bloggers.com/illustrated-guide-to-roc-and-auc/
df = cbind(fit = if_else(previsao=="br",1,0), class = if_else(test$book=="br",1,0)) %>% as.data.frame()
plot_pred_type_distribution(df,0.5)
# Chunk 34: nb
# Naive Bayes ----
set.seed(825)
antes = Sys.time()
book_nb <- train(book ~.,
data=train,
method= "nb",
laplace =1,
trControl = trainControl(method = "cv",number = 10))
time_nb <- Sys.time() - antes
Sys.time() - antes
previsao  = predict(book_nb, test)
confusionMatrix(previsao, test$book)
# https://www.r-bloggers.com/illustrated-guide-to-roc-and-auc/
df = cbind(fit = if_else(previsao=="br",1,0), class = if_else(test$book=="br",1,0)) %>% as.data.frame()
plot_pred_type_distribution(df,0.5)
# Chunk 35: glm
# Modelo logístico ----
set.seed(825)
antes = Sys.time()
book_glm <- train(book ~.,
data=train,
method = "glm",                                         # modelo generalizado
family = binomial(link = 'logit'),                      # Familia Binomial ligacao logit
trControl = trainControl(method = "cv", number = 10))   # validacao cruzada
time_glm <- Sys.time() - antes
Sys.time() - antes
suppressMessages(library(ggfortify))
autoplot(book_glm$finalModel, which = 1:6, data = train,
colour = 'book', label.size = 3,
ncol = 3) + theme_classic()
previsao  = predict(book_glm, test)
confusionMatrix(previsao, test$book)
df = cbind(fit = if_else(previsao=="br",1,0), class = if_else(test$book=="br",1,0)) %>% as.data.frame()
plot_pred_type_distribution(df,0.5)
# Chunk 36
# "Dados esses modelos, podemos fazer declarações estatísticas sobre suas diferenças de desempenho? Para fazer isso, primeiro coletamos os resultados de reamostragem usando resamples." - caret
resamps <- resamples(list(knn = book_knn,
rf = book_rf,
nb = book_nb,
glm = book_glm))
summary(resamps)
# Chunk 37
c( knn= time_knn,rf = time_rf,nb = time_nb,glm = time_glm)
# Chunk 38
splom(resamps)
# Chunk 39
bwplot(resamps)
# Chunk 40
difValues <- diff(resamps)
# plot:
bwplot(difValues)
tidy_resamps <-
resamps %>%
broom::tidy() %>%
select(glm,knn,rf)
tidy_resamps %>%
purrr::map_dbl(function(x) shapiro.test(x)$p.value)
mtcars
split(mtcars, clyr)
split(mtcars, cly)
split(mtcars, clr)
split(mtcars, cyr)
split(mtcars, cyl)
split(mtcars, "cyl")
split(mtcars, cyl)
split(mtcars, mtcars$cyl)
setwd("~/Github/site-hugo")
blogdown:::serve_site()
blogdown:::serve_site()
setwd("~/Github/site-hugo/content/post/2018-06-24-brasil-argentina-tidytext-ml")
suppressMessages(library(dplyr))
suppressMessages(library(kableExtra))
suppressMessages(library(magrittr))
base <- read.csv("original_books.csv") %>% as_tibble()
devtools::source_url("https://raw.githubusercontent.com/gomesfellipe/functions/master/wordcloud_sentiment.R")
# Obtendo nuvem e salvando tabela num objeto com nome teste:
df <- wordcloud_sentiment(base$text,
type = "text",
sentiment = F,
excludeWords = c("nao",letters,LETTERS),
ngrams = 2,
tf_idf = F,
max = 100,
freq = 10,
horizontal = 0.9,
textStemming = F,
print=T)
suppressMessages(library(stringr))
suppressMessages(library(tidytext))
devtools::source_url("https://raw.githubusercontent.com/gomesfellipe/functions/master/clean_tweets.R")
devtools::source_url("https://raw.githubusercontent.com/gomesfellipe/functions/master/rm_accent.R")
original_books = base %>%
mutate(text = clean_tweets(text) %>% enc2native() %>% rm_accent())
#Removendo stopwords:
excludewords=c("[:alpha:]","[:alnum:]","[:digit:]","[:xdigit:]","[:space:]","[:word:]",
LETTERS,letters,1:10,
"hat","trick","bc","de","tem","twitte","fez",
'pra',"vai","ta","so","ja","rt")
stop_words = data_frame(word = c(stopwords::stopwords("pt"), excludewords))
tidy_books <- original_books %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
suppressMessages(library(dplyr))
suppressMessages(library(kableExtra))
suppressMessages(library(magrittr))
base <- read.csv("original_books.csv") %>% as_tibble()
suppressMessages(library(stringr))
suppressMessages(library(tidytext))
devtools::source_url("https://raw.githubusercontent.com/gomesfellipe/functions/master/clean_tweets.R")
devtools::source_url("https://raw.githubusercontent.com/gomesfellipe/functions/master/rm_accent.R")
original_books = base %>%
mutate(text = clean_tweets(text) %>% enc2native() %>% rm_accent())
#Removendo stopwords:
excludewords=c("[:alpha:]","[:alnum:]","[:digit:]","[:xdigit:]","[:space:]","[:word:]",
LETTERS,letters,1:10,
"hat","trick","bc","de","tem","twitte","fez",
'pra',"vai","ta","so","ja","rt")
stop_words = data_frame(word = c(stopwords::stopwords("pt"), excludewords))
tidy_books <- original_books %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
suppressMessages(library(stringr))
suppressMessages(library(tidytext))
devtools::source_url("https://raw.githubusercontent.com/gomesfellipe/functions/master/clean_tweets.R")
devtools::source_url("https://raw.githubusercontent.com/gomesfellipe/functions/master/rm_accent.R")
original_books = base %>%
mutate(text = clean_tweets(text) %>% enc2native() %>% rm_accent())
suppressMessages(library(stringr))
suppressMessages(library(tidytext))
devtools::source_url("https://raw.githubusercontent.com/gomesfellipe/functions/master/clean_tweets.R")
devtools::source_url("https://raw.githubusercontent.com/gomesfellipe/functions/master/catch_error.R")
original_books = base %>%
mutate(text = clean_tweets(text) %>% enc2native() %>% rm_accent())
#Removendo stopwords:
excludewords=c("[:alpha:]","[:alnum:]","[:digit:]","[:xdigit:]","[:space:]","[:word:]",
LETTERS,letters,1:10,
"hat","trick","bc","de","tem","twitte","fez",
'pra',"vai","ta","so","ja","rt")
stop_words = data_frame(word = c(stopwords::stopwords("pt"), excludewords))
tidy_books <- original_books %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
devtools::source_url("https://raw.githubusercontent.com/gomesfellipe/functions/master/clean_tweets.R")
devtools::source_url("https://raw.githubusercontent.com/gomesfellipe/functions/master/catch_error.R")
original_books = base %>%
mutate(text = clean_tweets(text) %>% enc2native() %>% rm_accent())
base
base %>%
mutate(text = clean_tweets(text))
suppressMessages(library(stringr))
suppressMessages(library(tidytext))
devtools::source_url("https://raw.githubusercontent.com/gomesfellipe/functions/master/clean_tweets.R")
devtools::source_url("https://raw.githubusercontent.com/gomesfellipe/functions/master/rm_accent.R")
original_books = base %>%
mutate(text = clean_tweets(text) %>% enc2native() %>% rm_accent())
#Removendo stopwords:
excludewords=c("[:alpha:]","[:alnum:]","[:digit:]","[:xdigit:]","[:space:]","[:word:]",
LETTERS,letters,1:10,
"hat","trick","bc","de","tem","twitte","fez",
'pra',"vai","ta","so","ja","rt")
stop_words = data_frame(word = c(stopwords::stopwords("pt"), excludewords))
tidy_books <- original_books %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
devtools::source_url("https://raw.githubusercontent.com/gomesfellipe/functions/master/clean_tweets.R")
devtools::source_url("https://raw.githubusercontent.com/gomesfellipe/functions/master/rm_accent.R")
original_books = base %>%
mutate(text = clean_tweets(text) %>% enc2native() %>% rm_accent())
devtools::source_url("https://raw.githubusercontent.com/gomesfellipe/functions/master/clean_tweets.R")
devtools::source_url("https://github.com/gomesfellipe/functions/blob/master/catch_error.R")
original_books = base %>%
mutate(text = clean_tweets(text) %>% enc2native() %>% rm_accent())
devtools::source_url("https://raw.githubusercontent.com/gomesfellipe/functions/master/clean_tweets.R")
devtools::source_url("https://github.com/gomesfellipe/functions/blob/master/catch_error.R")
devtools::source_url("https://raw.githubusercontent.com/gomesfellipe/functions/master/clean_tweets.R")
devtools::source_url("https://raw.githubusercontent.com/gomesfellipe/functions/master/catch_error.R")
original_books = base %>%
mutate(text = clean_tweets(text) %>% enc2native() %>% rm_accent())
suppressMessages(library(stringr))
suppressMessages(library(tidytext))
devtools::source_url("https://raw.githubusercontent.com/gomesfellipe/functions/master/clean_tweets.R")
devtools::source_url("https://raw.githubusercontent.com/gomesfellipe/functions/master/catch_error.R")
original_books = base %>%
mutate(text = clean_tweets(text) %>% enc2native() %>% rm_accent())
#Removendo stopwords:
excludewords=c("[:alpha:]","[:alnum:]","[:digit:]","[:xdigit:]","[:space:]","[:word:]",
LETTERS,letters,1:10,
"hat","trick","bc","de","tem","twitte","fez",
'pra',"vai","ta","so","ja","rt")
stop_words = data_frame(word = c(stopwords::stopwords("pt"), excludewords))
tidy_books <- original_books %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
devtools::source_url("https://raw.githubusercontent.com/gomesfellipe/functions/master/clean_tweets.R")
original_books = base %>%
mutate(text = clean_tweets(text) %>% enc2native() %>% rm_accent())
suppressMessages(library(abjutils))
original_books = base %>%
mutate(text = clean_tweets(text) %>% enc2native() %>% rm_accent())
suppressMessages(library(dplyr))
suppressMessages(library(kableExtra))
suppressMessages(library(magrittr))
base <- read.csv("original_books.csv") %>% as_tibble()
suppressMessages(library(stringr))
suppressMessages(library(tidytext))
suppressMessages(library(abjutils))
devtools::source_url("https://raw.githubusercontent.com/gomesfellipe/functions/master/clean_tweets.R")
original_books = base %>%
mutate(text = clean_tweets(text) %>% enc2native() %>% rm_accent())
#Removendo stopwords:
excludewords=c("[:alpha:]","[:alnum:]","[:digit:]","[:xdigit:]","[:space:]","[:word:]",
LETTERS,letters,1:10,
"hat","trick","bc","de","tem","twitte","fez",
'pra',"vai","ta","so","ja","rt")
stop_words = data_frame(word = c(stopwords::stopwords("pt"), excludewords))
tidy_books <- original_books %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
devtools::source_url("https://raw.githubusercontent.com/gomesfellipe/functions/master/clean_tweets.R")
original_books = base %>%
mutate(text = clean_tweets(text) %>% enc2native() %>% rm_accent())
devtools::source_url("https://raw.githubusercontent.com/gomesfellipe/functions/master/clean_tweets.R")
original_books = base %>%
mutate(text = clean_tweets(text) %>% enc2native() %>% rm_accent())
devtools::source_url("https://raw.githubusercontent.com/gomesfellipe/functions/master/clean_tweets.R")
original_books = base %>%
mutate(text = clean_tweets(text) %>% enc2native() %>% rm_accent())
