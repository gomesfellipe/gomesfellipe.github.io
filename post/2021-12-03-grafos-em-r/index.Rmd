---
title: Vou te provar que da para fazer Grafos bonitos em R!
author: Fellipe Gomes
date: '2021-12-03'
slug: []
categories:
  - Tidyverse
  - Text Mining
  - R
  - Pr√°tica
  - Data mining
  - Ciencia de dados
  - Automa√ß√£o
  - Analise Explorat√≥ria
  - Analise Mutivariada
  - grafo
  - web scrapping
  - ggplot
tags:
  - Tidyverse
  - text mining
  - strings
  - RStudio
  - R
  - gomesfellipe
  - Estatistica
  - Data Mining
  - grafo
  - web scrappnig
  - ggplot
description: 'Neste post vamos coletar not√≠cias via web scrapping, detectar entidades dos textos e criar um grafo utilizando ggplot2'
featured: 'img1.png'
featuredalt: 'Pic 28'
featuredpath: 'date'
linktitle: ''
type: "post"
output:
  blogdown::html_page:
    toc: true
    toc_depth: 2
image_preview: 'img1.png'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = F, warning = F, error = F, message = F)
```

# Introdu√ß√£o e contexto

Durante os anos de 2020 e 2021 fiz um [MBA Executivo em Business Analytics e Big Data](https://educacao-executiva.fgv.br/df/brasilia/cursos/mba-pos-graduacao/mba-presencial/mba-executivo-em-business-analytics-e-big-data) na FGV e uma das disciplinas que gostei bastante abordou a an√°lise de m√≠dias sociais com t√©cnicas de minera√ß√£o de texto e processamento de linguagem natural.

No trabalho final fomos desafiados a extrair dados da internet via api ou scraping, aplicar a metodologia apropriada para extrair informa√ß√µes de interesse e contruir um Grafo. 

Como esse gr√°fico deu mais de trabalho do que eu esperava e fiquei bem satisfeito com o resultado final, resolvi fazer uma nova an√°lise para praticar e publicar aqui no blog, espero que gostem!

## O que s√£o Grafos?

üìé Segundo o Wikipedia: 

> "A teoria dos grafos √© um ramo da matem√°tica que estuda as rela√ß√µes entre os objetos de um determinado conjunto"

S√£o muito √∫teis para an√°lises de redes sociais, redes de amizades ou qualquer rede com rela√ß√µes de depend√™ncias. Existem muitos tipos de grafos como conectados, desconectados, esparsos, densos, direcionados, n√£o direcionados e por ai vai... 

Al√©m disso existe toda uma nomenclatura espec√≠fica, mas n√£o entrarei em detalhes te√≥ricos neste post pois tamb√©m estou estudado sobre o tema! Caso queira aprofundar na teoria por tr√°s recomendo [este material](http://faculty.ucr.edu/~hanneman/nettext/index.html) gratuito muito bom!

## Como contruir um?

No curso que fiz aprendemos a mexer no [Gephi](https://gephi.org/) para a contru√ß√£o desses Grafos (ferramenta incr√≠vel, diga-se de passagem) por√©m ouvi dizer diversas vezes, tanto dentro quanto fora da FGV, que R e Python eram muito limitados para constru√ß√£o de Grafos bonitos e que esse software sempre a melhor op√ß√£o.

Apesar do enorme potencial do Gephi, fiquei um pouco entediado estudando-o pois n√£o sou grande f√£ de ferramentas *point-and-click* e quando o professor falou que a escolha da ferramenta para a constru√ß√£o do Grafo era livre, resolvi tentar faz√™-lo em R!

# Carregar depend√™ncias

Pacotes utilizados neste post:

```{r}
library(rvest)     # web scrapping
library(dplyr)     # manipulate data
library(purrr)     # functional prog
library(stringr)   # str toolkit
library(spacyr)    # ner
library(igraph)    # base graph
library(tidygraph) # tidy graph
library(ggraph)    # plot graph
```

# Fonte dos dados

Os dados utilizados neste post foram coletados via web scrapping do site do [G1 - Globo](https://g1.globo.com/). Optei por trabalhar com textos jornal√≠sticos neste post pois apresentam a vantagem de serem bem escritos, o que facilita na tarefa de minera√ß√£o de texto.

Tamb√©m fiz um grafo analisando tweets sobre a CPI da pandemia [que ser√° apresentado como b√¥nus no final deste post](.#b√¥nus) e para quem tiver curiosidade de conferir [os c√≥digos](https://github.com/gomesfellipe/cpi_da_pandemia) vai notar que foi necess√°rio um tratamento muito mais extensivo para corrigir os nomes de cada um dos senadores, deputados e personagens pol√≠ticos detectados.

Confira abaixo todos os c√≥digos necess√°rios para realizar tal extra√ß√£o:

<details>
<summary>(*Clique aqui para exibir as fun√ß√µes `scrape_post_links` e `scrape_post_body` *)</summary>

```{r}
# Funcao para coletar os links de cada noticia
scrape_post_links <- function(site) {
  cat(paste0(site, "\n"))
  
  source_html <- read_html(site)
  
  links <- source_html %>%
    html_nodes("div.widget--info__text-container") %>%
    html_nodes("a") %>%
    html_attr("href")
  
  links <- links[!is.na(links)]
  
  return(links)
}

# Funcao para coletar o texto da materia em cada link
scrape_post_body <- function(site) { 
  
  text <- tryCatch({
    cat(paste0(site, "\n"))
    body <- site %>%
      read_html %>%
      html_nodes("article") %>%
      html_nodes("p.content-text__container")  %>%
      html_text %>% 
      paste(collapse = '')
    
  }, error = function(e){
    cat(paste("ERRO 404", "\n"))
    body <- NA
  })
  
  return(body)
}

# criar matriz de adjacencias
get_adjacent_list <- function(edge_list) {
  gtools::combinations(length(edge_list), 2, edge_list)  
}
```

</details>
&nbsp;

```{r, eval = FALSE}
# raiz
root <- "https://g1.globo.com/busca/?q=economia+brasil"

# gerar links das proximas 100 paginas
all_pages <- c(root, paste0(root, "&page=", 1:50))

# coletar os links dos posts de cada pagina
all_links <- map(all_pages, scrape_post_links) %>% unlist()

# extrair urls
cleaned_links <- map_chr(all_links, ~{
  .x %>% 
    urltools::param_get() %>% 
    pull(u) %>% 
    urltools::url_decode()
})

# reter apenas links que falam de economia
cleaned_links <- cleaned_links %>% .[str_detect(.,  "g1.globo.com/economia")]

# nao reter links do globoplay
cleaned_links <- cleaned_links %>% .[!str_detect(.,  "globoplay")]

# coletar conteudo de cada link
data <- map_chr(cleaned_links, scrape_post_body) %>% unique()
```

# NER - Named Entity Recognition

Utilizaremos um modelo de reconhecimento de entidades pr√©-treinado fornecido pela [Spacy](https://spacy.io/) (que fornece essa e muitas outras solu√ß√µes interessantes quando se trata de processamento de linguagem natural).

Primeiramente vamos configurar o `spacyr` na m√°quina para utilizar o modelo pr√© treinado para reconhecimento de entidades em portugu√™s:

```{r, eval = F}
# Executar apenas 1 vez
spacyr::spacy_install()
spacy_download_langmodel("pt_core_news_sm")
```

Inicializar modelo pr√©-treinado em portugu√™s:

```{r, eval = F}
spacy_initialize(model="pt_core_news_sm")
```

Aplicar modelo carregado para o reconhecimento de entidades:

```{r, eval = F}
entities <- spacy_extract_entity(data)
entities
```

Filtrar apenas entidades cujo tipo s√£o **pessoas** ou **organiza√ß√µes**:

```{r, eval = F}
filtered_entities <- 
  entities %>% 
  filter(ent_type=='ORG'| ent_type=='PER')
```

```{r, eval = F, echo = F}
saveRDS(filtered_entities, "filtered_entities.rds")
```

# Preparar dados

```{r, echo = F}
filtered_entities <- readRDS("filtered_entities.rds")
```

Precisamos criar uma lista de arestas:

```{r}
edges <- 
  filtered_entities %>%
  group_by(doc_id) %>%
  summarise(entities = paste(text, collapse = ",")) %>% 
  pull(entities) %>% 
  str_split(",") %>% 
  map(~unique(unlist(.x))) %>% 
  .[map_dbl(., length) != 1]
```

Agora criaremos a matriz de adjac√™ncias, que envolvem todas as combina√ß√µes 2 a 2 das entidades detectadas em cada not√≠cia: 

```{r}
adjacent_matrix <-
  map_dfr(edges, ~ as.data.frame(get_adjacent_list(.x))) %>% 
  as_tibble() %>% 
  set_names(c('item1', 'item2'))
```

Aplicaremos algum tratamento para padronizar as entidades, reter apenas combina√ß√µes que aconteceram pelo menos 3 vezes e remover algum res√≠duo que veio no processo de NER:

```{r}
# Padronizar entidades
adjacent_matrix <- adjacent_matrix %>% 
  mutate_all(~.x %>% 
               str_replace_all("Funda√ß√£o Getulio Vargas", "FGV") %>% 
               str_replace_all("FMI", "Fundo Monet√°rio Internacional") %>% 
               str_replace_all("Paulo Guedes", "Guedes") %>% 
               str_replace_all("Estados Unidos( da Am[√©e]rica)?", "EUA") %>% 
               str_replace_all("Donald Trump", "Trump") %>% 
               str_replace_all("CEF", "Caixa Econ√¥mica Federal") %>% 
               str_replace_all("CMN", "Conselho Monet√°rio Nacional") %>% 
               str_replace_all("Cl[√°a]udio Considera", "Cl√°udio") %>% 
               str_replace_all("OCDE", "Organiza√ß√£o para a Coopera√ß√£o e
                               Desenvolvimento Econ√¥mico") %>% 
               str_replace_all("(Andr√© )?Brand√£o", "Andr√© Brand√£o") %>% 
               str_replace_all("(Maur[i√≠]cio )?Macri", "Mauricio Macri") %>% 
               str_remove_all("^(?i)(no|de)\\s")
             
             )

# remover residuos
{
  entities_to_drop <- c("Assine", "Google Podcasts", "Spotify", "Focus do",
                        "Focus", "Segundo", "Ningu√©m", "Haver√°", "G1",
                        "Come√ßa", "LEIA", "R$", "Considera", "Caixa Aqui")
  
  weighted_edgelist <- adjacent_matrix %>%
    filter_at(1:2, ~ !.x %in% entities_to_drop) %>% 
    group_by(item1, item2) %>%
    summarise(n=n()) %>% 
    ungroup() %>% 
    filter(n>3) 
}
```

Definir alguns objetos para o grafo:

```{r}
# Instanciar objeto das setas
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

# Definir pesos conforme numero de ocorrencias
subt <- weighted_edgelist

# Instanciar objeto dos vertices
vert <- subt %>% 
  tidyr::gather(item, word, item1, item2) %>%
  group_by(word) %>% 
  summarise(n = sum(n))

# Obter componentes para colorir os clusters do grafo
tidy_graph_components <- 
  subt  %>%
  select(item1, item2) %>% 
  as.matrix() %>%
  graph.edgelist(directed = FALSE)  %>%
  as_tbl_graph() %>% 
  activate("edges") %>% 
  # definir pesos como numero de ocorrencias
  mutate(weight = subt$n) %>% 
  activate("nodes") %>% 
  # obter clusters:
  mutate(component = as.factor(tidygraph::group_edge_betweenness()))
  # outros tipos de agrupamentos:
  # tidygraph.data-imaginist.com/reference/group_graph.html 
  
# Atualizar vertice para incluir grupos
vert <- vert %>% 
  left_join( as.data.frame(activate(tidy_graph_components, "nodes")) %>% 
               rename(word = name))
```

Finalmente, vamos criar o grafo utilizando `ggplot2`:

```{r}
set.seed(1)
subt %>%
  graph_from_data_frame(vertices = vert) %>%
  # https://www.data-imaginist.com/2017/ggraph-introduction-layouts/ # layouts
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches'), color = "#D9D9D9A0") +
  geom_node_point() + 
  geom_node_text(aes(label = name, size = n, alpha = n, color = component),# color = "#EAFF00",
                 repel = TRUE, point.padding = unit(0.2, "lines"),
                 show.legend = F) +
  scale_size(range = c(2,10)) +
  scale_alpha(range = c(0.5,1))+ 
  theme_dark() + 
  theme(
    panel.background = element_rect(fill = "#2D2D2D"),
    legend.key = element_rect(fill = "#2D2D2D")
  ) +
  theme_graph(background = "black")
```

<center>
![](/post/2021-12-03-grafos-em-r/grafo.png){width=95%} 
</center>


üìå Interpreta√ß√£o

Este grafo resume algumas informa√ß√µes interessantes sobre como o cen√°rio da economia no brasil estava no dia 30 de novembro de 2021. Vejamos alguns pontos relevantes que podem ser envontrados no cen√°rio atual:

<div class="w3-panel w3-pale-green w3-border">
&nbsp; ‚òû Bolsa familia

O Aux√≠lio Brasil √© referido como o "Novo Bolsa Fam√≠lia" pelos jornais e por isso deve ter sido criada tal rela√ß√£o no Grafo. J√° a Caixa Econ√¥mica Federal √© o agente que executa os pagamentos.
</div>

<div class="w3-panel w3-pale-red w3-border">
&nbsp; ‚òû Guedes

Paulo Guedes √© nosso atual ministro da economia e envolta de seu nome aparecem diversos assuntos que est√£o em pauta atualmente como a PEC dos precat√≥rios, (a privatiza√ß√£o da) Petrobr√°s, Copom, IPCA, Aux√≠lio Brasil dentre outros.
</div>

<div class="w3-panel w3-pale-yellow w3-border">
&nbsp; ‚òû Fundo Monet√°rio Internacional

O FMI [trabalha para melhorar as economias dos pa√≠ses](https://pt.wikipedia.org/wiki/Fundo_Monet%C3%A1rio_Internacional) e al√©m da Argentina estar endividada e em acordo com o FMI, √© √©poca de elei√ß√£o, o que explica haver alguns personagens de sua pol√≠tica relacionados.
</div>

Salvar localmente em alta resolu√ß√£o:

```{r}
ggsave(filename = 'grafo.png', width = 8, height = 6, device='png', dpi=700)
```

O legal de salvar em alta resolu√ß√£o √© poder dar zoom e navegar pelo grafo!

# B√¥nus

Antes de criar este post trabalhei em um [outro grafo](https://github.com/gomesfellipe/cpi_da_pandemia) com banco de dados de aproximadamente 27GB de tweets coletados e fornecidos gentilmente pelo [Janderson Toth](https://twitter.com/trifenol) (Para quem n√£o o conhe√ße, recomendo fortemente [segui-lo no linkedin](https://br.linkedin.com/in/trifenol) pois ele tem compartilhado uma s√©rie de posts com insights obtidos destes dados!)

<center>
![](/post/2021-12-03-grafos-em-r/grafo2.png){width=95%} 
</center>

Para quem tiver interesse, o c√≥digo est√° [dispon√≠vel no github](https://github.com/gomesfellipe/cpi_da_pandemia)!

# Conclus√£o

Convenhamos que, de fato, criar um grafo no R n√£o √© uma tarefa super simples. No Gelphi √© poss√≠vel criar grafos at√© mais bonitos que este, por√©m, no longo prazo, ganhamos em produtividade e em escalabilidade pois poder√≠amos reaproveitar muito c√≥digo e tranquilamente desenvolver uma rotina para criar novos grafos a partir de dados streaming, por exemplo, automatizando todo o processo! 

# Outras bibliotecas para constru√ß√£o de grafos

Depois de conversar com algumas pessoas que leram o post, achei que merecia um update com mais id√©ias de mais bibliotecas que poderiam ter sido utilizadas:

* [cheddar](https://cran.r-project.org/web/packages/cheddar/vignettes/PlotsAndStats.pdf)
* [bipartite](https://cran.r-project.org/web/packages/bipartite/bipartite.pdf)
* [ggbipart](https://pedroj.github.io/bipartite_plots/)
* [diagrameR](https://rich-iannone.github.io/DiagrammeR/)
* [visNetwork](https://cran.r-project.org/web/packages/visNetwork/vignettes/Introduction-to-visNetwork.html)


