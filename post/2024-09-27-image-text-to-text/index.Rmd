---
title: Extra√ß√£o de informa√ß√µes de imagens com IA Generativa
author: Fellipe Gomes
date: '2024-09-27'
slug: []
categories:
- Fundamentos de Data Science
- Intelig√™ncia Artificial
- Machine Learning
- Programa√ß√£o e Ferramentas
tags:
- chatgpt
- data-science
- genai
- ia-generativa
- inteligencia-artificial
- llama
- llama2
- llava
- llm
- lmm
description: Neste post, exploraremos como utilizar o modelo Llava para gerar r√≥tulos
  descritivos de imagens, usando dados do conjunto COCO-2017.
featured: img.png
featuredalt: Extra√ß√£o de informa√ß√µes de imagens com GenAI com o modelo Llava
featuredpath: date
type: post
output:
  blogdown::html_page:
    toc: false
    toc_depth: 1
image_preview: img.png
schema_type: TechArticle
educational_level: Beginner
learning_resource_type:
- Tutorial
- Code Example
- Case Study
og_image: https://gomesfellipe.github.io/img/2024/09/img.png
og_type: article
article_section: Artificial Intelligence
twitter_card: summary_large_image
twitter_image: https://gomesfellipe.github.io/img/2024/09/img.png
keywords: chatgpt, data-science, genai, ia-generativa, inteligencia-artificial, llama,
  llama2, llava, llm, lmm, data science, machine learning, analytics
robots: index, follow
subject: Data Science and Machine Learning
classification: Technology
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(python.reticulate = FALSE, eval = FALSE)
```

# Caso de Uso de IA Generativa: Extra√ß√£o de Informa√ß√µes de Imagens com o Modelo Llava

GenAI refere-se a modelos de intelig√™ncia artificial capazes de gerar conte√∫do novo e criativo a partir de dados de entrada. Seu uso est√° revolucionando a maneira como processamos dados n√£o estruturados, como imagens, √°udios, textos, v√≠deos, etc. Trabalhar com modelos pr√©-treinados (i.e., que j√° foram treinados com grandes conjuntos de dados) e adapt√°-los para necessidades espec√≠ficas tem sido um divisor de √°guas.

Neste post, vamos explorar a utiliza√ß√£o do modelo Llava (Large Language and Vision Assistant) para extrair r√≥tulos descritivos de imagens e tamb√©m discutir como comparar a qualidade das previs√µes geradas com m√©tricas espec√≠ficas para avaliar a performance desse tipo de modelo.

## Por que o Modelo Llava?

O modelo [Llava](https://llava-vl.github.io/) √© uma alternativa de c√≥digo aberto ao [GPT-4 Vision](https://chat-gpt-5.ai/capabilities-of-gpt-4v/) da OpenAI (que se destaca neste dom√≠nio, mas sua aplica√ß√£o √© restrita devido sua natureza propriet√°ria e comercial) que foi treinado em grandes conjuntos de dados multimodais, sendo capaz de compreender e gerar descri√ß√µes textuais para imagens. 

Essa capacidade de "conversar com imagens" tendo o mesmo "poder" de um LLM, possibilita seu uso em muitas solu√ß√µes desenvolvidas por cientistas de dados no mundo real, como:

1. **Classifica√ß√£o de produtos em e-commerce**: gera√ß√£o de descri√ß√µes detalhadas de roupas, acess√≥rios, eletr√¥nicos, etc.
2. **Detec√ß√£o de defeitos em linhas de produ√ß√£o**: identifica√ß√£o de falhas em produtos para automa√ß√£o e controle de qualidade.
3. **Diagn√≥stico m√©dico por imagens**: auxiliar na detec√ß√£o precoce de doen√ßas a partir de descri√ß√µes detalhadas de imagens m√©dicas.
4. **Reconhecimento de placas de carros**: transcri√ß√£o autom√°tica de textos de placas e caracter√≠sticas de ve√≠culos.
5. **Identifica√ß√£o de sinais de tr√¢nsito**: aplica√ß√£o em ve√≠culos aut√¥nomos para navega√ß√£o e identifica√ß√£o de sinais.
6. **An√°lise de alimentos para calcular nutri√ß√£o**: extra√ß√£o autom√°tica de informa√ß√µes nutricionais de fotos ou r√≥tulos de alimentos.
7. **Identifica√ß√£o de animais em c√¢meras de vida selvagem**: gerar descri√ß√µes detalhadas de animais detectados, ajudando pesquisadores a automatizar o monitoramento da vida selvagem.
8. **Detec√ß√£o de aglomera√ß√µes em eventos**: analisar imagens de c√¢meras de seguran√ßa para identificar a presen√ßa de grandes grupos de pessoas em eventos ou lugares p√∫blicos, √∫til em gest√£o de multid√µes ou para quest√µes de seguran√ßa.

## Dataset COCO-2017

O [COCO](https://cocodataset.org/) (Common Objects in Context) √© um dataset amplamente utilizado em vis√£o computacional. Ele √© um dos maiores conjuntos de imagens do dia a dia com objetos em diferentes contextos, com anota√ß√µes detalhadas fornecidas por humanos como tags, caixa delimitadora, pol√≠gono que segmenta a imagem detectando objetos bem como sua descri√ß√£o. Isso o torna ideal para testar o desempenho desse tipo de modelo para gera√ß√£o de legendas.

<center>
<div style="display: flex; width: 100%;">
<div style="width: 50%;">
<img src="/post/2024-09-27-image-text-to-text/coco1.png" alt="Imagem 2" style="width: 100%;">
</div>
<div style="width: 50%;">
<img src="/post/2024-09-27-image-text-to-text/coco2.png" alt="Imagem 2" style="width: 100%;">
</div>
</div>
<center>
<small>
Imagem do COCO Dataset com e sem anota√ß√£o obtida na [se√ß√£o explorat√≥ria](https://cocodataset.org/#explore) das imagens
</small>
</center>
</center>

# Preparando o Ambiente

Utilizei o ambiente do Kaggle para desenvolvimento deste notebook, que disponibiliza a utiliza√ß√£o de GPUs. Atrav√©s do Hardware Accelerator utilizaremos a [NVIDIA TESLA P100 GPU](https://www.kaggle.com/docs/efficient-gpu-usage).

<details>
<summary>*Expandir c√≥digo*</summary>
```{}
%%capture
!pip -qqq install bitsandbytes accelerate rouge-score pycocoevalcap bert_score
!pip install -U nltk

import os
import re
import json
import pandas as pd
import numpy as np
from tqdm import tqdm

import seaborn as sns
import matplotlib.pyplot as plt

from PIL import Image
import requests
from io import BytesIO
from IPython.display import HTML
import base64

import torch
from transformers import pipeline, AutoProcessor, BitsAndBytesConfig

from nltk.translate.bleu_score import sentence_bleu
from rouge_score import rouge_scorer
from bert_score import score as bert_score
from nltk.translate.meteor_score import meteor_score

from transformers import logging
import warnings

logging.set_verbosity_error()
warnings.filterwarnings("ignore", "use_inf_as_na")
```

</details>

<br>

# Carregar dados

Por fins de praticidade para este post, selecionei uma amostra de 10 imagens aleat√≥rias do dataset COCO - (Common Objects in Context) no site <https://cocodataset.org> (onde √© poss√≠vel ter uma descri√ß√£o detalhada do conjunto de dados, incluindo seu [paper](https://arxiv.org/abs/1405.0312) para aprofundamento), para avaliar o desempenho do modelo. 

<details>
<summary>*Expandir c√≥digo*</summary>

```{python}
df_sample = pd.DataFrame({
  'coco_url': [
    'http://images.cocodataset.org/train2017/000000058822.jpg',
    'http://images.cocodataset.org/train2017/000000530396.jpg',
    'http://images.cocodataset.org/train2017/000000097916.jpg',
    'http://images.cocodataset.org/train2017/000000418492.jpg',
    'http://images.cocodataset.org/train2017/000000022304.jpg',
    'http://images.cocodataset.org/train2017/000000295999.jpg',
    'http://images.cocodataset.org/train2017/000000406616.jpg',
    'http://images.cocodataset.org/train2017/000000370926.jpg',
    'http://images.cocodataset.org/train2017/000000005612.jpg',
    'http://images.cocodataset.org/train2017/000000146436.jpg'
  ],
  'caption': [
    'A laptop sitting on a desk with a cell phone and mouse.',
    'A black bear walking through the grass field.',
    'a person who is surfing in the ocean.',
    'A young boy standing on a sandy beach holding a flag.',
    'A man surfing on a wave in the ocean.',
    'A herd of cows, grazing in a field.',
    'There is a cutting board and knife with chopped apples and carrots.',
    'A long yellow school bus is parked on a city street.\n',
    'A black and white horse standing in the middle of a field.',
    'A man in a red jacket looking at his phone.'
    ]})
    
# Fun√ß√£o para verificar se o caminho √© uma URL
def is_url(path):
    return path.startswith('http://') or path.startswith('https://')

# Fun√ß√£o simplificada para gerar o thumbnail e convert√™-lo em base64 diretamente
def process_image(path):
    try:
        if is_url(path):
            # Se for uma URL, baixar a imagem
            response = requests.get(path)
            response.raise_for_status()  # Verifica se houve algum erro no download
            image = Image.open(BytesIO(response.content))  # Abrir a imagem do conte√∫do da resposta
        else:
            # Se for um caminho local, abrir a imagem diretamente
            image = Image.open(path)
        
        # Criar uma miniatura da imagem (thumbnail) com tamanho m√°ximo de 150x150
        image.thumbnail((150, 150), Image.LANCZOS)
        
        # Salvar a imagem em um buffer de mem√≥ria e convert√™-la para base64
        with BytesIO() as buffer:
            image.save(buffer, 'jpeg')
            image_base64 = base64.b64encode(buffer.getvalue()).decode()
        
        # Retornar a string HTML com a imagem embutida no formato base64
        return f'<img src="data:image/jpeg;base64,{image_base64}">'
    
    except Exception as e:
        # Em caso de erro, retornar uma string vazia ou uma mensagem de erro
        return f"<p>Erro ao carregar imagem: {e}</p>"

# Aplicar o processamento de imagens diretamente no DataFrame
df_sample['image'] = df_sample['coco_url'].map(process_image)  # Pode ser URL ou caminho local

# Exibir as legendas e imagens formatadas em HTML
HTML(df_sample[['image', 'coco_url', 'caption']].head().to_html(escape=False))
```

</details>
<br>
 
<img src="/post/2024-09-27-image-text-to-text/df1.png" style="width: 100%;">

Caso voc√™ precise de mais imagens para testar, tamb√©m √© poss√≠vel encontrar uma [vers√£o disponibilizada no Kaggle](https://www.kaggle.com/datasets/awsaf49/coco-2017-dataset/data) .

# Carregar modelo

Utilizaremos uma vers√£o de 7 bilh√µes de par√¢metros do modelo ["LLaVA 1.5"](https://huggingface.co/llava-hf/llava-1.5-7b-hf) (Language and Vision Assistant), dispon√≠vel no HuggingFace (Uma plataforma onde a comunidade de Machine Learning colabora com modelos, dados e aplica√ß√µes) treinada para tarefas de gera√ß√£o de texto a partir de imagens.

```{python}
%%time

model_id = "llava-hf/llava-1.5-7b-hf"

# Configura√ß√£o de quantiza√ß√£o do modelo, que permite reduzir o uso de mem√≥ria sem 
# comprometer muito a precis√£o. Aqui estamos configurando para usar quantiza√ß√£o em 4 bits.
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,  
    bnb_4bit_use_double_quant=True,  
    bnb_4bit_quant_type="nf4",  
    bnb_4bit_compute_dtype=torch.bfloat16  
)

# Cria√ß√£o de um pipeline de processamento de imagens para gera√ß√£o de texto
# O pipeline √© configurado para a tarefa "image-to-text"
pipe = pipeline(
    "image-to-text", 
    model=model_id, 
    model_kwargs={
        "quantization_config": quantization_config,
        "low_cpu_mem_usage": True
    }
)

# Carregar o processador associado respons√°vel por pr√©-processar
# as imagens de entrada e preparar os dados para serem inseridos no modelo
processor = AutoProcessor.from_pretrained(model_id)
```

```
CPU times: user 28.7 s, sys: 28.1 s, total: 56.8 s
Wall time: 6min 26s
```

<div class="w3-panel w3-pale-blue w3-border">
&nbsp; **üìå Nota:** A quantiza√ß√£o √© uma t√©cnica para reduzir o tamanho do modelo, perdendo um pouco de performance para otimizar o desempenho e rodar em m√°quinas com mem√≥ria limitada.
</div>

# Prompt Engineering

Uma ampla variedade de [t√©cnicas](https://www.promptingguide.ai/pt) poderiam ser aplicadas para desenvolver [prompts](https://python.langchain.com/docs/how_to/multimodal_prompts/) mais eficazes (inclusive com [LangChain](https://python.langchain.com/docs/introduction/), como fiz no [√∫ltimo post](https://gomesfellipe.github.io/post/2024-05-26-detec-o-de-linguagem-t-xica-com-o-llm-gemma-e-langchain/)) ou especializar o modelo com ajuste fino visando obter resultados otimizados. No entanto, como este n√£o √© o foco do post, usarei um prompt simples e direto para estabelecer um baseline para avaliar as capacidades do modelo com o m√≠nimo de esfor√ßo.

```{python}
# Cada valor em "content" tem que ser uma lista de dicion√°rio com os tipos ("text", "image") 
conversation = [
    {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe this image in a few words:"},
          {"type": "image"},
        ]
    },
]

# Formata a conversa (que pode incluir texto e imagens) no formato correto que o modelo entende.
prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)
```

O [prompt](https://colab.research.google.com/drive/1qsl6cd2c8gGtEW1xV5io7S8NHh-Cp1TV?usp=sharing#scrollTo=JvvtplWDRvfu) deve ser especificado no seguinte formato:

```
USER: <image>
<prompt>
ASSISTANT:
```

# Infer√™ncia

Com o modelo devidamente configurado e o prompt ajustado, estamos prontos para executar o pipeline de infer√™ncia. A vantagem de utilizar [pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines#multimodal) √© que eles abstraem boa parte da codifica√ß√£o complexa, proporcionando uma interface simples e eficiente. Essa API vers√°til √© dedicada a v√°rias tarefas, como NER (Reconhecimento de Entidades), An√°lise de Sentimentos, Extra√ß√£o de Features e Question Answering.

```{python}
for i in tqdm(range(df_sample.shape[0])):
    
    # preparar objetos do loop
    coco_url = df_sample.iloc[i]['coco_url']
    caption = df_sample.iloc[i]['caption']
    index = df_sample.iloc[i].name
    
    # Obter imagem
    response = requests.get(coco_url)
    image = Image.open(BytesIO(response.content))
    
    # Realizar a infer√™ncia usando o pipeline e o prompt gerado
    outputs = pipe(image, prompt=prompt, generate_kwargs={"max_new_tokens": 32})
    
    # Processar o texto gerado para extrair a parte relevante
    result = outputs[0]['generated_text'].split('ASSISTANT:', 1)[1].strip()
    
    # Adicionar o resultado da infer√™ncia √† nova coluna 'llm' do DataFrame
    df_sample.loc[index, 'llm'] = result
```

```
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:41<00:00,  4.20s/it]
```

Ap√≥s a execu√ß√£o do modelo, veja como ficaram os resultados:

<details>
<summary>*Expandir c√≥digo*</summary>

```{python}
# Fun√ß√£o para destacar as palavras
def highlight_diff(caption, llm):
    # Divide as frases em palavras
    caption_words = caption.replace(".", "").split()
    llm_words = llm.replace(".", "").split()
    
    # Converte as palavras em conjuntos para encontrar a interse√ß√£o
    caption_set = set(caption_words)
    llm_set = set(llm_words)
    
    # Calcula as palavras que n√£o est√£o na interse√ß√£o
    caption_highlighted = " ".join([f'<span style="color:red">{word}</span>' if word not in llm_set else word for word in caption_words])
    llm_highlighted = " ".join([f'<span style="color:red">{word}</span>' if word not in caption_set else word for word in llm_words])
    
    return caption_highlighted, llm_highlighted

# Aplica a fun√ß√£o a cada linha do DataFrame e cria novas colunas
df_sample['highlighted_caption'], df_sample['highlighted_llm'] = zip(*df_sample.apply(lambda row: highlight_diff(row['caption'], row['llm']), axis=1))

# Exibir o DataFrame formatado com HTML
HTML(df_sample[['image', 'highlighted_caption', 'highlighted_llm']].to_html(escape=False))
```

</details>
<br>

<img src="/post/2024-09-27-image-text-to-text/df2.png" alt="extra√ß√£o de r√≥tulos descritivos de imagens com Llava" style="width: 100%;">

Destaquei em vermelho as palavras que diferem entre a legenda original do dataset e a previs√£o gerada pelo nosso modelo de linguagem. 

<div class="w3-panel w3-pale-yellow w3-border">
&nbsp; üí≠ Apesar de algumas diferen√ßas sutis entre as duas vers√µes, como 'looking at his phone' e 'looking at his <span style="color:red;">cell</span> phone', a ideia principal permanece bastante coerente com o que vemos nas imagens. Em alguns casos, como no item 3, a descri√ß√£o gerada pelo modelo, 'holding a <span style="color:red;">kite</span>', parece at√© mais apropriada do que a fornecida pelo dataset, 'holding a <span style="color:red;">flag</span>'.

</div>



Agora, o pr√≥ximo passo ser√° quantificar essas diferen√ßas de maneira num√©rica.


# Avaliar modelo

Para medir a precis√£o das legendas geradas, aplicaremos quatro m√©tricas amplamente usadas:

- **[BLEU](https://aclanthology.org/P02-1040.pdf) (Bilingual Evaluation Understudy Score)**: Amplamente utilizada para medir a qualidade de tradu√ß√µes autom√°ticas, mede a **sobreposi√ß√£o de n-gramas** entre a tradu√ß√£o gerada por um modelo e as tradu√ß√µes de refer√™ncia, atribuindo uma pontua√ß√£o que varia de 0 a 1 (aplica tamb√©m um fator de penaliza√ß√£o para evitar que tradu√ß√µes curtas sejam favorecidas);
- **[ROUGE-L](https://aclanthology.org/W04-1013.pdf) (Recall-Oriented Understudy for Gisting Evaluation)**: Muito utilizado em tarefa de sumariza√ß√£o de textos, considera a sequ√™ncia mais longa de palavras que aparecem em ambas as refer√™ncias e previs√µes, medindo a capacidade de preservar a **ordem das palavras**;
- **[METEOR](https://www.cs.cmu.edu/~alavie/METEOR/) (Metric for Evaluation of Translation with Explicit ORdering)**: Baseada na m√©dia harm√¥nica da precis√£o e recall de n-gramas, com recall ponderado mais alto do que a precis√£o. Essa m√©trica METEOR foi projetada para corrigir alguns dos problemas (como encontrar sin√¥nimos) nas m√©tricas BLEU e ROGUE;
- **[BERTScore](https://huggingface.co/spaces/evaluate-metric/bertscore)**: Usa embeddings (representa√ß√µes sem√¢nticas) obtidas a partir do modelo BERT para comparar a similaridade sem√¢ntica entre as descri√ß√µes geradas e as de refer√™ncia.

<details>
<summary>*Expandir c√≥digo*</summary>

```{python}
# Fun√ß√µes para calcular as m√©tricas
def calcular_bleu(referencias, previsao):
    return sentence_bleu([referencias.split(" ")], previsao.split(" "),weights = [1])

def calcular_rouge(referencias, previsao):
    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
    return scorer.score(referencias, previsao)['rougeL'].fmeasure

def calcular_meteor(referencias, previsao):
    return meteor_score([referencias.split(" ")], previsao.split(" "))

def calcular_bertscore(referencias, previsao):
    P, R, F1 = bert_score([previsao], [referencias], lang="en", verbose=True)
    return F1.mean().item()
```

</details>
<br>

```{python}
%%capture

# Avaliar as amostras no DataFrame
resultados = []
for i, row in df_sample.iterrows():
    
    referencias = row['caption'].replace(".", "")
    previsao = row['llm'].replace(".", "")
    
    bleu = calcular_bleu(referencias, previsao)
    rouge = calcular_rouge(referencias, previsao)
    meteor = calcular_meteor(referencias, previsao)
    bert = calcular_bertscore(referencias, previsao)
    
    resultados.append([referencias, previsao, bleu, rouge, meteor, bert])

# Converter os resultados para um DataFrame
df_resultados = pd.DataFrame(resultados, columns=['caption', 'llm', 'BLEU', 'ROUGE', 'METEOR', 'BERTScore'])
```

Vejamos os resultados:

<details>
<summary>*Expandir c√≥digo*</summary>

```{python}
# Configurar o tema do Seaborn
sns.set_theme(style="white", rc={"axes.facecolor": (0, 0, 0, 0)})

# Reformatar o DataFrame para o formato long
df_long = df_resultados[['BLEU', 'ROUGE', 'METEOR', 'BERTScore']].melt(var_name="M√©trica", value_name="Valor")

# Calcular a m√©dia de cada m√©trica
mean_values = df_long.groupby('M√©trica')['Valor'].mean().reset_index()

# Inicializar o objeto FacetGrid
pal = sns.cubehelix_palette(len(df_long['M√©trica'].unique()), rot=-.25, light=.7)
g = sns.FacetGrid(df_long, row="M√©trica", hue="M√©trica", aspect=6, height=1.5, palette=pal)

# Desenhar as densidades
g.map(sns.kdeplot, "Valor", 
      bw_adjust=.5, clip_on=False, 
      fill=True, alpha=1, linewidth=1.5)
g.map(sns.kdeplot, "Valor", clip_on=False, color="w", lw=2, bw_adjust=.5)

# Adicionar linha de refer√™ncia
g.refline(y=0, linewidth=2, linestyle="-", color=None, clip_on=False)

# Fun√ß√£o para rotular o gr√°fico
def label(x, color, label):
    ax = plt.gca()
    # Localizar a m√©dia correspondente √† m√©trica
    mean_value = mean_values[mean_values['M√©trica'] == label]['Valor'].values[0]
    ax.text(0, .4, f"{label} (M√©dia: {mean_value:.2f})", fontweight="bold", color=color,
            ha="left", va="center", transform=ax.transAxes, fontsize=20)

g.map(label, "Valor")

# Ajustar espa√ßamento entre subplots manualmente
g.figure.subplots_adjust(hspace=0.2)

# Remover detalhes desnecess√°rios dos eixos
g.set_titles("")
g.set(yticks=[], ylabel="")
g.despine(bottom=True, left=True)

# Configurar o eixo x
g.set(xlim=(0.4, 1), xticks=np.arange(0.4, 1.05, 0.1))  # Limites e ticks do eixo x

# Remover r√≥tulos do eixo x em cada subplot
for ax in g.axes.flat:
    ax.set_xlabel("")  # Remover r√≥tulo do eixo x
    ax.tick_params(axis='x', labelsize=16)  # Aumentar o tamanho da fonte dos ticks do eixo x

# Exibir o gr√°fico
plt.show()
```

</details>
<br>

<center><img src="/post/2024-09-27-image-text-to-text/metrics.png" alt="metricas da extra√ß√£o de r√≥tulos descritivos de imagens com Llava" style="width: 80%;"></center>


<div class="w3-panel w3-pale-yellow w3-border">
&nbsp; **üìå Insights ao Avaliar as M√©tricas do Modelo: ** 

- As m√©tricas baseadas em **n-grams e na correspond√™ncia de palavras** mostraram desempenho **subestimado**. Embora o modelo tenha apresentado algumas varia√ß√µes na escolha das palavras, as frases geradas mantiveram um sentido geral muito semelhante ao que √© retratado nas imagens.

- Por outro lado, a m√©trica baseada em **embeddings**, que avalia o significado **sem√¢ntico** das frases, apresentou resultados **significativamente superiores**. Essa abordagem se mostrou mais congruente em avaliar a similaridade das descri√ß√µes geradas e a descri√ß√£o informada do conte√∫do visual das imagens.

- √â importante ressaltar que nosso **prompt** foi mantido na forma **mais simples poss√≠vel** e que o conjunto de dados abrange um **escopo bastante amplo**. Com isso, acredito que o modelo ainda tem muito potencial para oferecer resultados ainda mais robustos, sem a necessidade de ajustes finos, em tarefas mais espec√≠ficas.

</div>


# Conclus√£o

O uso da GenAI com o modelo Llava oferece uma solu√ß√£o eficiente para a extra√ß√£o de features de imagens em Python, possibilitando a cria√ß√£o de descri√ß√µes ricas e detalhadas. Ao comparar a qualidade das sa√≠das com m√©tricas como BLEU, podemos garantir que o modelo esteja oferecendo resultados satisfat√≥rios para as necessidades do projeto.

Se voc√™ deseja automatizar processos de an√°lise de imagens, explorar a cria√ß√£o de modelos customizados ou otimizar a organiza√ß√£o de dados visuais, a utiliza√ß√£o de GenAI com modelos como o Llava pode ser um divisor de √°guas em seus projetos.

Se este conte√∫do foi √∫til, continue acompanhando o blog para mais tutoriais sobre intelig√™ncia artificial e Python!

# Refer√™ncias

- <https://huggingface.co/llava-hf/llava-1.5-7b-hf>
- <https://github.com/haotian-liu/LLaVA>
- <https://colab.research.google.com/drive/1qsl6cd2c8gGtEW1xV5io7S8NHh-Cp1TV?usp=sharing#scrollTo=6Bx8iu9jOssW>
- <https://cocodataset.org/#explore>
- <https://www.kaggle.com/datasets/awsaf49/coco-2017-dataset/>

