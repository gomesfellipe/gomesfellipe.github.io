&lt;?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Teoria on Fellipe Gomes - Data Science Blog</title>
    <link>https://gomesfellipe.github.io/categories/teoria/</link>
    <description>Últimos posts sobre Data Science, Machine Learning e R</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>pt-br</language>
    <managingEditor>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</managingEditor>
    <webMaster>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</webMaster>
    <lastBuildDate>Sat, 28 Jul 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://gomesfellipe.github.io/categories/teoria/" rel="self" type="application/rss+xml" />
    <item>
      <title>modelo bayesiano do zero</title>
      <link>https://gomesfellipe.github.io/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero/</link>
      <pubDate>Sat, 28 Jul 2018 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero/</guid>
      <description>Um pouco sobre as duas grandes escolas de inferência, contas e implementação de um modelo linear bayesiano na mão para dados simulados e para dados reais</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/plotly-binding/plotly.js"></script>
<script src="/rmarkdown-libs/typedarray/typedarray.min.js"></script>
<script src="/rmarkdown-libs/jquery/jquery.min.js"></script>
<link href="/rmarkdown-libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="/rmarkdown-libs/crosstalk/js/crosstalk.min.js"></script>
<link href="/rmarkdown-libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="/rmarkdown-libs/plotly-main/plotly-latest.min.js"></script>


<div id="modelagem-estatística-e-as-duas-grandes-escolas-de-inferência" class="section level1">
<h1>Modelagem estatística e as duas grandes escolas de inferência</h1>
<p>Através da modelagem estatística é possível tomar decisões sobre diversos assuntos de interesse como por exemplo na análise de risco de crédito, previsões de quantidade de chuva em um dado local, estimativas de erros ou falhas de um novo produto ou serviço além de diversas áreas como na Educação, Economia, nas Ciências Sociais, Saúde etc.</p>
<p>Muitas vezes os parâmetros das distribuições em estudo podem ser desconhecidos e existe o desejo de se inferir sobre eles. Existem duas grandes escolas de inferência: a clássica e a bayesiana. A clássica trata esses parâmetros como quantidades fixas e não atribui distribuição a eles, a estimação desses parâmetros é dada através da função de verossimilhança, enquanto que na escola bayesiana atribui-se uma distribuição, chamada de distribuição a priori, ao conjunto de parâmetros desconhecidos quantificando a sua crença sobre esse conjunto e a estimação dos parâmetros é dada através da distribuição à posteriori, que é proporcional ao produto da função de verossimilhança com a distribuição a priori.</p>
<p>O interesse pela modelagem estatística através da abordagem bayesiana surgiu a partir de um projeto de iniciação científica quando cursava o 6º período do curso de Graduação em Estatística que tinha como objetivo o cálculo e apresentação de estatísticas descritivas para ajudar uma pesquisadora. Após obter os resultados da análise exploratória e descritiva, notei, junto com meu orientador, que havia possibilidade de dar continuidade ao estudo a partir de uma abordagem estatística mais elaborada. Sendo assim, outro projeto de iniciação científica foi iniciado em seguida com a finalidade de me preparar para utilizar um modelo linear hierárquico bayesiano sob os dados disponibilizados pela pesquisadora em minha monografia.</p>
<p>Caso tenha interesse em conferir o projeto com o estudo sobre modelos hierárquicos bayesianos, disponibilizei os resultados e os códigos em meu github <a href="https://github.com/gomesfellipe/projeto_modelos_hierarquicos_bayesianos">neste repositório</a>. Neste post farei uma breve introdução sobre o ajuste de um modelo linear bayesiano simples e os resultados obtidos (utilizando uma distribuição a priori não informativa). Os resultados obtidos serão comparados com os resultados obtidos com o ajuste de um modelo de regressão linear através da abordagem clássica.</p>
<div id="distribuição-a-priori" class="section level2">
<h2>Distribuição a priori</h2>
<p>Para o estudo, optou-se pela utilização de valores elevados para variância a priori (também consideradas como “não informativas”, fazendo uma analogia à modelos clássicos) obtendo ajustes que atribuem maior importância à informação provinda da amostra.</p>
<p>Portanto com valores elevados para variância da distribuição a priori (consideradas como “não informativas”) foram obtida a distribuição a posteriori de um parâmetro <span class="math inline">\(\theta\)</span> que contém toda a informação probabilística a respeito deste parâmetro e quando a forma analítica dessa distribuição é conhecida o gráfico da <a href="https://pt.wikipedia.org/wiki/Fun%C3%A7%C3%A3o_densidade">fdp</a> pode ilustrar o comportamento probabilístico do parâmetro de interesse e auxiliar em alguma tomada de decisão, porém, quando a forma analítica não é conhecida ou é muito custosa de ser obtida, pode-se recorrer a métodos de simulação tais como os métodos MCMC.</p>
</div>
<div id="amostrador-de-gibbs---método-mcmc" class="section level2">
<h2>Amostrador de Gibbs - método MCMC</h2>
<p>Com os avanços dos métodos de MCMC, surgiu o amostrador de Gibbs, proposto por <span class="citation">@GemanGeman</span> e tornou-se popular por <span class="citation">@GelfandSmith</span>, falo um pouco mais sobre o algoritmo no <a href="https://github.com/gomesfellipe/projeto_modelos_hierarquicos_bayesianos/blob/master/texto.pdf">texto do projeto</a>.</p>
<p>Como a convergência ocorre após o aquecimento (ou burn-in), é comum usar os valores de <span class="math inline">\(\theta^{(a)}\)</span>, <span class="math inline">\(\theta^{(a+t)}\)</span>, <span class="math inline">\(\theta^{(a+2t)}\)</span>,… para compor a amostra de <span class="math inline">\(\theta\)</span>, sendo <span class="math inline">\(a-1\)</span> o número de iterações iniciais do aquecimento e <span class="math inline">\(t\)</span> o espaçamento utilizado para diminuir a autocorrelação dos parâmetros. Maiores detalhes podem ser vistos em <span class="citation">@Gamerman06</span>.</p>
</div>
</div>
<div id="ao-que-interessa" class="section level1">
<h1>Ao que interessa</h1>
<p>O objetivo deste post é apresentar e comparar os resultados do ajuste de um modelo linear bayesiano simples utilizando uma distribuição a priori não informativa com o modelo de regressão linear simples para dados simulados e para dados reais.</p>
<p>Diversas funções foram criadas ao longo o estudo para conferir o comportamento das cadeias geradas e os resultados do ajuste do modelo, aproveitarei essas funções para este post importando do <a href="https://github.com/gomesfellipe/projeto_modelos_hierarquicos_bayesianos/blob/master/dependencies.R">repositório no github</a> da seguinte maneira:</p>
<pre class="r"><code>path_to_dep &lt;- &quot;https://raw.githubusercontent.com/gomesfellipe/projeto_modelos_hierarquicos_bayesianos/master/dependencies.R&quot;
devtools::source_url(path_to_dep, encoding=&quot;UTF-8&quot;)</code></pre>
</div>
<div id="ajuste-do-modelo-para-dados-simulados" class="section level1">
<h1>Ajuste do modelo para dados simulados</h1>
<p>Suponha então um exemplo em que a população de interesse tenha distribuição normal com média <span class="math inline">\(\beta_0 + \beta_1 X\)</span>, sendo <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span> desconhecidos e variância <span class="math inline">\(\sigma^2\)</span> desconhecida. Seja <span class="math inline">\(\tau=\frac{1}{\sigma^2}\)</span> o parâmetro chamado de precisão.</p>
<p>O parâmetro <span class="math inline">\(\beta_0\)</span> é conhecido como intercepto ou coeficiente linear e o <span class="math inline">\(\beta_1\)</span> como coeficiente angular. Além disso, suponha que as unidades dessa população sejam iid. Dessa forma, tem-se que as unidades dessa população tem a seguinte distribuição:</p>
<p><span class="math display">\[
Y_i \stackrel{iid}{\sim} N(\beta_0 + \beta_1 X_i,\frac{1}{\tau}), 
\]</span></p>
<p>onde <span class="math inline">\(i=1,...,N\)</span>.</p>
<p>Para o estudo do modelo primeiramente foi utilizado um conjunto de dados simulados utilizando uma amostra de tamanho <span class="math inline">\(N=1000\)</span> e com os seguintes parâmetros “desconhecidos” dos quais desejamos estimar: <span class="math inline">\(\beta_0 = 1\)</span>, <span class="math inline">\(\beta_1 = 0,5\)</span>, <span class="math inline">\(\tau = 2\)</span>. A amostra será simulada segundo a variável aleatória: <span class="math inline">\(X_i ~ N(0,1)\)</span> e em seguida os parâmetros deste modelo, denotados por <span class="math inline">\(\theta = (\beta_0, \beta_1, \tau)\)</span> foram estimados usando o paradigma Bayesiano.</p>
<div id="gerando-a-amostra" class="section level2">
<h2>Gerando a amostra</h2>
<p>A amostra que foi simulada foi obtida da seguinte maneira:</p>
<pre class="r"><code># Amostra que sera utilizada:

set.seed(12)
n   &lt;- 1000                 # N=1000
b0  &lt;- 1                    # \beta_0 = 1
b1  &lt;- 0.5                  # \beta_1 = 0,5
tau &lt;- 2                    # \tau = 2 e 
x   &lt;- rnorm(n)             # X_i ~ N(0,1), logo:
y   &lt;- b0 + b1 * x + rnorm(n,0,sqrt(1/tau))</code></pre>
<p>Obtendo-se uma amostra de tamanho <span class="math inline">\(n\)</span>, pode-se inferir sob os parâmetros desconhecidos <span class="math inline">\(\theta = (\beta_0, \beta_1, \tau)\)</span> através da distribuição a posteriori e para obter essa distribuição faz-se necessário calcular a função de verossimilhança, que pode ser obtida da seguinte forma:</p>
<p><span class="math display">\[
p(y| \beta_0, \beta_1 , \tau) =\prod^n_{i=1} p(y_i | \beta_0, \beta_1, \tau )  
\]</span></p>
<p>portanto</p>
<p><span class="math display">\[
p(y| \beta_0, \beta_1 , \tau) = \prod_{i=1}^n \frac{ \sqrt{\tau} }{ \sqrt{2\pi} } exp { - \frac{\tau}{2} ( y_i - \beta_0 - \beta_1 x_i )^2 }
\]</span></p>
<p>onde <span class="math inline">\(y = (y_1, ..., y_n)\)</span> é a amostra coletada. O valor p para o teste de Shapiro para conferir a suposição de normalidade da variável resposta foi de 0.6181791 enquanto que o valor p para conferir a normalidade da variável explicativa foi de 0.7413229.</p>
</div>
<div id="distribuição-a-priori-1" class="section level2">
<h2>Distribuição a priori</h2>
<p>Durante o estudo diversos valores os parâmetros a priori foram selecionados para que fosse possível avaliar a sensibilidade da qualidade da escolha da distribuição priori, aqui será apresentado os resultados obtidos com valores elevados para variância a priori (também consideradas como “não informativas”, fazendo uma analogia à modelos clássicos) que ajusta o modelo atribuindo maior importância à informação provinda da amostra.</p>
<p>Considere a priori que os parâmetros sejam independentes e que</p>
<p><span class="math display">\[
\beta_0 \sim N(m_0,\sigma_0^2),  \\
\beta_1 \sim N(m_1,\sigma_1^2) \mbox{ e }  \\
\tau    \sim G(a,b).
\]</span></p>
<p>Portanto, para a estimação foram utilizados os seguintes hiperparâmetros : <span class="math inline">\(m_0 = m_1 = 0\)</span>, <span class="math inline">\(\sigma_0^2 = \sigma_1^2 = 100\)</span>, <span class="math inline">\(a=0,1\)</span> e <span class="math inline">\(b=0,1\)</span></p>
<p>No R:</p>
<pre class="r"><code>#Parametros para b0 ~ N(mu0, sig0)
mu0 &lt;-  0
sig0 &lt;-  1000

#Parametros para b1 ~ N(mu1, sig1)
mu1 &lt;-  0
sig1 &lt;-  1000

#Parametros para tau ~ G(a,b)
a &lt;-  0.1
b &lt;-  0.1</code></pre>
<p>Dessa forma, tem-se que a distribuição conjunta a priori possui a seguinte forma:</p>
<p><span class="math display">\[
 p(\beta_0, \beta_1 , \tau) \propto exp\Big\{-\frac{1}{2\sigma_0^2}( \beta_0 - m_0)^2\Big\} exp\Big\{-\frac{1}{2\sigma_1^2}( \beta_1 - m_1)^2\Big\} \tau^{a-1}exp \{-b \tau\}.
\]</span></p>
</div>
<div id="distribuição-a-posteriori" class="section level2">
<h2>Distribuição a posteriori</h2>
<p>Combinando a função de verossimilhança com a distribuição a priori, obtêm-se a distribuição a posteriori que é proporcional a:</p>
<p><span class="math display">\[
p(\beta_0, \beta_1 , \tau|y) \propto \tau^{\frac{n}{2}+a-1} exp \left\{ -\frac{\tau}{2} \sum^n_{i=1} (y_i - \beta_0 - \beta_1 x_i)^2 - b\tau  - \frac{1}{2\sigma_0^2}(\beta_0-m_0)^2  \right\} \times   exp\left\{- \frac{1}{2\sigma_1^2}(\beta_1-m_1)^2  \right\} . 
\]</span></p>
<p>Note que essa distribuição é multivariada e não possui forma analítica conhecida. Sendo assim, recorre-se aos métodos de MCMC para se obter amostras dessa distribuição. E então faz-se necessário obter as DCCP de <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span> e <span class="math inline">\(\tau\)</span>.</p>
</div>
<div id="implementando-o-amostrador-de-gibbs" class="section level2">
<h2>Implementando o amostrador de Gibbs</h2>
<p>O tamanho da cadeia foi de 30000 simulações e o <em>burn-in</em> (ou amostra de aquecimento) utilizado considerada após o ajuste foi de 15000. no R:</p>
<pre class="r"><code>nsim           &lt;-  3*10000
burnin         &lt;-  nsim / 2 
cadeia.b0      &lt;-  rep(0,nsim)
cadeia.b1      &lt;-  rep(0,nsim)
cadeia.tau     &lt;-  rep(0,nsim)

# Chutes iniciais: 
cadeia.b0[1]    &lt;-  0
cadeia.b1[1]    &lt;-  0
cadeia.tau[1]   &lt;-  1</code></pre>
<div id="calculos-para-implementar-o-algoritimo-na-mão" class="section level3">
<h3>Calculos para implementar o algoritimo na mão</h3>
<p>Para a implementação do algoritmo, fez-se necessário o cálculo das distribuições condicionais completas a posteriori (DCCP), primeiramente veja os resultados obtidos para <span class="math inline">\(\tau\)</span>:</p>
<ul>
<li>DCCP de <span class="math inline">\(\tau\)</span>:</li>
</ul>
<p><span class="math display">\[
\tau|y_1, ...,y_n,\beta_0, \beta_1 \sim Gama ( \frac{n}{2}+a,b+\frac{1}{2} \sum^n_{i=1}(y_i-\beta_0-\beta_1 x_i)^2 ) 
\]</span></p>
<p>Em seguida, veja o resultado obtido para <span class="math inline">\(\beta_0\)</span>, o coeficiente linear da reta, isto é, a altura em que a reta de regressão intercepta o eixo dos <span class="math inline">\(Y\)</span>’s:</p>
<ul>
<li>DCCP de <span class="math inline">\(\beta_0\)</span>:</li>
</ul>
<p><span class="math display">\[
\beta_0 | y_1,...,y_n , \tau,\beta_1 \sim N(\dfrac{(\tau\sum^n_{i=1}y_i - \tau\beta_1\sum^n_{i=1}x_i  +\frac{m_0}{\sigma_0^2})}{ \tau n + \frac{1}{\sigma_0^2}},  (n\tau +   \frac{1}{\sigma_0^2} )^{-1})
\]</span></p>
<p>Por fim, veja o resultado obtido para <span class="math inline">\(\beta_1\)</span>, é o coeficiente angular da reta, ou seja, é o a variação esperada na variável <span class="math inline">\(Y\)</span> quando a variável explicativa é acrescida de 1 unidade:</p>
<ul>
<li>DCCP de <span class="math inline">\(\beta_1\)</span>:</li>
</ul>
<p><span class="math display">\[
\beta_1 | y_1,...,y_n , \tau,\beta_0 \sim N(\frac{\tau\sum^n_{i=1}x_i y_i  - \tau\beta_0\sum^n_{i=1}x_i + \frac{m_1}{\sigma_1^2}}{\tau \sum^n_{i=1}x_i^2 + \frac{1}{\sigma_1^2}}, ( \tau \sum^n_{i=1}x_i^2 + \frac{1}{\sigma_1^2} )^{-1})
\]</span></p>
<p>Agora que todas as distribuições condicionais completas estão calculadas o algorítimo já pode ser implementado, no R foi feito da seguinte maneira: (note que as linhas que foram comentadas executariam uma barra de carregamento, com ilustrado em seguida)</p>
<pre class="r"><code># pb &lt;- txtProgressBar(min = 0, max = nsim, style = 3) # iniciando barra de processo
for (k in 2:nsim){
  
  #Cadeia tau
  cadeia.tau[k]   &lt;-  rgamma(1, (n/2) + a, b + (sum((y - cadeia.b0[k-1] - (cadeia.b1[k-1]*x))^2)/2))
  
  # Cadeia B0
  c0              &lt;-  (n*cadeia.tau[k]) + (1/sig0)
  m0              &lt;-  (cadeia.tau[k]*sum(y) - (cadeia.tau[k]*cadeia.b1[k-1]*sum(x)) + (mu0/sig0))/c0
  cadeia.b0[k]    &lt;-  rnorm(1, m0, 1/sqrt(c0))
  
  # Cadeia B1
  c1              &lt;-   (sum(x^2)*cadeia.tau[k]) + (1/sig1)
  m1              &lt;-   ((cadeia.tau[k]*sum(x*y)) - (cadeia.tau[k]*cadeia.b0[k]*sum(x)) + (mu1/sig1))/c1
  cadeia.b1[k]    &lt;-   rnorm(1, m1, 1/sqrt(c1))
  
  # setTxtProgressBar(pb, k)
  
}# ;close(pb) #Encerrando barra de processo</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/loading.png" /></p>
</div>
<div id="resultados-da-cadeia" class="section level3">
<h3>Resultados da cadeia</h3>
<p>A seguir definiremos a variável <code>inds</code> que indica os valores após a amostra de aquecimento (ou <em>burn-in</em>), a variável <code>real</code> que contém os valores reais utilizados para gerar a amostra para conferir se o modelo foi capaz de recuperá-los, os nomes dos parâmetros e os resultados das cadeias foram agregados em uma matriz:</p>
<pre class="r"><code># Juntando resultados:
inds    &lt;- seq(burnin, nsim) # Definindo os indices
real    &lt;- c(b0, b1, tau)
name    &lt;- c(expression(beta[0]), expression(beta[1]), expression(tau))
results &lt;- cbind(cadeia.b0, cadeia.b1, cadeia.tau) %&gt;% as.data.frame() %&gt;% .[inds, ] %T&gt;% head</code></pre>
<div id="histograma-e-densidade" class="section level4">
<h4>Histograma e densidade</h4>
<p>A figura abaixo apresenta os histogramas junto com as densidades de três cadeias obtidas ao se inicializar o amostrador em pontos diferentes de todos os parâmetros contidos em <span class="math inline">\(\theta\)</span> e uma linha vermelha indicará o valor do real parâmetro utilizado para estimar a cadeia.</p>
<pre class="r"><code>g1 &lt;- hist_den(results[,1],name = name[1], p = real[1])
g2 &lt;- hist_den(results[,2],name = name[2], p = real[2])
g3 &lt;- hist_den(results[,3],name = name[3], p = real[3])
grid.arrange(g1,g2,g3,ncol=1)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<div id="cadeia" class="section level4">
<h4>Cadeia</h4>
<p>A figura abaixo apresenta os traços das cadeias dos parâmetros amostrados exibindo o intervalo de credibilidade com a linha pontilhada em azul e o valor verdadeiro do parâmetro em vermelho. Note que há indícios de convergência.</p>
<pre class="r"><code># Cadeia
cadeia(results, name, real)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>é possível notar que todos os intervalos de credibilidade contêm o parâmetro populacional real utilizado para gerar a amostra.</p>
</div>
<div id="autocorrelação" class="section level4">
<h4>Autocorrelação</h4>
<p>A figura abaixo apresenta os gráficos de autocorrelação, que indicam se houve a influência dos “valores vizinhos” dos parâmetros amostrados. Note que parece haver independência entre as interações.</p>
<pre class="r"><code># ACF
FAC(results)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>é possível notar que nenhuma das cadeias apresentaram estimativas autocorrelacionada</p>
</div>
<div id="estimativas" class="section level4">
<h4>Estimativas</h4>
<p>Agora que já foi verificado que a cadeia se comportou de maneira satisfatória, veja os resultados obtidos sobre as estimativas dos parâmetros através do algoritmo. apresenta os resumos a posteriori dos parâmetros amostrados.</p>
<pre class="r"><code>coef &lt;- coeficientes(results, real = real) %&gt;% as.data.frame()

tabela_coeficientes(coef)</code></pre>
<div id="htmlwidget-1" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"visdat":{"11b9832b6bfa0":["function () ","plotlyVisDat"]},"cur_data":"11b9832b6bfa0","attrs":{"11b9832b6bfa0":{"columnorder":[1,2,3,4,5],"columnwidth":[80,80,80,80,80],"header":{"values":["Média","Desv. Pad.","IC inf","IC sup","Real"],"line":{"color":"#506784"},"fill":{"color":"#1F8FFFB4"},"align":["center","center","center","center","center"],"font":{"color":"white","size":15},"height":40},"cells":{"values":[[1.0244,0.4933,1.9001],[0.023,0.0241,0.085],[0.9792,0.4464,1.7371],[1.0697,0.5409,2.0695],[1,0.5,2]],"line":{"color":"#506784"},"fill":{"color":["#1F8FFF58","white","white","white","#1F8FFF58"]},"align":["center","center","center","center","center"],"font":{"color":["white","#506784","#506784","#506784","white"],"size":12},"height":30},"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"table"}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"hovermode":"closest","showlegend":false},"source":"A","config":{"showSendToCloud":false},"data":[{"columnorder":[1,2,3,4,5],"columnwidth":[80,80,80,80,80],"header":{"values":["Média","Desv. Pad.","IC inf","IC sup","Real"],"line":{"color":"#506784"},"fill":{"color":"#1F8FFFB4"},"align":["center","center","center","center","center"],"font":{"color":"white","size":15},"height":40},"cells":{"values":[[1.0244,0.4933,1.9001],[0.023,0.0241,0.085],[0.9792,0.4464,1.7371],[1.0697,0.5409,2.0695],[1,0.5,2]],"line":{"color":"#506784"},"fill":{"color":["#1F8FFF58","white","white","white","#1F8FFF58"]},"align":["center","center","center","center","center"],"font":{"color":["white","#506784","#506784","#506784","white"],"size":12},"height":30},"type":"table","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>Como se trata de uma amostra simulada é possível comparar as estimativas com os valores reais que geraram a amostra e os valores estão muito próximos da média (todos eles estão incluídos no intervalo de credibilidade).</p>
</div>
</div>
<div id="comparando-com-o-modelo-linear-clássico" class="section level3">
<h3>Comparando com o modelo linear clássico</h3>
<p>Agora que os resultados sob o paradigma bayesiano já foram conferidos será ajustado um modelo de regressão linear simples pelo método dos mínimos quadrados através da função <code>lm()</code> sob o paradigma clássico para comparar com os resultados de um modelo de regressão linear simples sob o paradigma bayesiano utilizando os resultados calculados.</p>
<pre class="r"><code># Reta do modelo classico
plot(x, y)
modelo.classico &lt;- lm(y ~ 1 + x)
a.classico      &lt;- modelo.classico$coefficients[1]
b.classico      &lt;- modelo.classico$coefficients[2]
abline(a        &lt;- a.classico, b = b.classico, col = &quot;blue&quot;)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>O modelo estimado para estes dados sob o paradigma da inferência clássica foi o seguinte: <span class="math inline">\(\hat{y} = 1.0245 x + 0,4933\)</span>, o que mostra que as estimativas de <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span> foram muito parecidas com as estimativas sob o paradigma da inferência bayesiana.</p>
<pre class="r"><code># Reta do modelo bayesiano
plot(x, y)
a.bayes  &lt;-  mean(results[, 1])
b.bayes  &lt;-  mean(results[, 2])
abline(a = a.bayes, b = b.bayes, col = &quot;red&quot;)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>A figura apresenta o gráfico de dispersão entre as variáveis da amostra simulada e as retas dos ajustes de ambos os modelos:</p>
<pre class="r"><code>library(stringr)
library(ggplot2)
library(ggExtra)

# Texto da imagem
text.classico &lt;- str_c(&quot;Modelo Classico: &quot;,&quot;y = &quot;,round(a.classico,4),&quot; x + &quot;,round(b.classico,4))
text.bayes    &lt;- str_c(&quot;Modelo Bayesiano: &quot;,&quot;y = &quot;,round(a.bayes,4),&quot; x + &quot;,round(b.bayes,4))

# Gerando o e ambos:
cbind(y, x) %&gt;%
  as.data.frame %&gt;%
    ggplot(aes(y = y, x = x)) +
    geom_point() +
    geom_smooth(method = &quot;lm&quot;, se = F, col = &quot;red&quot;) +
    theme_classic() +
    geom_abline(slope = b.bayes,
    intercept = a.bayes,
    col = &quot;blue&quot;) +
    labs(title = &quot;&quot;,
    x = &quot;Covariável&quot;,
    y = &quot;Reposta&quot;)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>Agora que os resultados no algoritmo já foram conferidos e avaliados de maneira satisfatória utilizando os dados simulados, é a vez de fazer o ajuste para dados reais.</p>
</div>
</div>
</div>
<div id="ajuste-do-modelo-para-dados-reais" class="section level1">
<h1>Ajuste do modelo para dados reais</h1>
<p>O conjunto de dados que será utilizado como exemplo foi disponibilizado por <span class="citation">@Ezekiel_cars</span> e hoje faz parte do conjunto de banco de dados nativos do R (a base de dados pode ser obtida ao escrever <code>cars</code> no console). Os dados informam a velocidade dos carros e as distâncias tomadas para parar, esses dados foram registrados na década de 1920 e são de grande utilidade didática até os dias de hoje.</p>
<p>Considere que deseja-se modelar a velocidade dos carros de acordo com as distâncias tomadas para parar, portanto a variável resposta será a velocidade e a variável explicativa do modelo será a distância tomada para parar.</p>
<div id="amostra-utilizada" class="section level2">
<h2>Amostra utilizada</h2>
<pre class="r"><code>y    &lt;-  cars$speed
x    &lt;-  cars$dist
n    &lt;-  nrow(cars)</code></pre>
<p>o valor p para o teste de Shapiro para conferir a suposição de normalidade da variável resposta foi de 0.4576319 enquanto que o valor p para conferir a normalidade da variável explicativa foi de 0.0390997</p>
</div>
<div id="distribuição-a-priori-2" class="section level2">
<h2>Distribuição a priori</h2>
<p>Serão utilizados os mesmos valores que foram propostos na simulação como hiperparametros e chutes iniciais para a cadeia, o código usado foi exatamente o mesmo.</p>
</div>
<div id="resultados-da-cadeia-1" class="section level2">
<h2>Resultados da cadeia</h2>
<p>Definiremos novamente a variável <code>inds</code> que indica os valores após a amostra de aquecimento (ou <em>burn-in</em>), desta vez não haverá a variável <code>real</code> pois não conhecemos os valores reais utilizados para gerar a amostra para conferir se o modelo foi capaz de recuperá-los. Desta vez utilizaremos a variável <code>classico</code>, que guarda os valores obtidos com o ajuste do modelo linear pela abordagem clássica.</p>
<pre class="r"><code># Juntando resultados:
inds     &lt;- seq(burnin, nsim) # Definindo os indices
results  &lt;- cbind(cadeia.b0, cadeia.b1, cadeia.tau) %&gt;% as.data.frame() %&gt;% .[inds, ]
classico &lt;- c(coefficients(lm(cars)), 1 / var(lm(cars)$residuals))
name     &lt;- c(expression(beta[0]), expression(beta[1]), expression(tau))</code></pre>
<div id="histograma-e-densidade-1" class="section level4">
<h4>Histograma e densidade</h4>
<p>A figura abaixo exibe os histogramas com as densidades de três cadeias obtidas ao se iniciar o amostrador em pontos diferentes de todos os parâmetros <span class="math inline">\(\theta\)</span> mas dessa vez sem a linha vermelha que indicava o valor do parâmetro real pois agora ele é desconhecido.</p>
<pre class="r"><code>g1 &lt;- hist_den(results[, 1], name = name[1])
g2 &lt;- hist_den(results[, 2], name = name[2])
g3 &lt;- hist_den(results[, 3], name = name[3])
grid.arrange(g1, g2, g3, ncol = 1)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>Nota-se que ambas as cadeias convergiram uma mesma distribuição e que as últimas três cadeias apresentaram valores próximos.</p>
</div>
<div id="cadeias" class="section level4">
<h4>Cadeias</h4>
<p>A figura abaixo apresenta os traços das cadeias dos parâmetros amostrados. Note que há indícios de convergência.</p>
<pre class="r"><code>cadeia(results,name)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
</div>
<div id="autocorrelação-1" class="section level4">
<h4>Autocorrelação</h4>
<p>A Figura abaixo apresenta os gráficos de autocorrelação dos parâmetros amostrados.</p>
<pre class="r"><code>FAC(results)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>É possível notar que apenas nas primeiras defasagens das cadeias das estimativas para os parâmetros <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span> se apresentaram de forma autocorrelacionada e que a partir dessa defasagem o gráfico de autocorrelação se apresentou de forma desejável.</p>
</div>
<div id="estimativas-1" class="section level4">
<h4>Estimativas</h4>
<p>Como todas as características da cadeia gerada foram avaliadas de maneira satisfatória agora será possível conferir o ajuste dos parâmetros de maneira mais segura pois já foi constatada a convergência da cadeia</p>
</div>
<div id="comparando-com-o-modelo-linear-clássico-1" class="section level4">
<h4>Comparando com o modelo linear clássico</h4>
<p>Agora que os resultados sob o paradigma bayesiano já foram conferidos novamente será ajustado um modelo de regressão linear simples pelo método dos mínimos quadrados sob o paradigma clássico para comparar com os resultados do um modelo de regressão linear simples sob o paradigma bayesiano utilizando os resultados calculados na seção.</p>
<pre class="r"><code># Reta do modelo classico 
plot(x, y)
modelo.classico &lt;- lm(y ~ 1 + x)
a.classico      &lt;- modelo.classico$coefficients[1]
b.classico      &lt;- modelo.classico$coefficients[2]
abline(a        &lt;- a.classico, b = b.classico, col = &quot;blue&quot;)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<pre class="r"><code># Reta do modelo bayesiano
plot(x, y)
a.bayes &lt;- mean(results[, 1])
b.bayes &lt;- mean(results[, 2])
abline(a = a.bayes, b = b.bayes, col = &quot;red&quot;)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>A Tabela abaixo apresenta o resumo a posteriori dos parâmetros estimados da cadeia e note que esta tabela não conta com a coluna dos valores reais como no exemplo anterior e sim as estimativas sob o paradigma clássico.</p>
<pre class="r"><code>coef &lt;- 
  coeficientes(results,real = classico) %&gt;% as.data.frame()

tabela_coeficientes(coef)</code></pre>
<div id="htmlwidget-2" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-2">{"x":{"visdat":{"11b981cae4251":["function () ","plotlyVisDat"]},"cur_data":"11b981cae4251","attrs":{"11b981cae4251":{"columnorder":[1,2,3,4,5],"columnwidth":[80,80,80,80,80],"header":{"values":["Média","Desv. Pad.","IC inf","IC sup","Real"],"line":{"color":"#506784"},"fill":{"color":"#1F8FFFB4"},"align":["center","center","center","center","center"],"font":{"color":"white","size":15},"height":40},"cells":{"values":[[8.2374,0.1663,0.1083],[0.8481,0.017,0.0214],[6.5848,0.1326,0.0699],[9.9239,0.1997,0.1542],[8.2839,0.1656,0.1025]],"line":{"color":"#506784"},"fill":{"color":["#1F8FFF58","white","white","white","#1F8FFF58"]},"align":["center","center","center","center","center"],"font":{"color":["white","#506784","#506784","#506784","white"],"size":12},"height":30},"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"table"}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"hovermode":"closest","showlegend":false},"source":"A","config":{"showSendToCloud":false},"data":[{"columnorder":[1,2,3,4,5],"columnwidth":[80,80,80,80,80],"header":{"values":["Média","Desv. Pad.","IC inf","IC sup","Real"],"line":{"color":"#506784"},"fill":{"color":"#1F8FFFB4"},"align":["center","center","center","center","center"],"font":{"color":"white","size":15},"height":40},"cells":{"values":[[8.2374,0.1663,0.1083],[0.8481,0.017,0.0214],[6.5848,0.1326,0.0699],[9.9239,0.1997,0.1542],[8.2839,0.1656,0.1025]],"line":{"color":"#506784"},"fill":{"color":["#1F8FFF58","white","white","white","#1F8FFF58"]},"align":["center","center","center","center","center"],"font":{"color":["white","#506784","#506784","#506784","white"],"size":12},"height":30},"type":"table","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>O modelo estimado sob este paradigma pode ser escrito da seguinte maneira: <span class="math inline">\(\hat{y} = 8,2839 x + 0,1656\)</span>, ou seja, os valores de <span class="math inline">\(\beta_0\)</span> e de <span class="math inline">\(\beta_1\)</span> novamente foram muito próximos dos parâmetros obtidos ao estimar sob o paradigma clássico.</p>
</div>
<div id="comparando-de-forma-visual" class="section level4">
<h4>Comparando de forma visual</h4>
<p>A Figura ilustra o gráfico de dispersão dos dados citados acima, com a intenção de exibir quanto uma variável é afetada por outra, onde no eixo vertical representa a velocidade do carro e no eixo horizontal a distância tomada para parar.</p>
<p>Além do comportamento das variáveis, neste gráfico é exibido também os resultados obtidos do ajuste ao se utilizar o método de mínimos quadrados (representada pela linha em vermelho) para estimar os parâmetros e o ajuste do modelo ao se utilizar o método apresentado acima em (representada pela linha azul).</p>
<pre class="r"><code># Texto da imagem
text.classico &lt;- str_c(&quot;Modelo Classico: &quot;,&quot;y = &quot;,round(a.classico,4),&quot; x + &quot;,round(b.classico,4))
text.bayes    &lt;- str_c(&quot;Modelo Bayesiano: &quot;,&quot;y = &quot;,round(a.bayes,4),&quot; x + &quot;,round(b.bayes,4))

#Gerando o scatter.plot
cbind(y, x) %&gt;%
  as.data.frame %&gt;%
  ggplot(aes(y = y, x = x)) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, se = F, col = &quot;red&quot;) +
  theme_classic() +
  geom_abline(slope = b.bayes,
              intercept = a.bayes,
              col = &quot;blue&quot;) +
  labs(title = &quot;Relação entre a Distância e a Velocidade com \nreta do modelo linear clássico vs bayesiano&quot;,
       x = &quot;Distância&quot;,
       y = &quot;Velocidade&quot;)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>É possível notar que os coeficientes calculados foram muito parecidos, mesmo apresentando pequenas diferenças decimais no valor dos coeficientes ainda é possível notar que as retas estão basicamente sobrepostas, ou seja, os valores estimados em ambas as abordagens foram praticamente os mesmos.</p>
<p>Apesar dos valores dos ajustes terem apresentado basicamente os mesmo resultados, a maneira de se conferir a qualidade do ajuste é diferente em ambas as abordagens. Enquanto sob o paradigma clássico o ajuste do modelo pode ser checado ao avaliar os pre-supostos quanto à distribuição dos resíduos, como recomenda <span class="citation">@GaussClarice</span>, ao utilizar um método de MCMC faz-se necessário conferir também outros aspectos como por exemplo se houve convergência da cadeias além do comportamento das autocorrelações, vide <span class="citation">@migon</span>.</p>
</div>
</div>
</div>
<div id="conclusão" class="section level1">
<h1>Conclusão</h1>
<p>O uso do algorítmo para simular os dados da implementação do modelo hierárquico bayesiano envolveu diversas etapas. Inicialmente foi necessária a revisão de literatura para a compreensão dos métodos que seriam utilizados na implementação do algoritmo, bem como em seu desenvolvimento. Essa pesquisa funcionou de maneira muito didática, de forma que a cada semana a abordagem pudesse envolver maior grau de complexidade.</p>
<p>Durante o estudo, diversos valores de parâmetros a priori foram selecionados para que fosse possível avaliar a sensibilidade da qualidade da escolha da distribuição a priori. Observou-se que valores elevados para variância a priori (também consideradas como “não informativas” - fazendo uma analogia à modelos clássicos) obtiveram melhores ajustes atribuindo maior importância à informação provinda da amostra.</p>
<p>O estudo com dados simulados facilitou o entendimento do algoritmo pois foi possível notar com facilidade a inadequabilidade das escolhas das prioris, que resultavam em estimativas muito distante do parâmetro populacional que gerou a amostra.</p>
</div>
<div id="referências" class="section level1">
<h1>Referências</h1>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero/">modelo bayesiano do zero</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Aprendizado Não Supervisionado</category>
      <category>Bayes</category>
      <category>Inferência Bayesiana</category>
      <category>Estatistica</category>
      <category>Modelagem Estatistica</category>
      <category>Probabilidade</category>
      <category>R</category>
      <category>Simulação</category>
      <category>Teoria</category>
      <category domain="tag">bayes</category>
      <category domain="tag">Estatistica</category>
      <category domain="tag">gomesfellipe</category>
      <category domain="tag">jags</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">modelagem estatistica</category>
      <category domain="tag">modelos generalizados</category>
      <category domain="tag">modelos lineares</category>
      <category domain="tag">probabilidade</category>
      <category domain="tag">R</category>
      <category domain="tag">regression</category>
      <category domain="tag">Teoria</category>
    </item>
    <item>
      <title>O que são CheatSheets, gamificação e por que aprender R é tão divertido?</title>
      <link>https://gomesfellipe.github.io/post/2018-02-17-cheatsheet-gamificacao-r/cheatsheet-gamificacao-r/</link>
      <pubDate>Sat, 17 Feb 2018 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2018-02-17-cheatsheet-gamificacao-r/cheatsheet-gamificacao-r/</guid>
      <description>Você costuma ler o manual de instruções? Veja como equipes têm trabalhado para contribuir e facilitar o aprendizado da linguagem R ampliando a intersecção entre a curiosidade de nossa infancia e o amadurecimento. Programar se torna uma tarefa divertida e prática mas sem abandonar o manual de instruções escrito por quem sabe do que esta falando!</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="você-costuma-ler-o-manual" class="section level1">
<h1>Você costuma ler o manual?</h1>
<p>Quando eramos crianças geralmente não tinhamos o costume de ler o manual das coisas não é mesmo? Particularmente eu sempre gostei de aprender como as coisas funcionavam diretamente com a prática para poder usá-las depois. Adorava buscar entender como as coisas se encaixavam ao montar os brinquedinhos do kinder-ovo sem ler as instruções ou criar diferentes combinações com lego customizados, por exemplo. Acredito que isso seja da natureza de toda criança!</p>
<div class="col2">
<p>Acontece que com o passar dos anos vamos adquirindo conhecimento e começamos a perceber que quanto mais aprendemos maior a quantidade de coisas novas que ainda temos a aprender.</p>
<p>Especialmente ao ler noticias do tipo: <a href="http://www.jornalciencia.com/einstein-estava-certo-cientistas-detectam-ondas-gravitacionais-comprovando-a-teoria-da-relatividade-geral/">“Einstein estava certo a cem anos atrás!”</a> depois de ele já ter tomado nota das ondas gravitacionais a tantos anos.. Um dever de casa que durou 100 anos foi deixado por um gênio e isso serve para nos lembra como somos “pequenos”, o quanto é importante seguir boas referências nos apoiando em ombros de gigantes para enxergar mais longe!</p>
<p>(Quem ai aos 22 anos, desenvolveu o cálculo infinitesimal, as bases da teoria das cores, contribuiu com o estudo da ótica, formulou conceitos sobre as leis do movimento planetário e virou lenda com um famoso incidente da maçã que levou a formular a teoria da gravidade? Será que <a href="https://www.oficinadanet.com.br/post/15839-isaac-newton-o-maior-genio-de-todos-os-tempos">Isaac Newton</a> conhece alguem?)</p>
<p><img src="http://i.giphy.com/IZ4EXtpPkamXe.gif" /></p>
</div>
<p>Isso faz pensar em como é importante ouvir (ou ler) quem entende do assunto para darmos nossos próximos passos</p>
</div>
<div id="mas-o-r-tem-manual" class="section level1">
<h1>Mas o R tem manual?</h1>
<p>Seja estudando estatística, programação em R, qualquer outra matéria ou mesmo configurando seu relógio, montando um daqueles móveis complicados ou que seja montando um avião! Nem tudo precisa ser um quebra cabeça, não importa o quão ávido por saber, consultar o manual (ou um livro se torna uma tarefa fundamental para darmos o próximo passo!</p>
<p>A medida que vamos avançando no aprendendizado da linguagem R, mais consultas ao <a href="https://www.r-project.org/help.html">“Help”</a> vão sendo realizadas. Isso ocorre também quando avançamos no estudo de qualquer área, acaba sendo natural elevar o número de consultas ao “manual”.</p>
<p>Acontece que nem sempre encontramos explicações detalhadas ou suficientes no Help para solucionar nossos problemas e em busca de mais detalhes e referencias quase sempre podemos encontrar <a href="http://r-pkgs.had.co.nz/vignettes.html">vignettes</a> (que são guias longos para os pacotes, geralmente com exemplos reprodutíveis e algumas dicas para o uso) em uma breve pesquisa escrevendo: “Nome do pacote” + “CRAN” na busca do Google e geralmente logo no inicio já existe uma ou mais referências além do Help do RStudio no <a href="https://cran.r-project.org/">CRAN</a> disponibilizado pelos desenvolvedores dos pacotes.</p>
</div>
<div id="aprendendo-com-a-prática" class="section level1">
<h1>Aprendendo com a prática</h1>
<p>Algo muito legal disponibilizado pela RStudio dentre os <a href="https://www.rstudio.com/resources">recursos em seu site oficial</a> são as <a href="https://www.rstudio.com/resources/cheatsheets/">CheatSheets</a> e a <a href="https://www.rstudio.com/online-learning/">aprendizagem online</a> onde é apresentada a <a href="https://www.datacamp.com/">DataCamp</a> que tornam fácil aprender e usar alguns dos pacotes mais utilizados unindo a ideia da “consulta ao manual” e a ideia de se aprender na prática.</p>
<p>Além desses recursos ainda existe <a href="https://www.kaggle.com/">kaggle</a> que funciona quase como uma plataforma de “jogos com a ciência de dados” onde competidores comparam resultados dos ajustes de seus modelos, análises descrtias e relatórios valendo premios em dinheiro!</p>
<p>Divertido como quando eramos crianças!</p>
<p>Veja a seguir uma breve explicação e referências para acessar esses recursos</p>
<div id="cheatsheets" class="section level2">
<h2>Cheatsheets</h2>
<p>Ao longo do tempo novas cheatsheets vão sendo adicionadas e todas elas estão disponíveis para download, além disso qualquer pessoa que quiser contribuir com a comunidade é <a href="https://www.rstudio.com/resources/cheatsheets/how-to-contribute-a-cheatsheet/">convidado pelos desenvolvedores</a> a enviar suas próprias CheatSheets!</p>
<p>Todas as CheatSheets apresentadas abaixo estão disponíveis no <a href="https://www.rstudio.com/resources/cheatsheets/">repositório de CheatSheets</a> :</p>
<div id="exemplos-do-básico" class="section level3">
<h3>Exemplos do básico:</h3>
<p>Informações básicas e fundamentais para o uso da IDE do RStudio e o uso da linguagem podem ser encontrados nessas CheatSheets:</p>
<div class="row">
<div class="col-sm-4">
<div class="figure">
<img src="https://image.slidesharecdn.com/rstudio-ide-cheatsheet-170605180146/95/rstudio-idecheatsheet-1-638.jpg?cb=1496686097" alt="" />
<p class="caption">RStudio IDE</p>
</div>
</div>
<div class="col-sm-4">
<div class="figure">
<img src="https://s2.studylib.es/store/data/008818835_1-c476932320d9a4cd7e891da23012aaa1.png" alt="" />
<p class="caption">Basics R</p>
</div>
</div>
<div class="col-sm-4">
<div class="figure">
<img src="https://i.pinimg.com/originals/d7/34/13/d734131229a11252c34f954df1fbd511.png" alt="" />
<p class="caption">Advanced R</p>
</div>
</div>
</div>
</div>
<div id="exemplos-de-recursos" class="section level3">
<h3>Exemplos de recursos:</h3>
<p>Existem também algumas CheatSheets para auxiliar no uso dos recursos oferecidos pela RStudio como por exemplo a para RMarkdown e Shinny:</p>
<div class="row">
<div class="col-sm-4">
<div class="figure">
<img src="https://d33wubrfki0l68.cloudfront.net/65dffd1bdcaa0025006262164d98e8068e8b4387/c3895/wp-content/uploads/2018/08/rmarkdown-2.0.png" alt="" />
<p class="caption">RMarkdown</p>
</div>
</div>
<div class="col-sm-4">
<div class="figure">
<img src="https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference-guide.png" alt="" />
<p class="caption">Guia de Referencia para RMarkdown</p>
</div>
</div>
<div class="col-sm-4">
<div class="figure">
<img src="https://mcmoutletonline.com/pics/shiny-cheat-sheet.png" alt="" />
<p class="caption">Shiny</p>
</div>
</div>
</div>
</div>
<div id="exemplos-de-utilidades" class="section level3">
<h3>Exemplos de utilidades</h3>
<p>Além das funcionalidades e recursos básicos disponíveis pela equipe da RStudio ainda contamos com uma enorme quantidade de pacotes que estão sendo desenvolvidos a todo momento com a finalidade de melhorar o desempenho de nossos programas e projetos, a seguir alguns exemplos de CheatSheets de pacotes que são bastante úteis no dia a dia do programador estatístico:</p>
<div id="exemplos-de-fornecidos-pela-rstudio" class="section level4">
<h4>Exemplos de fornecidos pela RStudio</h4>
<div class="row">
<div class="col-sm-4">
<div class="figure">
<img src="https://d33wubrfki0l68.cloudfront.net/ed91b01c08afed41e2df36b805e32c2c46e48857/21514/wp-content/uploads/2018/08/strings.png" alt="" />
<p class="caption">stringr - Para facilitar a manipulação de strings</p>
</div>
</div>
<div class="col-sm-4">
<div class="figure">
<img src="https://d33wubrfki0l68.cloudfront.net/3ae77f446a7470730f3dbb7b6489525494ac8bd5/57024/wp-content/uploads/2018/08/purrr.png" alt="" />
<p class="caption">purrr - Pacote com ferramentas de programação funcional</p>
</div>
</div>
<div class="col-sm-4">
<div class="figure">
<img src="https://d33wubrfki0l68.cloudfront.net/db69c3d03699d395475d2ac14d64f611054fa9a4/e98f3/wp-content/uploads/2018/08/data-transformation.png" alt="" />
<p class="caption">dplyr - Para facilidade e velocidade na manipulação de dados</p>
</div>
</div>
</div>
<p></br></p>
<div class="row">
<div class="col-sm-4">
<div class="figure">
<img src="https://d33wubrfki0l68.cloudfront.net/21d683072b0c21cbd9b41fc0e37a587ad26b9525/cbf41/wp-content/uploads/2018/08/data-visualization-2.1.png" alt="" />
<p class="caption">ggplot2 - Para apresentações visuais elegantes e práticas</p>
</div>
</div>
<div class="col-sm-4">
<div class="figure">
<img src="https://miro.medium.com/max/4582/1*W08jooOkrVu7iok96jsJpA.jpeg" alt="" />
<p class="caption">readr - Para facilitar a tarefa de importar arquivos</p>
</div>
</div>
<div class="col-sm-4">
<div class="figure">
<img src="https://image.slidesharecdn.com/devtools-cheatsheet-170605180143/95/devtools-cheatsheet-1-638.jpg?cb=1496685956" alt="" />
<p class="caption">devtools - Possibilitando o usuário criar seus próprios pacotes</p>
</div>
</div>
</div>
<p>Dentre muitos outros disponíveis no <a href="https://www.rstudio.com/resources/cheatsheets/">link para galeria de CheatSheets</a></p>
</div>
<div id="exemplos-fornecidos-por-contribuidores" class="section level4">
<h4>Exemplos fornecidos por contribuidores</h4>
<p>Como mostrado anteriormente, o pacote <a href="https://cran.r-project.org/web/packages/devtools/index.html"><code>devtools</code></a> possibilita que qualquer usuário crie e disponibilize seus próprios pacotes, então além dos pacotes a comunidade também contribuiu com diversas cheatsheets, veja algumas delas:</p>
<div class="row">
<div class="col-sm-4">
<div class="figure">
<img src="https://d33wubrfki0l68.cloudfront.net/ad16acdb544c1a9ca00c7dd175312a52f45e8979/7e9a2/wp-content/uploads/2015/01/caret-cheatsheet.png" alt="" />
<p class="caption">caret - Pacote muito famoso quando o assunto é Machine Learning</p>
</div>
</div>
<div class="col-sm-4">
<div class="figure">
<img src="https://d33wubrfki0l68.cloudfront.net/1267e8f809560bdbad86d15763be08302a471fb5/138a1/wp-content/uploads/2015/01/leaflet-cheatsheet-1.png" alt="" />
<p class="caption">leaflet - Para criar mapas interativos com facilidade</p>
</div>
</div>
<div class="col-sm-4">
<div class="figure">
<img src="https://wch.github.io/latexsheet/latexsheet-0.png" alt="" />
<p class="caption"><span class="math inline">\(\LaTeX\)</span> - Linguagem muito útil e fácil para escrever muitos tipos de documentos</p>
</div>
</div>
</div>
<p>Como podemos ver são muitas opções para consulta, ao encontrar o que nos torna mais confortável enquanto aprendemos a linguagem torna-se possível dar passos mais largos</p>
</div>
</div>
</div>
</div>
<div id="aprendizagem-online-e-a-gamificação" class="section level1">
<h1>Aprendizagem Online e a gamificação</h1>
<p>A <a href="https://en.wikipedia.org/wiki/Gamification">gamificação</a> é um conceito que vem sendo introduzido após a introdução da tecnologia na história. A idéia de se criar jogos para motivar e engajar as pessoas em atividades profissionais e a idéia de se estar em um jogo possibilita doses de motivação especialmente a quem gosta de competir.</p>
<div id="datacamp" class="section level2">
<h2>DataCamp</h2>
<center>
<img src="https://nhorton.people.amherst.edu/rstudio/datacamp.png" />
</center>
<p>No site oficial da <a href="https://www.rstudio.com/">RStudio</a> encontramos além das cheatsheets, dentre seus recursos existe a opção de <a href="https://www.rstudio.com/online-learning/">aprendizagem online</a> onde é apresentada a <a href="https://www.datacamp.com/">DataCamp</a> que é o primeiro e um dos mais importantes líderes em divulgar e ensinar Data Science, oferecendo treinamento baseado em habilidades, inovação técnica pioneira e cursos oferecidos pelos melhores educadores do mundo em data science!(Descrição deles no <a href="https://www.datacamp.com/about">site</a>)</p>
<p>Apesar do site ser pago, existem diversas opções de cursos gratuitos para quem esta começando e é muito simples e fácil aprender pelo site. Ao cumprir com exercícios e terminar os cursos o usuário ganha xp (pontos de experiencia) que registram sua evolução. Pode ser um ótimo investimento para aprender com os melhores do mundo e o legal de tudo isso é que nunca fica fácil!</p>
</div>
<div id="kaggle" class="section level2">
<h2>Kaggle</h2>
<center>
<div style="width:300px; height=200px">
<p><img src="https://upload.wikimedia.org/wikipedia/commons/7/7c/Kaggle_logo.png" /></p>
</div>
</center>
<p>O <a href="https://www.kaggle.com/">kaggle</a> é um playground para cientistas de dados, nele existem diversas modalidades e dentre elas uma das das que eu acho mais interessante são as competições de machine learning onde pessoas e empresas interessadas em adquirir ou até comparar seus resultados com os modelos feitos por uma enorme comunidade programando em diferentes linguagens estão testando para saber quem treina o modelo mais preciso e é possível acompanhar o código e o raciocínio de cientistas de dados do mundo inteiro de maneira muito simples!</p>
<p>Também existe a recompensa com pontos de experiencia para passar de nível, congratulação com medalhas dentre outras recopensas semelhantes ao dos games.</p>
<p>Existem os famosos <a href="https://www.kaggle.com/datasets">datasets</a> que são os bancos de dados fornecidos pelos usuários da plataforma e os <a href="https://www.kaggle.com/kernels">kernels</a> é um ótimo lugar para compartilhar seu trabalho e debater sobre resultados de projetos e idéias de aplicações de outras pessoas</p>
<p>Ao contrário da DataCamp, o Kaggle é gratuito e existem competições pagando até $100.000,00 para a pesosa (ou equipe) que apresentar os resultados mais satisfatórios!!</p>
</div>
</div>
<div id="para-refletir.." class="section level1">
<h1>Para refletir..</h1>
<div class="col2">
<center>
<div style="width:150px; height=300px">
<p><img src="https://cdn.pensador.com/img/authors/co/nf/confucio-2-l.jpg" /></p>
</div>
</center>
<blockquote>
<p>“Há três métodos de ganhar sabedoria: primeiro, por reflexão, que é o mais nobre; segundo, por imitação, que é o mais fácil; e o terceiro, por experiência, que é o mais amargo.”
- Confúcio</p>
</blockquote>
</div>
<p>Essa passagem de Confúcio deixa claro que não existe só um jeito de se aprender, com um mix de maneiras de se obter conhecimento fica um pouco menos difícil encontrar a sabedoria: estudar, exercitar e praticar se torna divertido quando se trata de programar em R!</p>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2018-02-17-cheatsheet-gamificacao-r/cheatsheet-gamificacao-r/">O que são CheatSheets, gamificação e por que aprender R é tão divertido?</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Estatistica</category>
      <category>Prática</category>
      <category>R</category>
      <category>Teoria</category>
      <category domain="tag">Estatistica</category>
      <category domain="tag">Prática</category>
      <category domain="tag">R</category>
      <category domain="tag">RStudio</category>
      <category domain="tag">cheatsheets</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">datacamp</category>
      <category domain="tag">gomesfellipe</category>
      <category domain="tag">gamification</category>
      <category domain="tag">gamificacao</category>
    </item>
    <item>
      <title>O paradoxo dos aniversários com simulação e probabilidade</title>
      <link>https://gomesfellipe.github.io/post/2018-01-20-simulacao-e-probabilidade/simulacao-e-probabilidade/</link>
      <pubDate>Sat, 20 Jan 2018 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2018-01-20-simulacao-e-probabilidade/simulacao-e-probabilidade/</guid>
      <description>Quanto você acha que é a probabiliddade num grupo de 23 pessoas escolhidas aleatoriamente que duas delas farão aniversário no mesmo dia? Acreditaria se eu te dissesse que essa chance é maior do que 50%? A probabilidade é contra intuitiva e neste post vamos demonstrar de forma analitica e atraves de simulação esse e outros resultados além de dissertar um pouco sobre a história e conceitos importantes de probabilidade</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="curiosidades-sobre-a-teoria-das-probabilidades" class="section level1">
<h1>Curiosidades sobre a teoria das probabilidades</h1>
<p>O uso de cálculo de probabilidades para avaliar incertezas já é utilizado a centenas de anos. Foram tantas áreas que se encontraram aplicações (como na medicina, jogos de azar, previsão do tempo…) que hoje não restam dúvidas de que os dados são onipresentes, ainda mais em plena era da informação.</p>
<p>Os conceitos de chances e de incertezas são tão antigos quando a própria civilização. Pessoas sempre tiveram que lidar com incertezas sobre o clima, suprimento de alimentos, suprimentos de água, risco de vida e tantas outras ameaças ao ser humano que o esforço para reduzir essas incertezas e seus efeitos passou a ser muito importante.</p>
<p>A ideia do jogo tem uma longa história,já no egito antigo em 2000 a.c foram encontrados em tumbas (<a href="https://pt.wikipedia.org/wiki/Jogo_de_azar#Hist%C3%B3ria">dados cúbicos com marcações praticamente idênticas às de dados modernos (wikipedia)</a>).</p>
<p>Segundo <span class="citation"><a href="#ref-DeGroot" role="doc-biblioref">DeGroot</a> (<a href="#ref-DeGroot" role="doc-biblioref">n.d.</a>)</span>, a teoria da probabilidade foi desenvolvida de forma constante desde o século XVII e tem sido amplamente aplicada em diversos campos de estudo. Hoje, a teoria da probabilidade é uma ferramenta importante na maioria das áreas de engenharia, ciência e gestão.</p>
<p>Muitos pesquisadores estão ativamente envolvidos na descoberta e no estabelecimento de novas aplicações de probabilidade em campos de química, meteorologia, fotografia de satélites, marketing, previsão de terremoto, comportamento humano, design de sistemas informáticos, finanças, genética e lei.</p>
<div id="conceitos-e-interpretações-para-probabilidades" class="section level2">
<h2>Conceitos e interpretações para probabilidades</h2>
<p>Além das muitas aplicações formais da teoria da probabilidade, o conceito de probabilidade entra em nossa vida cotidiana e conversa.</p>
<p>Muitas vezes ouvimos e usamos expressões como “<em>Provavelmente vai chover a amanhã à noite</em>,” “<em>É muito provável que o onibus atrase</em>,” ou “<em>As chances são altas de não poder se juntar a nós para almoçar esta tarde</em>.” Cada uma dessas expressões é baseada no conceito da probabilidade de que algum evento específico ocorrerá.</p>
<p>Existem três abordagens atualmente, as duas primeiras são:</p>
<div id="clássica" class="section level4">
<h4>Clássica</h4>
<ul>
<li><p>Se refere à subconjuntos unitários equiprováveis</p></li>
<li><p><span class="math inline">\(P(A)=\dfrac{\text{Número de elementos de }A}{\text{Número de elementos de }\Omega}\)</span></p></li>
</ul>
</div>
<div id="frequentista-ou-estatística" class="section level4">
<h4>Frequentista ou Estatística</h4>
<ul>
<li><p>Considera o limite de frequências relativas como o valor de probabilidade</p></li>
<li><p><span class="math inline">\(P(A)=lim_{n \rightarrow \infty} \frac{n_A}{n}\)</span></p></li>
</ul>
<p>onde <span class="math inline">\(n_A\)</span> é o nº de ocorrências de <span class="math inline">\(A\)</span> em <span class="math inline">\(n\)</span> repetições independentes do experimento</p>
</div>
<div id="definição-de-probabilidade" class="section level4">
<h4>Definição de probabilidade</h4>
<p>Segundo <span class="citation"><a href="#ref-Magalhaes" role="doc-biblioref">Magalhães</a> (<a href="#ref-Magalhaes" role="doc-biblioref">n.d.</a>)</span>, as definições acima possuem o apelo da intuição e permanecem sendo usadas para resolver inúmeros problemas, entretanto elas não são suficientes para uma formulação matemática rigorosa da probabilidade.</p>
<p>Aproximadamente em 1930 A. N. Kolmogorov apresentou um conjunto de axiomas matemáticos para definir probabilidade, permitindo incluir as definições anteriores como casos particulares.</p>
<p>Porém, como o verdadeiro significado da probabilidade ainda é um assunto altamente polêmico e está envolvido em muitas discussões filosóficas atuais sobre as bases da estatística e quando se trata de probabilidades, não adianta utilizar apenas a intuição pois nosso cérebro vai da bug!</p>
<p>A probabilidade é extremamente contra intuitiva e seu estudo deve sempre envolver uma vasta gama de exercícios para treinar nosso raciocínio analítico. Existem diversos problemas práticos que já ilustraram isso e um ótimo exemplo que todo mundo que já fez um curso básico de probabilidade já conhece, o <a href="https://pt.wikipedia.org/wiki/Paradoxo_do_anivers%C3%A1rio">Paradóxo do aniversário</a></p>
</div>
</div>
</div>
<div id="o-paradoxo-do-aniversário-ou-problema-dos-aniversários---feller68" class="section level1">
<h1>O paradoxo do aniversário (ou problema dos aniversários - Feller[68])</h1>
<p>Exemplo retirado do livro do <span class="citation"><a href="#ref-Feller" role="doc-biblioref">Feller</a> (<a href="#ref-Feller" role="doc-biblioref">n.d.</a>)</span>, questiona:</p>
<p>“Num grupo de <span class="math inline">\(n\)</span> pessoas, qual é a probabilidade de pelo menos duas delas fazerem aniversário no mesmo dia?”</p>
<p>Esse problema surpreende todo mundo porque dependendo do valor de <span class="math inline">\(n\)</span> pessoas, a probabilidade é bastante alta! Segundo veremos a probabilidade de isso ocorrer em uma turma de 23 pessoas ou mais escolhidas <strong>aleatoriamente</strong> é maior que <strong>50%</strong>!</p>
<p>Qual aluno de qualquer turma de probabilidade que nunca foi desafiado numa aposta pelo professor que tinha dois alunos com mesma data de aniversário na sala de aula e se deu conta que perderia em poucos minutos?</p>
<p>Vamos resolver esse problema tanto pela abordagem clássica quanto pela abordagem frequentista, para utilizar a segunda abordagem dados de muitas turmas de variados tamanhos serão simulados utilizando o <strong>R</strong> e podemos comparar os resultados e buscar alguma evidência de que os dados se distribuem de forma semelhante!</p>
<p><strong>Obs</strong>: Simular dados permitem imitar o funcionamento de, praticamente, qualquer tipo de operação ou processo (sistemas) do mundo real!</p>
</div>
<div id="probabilidade" class="section level1">
<h1>Probabilidade</h1>
<p>Considerando o ano com 365 dias, podemos assumir que <span class="math inline">\(n&lt;365\)</span> primeiramente devemos definir o espaço amostral <span class="math inline">\(\Omega\)</span> que será o conjunto de todas as sequências formadas com as datas dos aniversários (associamos cada data a um dos 365 dias do ano), defini-se:</p>
<p><em>experimento</em>: observar o aniversário de n pessoas</p>
<p><span class="math display">\[
\Omega = \{ (1,1,...,1),(1,2,53,...,201),(24,27,109,...,200),... \}
\]</span></p>
<p>portanto, sua cardinalidade será:</p>
<p><span class="math display">\[
\#\Omega = 365^n
\]</span></p>
<p>Definindo o evento:</p>
<p><span class="math display">\[
A = \text{pelo meno 2 alunos fazendo aniversário no mesmo dia em uma turma de tamanho }n
\]</span>
Observa-se que é um evento complicado de se calcular. Uma prática muito comum na teoria das probabilidades nestes casos é estudar o complementar do evento de interesse, veja:</p>
<p><span class="math display">\[
A^c = \text{nenhum dos alunos fazenndo aniversário no mesmo dia em uma turma de tamanho }n
\]</span></p>
<p>Agora basta fazer a conta:</p>
<p><span class="math display">\[
P(A^c)=\frac{\#A^c}{\#\Omega}=\frac{365 \times 364 \times ... \times (365-n+1)}{365^n}=\frac{365!}{365^n (365-n)!}
\]</span></p>
<p>segundo propriedades , se o evento é o complementar de todos n serem diferentes consequentemente o seguinte resultado é verdadeiro:</p>
<p><span class="math display">\[
p(A)=1- \frac{365!}{365^n (365-n)!}
\]</span></p>
<p>Agora que já sabemos a probabilidade de pelo menos duas pessoas fazerem aniversário no mesmo dia em uma turma de <span class="math inline">\(n\)</span> alunos, vejamos o comportamento deste ajuste e uma tabela com possíveis valores de <span class="math inline">\(n\)</span>:</p>
<p>Em R:</p>
<p>Utilizando expansão em série de Taylor (<a href="https://pt.wikipedia.org/wiki/Paradoxo_do_anivers%C3%A1rio#Aproxima%C3%A7%C3%B5es">mais informações</a>):</p>
<pre class="r"><code>birthday=function(x){
  a=1-exp(-(x^2)/(2*365))
  return(a)
}
birthday(23)</code></pre>
<pre><code>## [1] 0.5155095</code></pre>
<table class="table table-condensed">
<thead>
<tr>
<th style="text-align:right;">
n
</th>
<th style="text-align:right;">
P
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FFF5EB; width: 20.00%">5</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FFF5EB; width: 20.00%">0.0336668</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FEE6CE; width: 30.00%">15</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FEE6CE; width: 39.17%">0.2652457</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FDD0A2; width: 40.00%">25</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FDD0A2; width: 64.84%">0.5752117</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FDAE6B; width: 50.00%">35</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FDAE6B; width: 84.54%">0.8132683</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FD8D3C; width: 60.00%">45</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FD8D3C; width: 94.84%">0.9375864</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #F16913; width: 70.00%">55</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #F16913; width: 98.69%">0.9841381</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #D94801; width: 80.00%">65</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #D94801; width: 99.75%">0.9969349</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #A63603; width: 90.00%">75</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #A63603; width: 99.97%">0.9995496</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #7F2704; width: 100.00%">85</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #7F2704; width: 100.00%">0.9999497</span>
</td>
</tr>
</tbody>
</table>
<p>Em Python (função retirada do <a href="https://pt.wikipedia.org/wiki/Paradoxo_do_anivers%C3%A1rio#Implementa%C3%A7%C3%A3o_em_Python">wikpédia</a> para comparar os resultados):</p>
<pre class="python"><code>def birthday(x):
    p = (1.0/365)**x
    for i in range((366-x),366):
        p *= i
    return 1-p
    
print(&quot;%1.7f&quot; %(birthday(23))) #Arredondando para o mesmo numero de casas decimais default do R</code></pre>
<pre><code>## 0.5072972</code></pre>
<p>Tanto a aproximação do R quanto a do Python obtiveram resultados semelhantes</p>
<p>Vejamos como é o comportamento da curva teórica e as estimações:</p>
<p><img src="/post/2018-01-20-simulacao-e-probabilidade/simulacao-e-probabilidade_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Note que segundo a distribuição teórica, confirmamos que a probabilidade do evento ocorrer em uma turma de 23 pessoas ou mais escolhidas <strong>aleatoriamente</strong> é maior que <strong>50%</strong>!</p>
</div>
<div id="simulação" class="section level1">
<h1>Simulação</h1>
<p>Segundo o <a href="https://pt.wikipedia.org/wiki/Simula%C3%A7%C3%A3o">wikipédia</a>, a simulação “consiste em empregar formalizações em computadores, como expressões matemáticas ou especificações mais ou menos formalizadas, com o propósito de imitar um processo ou operação do mundo real”</p>
<p>Nossa simulação irá consistir em imitar o comportamento de um processo do mundo real utilizando o seguinte código para simular o experimento de <em>observar o aniversário de <span class="math inline">\(n\)</span> pessoas</em> milhares de vezes:</p>
<pre class="r"><code>N&lt;- 5000                                    #Numero de simulacoes do experimento

prob=0
for(n in 2:100){                            #Para n variand de 2 até 50
  cont_a=0                                  #Inicia o contador
  M=matrix(NA, N, n)                        #Delara uma matriz varia com as dimensoes desejadas  
  for(i in 1:N){                            #indice i que percorre todas as N linhas simuladas
    M[i,] = sample(1:365, n, replace = T)   #Sorteio de uma amosra de tamanho n de numeros de 1 a 365 
    linha=M[i,]                             #objeto linha recebe a linha simulada
    tab=table(linha)                        #objeto tab guarda a tabela de frequencias dessa amostra
    if(length(tab)&lt;n){                      #se o tamanho da tabela de frequencias for menor que o tamanho da turma
      cont_a=cont_a+1                       #contador recebe 1 pois duas pessoas fizeram aniversario no mesmo dia
    } 
  }
  prob[n]=cont_a/N                          #a probabilidade será a proporcao de pessoas que fazem aniversario no mesmo dia observadas em N amostra simuladas
}

prob[23]</code></pre>
<pre><code>## [1] 0.5088</code></pre>
<p>Notamos que o resultado observado é muito próximo d resultado calculado de acordo com a probabilidade teoria para a chance de se se encontrar pelo menos 2 pessoas que fazem aniversário em uma turma de 23 anos (<em>novamente ultrapassou os 50%!!!</em>)</p>
<p>Para efeito de comparação visual com a resolução anterior:</p>
<table class="table table-condensed">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
n
</th>
<th style="text-align:right;">
P
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
5
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FFF5EB; width: 20.00%">5</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FFF5EB; width: 20.00%">0.0236</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
15
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FEE6CE; width: 30.00%">15</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FEE6CE; width: 38.30%">0.2470</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
25
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FDD0A2; width: 40.00%">25</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FDD0A2; width: 65.21%">0.5754</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
35
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FDAE6B; width: 50.00%">35</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FDAE6B; width: 85.02%">0.8172</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
45
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FD8D3C; width: 60.00%">45</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FD8D3C; width: 95.00%">0.9390</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
55
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #F16913; width: 70.00%">55</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #F16913; width: 98.87%">0.9862</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
65
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #D94801; width: 80.00%">65</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #D94801; width: 99.85%">0.9982</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
75
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #A63603; width: 90.00%">75</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #A63603; width: 99.97%">0.9996</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
85
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #7F2704; width: 100.00%">85</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #7F2704; width: 100.00%">1.0000</span>
</td>
</tr>
</tbody>
</table>
<p><img src="/post/2018-01-20-simulacao-e-probabilidade/simulacao-e-probabilidade_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<div id="comparando" class="section level1">
<h1>Comparando</h1>
<p>Por fim, vejamos de forma visual se o comportamento dos resultados simulados estão de acordo com o resultado teórico calculado:</p>
<p><img src="/post/2018-01-20-simulacao-e-probabilidade/simulacao-e-probabilidade_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Como podemos ver o comportamento dos dados simulados foi muito similar ao da curva teórica calculada.</p>
</div>
<div id="modelagem-e-simulação-em-probabilidade" class="section level1">
<h1>Modelagem e simulação em probabilidade</h1>
<p>Existe uma vasta gama de aplicações de simulações como em projetos de análises de sistemas de manufatura, avaliação de requisitos não funcionais de hardware e software, avaliação de novas armas e táticas militares, reposição de estoque, projeto de sistemas de transporte, avaliações de serviços, aplicações estatísticas de cadeias MCMC…</p>
<p>Um simulador permite testar várias alternativas a um custo <strong>geralmente</strong> mais baixo do que no mundo real, possibilitando o melhor entendimento sobre o problema!</p>
</div>
<div id="referências" class="section level1 unnumbered">
<h1>Referências</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-DeGroot" class="csl-entry">
DeGroot, Morris H. n.d. <em>Probability and Statistics</em>. Vol. 4.
</div>
<div id="ref-Feller" class="csl-entry">
Feller, William. n.d. <em>An Introduction to Probability Theory and Its Applications</em>. Vol. 3.
</div>
<div id="ref-Magalhaes" class="csl-entry">
Magalhães, Mascos N. n.d. <em>Probabilidade e Variáveis Aleatóriasa</em>. Vol. 1.
</div>
</div>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2018-01-20-simulacao-e-probabilidade/simulacao-e-probabilidade/">O paradoxo dos aniversários com simulação e probabilidade</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Estatistica</category>
      <category>Modelagem Estatistica</category>
      <category>Analise Exploratória</category>
      <category>Teoria</category>
      <category>Simulação</category>
      <category>Probabilidade</category>
      <category domain="tag">gomesfellipe</category>
      <category domain="tag">Estatistica</category>
      <category domain="tag">Teoria</category>
      <category domain="tag">analise multivariada</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">modelagem estatistica</category>
      <category domain="tag">simulacao</category>
      <category domain="tag">probabilidade</category>
    </item>
    <item>
      <title>Análise Multivariada com R</title>
      <link>https://gomesfellipe.github.io/post/2018-01-04-analise-multivariada-em-r/analise-multivariada-em-r/</link>
      <pubDate>Thu, 04 Jan 2018 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2018-01-04-analise-multivariada-em-r/analise-multivariada-em-r/</guid>
      <description>Análise Multivariada Imagem do Wikpedia
Esse é o primeiro post do ano e como no ano de 2017 falou-se tanto das maravilhas computacionais desta onda do Big Data e em contra partida, identificamos que deste 2004 a popularidade pelo termo “estatística” vem diminuindo como mostrei em uma breve pesquisa neste post sobre a API do googletrends sinto que existe uma necessidade de se ampliar também a divulgação dos métodos estatísticos pois o aprofundamento na teoria é fundamental (é muito fácil achar resultados sem fundamento apenas “apertando botão”), como as ferramentas da estatística multivariada que muitas vezes servem como soluções para essas grandes quantidades de dados</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="análise-multivariada" class="section level1">
<h1>Análise Multivariada</h1>
<p><a href="https://commons.wikimedia.org/wiki/File:Multivariate_Gaussian.png">Imagem do Wikpedia</a></p>
<p>Esse é o primeiro post do ano e como no ano de 2017 falou-se tanto das maravilhas computacionais desta onda do Big Data e em contra partida, <a href="https://gomesfellipe.github.io/post/2017-12-12-google-trends-e-r/google-trends-e-r/">identificamos que deste 2004 a popularidade pelo termo “estatística” vem diminuindo como mostrei em uma breve pesquisa neste post sobre a API do googletrends</a> sinto que existe uma necessidade de se ampliar também a divulgação dos métodos estatísticos pois o aprofundamento na teoria é fundamental (é muito fácil achar resultados sem fundamento apenas “apertando botão”), como as ferramentas da estatística multivariada que muitas vezes servem como soluções para essas grandes quantidades de dados</p>
<p>Diversas vezes nos deparamos com bases de dados que envolvem além de muitas observações, muitas variáveis, especialmente nas análises de fenômenos ou processos sociais, psicológicos, educacionais e econômicos bem como na área da química, biologia, geologia, marketing, medicina, medicina veterinária, dentre muitas outras.</p>
<p>Com as ferramentas estatísticas da análise multivariada somos capazes de identificar muitos elementos que podem ter relevância na análise dos dados, dois exemplos de ferramentas importantes são as que permitem encontrar fatores que não são diretamente observáveis com base em um conjunto de variáveis observáveis e as que permitem agrupar conjuntos de dados que possuem características semelhantes com algorítimos computacionais (chamados de aprendizados não-supervisionados ou semi-supervisionados em machine learning) e a partir dai estudar as novas classificações.</p>
<p>Neste post será apresentado algumas soluções para o caso em que existe a necessidade de avaliar um grande conjunto de dados com muitas variáveis e não temos muitas informações a respeito.</p>
</div>
<div id="análise-fatorial" class="section level1">
<h1>Análise Fatorial</h1>
<p>Na análise fatorial buscamos fatores que explicam parte da variância total dos dados, os fatores são as somas das variâncias originais.</p>
<div id="objetivo-da-análise-fatorial" class="section level2">
<h2>Objetivo da análise fatorial:</h2>
<ul>
<li><p>Procura identificar fatores que não são diretamente observáveis, com base em um conjunto de variáveis observáveis.</p></li>
<li><p>Explicar a correlação ou covariância, entre um conjunto de variáveis, em termos de um número limitado de variáveis não-observáveis, chamadas de fatores ou variáveis latentes.</p></li>
<li><p>Em casos nos quais se tem um número grande de variáveis medidas e correlacionadas entre si, seria possível identificar-se um número menor de variáveis alternativas, não correlacionadas e que de algum modo sumarizassem as informações principais das variáveis originais.</p></li>
<li><p>A partir do momento em que os fatores são identificados, seus valores numéricos, chamados de escores, podem ser obtidos para cada elemento amostral. Conseqüentemente, estes escores podem ser utilizados em outras análises que envolvam outras técnicas estatísticas, como análise de regressão ou análise de variância, por exemplo.</p></li>
</ul>
</div>
<div id="etapas-para-realização" class="section level2">
<h2>Etapas para realização</h2>
<ul>
<li><p>Computação da matriz de correlações para as variáveis originais;</p></li>
<li><p>Extração de fatores</p></li>
<li><p>Rotação dos fatores para tonar a interpretação mais fácil;</p></li>
<li><p>Cálculo dos escores dos fatores</p></li>
</ul>
<div id="matriz-de-correlação" class="section level3">
<h3>Matriz de Correlação:</h3>
<ul>
<li>Teste de Bartlett - a hipótese nula da matriz de correlação ser uma matriz identidade ( <span class="math inline">\(| R | = 1\)</span> ), isto é, avalia se os componentes fora da diagonal principal são zero. O resultado significativo indica que existem algumas relações entre as variáveis.</li>
</ul>
<p>No R:</p>
<pre class="r"><code>Bartlett.sphericity.test &lt;- function(x)
{
  method &lt;- &quot;Teste de esfericidade de Bartlett&quot;
  data.name &lt;- deparse(substitute(x))
  x &lt;- subset(x, complete.cases(x)) # Omitindo valores faltantes
  n &lt;- nrow(x)
  p &lt;- ncol(x)
  chisq &lt;- (1-n+(2*p+5)/6)*log(det(cor(x)))
  df &lt;- p*(p-1)/2
  p.value &lt;- pchisq(chisq, df, lower.tail=FALSE)
  names(chisq) &lt;- &quot;X-squared&quot;
  names(df) &lt;- &quot;df&quot;
  return(structure(list(statistic=chisq, parameter=df, p.value=p.value,
                        method=method, data.name=data.name), class=&quot;htest&quot;))
}
Bartlett.sphericity.test(dados)</code></pre>
<pre><code>## 
##  Teste de esfericidade de Bartlett
## 
## data:  dados
## X-squared = 2590.3, df = 55, p-value &lt; 2.2e-16</code></pre>
<ul>
<li>Teste KMO (Kaiser-Meyer-Olkin) - avalia a adequação do tamanho amostra. Varia entre 0 e 1, onde: zero indica inadequado para análise fatorial, aceitável se for maior que 0.5, recomendado acima de 0.8.</li>
</ul>
<p>No R:</p>
<pre class="r"><code>kmo = function(x)
{
  x = subset(x, complete.cases(x))
  r = cor(x)
  r2 = r^2 
  i = solve(r) 
  d = diag(i) 
  p2 = (-i/sqrt(outer(d, d)))^2 
  diag(r2) &lt;- diag(p2) &lt;- 0 
  KMO = sum(r2)/(sum(r2)+sum(p2))
  MSA = colSums(r2)/(colSums(r2)+colSums(p2))
  return(list(KMO=KMO, MSA=MSA))
}

kmo(dados)</code></pre>
<pre><code>## $KMO
## [1] 0.5942236
## 
## $MSA
##         A         B         C         D         E         F         G         H 
## 0.6789278 0.9151657 0.6897541 0.3385536 0.8699746 0.3632508 0.5172135 0.4878681 
##         I         J         K 
## 0.4901580 0.4895023 0.4686937</code></pre>
<div id="tipos-de-correlação" class="section level4">
<h4>Tipos de correlação:</h4>
<p>Nem sempre é possível utilizar a correlação de pearson, porém, existem diversas outras maneiras de se saber qual a correlação dos dados. Podemos utilizar correlações como de Spearman, Policórica, etc.. Já fiz um post onde explico os <a href="https://gomesfellipe.github.io/post/tipos-de-relacoes-entre-variaveis/">diferentes tipos de relações entre os tipos de variáveis</a> e os <a href="https://gomesfellipe.github.io/post/tipos-de-correlacoes/">tipos de correlações</a> possíveis para avaliar a relação dessas variáveis.</p>
<p>Aqui um outro exemplo de como utilizar a correlação parcial</p>
<pre class="r"><code>partial.cor &lt;- function (x)
{
R &lt;- cor(x)
RI &lt;- solve(R)
D &lt;- 1/sqrt(diag(RI))
Rp &lt;- -RI * (D %o% D)
diag(Rp) &lt;- 0
rownames(Rp) &lt;- colnames(Rp) &lt;- colnames(x)
Rp
}
mat_anti_imagem &lt;- -partial.cor(dados[,1:10])
mat_anti_imagem</code></pre>
<pre><code>##              A           B            C           D           E            F
## A  0.000000000 -0.25871349 -0.872167400  0.01668860 -0.04245837 -0.011036934
## B -0.258713494  0.00000000 -0.204228090  0.05621580  0.09801359  0.060189694
## C -0.872167400 -0.20422809  0.000000000 -0.02459342 -0.00482831 -0.008201211
## D  0.016688604  0.05621580 -0.024593418  0.00000000 -0.18602037  0.806258927
## E -0.042458373  0.09801359 -0.004828310 -0.18602037  0.00000000 -0.140784264
## F -0.011036934  0.06018969 -0.008201211  0.80625893 -0.14078426  0.000000000
## G  0.006994874 -0.12746560  0.045529480 -0.75073120 -0.32160010 -0.792991683
## H -0.071792191  0.03375700  0.059785980 -0.03196914  0.08903510  0.001496987
## I  0.055698412 -0.03720345 -0.040061304  0.08243832 -0.03925037  0.084673934
## J -0.042609593  0.06775755  0.004145341 -0.06024239  0.05806892 -0.001404078
##              G            H           I            J
## A  0.006994874 -0.071792191  0.05569841 -0.042609593
## B -0.127465596  0.033756995 -0.03720345  0.067757550
## C  0.045529480  0.059785980 -0.04006130  0.004145341
## D -0.750731196 -0.031969142  0.08243832 -0.060242391
## E -0.321600101  0.089035097 -0.03925037  0.058068923
## F -0.792991683  0.001496987  0.08467393 -0.001404078
## G  0.000000000 -0.025016441 -0.05943429  0.009961615
## H -0.025016441  0.000000000 -0.55229599  0.044929237
## I -0.059434295 -0.552295987  0.00000000  0.056248550
## J  0.009961615  0.044929237  0.05624855  0.000000000</code></pre>
</div>
</div>
<div id="extração-de-fatores-via-componentes-principais" class="section level3">
<h3>Extração de fatores via componentes principais</h3>
<ul>
<li><p>Determinado o número de fatores necessários para representar os dados</p></li>
<li><p>Também é determinado o método que será utilizado, o mais utilizado é a análise de componentes principais</p></li>
</ul>
<div id="estimação-do-número-de-fatores-m" class="section level4">
<h4>Estimação do número de fatores m</h4>
<ul>
<li><p>Estimação do número de fatores m</p></li>
<li><p>Para a estimação de m, bastará extrair-se os autovalores da matriz de correlação amostral.</p></li>
<li><p>Observa-se quais autovalores são os mais importantes em termos de grandeza numérica.</p></li>
<li><p>os autovalores refletem a importância do fator se o número de fatores for igual ao número de variáveis então a soma dos autovetores é igual a soma das variâncias (pois cada variância será igual a 1).</p></li>
<li><p>Portanto a razão $ / 2 var $ indica proporção da variabilidade total explicada pelo fator</p></li>
</ul>
<p><strong>Critérios:</strong></p>
<ol style="list-style-type: decimal">
<li><p>A análise da proporção da variância total relacionada com cada autovalor (<span class="math inline">\(\lambda_i\)</span>). Permanecem aqueles autovalores que maiores proporções da variância total e, portanto, o valor de m será igual ao número de autovalores retidos;</p></li>
<li><p>A comparação do valor numérico de (<span class="math inline">\(\lambda_i\)</span>) com o valor 1. O valor de m será igual ao número de autovalores maiores ou iguais a 1.</p></li>
<li><p>Observação do gráfico scree-plot, que dispõe os valores de (<span class="math inline">\(\lambda_i\)</span>) ordenados em ordem decrescente. Por este critério, procura-se no gráfico um “ponto de salto”, que estaria representando um decréscimo de importância em relação à variância total. O valor de m seria então igual ao número de autovalores anteriores ao “ponto de salto”.</p></li>
</ol>
</div>
</div>
<div id="análise-de-componentes-principais" class="section level3">
<h3>Análise de componentes Principais:</h3>
<ul>
<li><p>Fatores são obtidos através da decomposição espectral da matriz de correlações, resultado em cargas fatoriais que indicam o quanto cada variável está associada a cada fator e os autovalores associados a cada um dos fatores envolvidos</p></li>
<li><p>São formadas combinações lineares das variáveis observadas.</p></li>
<li><p>O primeiro componente principal consiste na combinação que responde pela maior quantidade de variância na amostra.</p></li>
<li><p>O segundo componente responde pela segunda maior variância na amostra e não é correlacionado com o primeiro componente.</p></li>
<li><p>Sucessivos componentes explicam progressivamente menores porções de variância total da amostra e todos são não correlacionados uns aos outros.</p></li>
</ul>
<p>No R a análise de componentes principais pode ser realizada com as funções nativas <code>prcomp()</code> e a visualização pode ser realizada com a função <code>biplot</code> nativa do R ou com a função <code>autoplot()</code> do pacote <code>ggfortify</code> apresentado em um posto que eu <a href="https://gomesfellipe.github.io/post/2017-12-26-diagnostico-de-modelos/diagnostico-de-modelos/">comento sobre ajustes de modelos lineares</a>.</p>
<p>Neste exemplo utilizaremos <a href="https://github.com/vqv/ggbiplot/blob/master/R/ggscreeplot.r">as funções de código aberto encontrei nesse github</a> que permite elaborar o gráfico baseado em funções do <code>ggplot</code>, além disso também carregaremos o pacote deste Github. Veja:</p>
<pre class="r"><code>library(ggplot2)
library(ggfortify)
library(ggbiplot)
#Componentes principais:
acpcor=prcomp(dados, scale = TRUE)
summary(acpcor)</code></pre>
<pre><code>## Importance of components:
##                           PC1    PC2    PC3    PC4     PC5     PC6    PC7
## Standard deviation     1.7205 1.5835 1.2745 1.2107 1.02156 0.72100 0.6634
## Proportion of Variance 0.2691 0.2280 0.1477 0.1333 0.09487 0.04726 0.0400
## Cumulative Proportion  0.2691 0.4971 0.6447 0.7780 0.87285 0.92011 0.9601
##                           PC8     PC9    PC10    PC11
## Standard deviation     0.4953 0.33061 0.25079 0.14588
## Proportion of Variance 0.0223 0.00994 0.00572 0.00193
## Cumulative Proportion  0.9824 0.99235 0.99807 1.00000</code></pre>
<pre class="r"><code>ggbiplot(acpcor, obs.scale = 1, var.scale = 1,
   ellipse = TRUE, circle = TRUE) +
  scale_color_discrete(name = &#39;&#39;) +
  theme(legend.direction = &#39;horizontal&#39;, legend.position = &#39;top&#39;)</code></pre>
<p><img src="/post/2018-01-04-analise-multivariada-em-r/analise-multivariada-em-r_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code># autoplot(acpcor, label = TRUE, label.size = 1,
#          loadings = TRUE, loadings.label = TRUE, loadings.label.size  = 3)</code></pre>
<p>Para a observação do gráfico scree-plot podemos utilizar os comandos a seguir (com funções nativas do R ou mesmo com funções personalizadas como a que eu acabei de comentar <a href="https://github.com/vqv/ggbiplot/blob/master/R/ggscreeplot.r">disponivel nesse github</a></p>
<pre class="r"><code>#Com Funcao nativa do R:
# plot(1:ncol(dados), acpcor$sdev^2, type = &quot;b&quot;, xlab = &quot;Componente&quot;,
#      ylab = &quot;Variância&quot;, pch = 20, cex.axis = 1.3, cex.lab = 1.3)

#Ou funcao personalizada com ggplot2:
ggscreeplot(acpcor)</code></pre>
<p><img src="/post/2018-01-04-analise-multivariada-em-r/analise-multivariada-em-r_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<div id="rotação" class="section level4">
<h4>Rotação</h4>
<ul>
<li><p>Algumas variáveis são mais correlacionadas com alguns fatores do que outras.</p></li>
<li><p>Em alguns casos, a interpretação dos fatores originais pode não ser tarefa muito fácil devido à aparição de coeficientes de grandeza numérica similar, e não desprezível, em vários fatores diferentes.</p></li>
<li><p>O propósito da rotação é obter uma estrutura simples.</p></li>
<li><p>Em uma estrutura simples, cada fator tem carga alta somente para algumas variáveis, tornando mais fácil a sua identificação.</p></li>
<li><p>Tipos: Varimax, Quartimax, Equamax</p></li>
</ul>
<p>Aplicando a Varimax:</p>
<pre class="r"><code>k &lt;- 6 #6 fatores selecionados
carfat = acpcor$rotation[, 1:k] %*% diag(acpcor$sdev[1:k])
carfatr = varimax(carfat)</code></pre>
</div>
<div id="comunalidade" class="section level4">
<h4>Comunalidade</h4>
<ul>
<li><p>Índices atribuídos a variável original que expressam em % o quanto da variabilidade de cada variável é explicada pelo modelo</p></li>
<li><p>Designa-se por comunalidade (<span class="math inline">\(h^{2}_i\)</span>)a proporção da variância de cada variável explicada pelos fatores comuns.</p></li>
<li><p>As comunalidades variam entre 0 e 1, sendo 0 quando os fatores comuns não explicam nenhuma variância da variável e 1 quando explicam toda a sua variância.</p></li>
<li><p>Quando o valor das comunalidades é menor que 0,6 deve-se
pensar em: aumentar a amostra, eliminar as variáveis.</p></li>
</ul>
</div>
</div>
<div id="interpretar-o-modelo" class="section level3">
<h3>Interpretar o modelo</h3>
<ul>
<li><p>Feito pelas cargas fatoriais que são os parâmetros do modelo</p></li>
<li><p>Fatores expressam as covariâncias entre cada fator e as variáveis originais</p></li>
<li><p>Varimax ajuda a interpretar o modelo</p></li>
<li><p>Rotações ortogonais (para dependente) ; Rotações oblíquas (para independentes)</p></li>
</ul>
</div>
</div>
</div>
<div id="clusters" class="section level1">
<h1>Clusters</h1>
<p>Técnica estatística multivariada que tem como objetivo organizar um conjunto de objetos em um determinado nº de subconjuntos mutuamente exclusivos (clusters), de tal forma que os objetos em um mesmo cluster sejam semelhantes entre si,porém diferentes dos objetos nos outros clusters</p>
<p>Etapas para análise de clusters, que são comuns em qualquer análise (KDD):</p>
<ul>
<li>Seleção dos objetos a serem agrupados</li>
<li>Definir conjunto de atributos que caracterizam os objetos</li>
<li>Medida de dissimilaridade</li>
<li>Seleção de um algoritmo de agregação</li>
<li>Definição do número de clusters</li>
<li>Interpretação e validação dos clusters</li>
</ul>
<p>Critérios para a seleção:</p>
<ul>
<li>Selecionar variáveis diferentes entre si</li>
<li>Variáveis padronizadas (padronização mais comum é a Z-score)</li>
</ul>
<p>Existem algumas abordagens para a utilização das técnicas de análises de clusters, as diferenças entre os métodos hierárquicos e os não hierárquicos são as seguintes:</p>
<p>Métodos Hierárquicos são preferidos quando:</p>
<ul>
<li>Serão analisadas varias alternativas de agrupamento.</li>
<li>O tamanho da amostra é moderado ( de 300 a 1000 objetos )</li>
</ul>
<p>Métodos não-hierárquicos são preferidos quando:</p>
<ul>
<li>O número de grupos é conhecido.</li>
<li>Presença dos outliers, desde que os métodos não-hierárquicos são
menos influenciados por outliers.</li>
<li>Há um grande nº de objetos a serem agrupados.</li>
</ul>
<div id="método-hierárquico-de-agrupamento" class="section level2">
<h2>Método hierárquico de agrupamento</h2>
<p>É realizado em dois passos, o primeiro deles calcula-se a matriz de similaridade com o uso da função <em>dist()</em> (existem diversos tipos de distâncias que podem ser utilizadas aqui), o método utilizado será o de <strong>Ward</strong> (também poderíamos escolher o método da menor distância, maior distância ou a distância média).</p>
<p>Vantagens:</p>
<ul>
<li>Rápidos e exigem menos tempo de processamento.</li>
<li>Apresentam resultados para diferentes níveis de agregação.</li>
</ul>
<p>Desvantagens:</p>
<ul>
<li>Alocação de um objeto em um cluster é irrevogável</li>
<li>Impacto substancial dos outliers ( apesar do Ward ser o menos susceptível)</li>
<li>Não apropriados para analisar uma amostra muito extensa, pois a medida que o tamanho da amostra aumenta, a necessidade de armazenamento das distâncias cresce drasticamente</li>
</ul>
<p>Para bases grandes é melhor não usar este método pois precisa da matriz de distâncias.</p>
<p>Dentre os métodos, a menor distância pode ser ruim em muitas situações, pois coloca muitos objetos no mesmo cluster.</p>
<p>Geralmente utiliza-se o dendograma para a visualização dos clusters.</p>
<pre class="r"><code>#Construindo a matriz de similaridade:
matriz_similaridade = dist(iris[,-5],             #Conjunto de dados utilizados
                           &quot;euclidean&quot;            #medida de distância utilizada
                           )

#Construindo o agrupamento hierárquico aglomerativo:
agrupamento = hclust(matriz_similaridade,     #Matriz de similaridade calculada
                     &quot;ward.D&quot;                 #Método de agrupamento
                     )
#Converte hclust em dendrograma e plot:
hcd &lt;- as.dendrogram(agrupamento)

library(ggdendro)
# Tipo pode ser &quot;rectangle&quot; ou &quot;triangle&quot;
dend_data &lt;- dendro_data(hcd, type = &quot;rectangle&quot;)
# o que esta contido em dend_data:
names(dend_data)</code></pre>
<pre><code>## [1] &quot;segments&quot;    &quot;labels&quot;      &quot;leaf_labels&quot; &quot;class&quot;</code></pre>
<pre class="r"><code>plot(agrupamento,xlab=&quot;Matriz de similaridade&quot;,main = &quot;Dendograma&quot;, cex = 0.3)
#Construindo representacao de grupos - geração de vetores:
grupos = cutree(agrupamento,             #Variável calculada em hclust
                3                        #Quantidade de grupos desejados
                )

#Construindo o dendograma:
rect.hclust(agrupamento, k=3, border=&quot;red&quot;)</code></pre>
<p><img src="/post/2018-01-04-analise-multivariada-em-r/analise-multivariada-em-r_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Existem diversas outras maneiras de se visualizar dendogramas, veja a seguir um outro exemplo utilizando o pacote <code>ape</code>:</p>
<pre class="r"><code>library(ape)
plot(as.phylo(agrupamento), type = &quot;unrooted&quot;, cex = 0.6,
     no.margin = TRUE)</code></pre>
<p><img src="/post/2018-01-04-analise-multivariada-em-r/analise-multivariada-em-r_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Para mais informações de métodos de plot de dendogramas,talvez <a href="http://www.sthda.com/english/wiki/beautiful-dendrogram-visualizations-in-r-5-must-known-methods-unsupervised-machine-learning">essa página</a> possa ser útil.</p>
</div>
<div id="método-não-hierárquico-de-agrupamento-k-means" class="section level2">
<h2>Método não hierárquico de agrupamento K-means</h2>
<p>Esta é uma das mais populares abordagens de agrupamento de dados por partição. A partir de uma escolha inicial para os centroides, o algoritmo procede verificando quais exemplares são mais similares a quais centroides.</p>
<p>Vantagens:</p>
<ul>
<li>Tendem a maximizar a dispersão entre os centros de gravidade dos clusters (mantem os clusters bem separados)</li>
<li>Simplicidade de cálculo, calcula somente as distâncias entre os objetos e os centros de gravidade dos clusters</li>
</ul>
<p>Desvantagens:</p>
<ul>
<li>Depende dos conjuntos de sementes iniciais, principalmente se a seleção das sementes é aleatória</li>
<li>Não há garantias de um agrupamento ótimo dos objetos</li>
</ul>
<pre class="r"><code>#Construindo o agrupamento por particionamento:
c = kmeans(iris[,-5],          #Conjunto de dados utilizados
                 2,            #Número de  grupos a ser descoberto
                 iter.max=5    #Número máximo de iterações permitido no algorítmo
                 )</code></pre>
<p>Para efeito de visualização, podemos utilizar a seguinte função que encontra dois fatores principais a partir da análise fatorial e às utiliza como eixos</p>
<pre class="r"><code>plot_kmeans = function(df, clusters, runs) {
  suppressMessages(library(psych))
  suppressMessages(library(ggplot2))
  
  #cluster
  tmp_k = kmeans(df, centers = clusters, nstart = 100)
  
  #factor
  tmp_f = fa(df, 2, rotate = &quot;none&quot;)
  
  #collect data
  tmp_d = data.frame(matrix(ncol=0, nrow=nrow(df)))
  tmp_d$cluster = as.factor(tmp_k$cluster)
  tmp_d$fact_1 = as.numeric(tmp_f$scores[, 1])
  tmp_d$fact_2 = as.numeric(tmp_f$scores[, 2])
  tmp_d$label = rownames(df)
  
  #plot
  g = ggplot(tmp_d, aes(fact_1, fact_2, color = cluster)) + geom_point() + geom_text(aes(label = label), size = 3, vjust = 1, color = &quot;black&quot;)
  return(g)
}
plot_kmeans(iris[,-5], 3)</code></pre>
<p><img src="/post/2018-01-04-analise-multivariada-em-r/analise-multivariada-em-r_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<div id="análise-exploratória-dos-clusters" class="section level3">
<h3>Análise exploratória dos clusters</h3>
<p>Não vou me estender nessa parte, mas é bom esclarecer que após encontrar os clusters e de extrema importância realizar a análise exploratória deles para entender os comportamentos dos grupos identificados.</p>
<pre class="r"><code>#Conferindo os grupos formados:
c$cluster%&gt;%
  table()</code></pre>
<pre><code>## .
##  1  2 
## 53 97</code></pre>
<pre class="r"><code>c$cluster%&gt;%
  table()%&gt;%
  barplot(main=&quot;Frequências dos clusters&quot;, names.arg=c(&quot;Cluster 1&quot;, &quot;Cluster 2&quot;))</code></pre>
<p><img src="/post/2018-01-04-analise-multivariada-em-r/analise-multivariada-em-r_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
</div>
</div>
<div id="medidas-de-validação-e-estabilidade" class="section level2">
<h2>Medidas de validação e estabilidade</h2>
<div id="pseudo-f" class="section level4">
<h4>pseudo-F</h4>
<p>O número adequado de clusters (k) deve ser maximizar o pseudo-F:</p>
<p><span class="math display">\[
pseudo-F = \dfrac{  \dfrac{BSS}{k-1} } { \dfrac{WSS}{N-k}} =\dfrac{\textrm{Quadrado médio entre clusters}}{\textrm{Quadrado médio dentro dos clusters}}
\]</span></p>
</div>
<div id="libraryclvalid" class="section level4">
<h4>library(clvalid)</h4>
<p>Este pacote faz os cálculos das medidas que avaliam se os clusters são compactos, bem separados e estáveis.</p>
<p>Vejamos os tipos de medidas:</p>
<p><strong>Medidas de validação</strong>:</p>
<ol style="list-style-type: decimal">
<li>conectividade: relativa ao grau de vizinhança entre objetos em um mesmo cluster, varia
entre 0 e infinito e quanto menor melhor.</li>
<li>silhueta: homogeneidade interna, assume valores entre -1 e 1 e quanto mais próximo de 1
melhor.</li>
<li>índice de Dunn: quantifica a separação entre os agrupamentos, assume valores entre 0 e 1 e
quanto maior melhor.</li>
</ol>
<p><strong>Medidas de estabilidade</strong>:</p>
<ol style="list-style-type: decimal">
<li>APN - average proportion of non-overlap: proporção média de observações não
classificadas no mesmo cluster nos casos com dados completos e incompletos. Assume valor
no intervalo [0,1], próximos de 0 indicam agrupamentos consistentes.</li>
<li>AD - average distance: distância média entre observações classificadas no mesmo cluster
nos casos com dados completos e incompletos. Assume valores não negativos, sendo
preferíveis valores próximos de zero.</li>
<li>ADM - average distance between means: distância média entre os centroides quando as
observações estão em um mesmo cluster. Assume valores não negativos, sendo preferíveis
valores próximos de zero.</li>
<li>FOM - figure of merit: medida do erro cometido ao usar os centroides como estimativas das
observações na coluna removida. Assume valores não negativos, sendo preferíveis valores
próximos de zero.</li>
</ol>
<pre class="r"><code>library(clValid)

#Medidas de validação:
valida=clValid(iris[1:4],3,clMethods=c(&quot;hierarchical&quot;,&quot;kmeans&quot;),validation=&quot;internal&quot;)
summary(valida)</code></pre>
<pre><code>## 
## Clustering Methods:
##  hierarchical kmeans 
## 
## Cluster sizes:
##  3 
## 
## Validation Measures:
##                                  3
##                                   
## hierarchical Connectivity   4.4770
##              Dunn           0.1378
##              Silhouette     0.5542
## kmeans       Connectivity  10.0917
##              Dunn           0.0988
##              Silhouette     0.5528
## 
## Optimal Scores:
## 
##              Score  Method       Clusters
## Connectivity 4.4770 hierarchical 3       
## Dunn         0.1378 hierarchical 3       
## Silhouette   0.5542 hierarchical 3</code></pre>
<pre class="r"><code>#Medidas de estabilidade;
valida=clValid(iris[1:4],3,clMethods=c(&quot;hierarchical&quot;,&quot;kmeans&quot;),validation=&quot;stability&quot;)
summary(valida)</code></pre>
<pre><code>## 
## Clustering Methods:
##  hierarchical kmeans 
## 
## Cluster sizes:
##  3 
## 
## Validation Measures:
##                        3
##                         
## hierarchical APN  0.0912
##              AD   1.0596
##              ADM  0.3680
##              FOM  0.4209
## kmeans       APN  0.0630
##              AD   0.9390
##              ADM  0.1131
##              FOM  0.3935
## 
## Optimal Scores:
## 
##     Score  Method Clusters
## APN 0.0630 kmeans 3       
## AD  0.9390 kmeans 3       
## ADM 0.1131 kmeans 3       
## FOM 0.3935 kmeans 3</code></pre>
</div>
<div id="gráfico-da-silhueta" class="section level4">
<h4>Gráfico da silhueta:</h4>
<pre class="r"><code>library(cluster)
#Construindo a matriz de similaridade:
matriz_similaridade = dist(iris[,-5],             #Conjunto de dados utilizados
                           &quot;euclidean&quot;            #medida de distância utilizada
                           )

#Construindo o agrupamento hierárquico aglomerativo:
agrupamento = hclust(matriz_similaridade,     #Matriz de similaridade calculada
                     &quot;ward.D&quot;                 #Método de agrupamento
                     )

silhueta =silhouette(cutree(agrupamento,k=3),dist(iris[,-5]))
plot(silhueta,main=&quot;&quot;)</code></pre>
<p><img src="/post/2018-01-04-analise-multivariada-em-r/analise-multivariada-em-r_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<pre class="r"><code>summary(silhueta)</code></pre>
<pre><code>## Silhouette of 150 units in 3 clusters from silhouette.default(x = cutree(agrupamento, k = 3), dist = dist(iris[,  from     -5])) :
##  Cluster sizes and average silhouette widths:
##        50        64        36 
## 0.7994998 0.4115006 0.4670305 
## Individual silhouette widths:
##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -0.09013  0.39933  0.56701  0.55416  0.77690  0.85493</code></pre>
</div>
<div id="muitas-opções" class="section level3">
<h3>Muitas opções</h3>
<p>Como podemos observar, a análise de agrupamentos é um método exploratório. É útil para organizar conjuntos de dados que contam com características semelhantes.</p>
<p>É uma das principais técnicas da mineração de dados e já conta com grande variedade de algoritmos.</p>
</div>
</div>
</div>
<div id="referência" class="section level1">
<h1>Referência</h1>
<p><a href="https://www1.udel.edu/oiss/pdf/617.pdf">I Johnson e Wichern (2007). Applied Multivariate Statistical Analysis, 6th
Edition. Prentice-Hal</a></p>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2018-01-04-analise-multivariada-em-r/analise-multivariada-em-r/">Análise Multivariada com R</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Estatistica</category>
      <category>R</category>
      <category>Teoria</category>
      <category>Analise Mutivariada</category>
      <category>Nao supervisionado</category>
      <category>Clustering</category>
      <category domain="tag">Estatistica</category>
      <category domain="tag">gomesfellipe</category>
      <category domain="tag">R Markdown</category>
      <category domain="tag">R</category>
      <category domain="tag">Teoria</category>
      <category domain="tag">pca</category>
      <category domain="tag">kmeans</category>
      <category domain="tag">clustering</category>
      <category domain="tag">analise multivariada</category>
    </item>
    <item>
      <title>Pacotes do R para avaliar o ajuste de modelos</title>
      <link>https://gomesfellipe.github.io/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos/</link>
      <pubDate>Sun, 24 Dec 2017 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos/</guid>
      <description>Alguns pacotes úteis para avaliar o ajuste do modelo de forma rápida, precisa e elegante</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="funções-do-r-para-avaliar-o-ajuste-de-modelos" class="section level1">
<h1>Funções do R para avaliar o ajuste de modelos</h1>
<p>Traduzindo:</p>
<p>“<em>Essencialmente, todos os modelos estão errados, mas alguns são úteis</em>” - George E. P. Box</p>
<p>Se você estuda estatística provavelmente já deve saber quem é este simpático senhor. Box teve grande contribuição para a estatística. Foi aluno do Ronald Aylmer Fisher e ainda se casou com a filha dele!</p>
<p>Lendo um <a href="http://jaguar.fcav.unesp.br/RME/fasciculos/v27/v27_n4/A10_Millor.pdf">artigo sobre a vida de Fisher</a> um parágrafo me chamou atenção com uma fala de sua filha, que dizia o seguinte:</p>
<p>“Joan Fisher Box, filha de Fisher, em seu livro sobre a vida do pai, se referindo à péssima classificação dele em francês, escreveu: “… ele nunca teve muita paciência com irrelevâncias.” (Box, 1978)"</p>
<p>Fico imaginando o tamanho da contribuição desdes crânios para a comunidade se tivessem acesso a tantos mecanismos que temos hoje em dia e o que eles achariam relevantes..</p>
<p>Para o bom ajuste de um modelo, certamente; a inferência, as análises de desvios, os critérios de seleção de um modelo, conferir comportamento dos resíduos e avaliação das estatísticas de diagnósticos são muito relevantes.</p>
<p>No <a href="https://cran.r-project.org/">CRAN</a> já contamos com muitos pacotes disponíveis para nos auxiliar nessas avaliações, portanto vou mostrar aqui alguns pacotes com funções que já me ajudaram muito em avaliações de modelos indo além das funções nativas do R e do pacote <code>ggplot2</code> (Um excelente pacote para apresentações elegantes e práticas de resultados visuais).</p>
</div>
<div id="ggally" class="section level1">
<h1>GGally</h1>
<p>Este pacote é sensacional, existem funções muito relevantes nele para melhorar a nossa experiência com ajuste de modelos, as funções apresentadas aqui são baseadas na <a href="http://ggobi.github.io/ggally/#ggally">página de documentação GGally</a>, lá você pode conferir a documentação completa.</p>
<p>Primeiramente vamos carregar o pacote:</p>
<pre class="r"><code>library(GGally)</code></pre>
<p>Carregado o pacote, vejamos as principais funções que podem nos auxiliar.</p>
<div id="ggallyggcoef" class="section level2">
<h2><code>GGally::ggcoef</code></h2>
<p>O objetivo da função <code>GGally::ggcoef</code> é traçar rapidamente os coeficientes de um modelo.</p>
<p>Para um modelo linear:</p>
<pre class="r"><code>reg &lt;- lm(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width, data = iris)
ggcoef(reg)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Para um modelo logístico podemos utilizar o argumento <code>exponentiate = TRUE</code> e além disso, somos capazes de fazer diversas alterações no gráfico utilizando o <code>ggcoef()</code> veja alguns exemplo de argumentos que podem ser usados para personalizar como barras de erro e a linha vertical são plotadas:</p>
<pre class="r"><code>#Ajustando o modelo:
d &lt;- as.data.frame(Titanic)
log.reg &lt;- glm(Survived ~ Sex + Age + Class, family = binomial, data = d, weights = d$Freq)

#Elaborando o gráfico
ggcoef(
  log.reg,                      #O modelo a ser conferido
  exponentiate = TRUE,          #Para avaliar o modelo logístico
  vline_color = &quot;red&quot;,          #Reta em zero  
  #vline_linetype =  &quot;solid&quot;,   #Altera a linha de referência
  errorbar_color = &quot;blue&quot;,      #Cor da barra de erros
  errorbar_height = .25,
  shape = 18,                   #Altera o formato dos pontos centrais
  #size=3,                      #Altera o tamanho do ponto
  color=&quot;black&quot;,                #Altera a cor do ponto
  mapping = aes(x = estimate, y = term, size = p.value))+
  scale_size_continuous(trans = &quot;reverse&quot;) #Essa linha faz com que inverta o tamanho                 </code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="ggallyggduo" class="section level2">
<h2><code>GGally::ggduo</code></h2>
<p>O objetivo desta função é exibir dois dados agrupados em uma matriz de plotagem. Isso é útil para análise de correlação canônica, análise de séries temporais múltiplas e análise de regressão.</p>
<p>Os dados do exemplo apresentados aqui podem ser encontrados neste <a href="http://www.stats.idre.ucla.edu/r/dae/canonical-correlation-analysis">link</a></p>
<pre class="r"><code>data(psychademic)
head(psychademic)</code></pre>
<pre><code>##   locus_of_control self_concept motivation read write math science    sex
## 1            -0.84        -0.24          4 54.8  64.5 44.5    52.6 female
## 2            -0.38        -0.47          3 62.7  43.7 44.7    52.6 female
## 3             0.89         0.59          3 60.6  56.7 70.5    58.0   male
## 4             0.71         0.28          3 62.7  56.7 54.7    58.0   male
## 5            -0.64         0.03          4 41.6  46.3 38.4    36.3 female
## 6             1.11         0.90          2 62.7  64.5 61.4    58.0 female</code></pre>
<pre class="r"><code>psych_variables &lt;- attr(psychademic, &quot;psychology&quot;)
academic_variables &lt;- attr(psychademic, &quot;academic&quot;)</code></pre>
<pre class="r"><code>ggduo(
  psychademic, psych_variables, academic_variables,
  types = list(continuous = &quot;smooth_lm&quot;),
  title = &quot;Correlação entre as variáveis psicológicas e academicas&quot;,
  xlab = &quot;Psicológicos&quot;,
  ylab = &quot;Academicas&quot;
)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Uma vez que o <code>ggduo</code> não tem uma seção superior para exibir os valores de correlação, podemos usar uma função personalizada para adicionar a informação nas parcelas contínuas.</p>
<p>Criando uma função personalizada para informar a correlação entre as observações:</p>
<pre class="r"><code>lm_with_cor &lt;- function(data, mapping, ..., method = &quot;pearson&quot;) {
  x &lt;- eval(mapping$x, data)
  y &lt;- eval(mapping$y, data)
  cor &lt;- cor(x, y, method = method)
  ggally_smooth_lm(data, mapping, ...) +
    ggplot2::geom_label(
      data = data.frame(
        x = min(x, na.rm = TRUE),
        y = max(y, na.rm = TRUE),
        lab = round(cor, digits = 3)
      ),
      mapping = ggplot2::aes(x = x, y = y, label = lab),
      hjust = 0, vjust = 1,
      size = 5, fontface = &quot;bold&quot;,
      inherit.aes = FALSE # do not inherit anything from the ...
    )
}</code></pre>
<p>Portanto:</p>
<pre class="r"><code>ggduo(
  psychademic, psych_variables, academic_variables,
  types = list(continuous = &quot;smooth_lm&quot;),
  title = &quot;Correlação entre variáveis acadêmica e psicológica&quot;,
  xlab = &quot;Psicológica&quot;,
  ylab = &quot;Academica&quot;
)+
  theme(legend.position = &quot;bottom&quot;)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Para avaliar resíduos da uma regressão ajustada para cada uma das variáveis explanatórias vs. as variáveis explanatórias:</p>
<pre class="r"><code>dados &lt;- datasets::swiss

# Criando uma coluna &quot;fake&quot;:
dados$Residual &lt;- seq_len(nrow(dados))

# Calculando todos os resíduos que serão exibidos:
colunas=2:6  #Informe as colunas que contem as variaveis explanatorias
residuals &lt;- lapply(dados[colunas], function(x) {
  summary(lm(Fertility ~ x, data = dados))$residuals
})
# Calculando um intervalo constante para todos os resíduos
y_range &lt;- range(unlist(residuals))

# Função modificada para mostrar os resíduos:

lm_or_resid &lt;- function(data, mapping, ..., line_color = &quot;red&quot;, line_size = 1) {
  if (as.character(mapping$y) != &quot;Residual&quot;) {
    return(ggally_smooth_lm(data, mapping, ...))
  }

  # Criando os resíduos para apresentar:
  resid_data &lt;- data.frame(
    x = data[[as.character(mapping$x)]],
    y = residuals[[as.character(mapping$x)]]
  )

  ggplot(data = data, mapping = mapping) +
    geom_hline(yintercept = 0, color = line_color, size = line_size) +
    ylim(y_range) +
    geom_point(data = resid_data, mapping = aes(x = x, y = y), ...)

}

# Plote os dados:
ggduo(
  dados,
  2:6, c(1,7),
  types = list(continuous = lm_or_resid)
)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
</div>
<div id="ggallyggnostic" class="section level2">
<h2><code>GGally::ggnostic</code></h2>
<p>O <code>ggnostic</code> é um wrapper de exibição para <code>ggduo</code> que exibe diagnósticos de modelo completo para cada variável explicativa dada.</p>
<p>Por padrão, o ggduo exibe os valores residuais, o sigma do modelo de “leave-one-out”, os pontos de alavanca e a distância de Cook em relação a cada variável explicativa.</p>
<p>As linhas da matriz de plotagem podem ser expandidas para incluir valores ajustados, erro padrão dos valores ajustados, resíduos padronizados e qualquer uma das variáveis de resposta.</p>
<p>Se o modelo for um modelo linear, os asteriscos (*) são adicionados de acordo com a significância anova de cada variável explicativa.</p>
<p>A maioria das parcelas diagnósticas contêm linhas de referência para ajudar a determinar se o modelo está adequadamente instalado</p>
<p>Olhando para os conjuntos de dados do conjunto de dados <code>state.x77</code> ajustaremos um modelo de regressão múltipla para a expectativa de vida.</p>
<pre class="r"><code>#Dados que serão utilizados no exemplos:
state &lt;- as.data.frame(state.x77)
#Arrumando o nome das variaveis:
colnames(state)[c(4, 6)] &lt;- c(&quot;Life.Exp&quot;, &quot;HS.Grad&quot;)
# Ajustando o modelo completo:
model &lt;- lm(Life.Exp ~ ., data = state)
# Executando o stepwise para encontrar o melhor ajuste
model &lt;- step(model, trace = FALSE)</code></pre>
<p>Executando o diagnóstico deste modelo com a função <code>ggnostic()</code>:</p>
<pre class="r"><code># look at model diagnostics
ggnostic(model)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Para acessar as variáveis influentes do modelo podemos utilizar a função <code>influence.measures()</code>, veja:</p>
<pre class="r"><code>summary(influence.measures(model))</code></pre>
<pre><code>## Potentially influential observations of
##   lm(formula = Life.Exp ~ Population + Murder + HS.Grad + Frost,      data = state) :
## 
##            dfb.1_ dfb.Pplt dfb.Mrdr dfb.HS.G dfb.Frst dffit   cov.r   cook.d
## Alaska      0.41   0.18    -0.40    -0.35    -0.16    -0.50    1.36_*  0.05 
## California  0.04  -0.09     0.00    -0.04     0.03    -0.12    1.81_*  0.00 
## Hawaii     -0.03  -0.57    -0.28     0.66    -1.24_*   1.43_*  0.74    0.36 
## Nevada      0.40   0.14    -0.42    -0.29    -0.28    -0.52    1.46_*  0.05 
## New York    0.01  -0.06     0.00     0.00    -0.01    -0.07    1.44_*  0.00 
##            hat    
## Alaska      0.25  
## California  0.38_*
## Hawaii      0.24  
## Nevada      0.29  
## New York    0.23</code></pre>
<p>Esta função retorna as seguintes estatísticas:</p>
<table>
<colgroup>
<col width="23%" />
<col width="25%" />
<col width="17%" />
<col width="17%" />
<col width="17%" />
</colgroup>
<thead>
<tr class="header">
<th>DFBeta</th>
<th>DFFit</th>
<th>CovRatio</th>
<th>D.Cook</th>
<th>h</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Alteração no vetor estimado <span class="math inline">\(\hat \beta\)</span> ao se retirar o i-ésimo ponto da análise</td>
<td>Alteração provocada no valor ajustado pela retirada da observação <span class="math inline">\(i\)</span></td>
<td>Expressa o relação de covariancia</td>
<td>Medida de afastamento das estimativas ao retirar <span class="math inline">\(i\)</span> e também considera o resíduo estudentizado internamente</td>
<td>Elementos da diagonal da matriz H</td>
</tr>
</tbody>
</table>
<p>Vejamos então um exemplo de matriz de matriz de diagnóstico completo.</p>
<p>As seguintes linhas de código exibirão uma matriz de diagnóstico para o mesmo modelo:</p>
<pre class="r"><code>#Ajustando um modelo de exemplo:
flea_model &lt;- step(lm(head ~ ., data = flea), trace = FALSE)</code></pre>
<p>Todas as colunas possíveis e usando <code>ggally_smooth()</code> para exibir os pontos ajustados e as variáveis de resposta temos:</p>
<pre class="r"><code># default output
ggnostic(flea_model,
 #        mapping = ggplot2::aes(color = species),  #Para colorir segundo um fator
         columnsY = c(&quot;head&quot;, &quot;.fitted&quot;, &quot;.se.fit&quot;, &quot;.resid&quot;, &quot;.std.resid&quot;, &quot;.hat&quot;, &quot;.sigma&quot;, &quot;.cooksd&quot;),
        continuous = list(default = ggally_smooth, .fitted = ggally_smooth)
)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
<div id="ggallyggpairs" class="section level2">
<h2><code>GGally::ggpairs</code></h2>
<p>O <code>ggpairs</code> é uma forma especial de uma ggmatrix que produz uma comparação pairwise de dados multivariados. Por padrão, o ggpairs fornece duas comparações diferentes de cada par de colunas e exibe a densidade ou a contagem da variável respectiva ao longo da diagonal. Com diferentes configurações de parâmetros, a diagonal pode ser substituída pelos valores do eixo e rótulos variáveis.</p>
<pre class="r"><code>#Funcao de correlacoes
my_fn &lt;- function(data, mapping, method=&quot;lm&quot;, ...){
  p &lt;- ggplot(data = data, mapping = mapping) + 
    geom_point() + 
    geom_smooth(method=method, ...)
  p
}
data(tips, package = &quot;reshape&quot;)
#Correlaçoes cruzadas
ggpairs(tips, lower = list(continuous = my_fn))</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Existem muitos recursos ocultos dentro dos <code>ggpairs()</code> e muitos exemplos podem ser conferidos na internet para obter o máximo do <code>ggpairs()</code>.</p>
</div>
<div id="ggallyggscatmat" class="section level2">
<h2><code>GGally::ggscatmat</code></h2>
<p>A principal função é <code>ggscatmat</code>. É semelhante a <code>ggpairs()</code>, mas funciona apenas para dados multivariados puramente numéricos.</p>
<p>É mais rápido que ggpairs, porque é necessário fazer menos escolhas.</p>
<p>Ele cria uma matriz com diagramas de dispersão na diagonal inferior, densidades na diagonal e correlações escritas na diagonal superior.</p>
<p>A sintaxe é inserir o conjunto de dados, as colunas que deseja traçar, uma coluna de cores e um nível alfa.</p>
<pre class="r"><code>data(flea)
ggscatmat(flea, columns = 2:4, color=&quot;species&quot;, alpha=0.8)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
</div>
</div>
<div id="ggfottify" class="section level1">
<h1>ggfottify</h1>
<p>Outra opção interessante para avaliar o ajuste dos modelos é o pacote <a href="https://cran.r-project.org/web/packages/ggfortify/index.html">ggfottify</a>. Ele disponibiliza uma interface de traçado (como a função <code>plot(modelo_ajustado)</code>) de análise e gráficos em um estilo unificado, porém usando <code>ggplot2</code>.</p>
<p>Vamos então dar início carregando o pacote:</p>
<pre class="r"><code>library(ggfortify)</code></pre>
<p>Veja a seguir alguns dos gráficos disponíveis no R para a análise de resíduos:</p>
<pre class="r"><code>autoplot(flea_model, which = 1:6, ncol = 3, label.size = 3)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>Especificando as opções de plot</p>
<p>Algumas propriedades desses gráficos podem ser alteradas. Por exemplo, a opção <code>colour = 'dodgerblue3'</code> é para pontos de dados, o <code>smooth.colour = 'black'</code> é para linhas de suavização e <code>ad.colour = 'blue'</code> é para opções adicionais.</p>
<p>Veja ainda que ncol e nrow controlam o layout.</p>
<pre class="r"><code>autoplot(flea_model, which = 1:6, colour = &#39;dodgerblue3&#39;,
         smooth.colour = &#39;black&#39;, smooth.linetype = &#39;dashed&#39;,
         ad.colour = &#39;blue&#39;,
         label.size = 3, label.n = 5, label.colour = &#39;blue&#39;,
         ncol = 3)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>Além disso, você pode usar nomes de colunas para essas propriedades, vamos separar os grupos de machos e fêmeas por cores:</p>
<pre class="r"><code>autoplot(flea_model, which = 1:6, data = flea,
         colour = &#39;species&#39;, label.size = 3,
         ncol = 3)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>O que será que os crânios da estatística fariam diante de tantos recursos?</p>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos/">Pacotes do R para avaliar o ajuste de modelos</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Estatistica</category>
      <category>Modelagem Estatistica</category>
      <category>R</category>
      <category>Teoria</category>
      <category>Tidyverse</category>
      <category domain="tag">gomesfellipe</category>
      <category domain="tag">R</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">modelagem estatistica</category>
      <category domain="tag">Estatistica</category>
      <category domain="tag">Correlacoes</category>
      <category domain="tag">R Markdown</category>
      <category domain="tag">regression</category>
      <category domain="tag">Teoria</category>
      <category domain="tag">modelos lineares</category>
      <category domain="tag">modelos generalizados</category>
      <category domain="tag">ggfortify</category>
      <category domain="tag">GGally</category>
    </item>
    <item>
      <title>Ajustando Modelos Bayesianos com JAGS</title>
      <link>https://gomesfellipe.github.io/post/2017-12-18-bayesiana-jags-mcmcplot/bayesiana-jags-mcmcplot/</link>
      <pubDate>Mon, 18 Dec 2017 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2017-12-18-bayesiana-jags-mcmcplot/bayesiana-jags-mcmcplot/</guid>
      <description>Inferência bayesiana Imagem da Internet
Quando estamos falando de Inferência nosso objetivo normalmente é tentar verificar alguma informação sobre uma quantidade desconhecida.
Para isso devemos utilizar toda informação disponível, seja ela objetiva ou subjetiva (isto é, vinda de umam amostra ou de algum conhecimento préveo ou intuitivo)
Segundo o ponto de vista Bayesiano essa informação subjetiva também será incorporada na análise graças ao teorema de bayes.
Como no ponto de vista Bayesiano atribuímos aleatoriedade ao parâmetro, nossa “crença” será representada por uma distribuição de probabilidade (ou modelo probabilístico)</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="inferência-bayesiana" class="section level1">
<h1>Inferência bayesiana</h1>
<p><a href="https://www.flickr.com/photos/mattbuck007/3676624894/in/photolist-6ATEuo-9TK3TW">Imagem da Internet</a></p>
<p>Quando estamos falando de Inferência nosso objetivo normalmente é tentar verificar alguma informação sobre uma quantidade desconhecida.</p>
<p>Para isso devemos utilizar <strong>toda</strong> informação disponível, seja ela <strong>objetiva</strong> ou <strong>subjetiva</strong> (isto é, vinda de umam amostra ou de algum conhecimento préveo ou intuitivo)</p>
<p>Segundo o ponto de vista Bayesiano essa informação subjetiva também será incorporada na análise graças ao <a href="https://pt.wikipedia.org/wiki/Teorema_de_Bayes">teorema de bayes</a>.</p>
<p>Como no ponto de vista Bayesiano atribuímos aleatoriedade ao parâmetro, nossa “crença” será representada por uma distribuição de probabilidade (ou modelo probabilístico)</p>
<p><em>Teorema de bayes</em>:
<span class="math display">\[
p(\theta|x)=\frac{p(x,\theta)}{p(x)}=\frac{p(x|\theta)p(\theta)}{p(x)}
\]</span></p>
<p>onde:</p>
<ul>
<li><span class="math inline">\(p(x|\theta)\)</span>: função de verossimilhança (modelo)</li>
<li><span class="math inline">\(p(\theta)\)</span>: distribuição a priori</li>
<li><span class="math inline">\(p(x)\)</span>: distribuição marginal de <span class="math inline">\(x\)</span>.</li>
</ul>
<p>A estimação muitas vezes envolve o cálculo de integrais nada simples analiticamente porém, alguns algorítimos como o amostrador de Gibbs pode relizar aproximações muito relevantes.</p>
</div>
<div id="modelo-linear-bayesiano" class="section level1">
<h1>Modelo linear bayesiano</h1>
<p>Para entender como funciona o modelo bayesiano, primeiramente vamos começar com algo bem simples, suponha:</p>
<p><span class="math display">\[
Y_i \sim N(\mu_i,\tau)
\]</span>
onde <span class="math inline">\(\mu\)</span> é definido como <span class="math inline">\(\mu_i= X \mathbf{\beta}\)</span>.</p>
<p>Incialmente vamos considerar que não existe relação nenhuma, então utilizaremos a priori:</p>
<p><span class="math display">\[
\beta \sim N(0,\tau_{\beta})
\]</span></p>
<p>onde <span class="math inline">\(\tau\)</span> é conhecido.</p>
<p>Nem sempre é uma tarefa simples determinar a distribuição posteri de um modelo bayesiano e é neste ponto que o pacote <code>jags</code>será bastante útil (existem outras alternativas como o <a href="https://cran.r-project.org/package=R2WinBUGS">WinBugs</a>, <a href="https://cran.r-project.org/package=R2OpenBUGS">OpenBugs</a>, <a href="https://cran.r-project.org/web/packages/rstan/index.html">Stan</a>, mas aqui resolvi trazer apenas o <a href="https://cran.r-project.org/package=rjags">jags</a> por possuir vantagens bem interessantes.)</p>
</div>
<div id="jags" class="section level1">
<h1>Jags</h1>
<p>O pacote <a href="https://cran.r-project.org/package=R2jags"><code>R2jags</code></a> é exatamente o que seu nome significa: “<em>Just Another Gibbs Sampler</em>”. Possui as mesmas funcionalidades do nosso querido <a href="https://cran.r-project.org/package=R2OpenBUGS">OpenBugs</a> possibilitando também que seja utilizado inteiramente dentro do ambiente R.</p>
<p>Assim como o OpenBugs, ele também trabalha chamando o <a href="mcmc-jags.sourceforge.net/">software oficial que precisa ser baixado no site</a>.</p>
<p>Para começar a utilizar basta baixar o pacote e acessá-lo na biblioteca:</p>
<pre class="r"><code>library(R2jags)</code></pre>
</div>
<div id="declarando-o-modelo" class="section level1">
<h1>Declarando o modelo</h1>
<p>A base de dados que será utilizada para ajustar o modelo será a base nativa do R chamada <code>trees</code>:</p>
<pre class="r"><code>X&lt;-trees[,1:2] #Matriz de variáveis explanatórias
Y&lt;- trees[,3]  #Vetor da variável resposta
p &lt;- ncol(X)   #p é o número de parâmetros do modelo (nesse caso é o número de colunas)
n &lt;- nrow(X)   #n é o número de observações do modelo</code></pre>
<p>O modelo deve estar declarado e salvo em um arquivo <code>.txt</code> (ou mesmo um outro arquivo <code>.r</code>) da seguinte maneira:</p>
<pre class="r"><code>### Declarando o modelo Bayesiano
sink(&quot;linreg.txt&quot;)
cat(&quot;
    model {
    
    # Prioris
    for(j in 1:p)
    {
    beta[j] ~ dnorm(mu.beta, tau.beta)       
    }
    sigma ~ dunif(0, 100)            
    tau &lt;- 1/ (sigma * sigma)
    
    # Verossimilhança
    for (i in 1:n) {
    y[i] ~ dnorm(mu[i], tau)
    mu[i] &lt;- inprod(X[i,], beta)
    }

    }
    &quot;,fill=TRUE)
sink()</code></pre>
<p>Uma vez que o modelo esta declarado, é a hora de nomear os parametros da função que fará o ajuste do modelo</p>
<pre class="r"><code>#Parametros da Priori
mu.beta &lt;- 0
tau.beta &lt;- 0.001

#Set Working Directory
wd &lt;- getwd()

# Junte os dados em uma lista
win.data &lt;- list(X=X,y=Y,p=p,n=n,mu.beta=mu.beta,tau.beta=tau.beta)

# Função de inicialização
inits &lt;- function(){ list(beta=rnorm(p), sigma = rlnorm(1))}

# Os parametros que desejamos estimar
params &lt;- c(&quot;beta&quot;,&quot;sigma&quot;,&quot;tau&quot;)

# Caracteristicas do MCMC
n.burnin &lt;- 500                    #Número de iterações que serão descartadas
n.thin &lt;- 10                       #para economizar memória e tempo de computação se n.iter for grande
n.post &lt;- 5000  
n.chains &lt;- 3                      #Número de cadeias
n.iter &lt;- n.burnin + n.thin*n.post #Número de iterações</code></pre>
</div>
<div id="implementando-o-modelo" class="section level1">
<h1>Implementando o modelo</h1>
<p>Após ter em mãos todos esses resultados, já podemos ajustar o modelo com o comando <code>jags()</code>, veja:</p>
<pre class="r"><code>bayes.mod.fit &lt;-jags(data = win.data,
                     inits = inits,
                     parameters = params,
                     model.file = &quot;linreg.txt&quot;,  # O arquivo &quot;linreg.txt&quot; deve estar no mesmo diretório
                     n.iter = n.iter,
                     n.thin=n.thin,
                     n.burnin=n.burnin,
                     n.chains=n.chains,
                     working.directory=wd,DIC = T)</code></pre>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 31
##    Unobserved stochastic nodes: 3
##    Total graph size: 166
## 
## Initializing model</code></pre>
<pre class="r"><code>print(bayes.mod.fit, dig = 3)</code></pre>
<pre><code>## Inference for Bugs model at &quot;linreg.txt&quot;, fit using jags,
##  3 chains, each with 50500 iterations (first 500 discarded), n.thin = 10
##  n.sims = 15000 iterations saved
##          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
## beta[1]    5.045   0.435   4.183   4.757   5.043   5.324   5.916 1.001 15000
## beta[2]   -0.478   0.078  -0.633  -0.527  -0.477  -0.427  -0.324 1.001 15000
## sigma      6.448   0.904   4.995   5.805   6.335   6.970   8.502 1.001 15000
## tau        0.025   0.007   0.014   0.021   0.025   0.030   0.040 1.001 15000
## deviance 201.924   2.682 198.881 199.970 201.244 203.149 208.856 1.001  7200
## 
## For each parameter, n.eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).
## 
## DIC info (using the rule, pD = var(deviance)/2)
## pD = 3.6 and DIC = 205.5
## DIC is an estimate of expected predictive error (lower deviance is better).</code></pre>
<p>Com os resultados em mãos podemos avaliar o ajuste do modelo, o jags nos fornece os intervalos de credibilidade e o Rhat, que é a convergência da cadeia, a princípio vamos apenas considerar o fato de que quanto mais próximo de 1, melhor são as estimativas.</p>
<p>Não vou me extender neste post com a interpretação do modelo pois o objetivo esta sendo mostrar a funcionalidade do jags em conjunto com o R.</p>
</div>
<div id="diagnósticos-do-modelo-com-mcmcplots" class="section level1">
<h1>Diagnósticos do modelo com <code>mcmcplots</code></h1>
<p>Para o diagnóstico do modelo podemos utilizar o pacote <code>mcmcplots</code> que fornece de maneira bem agradável os resultados gerados pelo amostrador, primeiramente vamos carregar o pacote:</p>
<pre class="r"><code>library(mcmcplots)</code></pre>
<p>Em seguida precisar informar para o <code>R</code> que o resultado do algorítimo se trata de um objeto mcmc, portanto:</p>
<pre class="r"><code>bayes.mod.fit.mcmc &lt;- as.mcmc(bayes.mod.fit)
summary(bayes.mod.fit.mcmc)</code></pre>
<pre><code>## 
## Iterations = 1:49991
## Thinning interval = 10 
## Number of chains = 3 
## Sample size per chain = 5000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##               Mean       SD  Naive SE Time-series SE
## beta[1]    5.04490 0.435344 3.555e-03      3.555e-03
## beta[2]   -0.47754 0.077588 6.335e-04      6.335e-04
## deviance 201.92383 2.682384 2.190e-02      2.144e-02
## sigma      6.44763 0.903646 7.378e-03      7.359e-03
## tau        0.02542 0.006784 5.539e-05      5.524e-05
## 
## 2. Quantiles for each variable:
## 
##               2.5%       25%       50%       75%     97.5%
## beta[1]    4.18250   4.75721   5.04333   5.32437   5.91642
## beta[2]   -0.63255  -0.52732  -0.47726  -0.42674  -0.32376
## deviance 198.88143 199.97019 201.24393 203.14881 208.85648
## sigma      4.99470   5.80492   6.33492   6.96990   8.50193
## tau        0.01383   0.02058   0.02492   0.02968   0.04008</code></pre>
<p>O pacote nos fornece alguns tipos de gráficos para diagnóstico</p>
<pre class="r"><code>caterplot(bayes.mod.fit.mcmc)                #Observando todas as estimativas</code></pre>
<p><img src="/post/2017-12-18-bayesiana-jags-mcmcplot/bayesiana-jags-mcmcplot_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code>caterplot(bayes.mod.fit.mcmc,parms = params) #Observando as estimativas de todos os parâmetros menos o desvio</code></pre>
<p><img src="/post/2017-12-18-bayesiana-jags-mcmcplot/bayesiana-jags-mcmcplot_files/figure-html/unnamed-chunk-8-2.png" width="672" /></p>
<pre class="r"><code>denplot(bayes.mod.fit.mcmc)                  #Densidade das estimativas de cada cadeia</code></pre>
<p><img src="/post/2017-12-18-bayesiana-jags-mcmcplot/bayesiana-jags-mcmcplot_files/figure-html/unnamed-chunk-8-3.png" width="672" /></p>
<pre class="r"><code>traplot(bayes.mod.fit.mcmc,greek = T)        #Avaliando a convergência</code></pre>
<p><img src="/post/2017-12-18-bayesiana-jags-mcmcplot/bayesiana-jags-mcmcplot_files/figure-html/unnamed-chunk-8-4.png" width="672" /></p>
<p>E por fim, para diagnósticos rápidos, pode produzir arquivos html com traço, densidade e autocorrelação.</p>
<p>O comando traça tudo em uma página e os arquivos serão exibidos em seu navegador de internet padrão.</p>
<pre class="r"><code>mcmcplot(bayes.mod.fit.mcmc)</code></pre>
<p>Vai retornar um relatório resumido para todos os parâmetros como nesta <a href="https://introndatalab.com/wp-content/uploads/manually/20150405/MCMC%20Plots%20%20result2_files/attack%5B1,1%5D.png">imagem da internet</a> como:</p>
<p><img src="/post/2017-12-18-bayesiana-jags-mcmcplot/bayesiana-jags-mcmcplot_files/imagem1.png" /></p>
<p>Como o objetivo do post é trazer a funcionalidade do pacote, vou apenas deixar ilustrado quais são algumas das funções mais comumente utilizadas para avaliar estatísticamente o desempenho dos modelos.</p>
<p>Diagnosticos estatísticos do modelo:</p>
<pre class="r"><code>#Mais diagnosticos:
gelman.plot(bayes.mod.fit.mcmc)</code></pre>
<p><img src="/post/2017-12-18-bayesiana-jags-mcmcplot/bayesiana-jags-mcmcplot_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code>geweke.diag(bayes.mod.fit.mcmc)</code></pre>
<pre><code>## [[1]]
## 
## Fraction in 1st window = 0.1
## Fraction in 2nd window = 0.5 
## 
##  beta[1]  beta[2] deviance    sigma      tau 
##  -1.6717   1.1790  -0.4485   0.1854  -0.6815 
## 
## 
## [[2]]
## 
## Fraction in 1st window = 0.1
## Fraction in 2nd window = 0.5 
## 
##  beta[1]  beta[2] deviance    sigma      tau 
##  0.37278 -0.36960 -0.24342 -0.08007  0.30725 
## 
## 
## [[3]]
## 
## Fraction in 1st window = 0.1
## Fraction in 2nd window = 0.5 
## 
##  beta[1]  beta[2] deviance    sigma      tau 
## -0.15725  0.19911 -0.08445 -0.34043  0.35357</code></pre>
<pre class="r"><code>geweke.plot(bayes.mod.fit.mcmc)</code></pre>
<p><img src="/post/2017-12-18-bayesiana-jags-mcmcplot/bayesiana-jags-mcmcplot_files/figure-html/unnamed-chunk-10-2.png" width="672" /><img src="/post/2017-12-18-bayesiana-jags-mcmcplot/bayesiana-jags-mcmcplot_files/figure-html/unnamed-chunk-10-3.png" width="672" /><img src="/post/2017-12-18-bayesiana-jags-mcmcplot/bayesiana-jags-mcmcplot_files/figure-html/unnamed-chunk-10-4.png" width="672" /></p>
<pre class="r"><code>raftery.diag(bayes.mod.fit.mcmc)</code></pre>
<pre><code>## [[1]]
## 
## Quantile (q) = 0.025
## Accuracy (r) = +/- 0.005
## Probability (s) = 0.95 
##                                                 
##           Burn-in  Total Lower bound  Dependence
##           (M)      (N)   (Nmin)       factor (I)
##  beta[1]  20       39950 3746         10.70     
##  beta[2]  20       36200 3746          9.66     
##  deviance 20       37410 3746          9.99     
##  sigma    20       38030 3746         10.20     
##  tau      20       36800 3746          9.82     
## 
## 
## [[2]]
## 
## Quantile (q) = 0.025
## Accuracy (r) = +/- 0.005
## Probability (s) = 0.95 
##                                                 
##           Burn-in  Total Lower bound  Dependence
##           (M)      (N)   (Nmin)       factor (I)
##  beta[1]  20       38030 3746         10.20     
##  beta[2]  20       36800 3746          9.82     
##  deviance 20       37410 3746          9.99     
##  sigma    20       37410 3746          9.99     
##  tau      20       35610 3746          9.51     
## 
## 
## [[3]]
## 
## Quantile (q) = 0.025
## Accuracy (r) = +/- 0.005
## Probability (s) = 0.95 
##                                                 
##           Burn-in  Total Lower bound  Dependence
##           (M)      (N)   (Nmin)       factor (I)
##  beta[1]  20       37410 3746          9.99     
##  beta[2]  20       38030 3746         10.20     
##  deviance 20       37410 3746          9.99     
##  sigma    30       40620 3746         10.80     
##  tau      20       39300 3746         10.50</code></pre>
<pre class="r"><code>heidel.diag(bayes.mod.fit.mcmc)</code></pre>
<pre><code>## [[1]]
##                                        
##          Stationarity start     p-value
##          test         iteration        
## beta[1]  passed       1         0.292  
## beta[2]  passed       1         0.455  
## deviance passed       1         0.733  
## sigma    passed       1         0.881  
## tau      passed       1         0.816  
##                                      
##          Halfwidth Mean     Halfwidth
##          test                        
## beta[1]  passed      5.0481 0.012089 
## beta[2]  passed     -0.4780 0.002155 
## deviance passed    201.8829 0.073069 
## sigma    passed      6.4367 0.024544 
## tau      passed      0.0255 0.000187 
## 
## [[2]]
##                                        
##          Stationarity start     p-value
##          test         iteration        
## beta[1]  passed       1         0.246  
## beta[2]  passed       1         0.249  
## deviance passed       1         0.967  
## sigma    passed       1         0.950  
## tau      passed       1         0.770  
##                                      
##          Halfwidth Mean     Halfwidth
##          test                        
## beta[1]  passed      5.0386 0.011955 
## beta[2]  passed     -0.4765 0.002134 
## deviance passed    201.9023 0.068414 
## sigma    passed      6.4571 0.025014 
## tau      passed      0.0253 0.000188 
## 
## [[3]]
##                                        
##          Stationarity start     p-value
##          test         iteration        
## beta[1]  passed       1         0.657  
## beta[2]  passed       1         0.690  
## deviance passed       1         0.544  
## sigma    passed       1         0.813  
## tau      passed       1         0.873  
##                                      
##          Halfwidth Mean     Halfwidth
##          test                        
## beta[1]  passed      5.0480 0.012156 
## beta[2]  passed     -0.4781 0.002163 
## deviance passed    201.9863 0.076685 
## sigma    passed      6.4491 0.025385 
## tau      passed      0.0254 0.000188</code></pre>
</div>
<div id="diagnostico-de-convergencia-rapida-superdiag" class="section level1">
<h1>Diagnostico de convergencia rapida: <code>superdiag</code></h1>
<p>Uma função muito conveniente para analisar representações numéricas de diagnósticos em um ajuste é o pacote <code>superdiag</code> de Tsai, Gill e Rapkin, 2012 que trás uma série de estatísticas para avaliar o desempenho dos ajustes do modelo.</p>
<pre class="r"><code>library(superdiag)
superdiag(bayes.mod.fit.mcmc, burnin = 100)</code></pre>
<pre><code>## Number of chains = 3 
## Number of iterations = 5000 per chain before discarding the burn-in period
## Burn-in period = 100 per chain
## Sample size in total = 14703 
## 
## ****************** The Geweke diagnostic: ******************
## Windows:
##            chain 1 chain 2 chain 3
## From start     0.1  0.5420  0.2999
## From stop      0.5  0.3511  0.6893
## 
## Z-scores:
##           chain 1 chain 2  chain 3
## beta[1]  -1.85586  0.3331 -1.66699
## beta[2]   1.57605 -0.2271  1.53584
## deviance  0.02463  0.3356 -1.14324
## sigma    -0.15363 -0.8820 -0.33962
## tau      -0.09745  0.9937  0.01232
## 
## *************** The Gelman-Rubin diagnostic: ***************
## Potential scale reduction factors:
##          Point est. Upper C.I.
## beta[1]      1.0001      1.001
## beta[2]      1.0000      1.000
## deviance     1.0009      1.002
## sigma        1.0002      1.001
## tau          0.9999      1.000
## 
## Multivariate psrf: 1.0005
## 
## ************* The Heidelberger-Welch diagnostic ************
## Chain 1:
## epsilon=0.1, alpha=0.05                                       
##          Stationarity start     p-value
##          test         iteration        
## beta[1]  passed       1         0.1576 
## beta[2]  passed       1         0.2864 
## deviance passed       1         0.8399 
## sigma    passed       1         0.8207 
## tau      passed       1         0.7405 
##                                       
##          Halfwidth Mean      Halfwidth
##          test                         
## beta[1]  passed      5.04671 0.012211 
## beta[2]  passed     -0.47775 0.002177 
## deviance passed    201.89097 0.074094 
## sigma    passed      6.43566 0.024772 
## tau      passed      0.02549 0.000189 
## 
## Chain 2:
## epsilon=0.079, alpha=0.1                                       
##          Stationarity start     p-value
##          test         iteration        
## beta[1]  passed       1         0.3032 
## beta[2]  passed       1         0.3259 
## deviance passed       1         0.9562 
## sigma    passed       1         0.7462 
## tau      passed       1         0.5362 
##                                       
##          Halfwidth Mean      Halfwidth
##          test                         
## beta[1]  passed      5.03850 0.0120853
## beta[2]  passed     -0.47646 0.0021574
## deviance passed    201.90084 0.0693125
## sigma    passed      6.45467 0.0252168
## tau      passed      0.02536 0.0001894
## 
## Chain 3:
## epsilon=0.054, alpha=0.005                                       
##          Stationarity start     p-value
##          test         iteration        
## beta[1]  passed       1         0.5489 
## beta[2]  passed       1         0.5665 
## deviance passed       1         0.5038 
## sigma    passed       1         0.8038 
## tau      passed       1         0.8898 
##                                       
##          Halfwidth Mean      Halfwidth
##          test                         
## beta[1]  passed      5.04719 0.0122925
## beta[2]  passed     -0.47794 0.0021858
## deviance passed    201.98956 0.0775537
## sigma    passed      6.44893 0.0256817
## tau      passed      0.02544 0.0001937
## 
## *************** The Raftery-Lewis diagnostic ***************
## Chain 1:
## Convergence eps = 0.001
## Quantile (q) = 0.025
## Accuracy (r) = +/- 0.005
## Probability (s) = 0.95 
##                                                 
##           Burn-in  Total Lower bound  Dependence
##           (M)      (N)   (Nmin)       factor (I)
##  beta[1]  30       40170 3746         10.70     
##  beta[2]  20       36340 3746          9.70     
##  deviance 20       38200 3746         10.20     
##  sigma    20       38200 3746         10.20     
##  tau      20       36950 3746          9.86     
## 
## Chain 2:
## Convergence eps = 5e-04
## Quantile (q) = 0.25
## Accuracy (r) = +/- 0.001
## Probability (s) = 0.99 
## 
## You need a sample size of at least 1244044 with these values of q, r and s
## 
## Chain 3:
## Convergence eps = 0.005
## Quantile (q) = 0.25
## Accuracy (r) = +/- 5e-04
## Probability (s) = 0.999 
## 
## You need a sample size of at least 8120675 with these values of q, r and s
## 
## ************* The Hellinger distance diagnostic ************
## Between chains: 
##              Min     Max
## beta[1]  0.01735 0.02915
## beta[2]  0.02015 0.02620
## deviance 0.03155 0.03413
## sigma    0.01858 0.02731
## tau      0.01538 0.02810
## 
## Within chain 1:
##              980    1960    2940    3920
## beta[1]  0.05231 0.03952 0.04017 0.04259
## beta[2]  0.04261 0.05034 0.04320 0.04782
## deviance 0.05880 0.04060 0.06297 0.04311
## sigma    0.03871 0.03667 0.06465 0.04285
## tau      0.03668 0.03996 0.03633 0.04083
## 
## Within chain 2:
##              980    1960    2940    3920
## beta[1]  0.03098 0.04075 0.04281 0.03887
## beta[2]  0.03050 0.03770 0.03887 0.04216
## deviance 0.04541 0.03992 0.03390 0.04730
## sigma    0.04660 0.03876 0.03090 0.02866
## tau      0.03648 0.03773 0.02967 0.03589
## 
## Within chain 3:
##              980    1960    2940    3920
## beta[1]  0.03356 0.03988 0.03146 0.02986
## beta[2]  0.03425 0.04729 0.03175 0.03219
## deviance 0.05894 0.03553 0.05018 0.04509
## sigma    0.04392 0.04245 0.03858 0.03760
## tau      0.04089 0.03458 0.04512 0.03047</code></pre>
<p>Para finalizar, outra função que pode ser útil pata atualizando o modelo, se necessário - por exemplo, se não houver convergência ou pouca convergencia:</p>
<pre class="r"><code>bayes.mod.fit.upd &lt;- update(bayes.mod.fit, n.iter=1000)
bayes.mod.fit.upd &lt;- autojags(bayes.mod.fit)</code></pre>
</div>
<div id="muito-a-estudar" class="section level1">
<h1>Muito a estudar</h1>
<p>Assim como toda a Estatística, inferência bayesiana não funciona se a teoria não for aplicada corretamente. É uma ferramenta muito poderosa e necessita ser usada com cautela pois demanda bastante o uso de metodologias estatísticas.</p>
<p>Como dizia o tio Ben: “grandes poderes trazem grandes responsabilidades” então vamos tomar cuidado com os resultados que encontramos.</p>
</div>
<div id="referencias" class="section level1">
<h1>Referencias</h1>
<p><a href="http://recologia.com.br/2012/12/uma-primeira-olhada-em-estatistica-bayesiana-e-linguagem-bugs/">Uma primeira olhada em estatística bayesiana e linguagem BUGS por Augusto Ribas - blog Recologia</a></p>
<p><a href="http://www.users.csbsju.edu/~mgass/robert.pdf">John K. Kruschke 2014 Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan.2nd Edition. Academic Press / Elsevier.</a></p>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2017-12-18-bayesiana-jags-mcmcplot/bayesiana-jags-mcmcplot/">Ajustando Modelos Bayesianos com JAGS</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Estatistica</category>
      <category>R</category>
      <category>Teoria</category>
      <category>Bayes</category>
      <category>Inferência Bayesiana</category>
      <category>Modelagem Estatistica</category>
      <category domain="tag">Estatistica</category>
      <category domain="tag">gomesfellipe</category>
      <category domain="tag">R</category>
      <category domain="tag">jags</category>
      <category domain="tag">bayes</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">modelagem estatistica</category>
    </item>
    <item>
      <title>Manipulando dados com dplyr</title>
      <link>https://gomesfellipe.github.io/post/2017-12-07-manipulando-dados-com-dplyr/manipulando-dados-com-dplyr/</link>
      <pubDate>Thu, 07 Dec 2017 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2017-12-07-manipulando-dados-com-dplyr/manipulando-dados-com-dplyr/</guid>
      <description>Manipular bases de dados nunca foi tão simples</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="o-pacote-dplyr" class="section level1">
<h1>o pacote <code>dplyr</code>:</h1>
<p>A análise exploratória dos dados é uma tarefa de bastante relevância para entendermos a natureza dos dados e o tempo de análise gastro é muito precioso. É necessária bastante curiosidade e criatividade para fazer uma boa análise exploratória dos dados pois é difícil receber aqueles dados bonitinhos igual aos nativos do banco de dados do <strong>R</strong>.</p>
<p>Existem diversos pacotes para as mais variadas necessidades dos cientistas de dados (ou qualquer pessoa que precise fazer alguma análise ou programação estatística) disponíveis no <a href="https://cran.r-project.org/">CRAN</a> e hoje quero registrar aqui algumas das funcionalidades do pacote <a href="https://cran.r-project.org/package=dplyr">dplyr</a> que são muito úteis.</p>
<p>É um dos pacotes mais poderosos e populares do R, desenvolvido por Hadley Wickham, faz a exploração de dados e permite a manipulação de dados de forma fácil e rápida no R.</p>
<p>Segundo sua descrição no <a href="https://cran.r-project.org/">CRAN</a>, o <a href="https://cran.r-project.org/package=dplyr">dplyr</a> é definido como uma ferramenta rápida e consistente para trabalhar com data.frames como objetos, tanto na memória quanto fora da memória. Vamos conferir então o que de tão especial tem nesse pacote.</p>
<div id="operador-pipe" class="section level2">
<h2>Operador pipe: %&gt;%</h2>
<p>O operador <code>%&gt;%</code> é uma opção incrivelmente útil para a manipulação dos dados, funcionando com uma lógica diferente da nativa do <strong>R</strong>, que executa funções no formato <code>f(x)</code>, o pipe permite que façamos operações no formato <code>x %&gt;% f()</code> que basicamente funciona da maneira como raciocinamos: “Pega esse objeto e executa isso, depois isso, depois isso…”
Realiza múltiplas ações sem guardar os passos intermediários.</p>
<!-- Vejamos um resumo dos dados: -->
<p>Vamos começar carregando o pacote:</p>
<pre class="r"><code># Carregando o pacote dplyr
suppressMessages(library(dplyr))</code></pre>
</div>
<div id="selecionando-n-linhas-aleatorias-função-sample_n" class="section level2">
<h2>Selecionando n linhas aleatorias: função <code>sample_n()</code></h2>
<pre class="r"><code># Selecionando 5 linhas aleatoriamente
df%&gt;%
  sample_n(5)</code></pre>
</div>
<div id="removendo-linhas-duplicadas-função-distinct" class="section level2">
<h2>Removendo linhas duplicadas: função <code>distinct()</code></h2>
<div id="baseado-em-todas-as-variáveis" class="section level3">
<h3>Baseado em todas as variáveis</h3>
<pre class="r"><code># excluindo linhas iguais
df%&gt;%
  distinct()</code></pre>
</div>
<div id="baseado-em-uma-variável" class="section level3">
<h3>Baseado em uma variável</h3>
<pre class="r"><code># excluindo linhas que possuem Datas iguais
df%&gt;%
  distinct(Datas)</code></pre>
</div>
<div id="baseado-em-mais-de-uma-variável" class="section level3">
<h3>Baseado em mais de uma variável</h3>
<pre class="r"><code># excluindo linhas que possuem ano e consumo iguais
df%&gt;%
  distinct(ano, consumo)</code></pre>
</div>
</div>
<div id="selecionando-colunas-variáveis-função-select" class="section level2">
<h2>Selecionando colunas (variáveis): função <code>select()</code></h2>
<pre class="r"><code># Selecionando a variavel ano e  todas as variáveis de sucesso até tonalidade na df
df%&gt;%
  select(ano, sucesso:tonalidade)

# Selecionando todas as variaveis com exceção de ano e id
df%&gt;%
  select(-c(ano,id))


# Selecionando todas as variaveis cujo nome inicia com e
df%&gt;%
  select(starts_with(&quot;a&quot;))</code></pre>
<div id="podem-ser-úteis-também-ends_with-e-contains." class="section level4">
<h4>Podem ser úteis também <code>ends_with()</code> e <code>contains()</code>.</h4>
</div>
</div>
<div id="reordenando-as-as-colunas-das-variáveis-função-select" class="section level2">
<h2>Reordenando as as colunas das variáveis: função <code>select()</code></h2>
<pre class="r"><code># reorganiza o data frame, iniciando com a variável Datas e depois as demais
df%&gt;%
  select(Datas,everything())</code></pre>
</div>
<div id="renomeando-variáveis-função-rename" class="section level2">
<h2>Renomeando variáveis: função <code>rename()</code></h2>
<pre class="r"><code># Renomeando a variável Datas para micro
df%&gt;%
  rename(Dia = Datas)</code></pre>
</div>
<div id="selecionando-um-subconjunto-de-linhas-que-satisfazem-uma-ou-mais-condições-função-filter" class="section level2">
<h2>Selecionando um subconjunto de linhas que satisfazem uma ou mais condições: função <code>filter()</code></h2>
<pre class="r"><code># Selecionando somente os indivíduos do sexo masculino
df%&gt;%
  filter( sexo == &quot;Masculino&quot;)


# Selecionando somente os indivíduos do sexo masculino e branco
df%&gt;%
  filter( sexo == &quot;Masculino&quot; &amp; tonalidade == &quot;Branco&quot;)


# Selecionando somente os indivíduos com consumo de 1 a 3 anos e 4 a 7 anos
df%&gt;%
  filter( consumo %in% c(&quot;1 a 3&quot;,&quot;4 a 7&quot;))


# Selecionando indivíduos que ou são homens solteiros ou são mulheres casadas
df%&gt;%
  filter( (estado_civil==&quot;Solteiro&quot; &amp; sexo ==&quot;Masculino&quot;) | (estado_civil==&quot;Casado&quot; &amp; sexo ==&quot;Feminino&quot;) )</code></pre>
</div>
<div id="ordenando-seus-data-frames-função-arrange" class="section level2">
<h2>Ordenando seus data frames: função <code>arrange()</code></h2>
<pre class="r"><code># Ordenando os dados pela variável ano de forma crescente
df%&gt;%
  arrange( ano )

# Ordenando os dados pela variável ano e consumo
df%&gt;%
  arrange( ano ,consumo)

# Ordenando os dados pela variável ano de forma decrescente
df%&gt;%
  arrange( desc(ano) )</code></pre>
</div>
<div id="criando-uma-nova-variável-função-mutate-e-transmute" class="section level2">
<h2>Criando uma nova variável: função <code>mutate()</code> e <code>transmute()</code></h2>
<pre class="r"><code># Criando a variável ano ao quadrado
df%&gt;%
  mutate( ano2 = ano**2 )

# Criando a variável Dia e a variável Mes
df%&gt;%
  mutate( Dia = substring(Datas,1,2), Mes = substring(mensalidade,1,1) )

# Se você quiser somente manter as variáveis criadas
df18 = transmute(mensalidade = substring(Datas,1,2), Mes = substring(mensalidade,1,1) )</code></pre>
</div>
<div id="resumindo-variáveis-função-summarize" class="section level2">
<h2>Resumindo variáveis: função <code>summarize()</code></h2>
<pre class="r"><code># Calculando a media e a mediana da variável ano
df%&gt;%
  summarise(  media.ano = mean(ano), mediana.ano = median(ano))</code></pre>
</div>
<div id="resumindo-variáveis-por-grupo-função-group_by-e-summarize" class="section level2">
<h2>Resumindo variáveis por grupo: função <code>group_by()</code> e <code>summarize()</code></h2>
<pre class="r"><code># Calculando a media da variável ano para as combinações entre sexo, consumo e estado civil e a frequencia de indivíduos em cada combinação
df%&gt;%
  group_by(sexo, consumo, estado_civil)%&gt;%
  summarise(media.ano = mean(ano),frequencia=n())


# Calculando a media da variável ano para as combinações entre ano legal, consumo e estado civil e a frequencia de indivíduos em cada combinação
df%&gt;%
  group_by(id, consumo, estado_civil)%&gt;%
  summarise(media.ano = mean(ano),frequencia=n())</code></pre>
</div>
<div id="operador-pipe-1" class="section level2">
<h2>Operador pipe: %&gt;%</h2>
<p>Portanto, o operador <code>%&gt;%</code> realiza múltiplas ações sem guardar os passos intermediários. Mais alguns exemplos:</p>
<pre class="r"><code># Selecionando as variáveis ano e id
df %&gt;%
  select(ano,id)


df %&gt;% select(-estado_civil) %&gt;%
  filter(sexo==&quot;Masculino&quot;) %&gt;%
  group_by(tonalidade,consumo) %&gt;%
  summarise(maximo=max(ano),media=mean(ano))</code></pre>
</div>
<div id="aplicando-funções-em-linhas" class="section level2">
<h2>Aplicando funções em linhas</h2>
<pre class="r"><code>df%&gt;%
  mutate(ano2 = ano**2 )%&gt;%
  rowwise() %&gt;% mutate(Max= max(ano:ano2)) %&gt;%
  select(ano,ano2,Max)</code></pre>
</div>
</div>
<div id="fazendo-a-união-de-banco-de-dados-distintos" class="section level1">
<h1>Fazendo a união de banco de dados distintos</h1>
<div id="combinando-duas-bases-de-dados" class="section level2">
<h2>Combinando duas bases de dados</h2>
<p>O pacote dplyr possui um conjunto de funções que nos auxiliam a combinar dos data frames do nosso interesse.</p>
<div id="inner_join" class="section level3">
<h3>inner_join</h3>
<pre class="r"><code># Função inner_join: Combina as duas bases incluindo todas as variáveis de ambas as bases e todas as linhas comuns as duas bases
inner_join(df1,df2,by=&quot;ID&quot;)

inner_join(df1,df3,by=c(&quot;ID&quot;=&quot;Identificacao&quot;))</code></pre>
</div>
<div id="left_join" class="section level3">
<h3>left_join</h3>
<pre class="r"><code># Função left_join: Combina as duas bases incluindo todas as variáveis de ambas as bases e todas as linhas da base a esquerda
left_join(df1,df2,by=&quot;ID&quot;)</code></pre>
</div>
<div id="right_join" class="section level3">
<h3>right_join</h3>
<pre class="r"><code># Função right_join: Combina as duas bases incluindo todas as variáveis de ambas as bases e todas as linhas da base a direita
right_join(df1,df2,by=&quot;ID&quot;)</code></pre>
</div>
<div id="full_join" class="section level3">
<h3>full_join</h3>
<pre class="r"><code># Função full_join: Combina as duas bases incluindo todas as variáveis de ambas as bases e todas as linhas de ambas as bases
full_join(df1,df2,by=&quot;ID&quot;)</code></pre>
</div>
<div id="semi_join" class="section level3">
<h3>semi_join</h3>
<pre class="r"><code># Função semi_join: Combina as duas bases incluindo as variáveis da basea a esquerda e todas as linhas comuns as duas bases
semi_join(df1,df2,by=&quot;ID&quot;)</code></pre>
</div>
<div id="anti_join" class="section level3">
<h3>anti_join</h3>
<pre class="r"><code># Função anti_join: Combina as duas bases incluindo as variáveis da base a esquerda e todas as linhas que não são comuns as duas bases
anti_join(df1,df2,by=&quot;ID&quot;)</code></pre>
</div>
</div>
<div id="combinando-dados-verticalmente" class="section level2">
<h2>Combinando dados verticalmente</h2>
<div id="juntando-por-linhas-comuns-com-intersect" class="section level3">
<h3>Juntando por linhas comuns com intersect</h3>
<pre class="r"><code>#Criando uma base com as linhas comus as duas bases
intersect(d1,d2)</code></pre>
</div>
<div id="juntando-todas-as-linhas-com-union" class="section level3">
<h3>Juntando todas as linhas com union</h3>
<pre class="r"><code>#Criando uma base unindo todas as linhas das duas bases
union(d1,d3)</code></pre>
</div>
<div id="base-com-linhas-distintas-nas-duas-bases-com-setdiff" class="section level3">
<h3>Base com linhas distintas nas duas bases com setdiff</h3>
<pre class="r"><code>#Criando uma base com as linhas distintas nas duas bases
setdiff(d1,d3)</code></pre>
</div>
<div id="empilhando-duas-bases-uma-sobre-a-outra-com-rbind" class="section level3">
<h3>Empilhando duas bases uma sobre a outra com rbind</h3>
<pre class="r"><code>#Empilhando duas bases, uma em cima da outra
rbind(d1,d3)</code></pre>
</div>
<div id="empilhando-duas-bases-lado-a-lado-com-cbind" class="section level3">
<h3>Empilhando duas bases lado a lado com cbind</h3>
<pre class="r"><code>#Empilhando duas bases, uma ao lado da outra
cbind(d3,d4)</code></pre>
<p>É realmente impressionante como este pacote pode impulsionar nossas habilidades na manipulação de dados! Espero que a partir de hoje o <code>%&gt;%</code> não seja mais visto coisa “esquisita”</p>
</div>
</div>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2017-12-07-manipulando-dados-com-dplyr/manipulando-dados-com-dplyr/">Manipulando dados com dplyr</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Estatistica</category>
      <category>R</category>
      <category>Teoria</category>
      <category>Tidyverse</category>
      <category>Data mining</category>
      <category domain="tag">Estatistica</category>
      <category domain="tag">gomesfellipe</category>
      <category domain="tag">R</category>
      <category domain="tag">Prática</category>
      <category domain="tag">Dplyr</category>
      <category domain="tag">Tidyverse</category>
      <category domain="tag">Data Mining</category>
    </item>
    <item>
      <title>Tipos de relações entre variáveis</title>
      <link>https://gomesfellipe.github.io/post/2017-12-02-tipos-de-relacoes-entre-variaveis/</link>
      <pubDate>Sat, 02 Dec 2017 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2017-12-02-tipos-de-relacoes-entre-variaveis/</guid>
      <description>Tipos de relações Vimos no último post sobre quais tipos de medidas de correlação e associação podem ser calculadas para identificar o grau de associação (ou dependência) entre as variáveis.
Já sabemos que esses coeficientes variam entre 0 e 1 ou entre -1 e +1, de maneira que a proximidade de zero indique a falta de associação entre elas.
Porém o que fazer com tantas métricas? Qual o cálculo mais aconselhado para as relações dois a dois de cada tipo de variáveis (medidas, quantidades, nomes, classes com algum tipo de ordem ou hierarquia)?</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="tipos-de-relações" class="section level1">
<h1>Tipos de relações</h1>
<p>Vimos no <a href="https://gomesfellipe.github.io/post/tipos-de-correlacoes/">último post</a> sobre quais tipos de medidas de correlação e associação podem ser calculadas para identificar o grau de associação (ou dependência) entre as variáveis.</p>
<p>Já sabemos que esses coeficientes variam entre 0 e 1 ou entre -1 e +1, de maneira que a proximidade de zero indique a falta de associação entre elas.</p>
<p>Porém o que fazer com tantas métricas? Qual o cálculo mais aconselhado para as relações dois a dois de cada tipo de variáveis (medidas, quantidades, nomes, classes com algum tipo de ordem ou hierarquia)?</p>
<p>Não basta chegar no R e fazer um <code>pairs(dados)</code> junto com <code>cor(dados)</code> e olhar aquele monte de números sem saber se eles apresentam algum resultado realmente relevante embasado na teoria estatística.</p>
<p>Vejamos então os tipos de relações possíveis e quais tipos de medidas podem ser utilizadas a seguir.</p>
</div>
<div id="numérica-x-numérica" class="section level1">
<h1>Numérica x Numérica</h1>
<p>Tipos de medidas que podem ser utilizadas:</p>
<ul>
<li>Pearson (Intensidade de relacionamento linear)</li>
<li>Spearman (Relação monotônica entre dados emparelhados)</li>
<li>Kendall (Correlação entre duas variáveis ordinais de amostras pequenas)</li>
</ul>
<div id="graficamente" class="section level2">
<h2>Graficamente</h2>
<p>Um jeito informal e intuitivo de avaliar a relação é verificar se existe relação linear entre as variáveis, além de identificar se esta relação é positiva, negativa ou inexistente.</p>
<div id="duas-variaveis" class="section level3">
<h3>Duas variaveis</h3>
<p>Algumas opções de como avaliar graficamente duas variáveis:</p>
<p><img src="/post/2017-12-02-tipos-de-relacoes-entre-variaveis_files/figure-html/unnamed-chunk-1-1.png" width="672" /><img src="/post/2017-12-02-tipos-de-relacoes-entre-variaveis_files/figure-html/unnamed-chunk-1-2.png" width="672" /></p>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="/post/2017-12-02-tipos-de-relacoes-entre-variaveis_files/figure-html/unnamed-chunk-1-3.png" width="672" /></p>
</div>
<div id="mais-de-duas-variáveis" class="section level3">
<h3>Mais de duas variáveis</h3>
<p>Quando existe a presença de mais de duas variáveis em estudo podemos utilizar outras características gráficas além do eixo x e y para identificar padrões, veja:</p>
<p><img src="/post/2017-12-02-tipos-de-relacoes-entre-variaveis_files/figure-html/unnamed-chunk-2-1.png" width="672" /><img src="/post/2017-12-02-tipos-de-relacoes-entre-variaveis_files/figure-html/unnamed-chunk-2-2.png" width="672" /><img src="/post/2017-12-02-tipos-de-relacoes-entre-variaveis_files/figure-html/unnamed-chunk-2-3.png" width="672" /></p>
</div>
</div>
<div id="normalidade" class="section level2">
<h2>Normalidade</h2>
<p>A suposição de normalidade é amplamente utilizada na estatística.</p>
<div id="graficamente-1" class="section level3">
<h3>Graficamente</h3>
<p>Avaliando a normalidade de forma visual com alguns comandos do ggplot:</p>
<pre class="r"><code>### Verificando a Normalidade Através do Histograma

# Criando um painel com o espaço de 4 gráficos
par(mfrow=c(2,2))

#preenchendo os quatro espaços com 4 histogramas (um para cada variável)
histogram=function(x){
  hist(x,prob=T)
  lines(density(x),col=&quot;red&quot;)
  curve(dnorm(x,mean(x), sd(x)),add=T,col=&quot;blue&quot;)
}
histogram(dados$GASTEDU)
histogram(dados$GASAUDE)
histogram(dados$GASLAZER)
histogram(dados$IDADE)</code></pre>
<p><img src="/post/2017-12-02-tipos-de-relacoes-entre-variaveis_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
<div id="qq-plot" class="section level3">
<h3>QQ-plot</h3>
<p>Compara os quantis dos dados com os quantis de uma normal padrão</p>
<pre class="r"><code>par(mfrow=c(2,2))
### Verificando a Normalidade Através do QQplot
qq = function(x){
  qqnorm(x,main = &quot;&quot;, xlab = &quot;Quantis teóricos N(0,1)&quot;, pch = 20)
qqline(x, lty = 1, col = &quot;red&quot;)
}

qq(dados$IDADE)
qq(dados$GASAUDE)
qq(dados$GASLAZER)
qq(dados$GASTEDU)</code></pre>
<p><img src="/post/2017-12-02-tipos-de-relacoes-entre-variaveis_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="qq-plot-com-envelope" class="section level3">
<h3>QQ plot com envelope</h3>
<p>Incluindo uma região de aceitação, para cada ponto constroi o intervalo de confiança</p>
<pre class="r"><code>#Envelope
envelope&lt;-function(x){
  n &lt;- length(x)
  nsim &lt;- 100 # Número de simulações
  conf &lt;- 0.95 # Coef. de confiança
  # Dados simulados ~ normal
  dadossim &lt;- matrix(rnorm(n*nsim, mean = mean(x), sd = sd(x)), nrow = n)
  dadossim &lt;- apply(dadossim,2,sort)
  # Limites da banda e média
  infsup&lt;-apply(dadossim,1,quantile, probs = c((1 - conf) / 2,(1 + conf) / 2))
  xbsim &lt;- rowMeans(dadossim)
  faixay &lt;- range(x, dadossim)
  qq0 &lt;- qqnorm(x, main = &quot;&quot;, xlab = &quot;Quantis teóricos N(0,1)&quot;, pch = 20, ylim = faixay)
  eixox &lt;- sort(qq0$x)
  lines(eixox, xbsim)
  lines(eixox, infsup[1,], col = &quot;red&quot;)
  lines(eixox, infsup[2,], col = &quot;red&quot;)
}

par(mfrow=c(2,2))
envelope(dados$GASTEDU)
envelope(dados$GASAUDE)
envelope(dados$GASLAZER)
envelope(dados$IDADE)</code></pre>
<p><img src="/post/2017-12-02-tipos-de-relacoes-entre-variaveis_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</div>
<div id="testes" class="section level3">
<h3>Testes</h3>
<p>A seguir, diversos testes de hipóteses para avaliar:</p>
<p><span class="math display">\[
H_0: \text{Dados Normais} \\
H_1: \text{Dados Não Normais} 
\]</span></p>
<p>A seguir uma função que criei colocando logo uma variedade de testes para fornecer diferentes evidências para nossa hipótese:</p>
<pre class="r"><code>normalidade&lt;-function(x){
t1 &lt;- ks.test(x, &quot;pnorm&quot;,mean(x), sd(x)) # KS  
t2 &lt;- lillie.test(x) # Lilliefors
t3 &lt;- cvm.test(x) # Cramér-von Mises
t4 &lt;- shapiro.test(x) # Shapiro-Wilk 
t5 &lt;- sf.test(x) # Shapiro-Francia
t6 &lt;- ad.test(x) # Anderson-Darling
t7&lt;-pearson.test(x) # Pearson Test of Normality

testes &lt;- c(t1$method, t2$method, t3$method, t4$method, t5$method,t6$method,t7$method)
valorp &lt;- c(t1$p.value, t2$p.value, t3$p.value, t4$p.value, t5$p.value,t6$p.value,t7$p.value)

resultados &lt;- cbind(valorp)
rownames(resultados) &lt;- testes
print(resultados, digits = 4)

}

normalidade(dados$GASAUDE)</code></pre>
<pre><code>##                                                valorp
## One-sample Kolmogorov-Smirnov test             0.9238
## Lilliefors (Kolmogorov-Smirnov) normality test 0.6494
## Cramer-von Mises normality test                0.6605
## Shapiro-Wilk normality test                    0.6297
## Shapiro-Francia normality test                 0.6286
## Anderson-Darling normality test                0.6346
## Pearson chi-square normality test              0.3249</code></pre>
</div>
</div>
<div id="dados-normais-relação-linear" class="section level2">
<h2>Dados normais + Relação linear</h2>
<p>Quando os dados são normais e a relação entre variáveis é linear, podemos utilizar os mesmos testes já comentados:</p>
<ul>
<li>Pearson</li>
<li>Spearman (amostras maiores)</li>
<li>Kendall (amostras pequenas)</li>
</ul>
<div id="coeficiente-de-correlação-de-pearson-rho" class="section level3">
<h3>Coeficiente de Correlação de Pearson <span class="math inline">\(\rho\)</span></h3>
<p>No R:</p>
<pre class="r"><code>#Matriz de correlações:
cor(dados$GASTEDU,dados$GASAUDE)</code></pre>
<pre><code>## [1] 0.77825</code></pre>
<p>Como saber se a correlação é significativa?</p>
<p><span class="math display">\[
H_0: \text{Não existe correlação} \\
H_1: \text{Existe correlação} 
\]</span></p>
<p>Aplicando o teste:</p>
<pre class="r"><code>#Teste de correlação:
cor.test(dados$GASTEDU,dados$GASAUDE,method = &quot;pearson&quot;)</code></pre>
</div>
</div>
<div id="dados-não-normais-eou-sem-relação-linear" class="section level2">
<h2>Dados não normais e/ou sem relação linear</h2>
<p>Quando os dados não se apresentam conforme a distribuição normal ou não apresentam relação linear, temos disponíveis o cálculo das seguintes correlações:</p>
<ul>
<li>Spearman (amostras maiores)</li>
<li>kendall (amostras pequenas)</li>
</ul>
<div id="coeficiente-de-correlação-de-spearman-rho" class="section level3">
<h3>Coeficiente de Correlação de Spearman <span class="math inline">\(\rho\)</span></h3>
<p>Ideal quando temos variáveis medidas apenas em uma escala ordinal.</p>
<p>Executando no R:</p>
<pre class="r"><code>#Teste de correlação:
cor.test(dados$GASTEDU,dados$GASAUDE,method = &quot;spearman&quot;)</code></pre>
</div>
<div id="coeficiente-de-correlação-de-kendall-tau-de-kendall" class="section level3">
<h3>Coeficiente de Correlação de Kendall (<span class="math inline">\(\tau\)</span> de kendall)</h3>
<p>Coeficiente de Kendall é, muitas vezes, interpretado como uma medida de concordância entre dois conjuntos de classificações relativas a um conjunto de objetos de estudo.</p>
<p>Vamos considerar apenas os 20 primeiros elementos da amostra:</p>
<p>Aplicação no R:</p>
<pre class="r"><code>#Teste de correlação:
cor.test(dados2$IDADE,dados2$GASAUDE,method = &quot;kendall&quot;)</code></pre>
</div>
</div>
</div>
<div id="ordinal-x-ordinal" class="section level1">
<h1>Ordinal x Ordinal</h1>
<p>Tipos de correlações possíveis para calcular:</p>
<ul>
<li>Spearman (amostras maiores)</li>
<li>kendall (amostras pequenas)</li>
</ul>
<p>Exemplo de uso de Spearman no R:</p>
<pre class="r"><code>cor(dados$ESCOLAR, dados$RENDA, method = &quot;spearman&quot;)
cor.test(dados$ESCOLAR, dados$RENDA, method = &quot;spearman&quot;)</code></pre>
<p>Exemplo de uso de Kendall com uma amostra menor:</p>
<pre class="r"><code>cor(dados2$ESCOLAR, dados2$RENDA, method = &quot;kendall&quot;)
cor.test(dados2$ESCOLAR, dados2$RENDA, method = &quot;kendall&quot;)</code></pre>
</div>
<div id="numérica-x-ordinal" class="section level1">
<h1>Numérica x Ordinal</h1>
<p>Independente de ser normal ou não</p>
<ul>
<li>Spearman (amostras maiores)</li>
<li>Kendall (amostras pequenas)</li>
<li>Comparações de grupos (Testes de Hipóteses)</li>
</ul>
<p>Exemplo de uso de Spearman no R:</p>
<pre class="r"><code>cor(dados$IDADE, dados$RENDA, method = &quot;spearman&quot;)
cor.test(dados$IDADE, dados$RENDA, method = &quot;spearman&quot;)</code></pre>
<p>Exemplo de uso de Kendall com uma amostra menor:</p>
<pre class="r"><code>cor(dados2$IDADE, dados2$RENDA, method = &quot;kendall&quot;)
cor.test(dados2$IDADE, dados2$RENDA, method = &quot;kendall&quot;)</code></pre>
</div>
<div id="nominal-x-nominal" class="section level1">
<h1>Nominal x Nominal</h1>
<p>Os termos nível nominal de
medida ou escala nominal são utilizadas para se referir
a àqueles dados que só podem ser categorizados. No
sentido estrito, não existe uma medida ou escala envolvida,
o que existe é apenas uma contagem.</p>
<p>Vamos avaliar a profissão e o estado civil primeiramente, precisamos da tabela de contingência.</p>
<p>Tabelas de Contingência (ou tabelas de freqüência de dupla entrada) são tabelas em que as frequências correspondem a duas classificações, uma classificação está nas linhas da tabela e a outra está nas colunas. Veja:</p>
<pre class="r"><code>tab=ftable(as.factor(dados$PROFI),
      as.factor(dados$ESTCIVIL),
      dnn=c(&quot;Profissão&quot;, &quot;EStado Civil&quot;))
tab</code></pre>
<pre><code>##           EStado Civil  1  2  3  4
## Profissão                         
## 1                      26 13 29  1
## 2                      24  6 21  0</code></pre>
<div id="qui-quadrado-de-independencia" class="section level2">
<h2>Qui-quadrado de independencia</h2>
<p><span class="math display">\[
H_0: \text{São independentes (Não associadas)} \\
H_1: \text{Não são independentes (São associadas) }
\]</span></p>
<p>Executando o teste:</p>
<pre class="r"><code>chisq.test(dados$PROFI, dados$ESTCIVIL)</code></pre>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  dados$PROFI and dados$ESTCIVIL
## X-squared = 2.2905, df = 3, p-value = 0.5143</code></pre>
<p><strong>OBS</strong>: Correção de YAKES quando existe alguma frequência esperada menor do que 5, veja:</p>
</div>
<div id="teste-exato-de-fisher" class="section level2">
<h2>Teste exato de fisher</h2>
<p>O teste qui-quadrado quando aplicado a amostras pequenas, como por exemplo com tamanho inferior a 20, veja:</p>
<pre class="r"><code>fisher.test(dados2$PROFI, dados2$ESTCIVIL)</code></pre>
<pre><code>## 
##  Fisher&#39;s Exact Test for Count Data
## 
## data:  dados2$PROFI and dados2$ESTCIVIL
## p-value = 0.5226
## alternative hypothesis: two.sided</code></pre>
</div>
<div id="medidas-de-associação" class="section level2">
<h2>Medidas de associação</h2>
<p>os testes fornecem apenas a resposta se as variáveis estão ou não correlacionadas. Para saber a intensidade desta relação, utilizam-se medidas de associação.</p>
<p>Considere as seguintes medidas:</p>
<ul>
<li><span class="math inline">\(\mathbf{\phi}\)</span> <strong>(phi)</strong> (é o R de pearson quando aplicado a tabelas 2x2)</li>
<li><strong>V de Crámer</strong></li>
<li><strong>Coeficiente de contingência</strong></li>
</ul>
<p>Ambos variam de 0 (ausência de associação) a 1 (associação muito forte).</p>
<pre class="r"><code>#Comando para tabela cruzada:
tab &lt;- xtabs(~ PROFI + ESTCIVIL, data = dados)

#Calcular as medidas de associação da tabela:
summary(assocstats(tab))</code></pre>
<pre><code>## 
## Call: xtabs(formula = ~PROFI + ESTCIVIL, data = dados)
## Number of cases in table: 120 
## Number of factors: 2 
## Test for independence of all factors:
##  Chisq = 2.2905, df = 3, p-value = 0.5143
##  Chi-squared approximation may be incorrect
##                     X^2 df P(&gt; X^2)
## Likelihood Ratio 2.6823  3  0.44324
## Pearson          2.2905  3  0.51435
## 
## Phi-Coefficient   : NA 
## Contingency Coeff.: 0.137 
## Cramer&#39;s V        : 0.138</code></pre>
<pre class="r"><code>#phi  (r aplicado na Tabela de 2x2 --&gt; Phi)
cor(dados$PROFI,dados$ESTCIVIL)  </code></pre>
<pre><code>## [1] -0.06972599</code></pre>
</div>
<div id="kappa" class="section level2">
<h2>Kappa</h2>
<p>É uma medida de concordância.</p>
<p><strong>Obs</strong>: Também pode ser utilizado o coeficiente de Kappa ponderado (pesquisar)</p>
<pre class="r"><code>#Kappa
medico1&lt;-sample(0:1,10, replace=T)
medico2&lt;-sample(0:1,10, replace=T)

#Kappa.test(x, y=NULL, conf.level=0.95)

fmsb::Kappa.test(medico1,medico2)</code></pre>
<pre><code>## $Result
## 
##  Estimate Cohen&#39;s kappa statistics and test the null hypothesis that
##  the extent of agreement is same as random (kappa=0)
## 
## data:  medico1 and medico2
## Z = -1.2649, p-value = 0.897
## 95 percent confidence interval:
##  -0.9680515  0.1680515
## sample estimates:
## [1] -0.4
## 
## 
## $Judgement
## [1] &quot;No agreement&quot;</code></pre>
</div>
</div>
<div id="nominal-x-ordinal" class="section level1">
<h1>Nominal x Ordinal</h1>
<p>Vamos avaliar a profissão e o estado civil primeiramente, precisamos da tabela de contingência:</p>
<pre class="r"><code>tab=ftable(as.factor(dados$PROFI),
      as.factor(dados$RENDA),
      dnn=c(&quot;Profissão&quot;, &quot;Renda&quot;))
tab</code></pre>
<pre><code>##           Renda  1  2  3  4
## Profissão                  
## 1                4 52  9  4
## 2                0 30 17  4</code></pre>
<div id="qui-quadrado-de-independencia-1" class="section level2">
<h2>Qui-quadrado de independencia</h2>
<p><span class="math display">\[
H_0: \text{São independentes (Não associadas)} \\
H_1: \text{Não são independentes (São associadas) }
\]</span></p>
<p>Executando o teste:</p>
<pre class="r"><code>chisq.test(dados$PROFI, dados$RENDA)</code></pre>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  dados$PROFI and dados$RENDA
## X-squared = 9.8864, df = 3, p-value = 0.01956</code></pre>
<p><strong>OBS</strong>: Correção de YAKES quando existe alguma frequência esperada menor do que 5, veja:</p>
</div>
<div id="teste-exato-de-fisher-1" class="section level2">
<h2>Teste exato de fisher</h2>
<p>O teste qui-quadrado quando aplicado a amostras pequenas, como por exemplo com tamanho inferior a 20, veja:</p>
<pre class="r"><code>fisher.test(dados2$PROFI, dados2$RENDA)</code></pre>
<pre><code>## 
##  Fisher&#39;s Exact Test for Count Data
## 
## data:  dados2$PROFI and dados2$RENDA
## p-value = 1
## alternative hypothesis: two.sided</code></pre>
</div>
<div id="medidas-de-associação-1" class="section level2">
<h2>Medidas de associação</h2>
<p>os testes fornecem apenas a resposta se as variáveis estão ou não correlacionadas. Para saber a intensidade desta relação, utilizam-se medidas de associação.</p>
<p>Considere as seguintes medidas:</p>
<ul>
<li><span class="math inline">\(\mathbf{\phi}\)</span> <strong>(phi) </strong> (é o R de pearson quando aplicado a tabelas 2x2)</li>
<li><strong>V de Crámer</strong></li>
<li><strong>Coeficiente de contingência</strong></li>
</ul>
<p>Ambos variam de 0 (ausência de associação) a 1 (associação muito forte).</p>
<pre class="r"><code>#Comando para tabela cruzada:
tab &lt;- xtabs(~ PROFI + RENDA, data = dados)

#Calcular as medidas de associação da tabela:
summary(assocstats(tab))</code></pre>
<pre><code>## 
## Call: xtabs(formula = ~PROFI + RENDA, data = dados)
## Number of cases in table: 120 
## Number of factors: 2 
## Test for independence of all factors:
##  Chisq = 9.886, df = 3, p-value = 0.01956
##  Chi-squared approximation may be incorrect
##                      X^2 df P(&gt; X^2)
## Likelihood Ratio 11.3123  3 0.010152
## Pearson           9.8864  3 0.019557
## 
## Phi-Coefficient   : NA 
## Contingency Coeff.: 0.276 
## Cramer&#39;s V        : 0.287</code></pre>
<pre class="r"><code>#phi  (r aplicado na Tabela de 2x2 --&gt; Phi)
cor(dados$PROFI,dados$RENDA)  </code></pre>
<pre><code>## [1] 0.231198</code></pre>
</div>
<div id="kappa-1" class="section level2">
<h2>Kappa</h2>
<p>Testa a concordância entre duas pessoas (a hipótese nula é de que a concordância é zero)</p>
<pre class="r"><code>#Kappa
medico1&lt;-sample(0:1,10, replace=T)
medico2&lt;-sample(0:1,10, replace=T)

#Kappa.test(x, y=NULL, conf.level=0.95)

fmsb::Kappa.test(medico1,medico2)</code></pre>
<pre><code>## $Result
## 
##  Estimate Cohen&#39;s kappa statistics and test the null hypothesis that
##  the extent of agreement is same as random (kappa=0)
## 
## data:  medico1 and medico2
## Z = 0.2538, p-value = 0.3998
## 95 percent confidence interval:
##  -0.4998102  0.6479584
## sample estimates:
## [1] 0.07407407
## 
## 
## $Judgement
## [1] &quot;Slight agreement&quot;</code></pre>
</div>
</div>
<div id="dicotônica-x-ordinal" class="section level1">
<h1>Dicotônica x Ordinal</h1>
<p>Uma variável dicotômica é uma variável qualitativa que só possui duas categorias.</p>
<p>Portanto a mesma abordagem utilizada em:</p>
<p>Dicotômica x Ordinal = Nominal x Ordinal = Nominal x Nominal</p>
</div>
<div id="nominal-x-numérca" class="section level1">
<h1>Nominal x Numérca</h1>
<div id="r2-do-ajuste-de-modelos-lineares" class="section level2">
<h2><span class="math inline">\(R^2\)</span> do ajuste de modelos lineares</h2>
<p>Pode-se ajustar um modelo de regressão linear simples e avaliar seu coeficiente de determinação, veja:</p>
<pre class="r"><code>#R2:
summary(lm(dados$GASAUDE~dados$ESTCIVIL))$r.squared</code></pre>
<pre><code>## [1] 0.0001015817</code></pre>
</div>
<div id="bisserial-pearson" class="section level2">
<h2>Bisserial = Pearson</h2>
<p>O pearson aplicada em uma relação de variável dicotômica com uma variável ordinal</p>
</div>
<div id="comparações-de-grupos" class="section level2">
<h2>Comparações de Grupos</h2>
<p>Quando por exemplo, trabalha-se com “renda por grupo”, existem muitas abordagens como o teste t ou anova como opções de testes paramétricos e muito mais</p>
</div>
</div>
<div id="correlação-parcial" class="section level1">
<h1>Correlação parcial</h1>
<div id="controlando-variável-numérica" class="section level2">
<h2>Controlando variável numérica</h2>
<p>Pode ser que queremos estudar a correlação entre x e y, porém existem uma variável z que também está correlacionada com alguma das duas variáveis, veja:</p>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="/post/2017-12-02-tipos-de-relacoes-entre-variaveis_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<pre><code>## [1] 0.7821115</code></pre>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="/post/2017-12-02-tipos-de-relacoes-entre-variaveis_files/figure-html/unnamed-chunk-27-2.png" width="672" /></p>
<pre><code>## [1] 0.7476177</code></pre>
<p><img src="/post/2017-12-02-tipos-de-relacoes-entre-variaveis_files/figure-html/unnamed-chunk-27-3.png" width="672" /></p>
<pre><code>## [1] 0.7821115</code></pre>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="/post/2017-12-02-tipos-de-relacoes-entre-variaveis_files/figure-html/unnamed-chunk-27-4.png" width="672" /></p>
<pre><code>## [1] 0.77825</code></pre>
<p>Isto implica que a variável educação é uma variável de confusão, veja as correlações:</p>
<pre><code>## Registered S3 method overwritten by &#39;GGally&#39;:
##   method from   
##   +.gg   ggplot2</code></pre>
<p><img src="/post/2017-12-02-tipos-de-relacoes-entre-variaveis_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p>O que acontece com a associação entre lazer e saúde quando controlamos a variável de confusão educação?</p>
<pre class="r"><code># correlação LAZER vc SAÚDE controlando o EDUCAÇÃO (correlação parcial de primeira ordem = um variável para controlar)
rp&lt;-ggm::pcor(c(&quot;GASLAZER&quot;, &quot;GASAUDE&quot;, &quot;GASTEDU&quot;),var(dados))  #controlando A EDUCAÇÃO

#Significância da Correlação Parcial

#Coeficiente de Determinação com base no Coef. de Pearson
r&lt;-cor(dados$GASLAZER,dados$GASAUDE) #sem controlar o lazer

#Coeficiente de Determinação com base na correlação parcial
pcor.test(rp,1,length(dados$GASAUDE))  #&quot;1&quot; porque só usamos uma variável de controle</code></pre>
<pre><code>## $tval
## [1] 5.922106
## 
## $df
## [1] 117
## 
## $pvalue
## [1] 3.259388e-08</code></pre>
<pre class="r"><code>data.frame(&quot;Sem correção&quot;=r^2, &quot;Com correção&quot;=rp^2)</code></pre>
<pre><code>##   Sem.correção Com.correção
## 1    0.6116985    0.2306242</code></pre>
</div>
<div id="controlando-variável-qualitativa" class="section level2">
<h2>Controlando variável Qualitativa</h2>
<p>A variável de controle (ou qualquer uma delas) pode ser dicotômica (categórica)</p>
<pre class="r"><code>#Visualmente:
ggplot(data = dados, aes(x = GASLAZER, y = GASAUDE,colour = as.factor(PROFI))) + geom_point()</code></pre>
<p><img src="/post/2017-12-02-tipos-de-relacoes-entre-variaveis_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<pre class="r"><code>#Sem controlar:
r=cor(dados$GASLAZER, dados$GASAUDE)
rp&lt;-pcor(c(&quot;GASLAZER&quot;, &quot;GASAUDE&quot;, &quot;PROFI&quot;),var(dados))
data.frame(&quot;Sem correção&quot;=r^2, &quot;Com correção&quot;=rp^2)</code></pre>
<pre><code>##   Sem.correção Com.correção
## 1    0.6116985    0.6162497</code></pre>
</div>
</div>
<div id="referências" class="section level1">
<h1>Referências</h1>
<p><a href="https://www.amazon.com.br/Practical-Nonparametric-Statistics-W-Conover/dp/0471160687">CONOVER, W. J. Pratical Nonparametric Statistics</a></p>
<p><a href="https://www.amazon.com.br/Estat%C3%ADstica-n%C3%A3o-Param%C3%A9trica-Para-Ci%C3%AAncias-Comportamento-ebook/dp/B06Y2F9NQY/ref=sr_1_2?s=books&amp;ie=UTF8&amp;qid=1515522153&amp;sr=1-2">SIEGEL, S. Estatística Não Paramétrica para as Ciências do Comportamento</a></p>
<p><a href="https://www.amazon.com.br/Estat%C3%ADstica-B%C3%A1sica-Pedro-Morettin/dp/8502207997">BUSSAB, W. de O.;MORETTIN, P. A. Estatística básica. 5 ed.</a></p>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2017-12-02-tipos-de-relacoes-entre-variaveis/">Tipos de relações entre variáveis</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Estatistica</category>
      <category>R</category>
      <category>Teoria</category>
      <category domain="tag">correlacoes</category>
      <category domain="tag">correlacao</category>
      <category domain="tag">ggplot2</category>
      <category domain="tag">r</category>
      <category domain="tag">gomesfellipe</category>
    </item>
    <item>
      <title>Tipos de Correlacoes</title>
      <link>https://gomesfellipe.github.io/post/2017-12-01-tipos-de-correlacoes/</link>
      <pubDate>Fri, 01 Dec 2017 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2017-12-01-tipos-de-correlacoes/</guid>
      <description>Correlações Tipos de Variáveis Tipos de Correlações Coeficiente de Correlação de Pearson Coeficiente de Correlação de Spearman \(\rho\) Coeficiente de Correlação de Kendall (\(\tau\) de kendall) Qui-quadrado de independencia Teste exato de fisher Medidas de associação \(\phi\) (phi) (é o R de pearson quando aplicado a tabelas 2x2) V de Crámer Coeficiente de contingência Kappa Mãos a obra Referências Correlações De maneira geral, quando estamos interessados em avaliar o grau de associação entre duas variáveis calculamos os coeficientes de associação ou correlação entre variáveis.</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#correlações">Correlações</a></li>
<li><a href="#tipos-de-variáveis">Tipos de Variáveis</a></li>
<li><a href="#tipos-de-correlações">Tipos de Correlações</a>
<ul>
<li><a href="#coeficiente-de-correlação-de-pearson">Coeficiente de Correlação de Pearson</a></li>
<li><a href="#coeficiente-de-correlação-de-spearman-rho">Coeficiente de Correlação de Spearman <span class="math inline">\(\rho\)</span></a></li>
<li><a href="#coeficiente-de-correlação-de-kendall-tau-de-kendall">Coeficiente de Correlação de Kendall (<span class="math inline">\(\tau\)</span> de kendall)</a></li>
<li><a href="#qui-quadrado-de-independencia">Qui-quadrado de independencia</a></li>
<li><a href="#teste-exato-de-fisher">Teste exato de fisher</a></li>
<li><a href="#medidas-de-associação">Medidas de associação</a>
<ul>
<li><a href="#phi-phi-é-o-r-de-pearson-quando-aplicado-a-tabelas-2x2"><span class="math inline">\(\phi\)</span> (phi) (é o R de pearson quando aplicado a tabelas 2x2)</a></li>
<li><a href="#v-de-crámer">V de Crámer</a></li>
<li><a href="#coeficiente-de-contingência">Coeficiente de contingência</a></li>
</ul></li>
<li><a href="#kappa">Kappa</a></li>
</ul></li>
<li><a href="#mãos-a-obra">Mãos a obra</a></li>
<li><a href="#referências">Referências</a></li>
</ul>
</div>

<!-- # Base de dados:  -->
<!-- ```{r,echo=F} -->
<!-- dados <- read_excel("contabilidade.xlsx") -->
<!-- kable(dados,"html")%>% -->
<!--   kable_styling()%>% -->
<!--   scroll_box(width = "700px", height = "250px") -->
<!-- ```  -->
<!-- É possível notar que existem diversos tipos de variáveis -->
<div id="correlações" class="section level1">
<h1>Correlações</h1>
<p>De maneira geral, quando estamos interessados em avaliar o grau de associação entre duas variáveis calculamos os <em>coeficientes de associação</em> ou <em>correlação</em> entre variáveis.</p>
<p>Essas medidas descrevem por meio de um único número a associação (ou dependência) entre duas variáveis.</p>
<p>De modo a facilitar a compreensão, esses coeficientes geralmente variam entre 0 e 1 ou entre -1 e +1, de maneira que a proximidade de zero indique a falta de associação entre elas.</p>
<p>Existem muitas medidas disponíveis para quantificar a associação entre variáveis, porém, um primeiro conceito que deve ser levado em conta é: quais são os tipos de variáveis?</p>
</div>
<div id="tipos-de-variáveis" class="section level1">
<h1>Tipos de Variáveis</h1>
<p>Existem dois tipos de variáveis que podem ser abordadas de maneiras diferentes, veja:</p>
<p><strong>Quantitativas</strong></p>
<ul>
<li>Continua: Medidas (Peso, altura, renda, dinheiro, comprimento)</li>
<li>Discreta: Contagem (qnt. de coisas)</li>
</ul>
<p><strong>Qualitativas</strong></p>
<ul>
<li>Nominais: Nomes</li>
<li>Ordinais: Quando é possível ordenar os arquivos</li>
</ul>
<p>E para cada relação ou associação que buscamos calcular, existe um tipo diferente de coeficiente, mas de maneira geral, todos eles possuem tais características em comum:</p>
</div>
<div id="tipos-de-correlações" class="section level1">
<h1>Tipos de Correlações</h1>
<p>Coeficientes de correlação informam:</p>
<ul>
<li>Intensidade
<ul>
<li>Fortemente relacionadas (Valores próximos de 1 ou -1)</li>
<li>Fracamente relacionadas (Valores próximos de 0)</li>
</ul></li>
<li>Direção
<ul>
<li>Positiva (Se ambas as variáveis crescem no mesmo sentido)</li>
<li>Negativa (Se as variáveis crescem em sentidos opostos)</li>
</ul></li>
<li>Significância</li>
</ul>
<p>IMPORTANTE: CORRELAÇÃO NÃO INDICA RELAÇÃO DE CAUSALIDADE</p>
<p>E além dos coeficientes de correlação, existem outras medidas de associação igualmente importantes, veja:</p>
<div id="coeficiente-de-correlação-de-pearson" class="section level2">
<h2>Coeficiente de Correlação de Pearson</h2>
<p>Sejam duas variáveis X e Y, ambas quantitativas, preferencialmente contínuas. A existência de relação linear entre essas variáveis pode ser detectada com auxílio do Diagrama de Dispersão, mas, também, com auxílio do Coeficiente de Correlação Linear de Pearson.</p>
</div>
<div id="coeficiente-de-correlação-de-spearman-rho" class="section level2">
<h2>Coeficiente de Correlação de Spearman <span class="math inline">\(\rho\)</span></h2>
<p>Utilizado quando não existe normalidade e/ou não existe relação linear, deve ser usado quando não se deseja utilizar nenhuma suposição de normalidade ou da presença de qualquer outra distribuição para a variável ou para a estatística de teste.</p>
<p>Este coeficiente se baseia nos postos das observações dentro de cada variável e se baseia sobre as diferenças entre os postos observados, nas variáveis X e Y, para um mesmo objeto de estudo.</p>
<p>Ideal quando temos variáveis medidas apenas em uma escala ordinal.</p>
</div>
<div id="coeficiente-de-correlação-de-kendall-tau-de-kendall" class="section level2">
<h2>Coeficiente de Correlação de Kendall (<span class="math inline">\(\tau\)</span> de kendall)</h2>
<p>O coeficiente de correlação Tau de Kendall serve para verificar se existe correlação entre duas variáveis ordinais. É um método adequado quando amostras têm tamanhos reduzidos, pois o método é mais preciso. E pode ser estendido a correlações parciais, quando o efeito de uma terceira variável, que age sobre X e Y, é retirado antes de determinar se X e Y estão relacionadas.</p>
<p>Coeficiente de Kendall é, muitas vezes, interpretado como uma medida de concordância entre dois conjuntos de classificações relativas a um conjunto de objetos de estudo.</p>
</div>
<div id="qui-quadrado-de-independencia" class="section level2">
<h2>Qui-quadrado de independencia</h2>
<p>Utiliza-se esta prova quando os dados da pesquisa se apresentam sob forma de frequências em categorias
discretas. Pode aplicar a prova <span class="math inline">\(\chi^2\)</span> para determinar a significância de diferenças entre dois grupos independentes e conseqüentemente, com respeito a frequências relativas com que os componentes do grupo se enquadram nas diversas categorias.</p>
<p>Suas hipóteses:</p>
<p><span class="math display">\[
H_0: \text{São independentes (Não associadas)} \\
H_1: \text{Não são independentes (São associadas) }
\]</span></p>
</div>
<div id="teste-exato-de-fisher" class="section level2">
<h2>Teste exato de fisher</h2>
<p>O teste qui-quadrado quando aplicado a amostras pequenas, como por exemplo com tamanho inferior a 20, veja:</p>
</div>
<div id="medidas-de-associação" class="section level2">
<h2>Medidas de associação</h2>
<p>os testes fornecem apenas a resposta se as variáveis estão ou não correlacionadas. Para saber a intensidade desta relação, utilizam-se medidas de associação.</p>
<p>Considere as seguintes medidas:</p>
<div id="phi-phi-é-o-r-de-pearson-quando-aplicado-a-tabelas-2x2" class="section level3">
<h3><span class="math inline">\(\phi\)</span> (phi) (é o R de pearson quando aplicado a tabelas 2x2)</h3>
<p>O coeficiente phi é uma medida de associação entre duas variáveis binárias. A interpretação é similar a de um coeficiente de correlação. Duas variáveis binárias são consideradas positivamente associadas se a maior parte dos dados (frequências) cai ao longo das células da diagonal (a e d maiores que b e c). E serão consideradas negativamente associadas se a maior parte dos dados cai fora da diagonal.</p>
</div>
<div id="v-de-crámer" class="section level3">
<h3>V de Crámer</h3>
<p>O coeficiente V de Cramer serve para medir associação em tabelas não quadradas.</p>
</div>
<div id="coeficiente-de-contingência" class="section level3">
<h3>Coeficiente de contingência</h3>
<p>O Coeficiente de Contingência C é uma medida de associação, relacionada à estatística de teste do teste qui-quadrado, e ajustada para diferentes tamanhos de amostra. Ele também está diretamente relacionado à estatística de teste do teste qui-quadrado e ao Coeficiente Phi (possui as mesmas vantagens e desvantagens de Phi).</p>
<p>Ambos variam de 0 (ausência de associação) a 1 (associação muito forte).</p>
</div>
</div>
<div id="kappa" class="section level2">
<h2>Kappa</h2>
<p>O coeficiente Kappa é uma medida de concordância inter observador e mede o grau de concordância além do que seria esperado só por conta do acaso. Muitas vezes é usado no lugar do teste de McNemar.</p>
<p><strong>Obs</strong>: Também pode ser utilizado o coeficiente de Kappa ponderado</p>
</div>
</div>
<div id="mãos-a-obra" class="section level1">
<h1>Mãos a obra</h1>
<p>É impressionante a gama de opções que já existe para avaliarmos variáveis por diversas perspectivas!</p>
<p>É bom ressaltar que é extremamente fácil se perder no meio de tantos resultados em tantas situações possíveis, por isso meu <a href="https://gomesfellipe.github.io/post/tipos-de-relacoes-entre-variaveis/">próximo post</a> irá tratar justamente dos diferentes tipos de relações entre variáveis e quais tipos de medidas são possíveis para cada caso, até a próxima!</p>
</div>
<div id="referências" class="section level1">
<h1>Referências</h1>
<p><a href="https://www.amazon.com.br/Practical-Nonparametric-Statistics-W-Conover/dp/0471160687">CONOVER, W. J. Pratical Nonparametric Statistics</a></p>
<p><a href="https://www.amazon.com.br/Estat%C3%ADstica-n%C3%A3o-Param%C3%A9trica-Para-Ci%C3%AAncias-Comportamento-ebook/dp/B06Y2F9NQY/ref=sr_1_2?s=books&amp;ie=UTF8&amp;qid=1515522153&amp;sr=1-2">SIEGEL, S. Estatística Não Paramétrica para as Ciências do Comportamento</a></p>
<p><a href="https://www.amazon.com.br/Estat%C3%ADstica-B%C3%A1sica-Pedro-Morettin/dp/8502207997">BUSSAB, W. de O. ;MORETTIN, P. A. Estatística básica. 5 ed.</a></p>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2017-12-01-tipos-de-correlacoes/">Tipos de Correlacoes</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>R</category>
      <category>Estatística</category>
      <category>Teoria</category>
      <category domain="tag">R</category>
      <category domain="tag">estatistica</category>
      <category domain="tag">correlacoes</category>
      <category domain="tag">pearson</category>
      <category domain="tag">spearman</category>
      <category domain="tag">kendall</category>
      <category domain="tag">qui-quadrado</category>
      <category domain="tag">teste exato defisher</category>
      <category domain="tag">v de cramer</category>
      <category domain="tag">kappa</category>
      <category domain="tag">gomesfellipe</category>
    </item>
  </channel>
</rss>