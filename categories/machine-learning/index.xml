&lt;?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Machine Learning on Fellipe Gomes - Data Science Blog</title>
    <link>https://gomesfellipe.github.io/categories/machine-learning/</link>
    <description>√öltimos posts sobre Data Science, Machine Learning e R</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>pt-br</language>
    <managingEditor>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</managingEditor>
    <webMaster>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</webMaster>
    <lastBuildDate>Fri, 27 Sep 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://gomesfellipe.github.io/categories/machine-learning/" rel="self" type="application/rss+xml" />
    <item>
      <title>Extra√ß√£o de informa√ß√µes de imagens com IA Generativa</title>
      <link>https://gomesfellipe.github.io/post/2024-09-27-image-text-to-text/</link>
      <pubDate>Fri, 27 Sep 2024 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2024-09-27-image-text-to-text/</guid>
      <description>Neste post, exploraremos como utilizar o modelo Llava para gerar r√≥tulos descritivos de imagens, usando dados do conjunto COCO-2017.</description>
      <content:encoded>&lt;![CDATA[
        


<div id="caso-de-uso-de-ia-generativa-extra√ß√£o-de-informa√ß√µes-de-imagens-com-o-modelo-llava" class="section level1">
<h1>Caso de Uso de IA Generativa: Extra√ß√£o de Informa√ß√µes de Imagens com o Modelo Llava</h1>
<p>GenAI refere-se a modelos de intelig√™ncia artificial capazes de gerar conte√∫do novo e criativo a partir de dados de entrada. Seu uso est√° revolucionando a maneira como processamos dados n√£o estruturados, como imagens, √°udios, textos, v√≠deos, etc. Trabalhar com modelos pr√©-treinados (i.e., que j√° foram treinados com grandes conjuntos de dados) e adapt√°-los para necessidades espec√≠ficas tem sido um divisor de √°guas.</p>
<p>Neste post, vamos explorar a utiliza√ß√£o do modelo Llava (Large Language and Vision Assistant) para extrair r√≥tulos descritivos de imagens e tamb√©m discutir como comparar a qualidade das previs√µes geradas com m√©tricas espec√≠ficas para avaliar a performance desse tipo de modelo.</p>
<div id="por-que-o-modelo-llava" class="section level2">
<h2>Por que o Modelo Llava?</h2>
<p>O modelo <a href="https://llava-vl.github.io/">Llava</a> √© uma alternativa de c√≥digo aberto ao <a href="https://chat-gpt-5.ai/capabilities-of-gpt-4v/">GPT-4 Vision</a> da OpenAI (que se destaca neste dom√≠nio, mas sua aplica√ß√£o √© restrita devido sua natureza propriet√°ria e comercial) que foi treinado em grandes conjuntos de dados multimodais, sendo capaz de compreender e gerar descri√ß√µes textuais para imagens.</p>
<p>Essa capacidade de ‚Äúconversar com imagens‚Äù tendo o mesmo ‚Äúpoder‚Äù de um LLM, possibilita seu uso em muitas solu√ß√µes desenvolvidas por cientistas de dados no mundo real, como:</p>
<ol style="list-style-type: decimal">
<li><strong>Classifica√ß√£o de produtos em e-commerce</strong>: gera√ß√£o de descri√ß√µes detalhadas de roupas, acess√≥rios, eletr√¥nicos, etc.</li>
<li><strong>Detec√ß√£o de defeitos em linhas de produ√ß√£o</strong>: identifica√ß√£o de falhas em produtos para automa√ß√£o e controle de qualidade.</li>
<li><strong>Diagn√≥stico m√©dico por imagens</strong>: auxiliar na detec√ß√£o precoce de doen√ßas a partir de descri√ß√µes detalhadas de imagens m√©dicas.</li>
<li><strong>Reconhecimento de placas de carros</strong>: transcri√ß√£o autom√°tica de textos de placas e caracter√≠sticas de ve√≠culos.</li>
<li><strong>Identifica√ß√£o de sinais de tr√¢nsito</strong>: aplica√ß√£o em ve√≠culos aut√¥nomos para navega√ß√£o e identifica√ß√£o de sinais.</li>
<li><strong>An√°lise de alimentos para calcular nutri√ß√£o</strong>: extra√ß√£o autom√°tica de informa√ß√µes nutricionais de fotos ou r√≥tulos de alimentos.</li>
<li><strong>Identifica√ß√£o de animais em c√¢meras de vida selvagem</strong>: gerar descri√ß√µes detalhadas de animais detectados, ajudando pesquisadores a automatizar o monitoramento da vida selvagem.</li>
<li><strong>Detec√ß√£o de aglomera√ß√µes em eventos</strong>: analisar imagens de c√¢meras de seguran√ßa para identificar a presen√ßa de grandes grupos de pessoas em eventos ou lugares p√∫blicos, √∫til em gest√£o de multid√µes ou para quest√µes de seguran√ßa.</li>
</ol>
</div>
<div id="dataset-coco-2017" class="section level2">
<h2>Dataset COCO-2017</h2>
<p>O <a href="https://cocodataset.org/">COCO</a> (Common Objects in Context) √© um dataset amplamente utilizado em vis√£o computacional. Ele √© um dos maiores conjuntos de imagens do dia a dia com objetos em diferentes contextos, com anota√ß√µes detalhadas fornecidas por humanos como tags, caixa delimitadora, pol√≠gono que segmenta a imagem detectando objetos bem como sua descri√ß√£o. Isso o torna ideal para testar o desempenho desse tipo de modelo para gera√ß√£o de legendas.</p>
<center>
<div style="display: flex; width: 100%;">
<div style="width: 50%;">
<p><img src="/post/2024-09-27-image-text-to-text/coco1.png" alt="Imagem 2" style="width: 100%;"></p>
</div>
<div style="width: 50%;">
<p><img src="/post/2024-09-27-image-text-to-text/coco2.png" alt="Imagem 2" style="width: 100%;"></p>
</div>
</div>
<center>
<small>
Imagem do COCO Dataset com e sem anota√ß√£o obtida na <a href="https://cocodataset.org/#explore">se√ß√£o explorat√≥ria</a> das imagens
</small>
</center>
</center>
</div>
</div>
<div id="preparando-o-ambiente" class="section level1">
<h1>Preparando o Ambiente</h1>
<p>Utilizei o ambiente do Kaggle para desenvolvimento deste notebook, que disponibiliza a utiliza√ß√£o de GPUs. Atrav√©s do Hardware Accelerator utilizaremos a <a href="https://www.kaggle.com/docs/efficient-gpu-usage">NVIDIA TESLA P100 GPU</a>.</p>
<details>
<summary>
<em>Expandir c√≥digo</em>
</summary>
<pre><code>%%capture
!pip -qqq install bitsandbytes accelerate rouge-score pycocoevalcap bert_score
!pip install -U nltk

import os
import re
import json
import pandas as pd
import numpy as np
from tqdm import tqdm

import seaborn as sns
import matplotlib.pyplot as plt

from PIL import Image
import requests
from io import BytesIO
from IPython.display import HTML
import base64

import torch
from transformers import pipeline, AutoProcessor, BitsAndBytesConfig

from nltk.translate.bleu_score import sentence_bleu
from rouge_score import rouge_scorer
from bert_score import score as bert_score
from nltk.translate.meteor_score import meteor_score

from transformers import logging
import warnings

logging.set_verbosity_error()
warnings.filterwarnings(&quot;ignore&quot;, &quot;use_inf_as_na&quot;)</code></pre>
</details>
<p><br></p>
</div>
<div id="carregar-dados" class="section level1">
<h1>Carregar dados</h1>
<p>Por fins de praticidade para este post, selecionei uma amostra de 10 imagens aleat√≥rias do dataset COCO - (Common Objects in Context) no site <a href="https://cocodataset.org" class="uri">https://cocodataset.org</a> (onde √© poss√≠vel ter uma descri√ß√£o detalhada do conjunto de dados, incluindo seu <a href="https://arxiv.org/abs/1405.0312">paper</a> para aprofundamento), para avaliar o desempenho do modelo.</p>
<details>
<summary>
<em>Expandir c√≥digo</em>
</summary>
<pre class="python"><code>df_sample = pd.DataFrame({
  &#39;coco_url&#39;: [
    &#39;http://images.cocodataset.org/train2017/000000058822.jpg&#39;,
    &#39;http://images.cocodataset.org/train2017/000000530396.jpg&#39;,
    &#39;http://images.cocodataset.org/train2017/000000097916.jpg&#39;,
    &#39;http://images.cocodataset.org/train2017/000000418492.jpg&#39;,
    &#39;http://images.cocodataset.org/train2017/000000022304.jpg&#39;,
    &#39;http://images.cocodataset.org/train2017/000000295999.jpg&#39;,
    &#39;http://images.cocodataset.org/train2017/000000406616.jpg&#39;,
    &#39;http://images.cocodataset.org/train2017/000000370926.jpg&#39;,
    &#39;http://images.cocodataset.org/train2017/000000005612.jpg&#39;,
    &#39;http://images.cocodataset.org/train2017/000000146436.jpg&#39;
  ],
  &#39;caption&#39;: [
    &#39;A laptop sitting on a desk with a cell phone and mouse.&#39;,
    &#39;A black bear walking through the grass field.&#39;,
    &#39;a person who is surfing in the ocean.&#39;,
    &#39;A young boy standing on a sandy beach holding a flag.&#39;,
    &#39;A man surfing on a wave in the ocean.&#39;,
    &#39;A herd of cows, grazing in a field.&#39;,
    &#39;There is a cutting board and knife with chopped apples and carrots.&#39;,
    &#39;A long yellow school bus is parked on a city street.\n&#39;,
    &#39;A black and white horse standing in the middle of a field.&#39;,
    &#39;A man in a red jacket looking at his phone.&#39;
    ]})
    
# Fun√ß√£o para verificar se o caminho √© uma URL
def is_url(path):
    return path.startswith(&#39;http://&#39;) or path.startswith(&#39;https://&#39;)

# Fun√ß√£o simplificada para gerar o thumbnail e convert√™-lo em base64 diretamente
def process_image(path):
    try:
        if is_url(path):
            # Se for uma URL, baixar a imagem
            response = requests.get(path)
            response.raise_for_status()  # Verifica se houve algum erro no download
            image = Image.open(BytesIO(response.content))  # Abrir a imagem do conte√∫do da resposta
        else:
            # Se for um caminho local, abrir a imagem diretamente
            image = Image.open(path)
        
        # Criar uma miniatura da imagem (thumbnail) com tamanho m√°ximo de 150x150
        image.thumbnail((150, 150), Image.LANCZOS)
        
        # Salvar a imagem em um buffer de mem√≥ria e convert√™-la para base64
        with BytesIO() as buffer:
            image.save(buffer, &#39;jpeg&#39;)
            image_base64 = base64.b64encode(buffer.getvalue()).decode()
        
        # Retornar a string HTML com a imagem embutida no formato base64
        return f&#39;&lt;img src=&quot;data:image/jpeg;base64,{image_base64}&quot;&gt;&#39;
    
    except Exception as e:
        # Em caso de erro, retornar uma string vazia ou uma mensagem de erro
        return f&quot;&lt;p&gt;Erro ao carregar imagem: {e}&lt;/p&gt;&quot;

# Aplicar o processamento de imagens diretamente no DataFrame
df_sample[&#39;image&#39;] = df_sample[&#39;coco_url&#39;].map(process_image)  # Pode ser URL ou caminho local

# Exibir as legendas e imagens formatadas em HTML
HTML(df_sample[[&#39;image&#39;, &#39;coco_url&#39;, &#39;caption&#39;]].head().to_html(escape=False))</code></pre>
</details>
<p><br></p>
<p><img src="/post/2024-09-27-image-text-to-text/df1.png" style="width: 100%;"></p>
<p>Caso voc√™ precise de mais imagens para testar, tamb√©m √© poss√≠vel encontrar uma <a href="https://www.kaggle.com/datasets/awsaf49/coco-2017-dataset/data">vers√£o disponibilizada no Kaggle</a> .</p>
</div>
<div id="carregar-modelo" class="section level1">
<h1>Carregar modelo</h1>
<p>Utilizaremos uma vers√£o de 7 bilh√µes de par√¢metros do modelo <a href="https://huggingface.co/llava-hf/llava-1.5-7b-hf">‚ÄúLLaVA 1.5‚Äù</a> (Language and Vision Assistant), dispon√≠vel no HuggingFace (Uma plataforma onde a comunidade de Machine Learning colabora com modelos, dados e aplica√ß√µes) treinada para tarefas de gera√ß√£o de texto a partir de imagens.</p>
<pre class="python"><code>%%time

model_id = &quot;llava-hf/llava-1.5-7b-hf&quot;

# Configura√ß√£o de quantiza√ß√£o do modelo, que permite reduzir o uso de mem√≥ria sem 
# comprometer muito a precis√£o. Aqui estamos configurando para usar quantiza√ß√£o em 4 bits.
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,  
    bnb_4bit_use_double_quant=True,  
    bnb_4bit_quant_type=&quot;nf4&quot;,  
    bnb_4bit_compute_dtype=torch.bfloat16  
)

# Cria√ß√£o de um pipeline de processamento de imagens para gera√ß√£o de texto
# O pipeline √© configurado para a tarefa &quot;image-to-text&quot;
pipe = pipeline(
    &quot;image-to-text&quot;, 
    model=model_id, 
    model_kwargs={
        &quot;quantization_config&quot;: quantization_config,
        &quot;low_cpu_mem_usage&quot;: True
    }
)

# Carregar o processador associado respons√°vel por pr√©-processar
# as imagens de entrada e preparar os dados para serem inseridos no modelo
processor = AutoProcessor.from_pretrained(model_id)</code></pre>
<pre><code>CPU times: user 28.7 s, sys: 28.1 s, total: 56.8 s
Wall time: 6min 26s</code></pre>
<div class="w3-panel w3-pale-blue w3-border">
<p>¬† <strong>üìå Nota:</strong> A quantiza√ß√£o √© uma t√©cnica para reduzir o tamanho do modelo, perdendo um pouco de performance para otimizar o desempenho e rodar em m√°quinas com mem√≥ria limitada.</p>
</div>
</div>
<div id="prompt-engineering" class="section level1">
<h1>Prompt Engineering</h1>
<p>Uma ampla variedade de <a href="https://www.promptingguide.ai/pt">t√©cnicas</a> poderiam ser aplicadas para desenvolver <a href="https://python.langchain.com/docs/how_to/multimodal_prompts/">prompts</a> mais eficazes (inclusive com <a href="https://python.langchain.com/docs/introduction/">LangChain</a>, como fiz no <a href="https://gomesfellipe.github.io/post/2024-05-26-detec-o-de-linguagem-t-xica-com-o-llm-gemma-e-langchain/">√∫ltimo post</a>) ou especializar o modelo com ajuste fino visando obter resultados otimizados. No entanto, como este n√£o √© o foco do post, usarei um prompt simples e direto para estabelecer um baseline para avaliar as capacidades do modelo com o m√≠nimo de esfor√ßo.</p>
<pre class="python"><code># Cada valor em &quot;content&quot; tem que ser uma lista de dicion√°rio com os tipos (&quot;text&quot;, &quot;image&quot;) 
conversation = [
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: [
          {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this image in a few words:&quot;},
          {&quot;type&quot;: &quot;image&quot;},
        ]
    },
]

# Formata a conversa (que pode incluir texto e imagens) no formato correto que o modelo entende.
prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)</code></pre>
<p>O <a href="https://colab.research.google.com/drive/1qsl6cd2c8gGtEW1xV5io7S8NHh-Cp1TV?usp=sharing#scrollTo=JvvtplWDRvfu">prompt</a> deve ser especificado no seguinte formato:</p>
<pre><code>USER: &lt;image&gt;
&lt;prompt&gt;
ASSISTANT:</code></pre>
</div>
<div id="infer√™ncia" class="section level1">
<h1>Infer√™ncia</h1>
<p>Com o modelo devidamente configurado e o prompt ajustado, estamos prontos para executar o pipeline de infer√™ncia. A vantagem de utilizar <a href="https://huggingface.co/docs/transformers/main_classes/pipelines#multimodal">pipelines</a> √© que eles abstraem boa parte da codifica√ß√£o complexa, proporcionando uma interface simples e eficiente. Essa API vers√°til √© dedicada a v√°rias tarefas, como NER (Reconhecimento de Entidades), An√°lise de Sentimentos, Extra√ß√£o de Features e Question Answering.</p>
<pre class="python"><code>for i in tqdm(range(df_sample.shape[0])):
    
    # preparar objetos do loop
    coco_url = df_sample.iloc[i][&#39;coco_url&#39;]
    caption = df_sample.iloc[i][&#39;caption&#39;]
    index = df_sample.iloc[i].name
    
    # Obter imagem
    response = requests.get(coco_url)
    image = Image.open(BytesIO(response.content))
    
    # Realizar a infer√™ncia usando o pipeline e o prompt gerado
    outputs = pipe(image, prompt=prompt, generate_kwargs={&quot;max_new_tokens&quot;: 32})
    
    # Processar o texto gerado para extrair a parte relevante
    result = outputs[0][&#39;generated_text&#39;].split(&#39;ASSISTANT:&#39;, 1)[1].strip()
    
    # Adicionar o resultado da infer√™ncia √† nova coluna &#39;llm&#39; do DataFrame
    df_sample.loc[index, &#39;llm&#39;] = result</code></pre>
<pre><code>100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:41&lt;00:00,  4.20s/it]</code></pre>
<p>Ap√≥s a execu√ß√£o do modelo, veja como ficaram os resultados:</p>
<details>
<summary>
<em>Expandir c√≥digo</em>
</summary>
<pre class="python"><code># Fun√ß√£o para destacar as palavras
def highlight_diff(caption, llm):
    # Divide as frases em palavras
    caption_words = caption.replace(&quot;.&quot;, &quot;&quot;).split()
    llm_words = llm.replace(&quot;.&quot;, &quot;&quot;).split()
    
    # Converte as palavras em conjuntos para encontrar a interse√ß√£o
    caption_set = set(caption_words)
    llm_set = set(llm_words)
    
    # Calcula as palavras que n√£o est√£o na interse√ß√£o
    caption_highlighted = &quot; &quot;.join([f&#39;&lt;span style=&quot;color:red&quot;&gt;{word}&lt;/span&gt;&#39; if word not in llm_set else word for word in caption_words])
    llm_highlighted = &quot; &quot;.join([f&#39;&lt;span style=&quot;color:red&quot;&gt;{word}&lt;/span&gt;&#39; if word not in caption_set else word for word in llm_words])
    
    return caption_highlighted, llm_highlighted

# Aplica a fun√ß√£o a cada linha do DataFrame e cria novas colunas
df_sample[&#39;highlighted_caption&#39;], df_sample[&#39;highlighted_llm&#39;] = zip(*df_sample.apply(lambda row: highlight_diff(row[&#39;caption&#39;], row[&#39;llm&#39;]), axis=1))

# Exibir o DataFrame formatado com HTML
HTML(df_sample[[&#39;image&#39;, &#39;highlighted_caption&#39;, &#39;highlighted_llm&#39;]].to_html(escape=False))</code></pre>
</details>
<p><br></p>
<p><img src="/post/2024-09-27-image-text-to-text/df2.png" alt="extra√ß√£o de r√≥tulos descritivos de imagens com Llava" style="width: 100%;"></p>
<p>Destaquei em vermelho as palavras que diferem entre a legenda original do dataset e a previs√£o gerada pelo nosso modelo de linguagem.</p>
<div class="w3-panel w3-pale-yellow w3-border">
<p>¬† üí≠ Apesar de algumas diferen√ßas sutis entre as duas vers√µes, como ‚Äòlooking at his phone‚Äô e ‚Äòlooking at his <span style="color:red;">cell</span> phone‚Äô, a ideia principal permanece bastante coerente com o que vemos nas imagens. Em alguns casos, como no item 3, a descri√ß√£o gerada pelo modelo, ‚Äòholding a <span style="color:red;">kite</span>‚Äô, parece at√© mais apropriada do que a fornecida pelo dataset, ‚Äòholding a <span style="color:red;">flag</span>‚Äô.</p>
</div>
<p>Agora, o pr√≥ximo passo ser√° quantificar essas diferen√ßas de maneira num√©rica.</p>
</div>
<div id="avaliar-modelo" class="section level1">
<h1>Avaliar modelo</h1>
<p>Para medir a precis√£o das legendas geradas, aplicaremos quatro m√©tricas amplamente usadas:</p>
<ul>
<li><strong><a href="https://aclanthology.org/P02-1040.pdf">BLEU</a> (Bilingual Evaluation Understudy Score)</strong>: Amplamente utilizada para medir a qualidade de tradu√ß√µes autom√°ticas, mede a <strong>sobreposi√ß√£o de n-gramas</strong> entre a tradu√ß√£o gerada por um modelo e as tradu√ß√µes de refer√™ncia, atribuindo uma pontua√ß√£o que varia de 0 a 1 (aplica tamb√©m um fator de penaliza√ß√£o para evitar que tradu√ß√µes curtas sejam favorecidas);</li>
<li><strong><a href="https://aclanthology.org/W04-1013.pdf">ROUGE-L</a> (Recall-Oriented Understudy for Gisting Evaluation)</strong>: Muito utilizado em tarefa de sumariza√ß√£o de textos, considera a sequ√™ncia mais longa de palavras que aparecem em ambas as refer√™ncias e previs√µes, medindo a capacidade de preservar a <strong>ordem das palavras</strong>;</li>
<li><strong><a href="https://www.cs.cmu.edu/~alavie/METEOR/">METEOR</a> (Metric for Evaluation of Translation with Explicit ORdering)</strong>: Baseada na m√©dia harm√¥nica da precis√£o e recall de n-gramas, com recall ponderado mais alto do que a precis√£o. Essa m√©trica METEOR foi projetada para corrigir alguns dos problemas (como encontrar sin√¥nimos) nas m√©tricas BLEU e ROGUE;</li>
<li><strong><a href="https://huggingface.co/spaces/evaluate-metric/bertscore">BERTScore</a></strong>: Usa embeddings (representa√ß√µes sem√¢nticas) obtidas a partir do modelo BERT para comparar a similaridade sem√¢ntica entre as descri√ß√µes geradas e as de refer√™ncia.</li>
</ul>
<details>
<summary>
<em>Expandir c√≥digo</em>
</summary>
<pre class="python"><code># Fun√ß√µes para calcular as m√©tricas
def calcular_bleu(referencias, previsao):
    return sentence_bleu([referencias.split(&quot; &quot;)], previsao.split(&quot; &quot;),weights = [1])

def calcular_rouge(referencias, previsao):
    scorer = rouge_scorer.RougeScorer([&#39;rougeL&#39;], use_stemmer=True)
    return scorer.score(referencias, previsao)[&#39;rougeL&#39;].fmeasure

def calcular_meteor(referencias, previsao):
    return meteor_score([referencias.split(&quot; &quot;)], previsao.split(&quot; &quot;))

def calcular_bertscore(referencias, previsao):
    P, R, F1 = bert_score([previsao], [referencias], lang=&quot;en&quot;, verbose=True)
    return F1.mean().item()</code></pre>
</details>
<p><br></p>
<pre class="python"><code>%%capture

# Avaliar as amostras no DataFrame
resultados = []
for i, row in df_sample.iterrows():
    
    referencias = row[&#39;caption&#39;].replace(&quot;.&quot;, &quot;&quot;)
    previsao = row[&#39;llm&#39;].replace(&quot;.&quot;, &quot;&quot;)
    
    bleu = calcular_bleu(referencias, previsao)
    rouge = calcular_rouge(referencias, previsao)
    meteor = calcular_meteor(referencias, previsao)
    bert = calcular_bertscore(referencias, previsao)
    
    resultados.append([referencias, previsao, bleu, rouge, meteor, bert])

# Converter os resultados para um DataFrame
df_resultados = pd.DataFrame(resultados, columns=[&#39;caption&#39;, &#39;llm&#39;, &#39;BLEU&#39;, &#39;ROUGE&#39;, &#39;METEOR&#39;, &#39;BERTScore&#39;])</code></pre>
<p>Vejamos os resultados:</p>
<details>
<summary>
<em>Expandir c√≥digo</em>
</summary>
<pre class="python"><code># Configurar o tema do Seaborn
sns.set_theme(style=&quot;white&quot;, rc={&quot;axes.facecolor&quot;: (0, 0, 0, 0)})

# Reformatar o DataFrame para o formato long
df_long = df_resultados[[&#39;BLEU&#39;, &#39;ROUGE&#39;, &#39;METEOR&#39;, &#39;BERTScore&#39;]].melt(var_name=&quot;M√©trica&quot;, value_name=&quot;Valor&quot;)

# Calcular a m√©dia de cada m√©trica
mean_values = df_long.groupby(&#39;M√©trica&#39;)[&#39;Valor&#39;].mean().reset_index()

# Inicializar o objeto FacetGrid
pal = sns.cubehelix_palette(len(df_long[&#39;M√©trica&#39;].unique()), rot=-.25, light=.7)
g = sns.FacetGrid(df_long, row=&quot;M√©trica&quot;, hue=&quot;M√©trica&quot;, aspect=6, height=1.5, palette=pal)

# Desenhar as densidades
g.map(sns.kdeplot, &quot;Valor&quot;, 
      bw_adjust=.5, clip_on=False, 
      fill=True, alpha=1, linewidth=1.5)
g.map(sns.kdeplot, &quot;Valor&quot;, clip_on=False, color=&quot;w&quot;, lw=2, bw_adjust=.5)

# Adicionar linha de refer√™ncia
g.refline(y=0, linewidth=2, linestyle=&quot;-&quot;, color=None, clip_on=False)

# Fun√ß√£o para rotular o gr√°fico
def label(x, color, label):
    ax = plt.gca()
    # Localizar a m√©dia correspondente √† m√©trica
    mean_value = mean_values[mean_values[&#39;M√©trica&#39;] == label][&#39;Valor&#39;].values[0]
    ax.text(0, .4, f&quot;{label} (M√©dia: {mean_value:.2f})&quot;, fontweight=&quot;bold&quot;, color=color,
            ha=&quot;left&quot;, va=&quot;center&quot;, transform=ax.transAxes, fontsize=20)

g.map(label, &quot;Valor&quot;)

# Ajustar espa√ßamento entre subplots manualmente
g.figure.subplots_adjust(hspace=0.2)

# Remover detalhes desnecess√°rios dos eixos
g.set_titles(&quot;&quot;)
g.set(yticks=[], ylabel=&quot;&quot;)
g.despine(bottom=True, left=True)

# Configurar o eixo x
g.set(xlim=(0.4, 1), xticks=np.arange(0.4, 1.05, 0.1))  # Limites e ticks do eixo x

# Remover r√≥tulos do eixo x em cada subplot
for ax in g.axes.flat:
    ax.set_xlabel(&quot;&quot;)  # Remover r√≥tulo do eixo x
    ax.tick_params(axis=&#39;x&#39;, labelsize=16)  # Aumentar o tamanho da fonte dos ticks do eixo x

# Exibir o gr√°fico
plt.show()</code></pre>
</details>
<p><br></p>
<center>
<img src="/post/2024-09-27-image-text-to-text/metrics.png" alt="metricas da extra√ß√£o de r√≥tulos descritivos de imagens com Llava" style="width: 80%;">
</center>
<div class="w3-panel w3-pale-yellow w3-border">
<p>¬† <strong>üìå Insights ao Avaliar as M√©tricas do Modelo: </strong></p>
<ul>
<li><p>As m√©tricas baseadas em <strong>n-grams e na correspond√™ncia de palavras</strong> mostraram desempenho <strong>subestimado</strong>. Embora o modelo tenha apresentado algumas varia√ß√µes na escolha das palavras, as frases geradas mantiveram um sentido geral muito semelhante ao que √© retratado nas imagens.</p></li>
<li><p>Por outro lado, a m√©trica baseada em <strong>embeddings</strong>, que avalia o significado <strong>sem√¢ntico</strong> das frases, apresentou resultados <strong>significativamente superiores</strong>. Essa abordagem se mostrou mais congruente em avaliar a similaridade das descri√ß√µes geradas e a descri√ß√£o informada do conte√∫do visual das imagens.</p></li>
<li><p>√â importante ressaltar que nosso <strong>prompt</strong> foi mantido na forma <strong>mais simples poss√≠vel</strong> e que o conjunto de dados abrange um <strong>escopo bastante amplo</strong>. Com isso, acredito que o modelo ainda tem muito potencial para oferecer resultados ainda mais robustos, sem a necessidade de ajustes finos, em tarefas mais espec√≠ficas.</p></li>
</ul>
</div>
</div>
<div id="conclus√£o" class="section level1">
<h1>Conclus√£o</h1>
<p>O uso da GenAI com o modelo Llava oferece uma solu√ß√£o eficiente para a extra√ß√£o de features de imagens em Python, possibilitando a cria√ß√£o de descri√ß√µes ricas e detalhadas. Ao comparar a qualidade das sa√≠das com m√©tricas como BLEU, podemos garantir que o modelo esteja oferecendo resultados satisfat√≥rios para as necessidades do projeto.</p>
<p>Se voc√™ deseja automatizar processos de an√°lise de imagens, explorar a cria√ß√£o de modelos customizados ou otimizar a organiza√ß√£o de dados visuais, a utiliza√ß√£o de GenAI com modelos como o Llava pode ser um divisor de √°guas em seus projetos.</p>
<p>Se este conte√∫do foi √∫til, continue acompanhando o blog para mais tutoriais sobre intelig√™ncia artificial e Python!</p>
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias</h1>
<ul>
<li><a href="https://huggingface.co/llava-hf/llava-1.5-7b-hf" class="uri">https://huggingface.co/llava-hf/llava-1.5-7b-hf</a></li>
<li><a href="https://github.com/haotian-liu/LLaVA" class="uri">https://github.com/haotian-liu/LLaVA</a></li>
<li><a href="https://colab.research.google.com/drive/1qsl6cd2c8gGtEW1xV5io7S8NHh-Cp1TV?usp=sharing#scrollTo=6Bx8iu9jOssW" class="uri">https://colab.research.google.com/drive/1qsl6cd2c8gGtEW1xV5io7S8NHh-Cp1TV?usp=sharing#scrollTo=6Bx8iu9jOssW</a></li>
<li><a href="https://cocodataset.org/#explore" class="uri">https://cocodataset.org/#explore</a></li>
<li><a href="https://www.kaggle.com/datasets/awsaf49/coco-2017-dataset/" class="uri">https://www.kaggle.com/datasets/awsaf49/coco-2017-dataset/</a></li>
</ul>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2024-09-27-image-text-to-text/">Extra√ß√£o de informa√ß√µes de imagens com IA Generativa</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">chatgpt</category>
      <category domain="tag">data-science</category>
      <category domain="tag">genai</category>
      <category domain="tag">ia-generativa</category>
      <category domain="tag">inteligencia-artificial</category>
      <category domain="tag">llama</category>
      <category domain="tag">llama2</category>
      <category domain="tag">llava</category>
      <category domain="tag">llm</category>
      <category domain="tag">lmm</category>
    </item>
    <item>
      <title>Detec√ß√£o de Linguagem T√≥xica com o LLM Gemma e LangChain</title>
      <link>https://gomesfellipe.github.io/post/2024-05-26-detec-o-de-linguagem-t-xica-com-o-llm-gemma-e-langchain/</link>
      <pubDate>Sun, 26 May 2024 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2024-05-26-detec-o-de-linguagem-t-xica-com-o-llm-gemma-e-langchain/</guid>
      <description>Neste post utilizaremos o modelo Gemma de IA generativa do Google com framework LangChain auxiliando na tarefa de prompt engineering</description>
      <content:encoded>&lt;![CDATA[
        


<div id="caso-de-uso-de-ia-generativa-detec√ß√£o-de-linguagem-t√≥xica-em-m√≠dias-sociais" class="section level1">
<h1>Caso de Uso de IA Generativa: Detec√ß√£o de Linguagem T√≥xica em M√≠dias Sociais</h1>
<hr />
<p>Neste post, realizaremos a tarefa de detec√ß√£o de linguagem t√≥xica em m√≠dias sociais usando o modelo <a href="https://ai.google.dev/gemma?hl=pt-br">Gemma</a> de IA generativa do Google com o framework <a href="https://www.langchain.com/">LangChain</a>. Vamos explorar como o texto de entrada afeta a sa√≠da do modelo e faremos alguma engenharia de prompts para direcion√°-lo √† tarefa necess√°ria.</p>
</div>
<div id="setup" class="section level1">
<h1>Setup</h1>
<p>Utilizaremos o ambiente do Kaggle para desenvolvimento deste notebook, que disponibiliza a utiliza√ß√£o de GPUs. Atrav√©s do <em>Hardware Accelerator</em> utilizaremos a <a href="https://www.kaggle.com/docs/efficient-gpu-usage">NVIDIA TESLA P100 GPU</a>.</p>
<div id="instalar-e-carregar-dependencias" class="section level2">
<h2>Instalar e carregar dependencias</h2>
<p>Vamos instalar as bibliotecas <code>accelerate</code> e <code>bitsandbytes</code> que possibilitam a quantiza√ß√£o de LLMs e algumas bibliotecas do framework LangChain</p>
<pre class="python"><code>!pip install accelerate
!pip install -i https://pypi.org/simple/ bitsandbytes
!pip install langchain langchain_huggingface langchain_community langchain_chroma</code></pre>
</div>
<div id="carregar-bibliotecas" class="section level2">
<h2>Carregar bibliotecas</h2>
<pre class="python"><code>import pandas as pd
import torch 
import re

from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
from langchain_huggingface import HuggingFacePipeline
from langchain_core.prompts.few_shot import PromptTemplate, FewShotPromptTemplate
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_chroma import Chroma</code></pre>
</div>
<div id="carregar-fun√ß√µes-auxiliares" class="section level2">
<h2>Carregar fun√ß√µes auxiliares</h2>
<p>Carregar uma fun√ß√£o para limpeza simples dos tweets.</p>
<details>
<summary>
<em>Clique aqui para ver os c√≥digos</em>
</summary>
<pre class="python"><code>def clean_tweet(text):
    &quot;&quot;&quot;
    src: https://github.com/lrdsouza/told-br-classifier
    &quot;&quot;&quot;
    text = text.replace(&#39;rt @user&#39;, &#39;&#39;)
    text = text.replace(&#39;@user&#39;, &#39;&#39;)
    pattern = re.compile(&#39;[^a-zA-Z0-9\s√°√©√≠√≥√∫√†√®√¨√≤√π√¢√™√Æ√¥√ª√£√µ√ß√Å√â√ç√ì√ö√Ä√à√å√í√ô√Ç√ä√é√î√õ√É√ï√á]&#39;)
    text = re.sub(r&#39;http\S+&#39;, &#39;&#39;, text)
    text = pattern.sub(r&#39; &#39;, text)
    text = text.replace(&#39;\n&#39;, &#39; &#39;)
    text = &#39; &#39;.join(text.split())
    return text</code></pre>
</details>
<p>¬†</p>
</div>
</div>
<div id="carregar-dados" class="section level1">
<h1>Carregar dados</h1>
<hr />
<p>Vamos utilizar o conjunto de dados <a href="https://github.com/JAugusto97/ToLD-Br">TolD-br</a>, um recurso interessante para o estudo da toxicidade em conte√∫dos online em portugu√™s brasileiro. Este dataset foi utilizado na competi√ß√£o <a href="https://www.kaggle.com/competitions/ml-olympiad-toxic-language-ptbr-detection">ML Olympiad - Toxic Language (PTBR) Detection</a>, organizada pelo <a href="https://www.youtube.com/@tensorflowugsp">TensorFlow UGSP</a> no Kaggle este ano. A competi√ß√£o convidou entusiastas de dados, cientistas e pesquisadores a desenvolverem modelos de machine learning capazes de classificar tweets em portugu√™s brasileiro como t√≥xicos ou n√£o t√≥xicos.</p>
<pre class="python"><code>train = pd.read_csv(&quot;/kaggle/input/ml-olympiad-toxic-language-ptbr-detection/train (2).csv&quot;)
test = pd.read_csv(&quot;/kaggle/input/ml-olympiad-toxic-language-ptbr-detection/test (4).csv&quot;)
sub = pd.read_csv(&quot;/kaggle/input/ml-olympiad-toxic-language-ptbr-detection/sample_submission.csv&quot;)</code></pre>
<p>Selecionar uma amostra para auxiliar no desenvolvimento do prompt para utiliza√ß√£o em novos dados:</p>
<pre class="python"><code>valid = train.sample(n=100, random_state=123)</code></pre>
<div id="preparar-dados" class="section level2">
<h2>Preparar dados</h2>
<p>Aplicar limpeza b√°sica para preparar os tweets.</p>
<details>
<summary>
<em>Clique aqui para ver os c√≥digos</em>
</summary>
<pre class="python"><code>train[&#39;text&#39;] = train.text.apply(lambda x: clean_tweet(x))
valid[&#39;text&#39;] = valid.text.apply(lambda x: clean_tweet(x))
test[&#39;text&#39;] = test.text.apply(lambda x: clean_tweet(x))</code></pre>
</details>
<p>¬†</p>
</div>
</div>
<div id="carregar-modelo" class="section level1">
<h1>Carregar Modelo</h1>
<hr />
<p>Neste notebook, faremos uso de um modelo da fam√≠lia <a href="https://ai.google.dev/gemma?hl=pt-br">Gemma</a>, desenvolvida pelo Google, que consiste em modelos leves e de c√≥digo aberto constru√≠dos com base em pesquisas e tecnologias empregadas no desenvolvimento dos modelos <a href="https://gemini.google.com/">Gemini</a></p>
<div class="w3-panel w3-pale-yellow w3-border">
<p>¬† <strong>üìå Nota:</strong> Para utilizar o modelo √© necess√°rio consentir com a <a href="https://www.kaggle.com/models/google/gemma/license/consent?returnUrl=%2Fmodels%2Fgoogle%2Fgemma%2Ftransformers">licen√ßa do Gemma</a> com o preenchimento de um formul√°rio dispon√≠vel na <a href="https://www.kaggle.com/models/google/gemma">p√°gina do modelo</a>.</p>
</div>
<p>Utilizaremos a implementa√ß√£o do <a href="https://huggingface.co/google/gemma-7b-it">Gemma-7b-instruct</a>, que √© uma variante ajustada por instru√ß√£o (IT) que pode ser usada para bate-papo e/ou seguir instru√ß√µes.</p>
<pre class="python"><code># Caminho para o modelo dispon√≠vel pelo ambiente do Kaggle
model_path = &quot;/kaggle/input/gemma/transformers/1.1-7b-it/1/&quot;

# Definir configuracoes de quantizacao para reduzir 
# o tamanho do modelo perdendo pouca performance
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=&quot;nf4&quot;,
    bnb_4bit_compute_dtype=torch.bfloat16
)

# Instanciar o tokenizador do LLM
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Instanciar o LLM
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    quantization_config=quantization_config,
    low_cpu_mem_usage=True, 
    device_map=&quot;auto&quot;
)</code></pre>
<p>Vamos carregar tamb√©m um modelo de embedding que utilizaremos para auxiliar na constru√ß√£o do nosso prompt:</p>
<pre class="python"><code>embeddings = HuggingFaceEmbeddings(model_name=&quot;all-MiniLM-L6-v2&quot;)</code></pre>
<div id="preparar-modelo" class="section level2">
<h2>Preparar modelo</h2>
<p>Os modelos da Hugging Face podem ser facilmente executados localmente utilizando a classe <code>HuggingFacePipeline</code>. O <a href="https://huggingface.co/models">Hugging Face Model Hub</a> √© um reposit√≥rio que abriga mais de 120 mil modelos, 20 mil conjuntos de dados e 50 mil aplicativos de demonstra√ß√£o (Spaces), todos de c√≥digo aberto e dispon√≠veis publicamente. Esta plataforma online permite que as pessoas colaborem facilmente e construam modelos de machine learning juntas.</p>
<pre class="python"><code># Instanciar um pipeline transformers
pipe = pipeline(
    model=model,
    tokenizer=tokenizer,
    task=&quot;text-generation&quot;,
    max_new_tokens=1,
)

# Passar o pipeline para a classe do LangChain
llm = HuggingFacePipeline(pipeline=pipe)</code></pre>
</div>
</div>
<div id="prompt-engineering" class="section level1">
<h1>Prompt Engineering</h1>
<hr />
<p>Para resolver este problema, criaremos um template de prompt que utiliza a estrat√©gia few-shot, que pode ser constru√≠do a partir de um conjunto de exemplos. O conjunto de exemplos ser√° din√¢mico, sendo constru√≠do com base em tweets que possuem a maior similaridade semantica com o tweet de entrada.</p>
<div id="preparar-exemplos" class="section level2">
<h2>Preparar exemplos</h2>
<p>Selecionaremos os exemplos candidatos do conjunto de dados de treino que n√£o estejam no dataset de valida√ß√£o. Cada exemplo de entrada deve ser um dicion√°rio onde:</p>
<ul>
<li><code>key</code>: o nome das vari√°veis de inputs do prompt;</li>
<li><code>values</code>: os valores dos inputs.</li>
</ul>
<pre class="python"><code># indices de instancias que nao estao no dataset de validacao (evitar leak)
idx_train_examples = train.loc[~train.index.isin(valid.index)].index

# organizar a lista com os exemplos candidatos
examples = [{&#39;tweet&#39;: train.text[i], 
             &#39;label&#39;: str(train.label[i])} for i in idx_train_examples]</code></pre>
</div>
<div id="criar-template-para-os-exemplos-com-prompttemplate" class="section level2">
<h2>Criar template para os exemplos com <code>PromptTemplate</code></h2>
<p>Agora precisamos instanciar um <code>PromptTemplate</code> para nosso prompt, que recebe um template das instru√ß√µes que desejamos passar para o LLM e os inputs que alimentam este template:</p>
<pre class="python"><code>example_template = &quot;&quot;&quot;
Tweet: {tweet}
Label: {label}
&quot;&quot;&quot;

# Instanciar o exemplo de prompt 
example_prompt = PromptTemplate(
    input_variables=[&quot;tweet&quot;, &quot;label&quot;], 
    template=example_template
)</code></pre>
</div>
<div id="inserir-exemplos-com-exampleselector" class="section level2">
<h2>Inserir exemplos com <code>ExampleSelector</code></h2>
<p>Agora vamos instanciar <code>SemanticSimilarityExampleSelector</code> para selecionar exemplos com base em sua semelhan√ßa com a entrada. Ele usa um modelo de embedding para calcular a similaridade entre a entrada e os exemplos de few-shot, bem como um armazenamento de vetores <a href="https://www.trychroma.com/">Chroma</a> para realizar a pesquisa do vizinho mais pr√≥ximo de maneira eficiente.</p>
<pre class="python"><code>example_selector = SemanticSimilarityExampleSelector.from_examples(
    examples = examples,
    embeddings = embeddings,
    vectorstore_cls = Chroma,
    k=3,
)</code></pre>
<p>Aumentar o n√∫mero de vizinhos mais pr√≥ximos n√£o garantir√° necessariamente resultados melhores. Normalmente k=6 no m√°ximo j√° √© suficiente. Se n√£o conseguir bons resultados assim, j√° seria mais indicado realizar um ajuste fino mesmo.</p>
</div>
<div id="preparar-o-fewshotprompttemplate" class="section level2">
<h2>Preparar o <code>FewShotPromptTemplate</code></h2>
<p>Finalmente, vamos definir a formata√ß√£o para a apresenta√ß√£o dos exemplos e, em seguida, usar <code>FewShotPromptTemplate</code> para para gerar o template final que ser√° utilizado como prompt com base nos valores de entrada.</p>
<pre class="python"><code>prefix = &quot;&quot;&quot;The following tweets are written in Brazilian Portuguese. \n\
You are a tweet classifier that identifies \
toxic language as 1 and non-toxic language as 0. \n\
Here are some examples:&quot;&quot;&quot;

suffix = &quot;&quot;&quot;
Tweet: {tweet} 
Label: &quot;&quot;&quot;

prompt = FewShotPromptTemplate(
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix=prefix,
    suffix=suffix,
    example_separator=&#39;\n&#39;,
    input_variables=[&quot;tweet&quot;],
)</code></pre>
<p>Vejamos como ser√° a formata√ß√£o do prompt para a classifica√ß√£o de cada tweet:</p>
<pre class="python"><code>print(prompt.format(tweet=valid.head(1).text.values[0]))</code></pre>
<pre><code>## The following tweets are written in Brazilian Portuguese. 
## You are a tweet classifier that identifies toxic language as 1 and non-toxic language as 0. 
## Here are some examples:
## 
## Tweet: caralho eu tenho q fazer alguma coisa mt importante mas eu esqueci o que √© ent√£o n deve ser importante
## Label: 1
## 
## Tweet: caralho as pessoas fazem me sentir a pessoa mais bosta e odiada poss√≠vel eu t√¥ bem
## Label: 0
## 
## Tweet: tenho quase certeza que isso e um homem escroto fingindo ser mulher kkkkkkkkk por um momento eu tbm pensei nisso
## Label: 0
## 
## Tweet: vei se um filho faz isso cmg eu pego o sandu√≠che e enfio no cu dele
## Label: </code></pre>
</div>
<div id="definir-custom-output-parsers" class="section level2">
<h2>Definir <code>Custom Output Parsers</code></h2>
<p>Para concluir a cadeia, vamos definir uma fun√ß√£o que funcione como um <a href="https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/custom/">Custom Output Parser</a>, que ser√° respons√°vel por pegar a sa√≠da do LLM e transform√°-la no formato mais adequado para nosso caso. Precisamos apenas do √∫ltimo caractere que ser√° retornado pelo LLM.</p>
<pre class="python"><code>def parse(response):
    &quot;&quot;&quot;Retorna apenas o ultimo caracter da sa√≠da do LLM&quot;&quot;&quot;
    return int(response[-1:])</code></pre>
</div>
<div id="definir-cadeira-langchain" class="section level2">
<h2>Definir Cadeira LangChain</h2>
<p><a href="https://python.langchain.com/v0.1/docs/modules/chains/">Chains</a> referem-se √† sequ√™ncias de chamadas - seja para um LLM, uma etapa de pr√©-processamento de dados, <a href="https://python.langchain.com/v0.1/docs/modules/tools/">tools</a>, ou ainda etapas de p√≥s-processamento do output gerado pelo modelo. As cadeias constru√≠das desta forma s√£o boas porque oferecem suporte nativo a streaming, ass√≠ncrono e infer√™ncia em batchs para uso.</p>
<pre class="python"><code>chain = prompt | llm | parse</code></pre>
<p>Vamos testar o comportamento da nossa cadeia em 1 tweet:</p>
<pre class="python"><code>print(f&quot;&quot;&quot;Tweet: {valid.head(1).text.values[0]}
Label: {valid.head(1).label.values[0]}
Predict: {chain.invoke({&#39;tweet&#39;:  valid.head(1).text.values[0]})}&quot;&quot;&quot;)</code></pre>
<pre><code>## Tweet: vei se um filho faz isso cmg eu pego o sandu√≠che e enfio no cu dele
## Label: 1
## Predict: 1</code></pre>
<p>Claramente um conte√∫do t√≥xico e que foi classificado corretamente. Mas como queremos realizar a chamada da nossa cadeia para diversos tweets do dataset de test, utilizaremos o m√©todo <code>.batch()</code> que executa a cadeia para uma lista de entradas:</p>
<pre class="python"><code>%%time
valid[&#39;predict&#39;] = chain.batch([{&#39;tweet&#39;: x} for x in valid.text])</code></pre>
<pre><code>## CPU times: user 1min 26s, sys: 24.8 s, total: 1min 51s
## Wall time: 1min 50s</code></pre>
</div>
<div id="avaliar-resultados" class="section level2">
<h2>Avaliar resultados</h2>
<p>Como a m√©trica de avalia√ß√£o da competi√ß√£o era a acur√°cia, vamos dar uma olhada em como ficou a matriz de confus√£o:</p>
<details>
<summary>
<em>Clique aqui para ver o c√≥digo do gr√°fico</em>
</summary>
<pre class="python"><code># Calcular m√©tricas
cm = confusion_matrix(valid.label, valid.predict)
acc=accuracy_score(valid.label, valid.predict)

# Configura√ß√µes de estilo do seaborn
sns.set(font_scale=1.2)
plt.figure(figsize=(5, 3))

# Plotar Matriz de Confus√£o para o m√©todo Vader em ingl√™s
sns.heatmap(cm, annot=True, fmt=&#39;d&#39;, cmap=&#39;binary&#39;, cbar=False,vmin=0, vmax=50,
            xticklabels=[&#39;N√£o T√≥xico&#39;, &#39;T√≥xico&#39;], yticklabels=[&#39;N√£o T√≥xico&#39;, &#39;T√≥xico&#39;])
plt.title(f&#39;Matriz de Confus√£o\nAcur√°cia: {acc:.0%}&#39;, fontsize=22)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.xlabel(&#39;Previsto&#39;, fontsize=14)
plt.ylabel(&#39;Real&#39;, fontsize=14)
plt.show()</code></pre>
</details>
<p>¬†</p>
<center>
<img src="/post/2024-05-26-detec-o-de-linguagem-t-xica-com-o-llm-gemma-e-langchain/cm.png" />
</center>
</div>
</div>
<div id="conclus√£o" class="section level1">
<h1>Conclus√£o</h1>
<hr />
<p>Embora nosso objetivo n√£o fosse alcan√ßar a perfei√ß√£o em termos de acur√°cia, at√© que o resultado foi satisfat√≥rio, dado o potencial dessa ferramenta para resolver uma ampla gama de problemas com poucas modifica√ß√µes nos c√≥digos. Existem muitos outros caminhos a serem explorados (inclusive recomendo assistir √† <a href="https://www.youtube.com/watch?v=bzU_STGxj7o&amp;t=6s">live no YouTube</a> em que os vencedores apresentaram solu√ß√µes muito mais eficientes), nosso foco aqui foi praticar, aplicar e documentar alguns conceitos interessantes e √∫teis sobre LLMs e LangChain.</p>
</div>
<div id="referencias" class="section level1">
<h1>Referencias</h1>
<hr />
<ul>
<li><a href="https://www.kaggle.com/competitions/ml-olympiad-toxic-language-ptbr-detection" class="uri">https://www.kaggle.com/competitions/ml-olympiad-toxic-language-ptbr-detection</a></li>
<li><a href="https://www.kaggle.com/models/google/gemma/transformers" class="uri">https://www.kaggle.com/models/google/gemma/transformers</a></li>
<li><a href="https://huggingface.co/google/gemma-1.1-7b-it" class="uri">https://huggingface.co/google/gemma-1.1-7b-it</a></li>
<li><a href="https://python.langchain.com/v0.1/docs/modules/model_io/prompts/few_shot_examples/" class="uri">https://python.langchain.com/v0.1/docs/modules/model_io/prompts/few_shot_examples/</a></li>
</ul>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2024-05-26-detec-o-de-linguagem-t-xica-com-o-llm-gemma-e-langchain/">Detec√ß√£o de Linguagem T√≥xica com o LLM Gemma e LangChain</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category>Texto e NLP</category>
      <category domain="tag">chatgpt</category>
      <category domain="tag">classification</category>
      <category domain="tag">data-science</category>
      <category domain="tag">gemma</category>
      <category domain="tag">google</category>
      <category domain="tag">inteligencia-artificial</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">llm</category>
      <category domain="tag">machine-learning</category>
      <category domain="tag">prophet</category>
    </item>
    <item>
      <title>An√°lise de Sentimentos com um &#34;ChatGPT&#34; de C√≥digo Aberto</title>
      <link>https://gomesfellipe.github.io/post/2024-04-20-sentiment-analysis-llama2/</link>
      <pubDate>Sat, 20 Apr 2024 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2024-04-20-sentiment-analysis-llama2/</guid>
      <description>Como executar localmente o LLM pr√©-treinado de c√≥digo aberto Llama2 para realizar uma an√°lise de sentimentos em Python</description>
      <content:encoded>&lt;![CDATA[
        

<div id="TOC">
<ul>
<li><a href="#por-que-an%C3%A1lise-de-sentimentos" id="toc-por-que-an√°lise-de-sentimentos">Por que An√°lise de Sentimentos?</a></li>
<li><a href="#por-que-large-language-models" id="toc-por-que-large-language-models">Por que Large Language Models?</a></li>
<li><a href="#o-que-faremos-aqui" id="toc-o-que-faremos-aqui">O que faremos aqui?</a></li>
<li><a href="#m%C3%A3os-a-obra" id="toc-m√£os-a-obra">M√£os a obra!</a>
<ul>
<li><a href="#iniciar-ambiente-de-trabalho" id="toc-iniciar-ambiente-de-trabalho">Iniciar ambiente de trabalho</a></li>
<li><a href="#carregar-dados" id="toc-carregar-dados">Carregar dados</a></li>
<li><a href="#informa%C3%A7%C3%B5es-gerais" id="toc-informa√ß√µes-gerais">Informa√ß√µes gerais</a></li>
<li><a href="#an%C3%A1lise-explorat%C3%B3ria" id="toc-an√°lise-explorat√≥ria">An√°lise Explorat√≥ria</a></li>
<li><a href="#an%C3%A1lise-de-sentimentos" id="toc-an√°lise-de-sentimentos">An√°lise de Sentimentos</a></li>
</ul></li>
<li><a href="#resultado-final" id="toc-resultado-final">Resultado Final</a></li>
<li><a href="#conclus%C3%A3o-e-discuss%C3%A3o" id="toc-conclus√£o-e-discuss√£o">Conclus√£o e Discuss√£o</a></li>
<li><a href="#refer%C3%AAncias" id="toc-refer√™ncias">Refer√™ncias</a></li>
</ul>
</div>

<style>
.column4 {
  float: left;
  width: 33%;
  padding: 10px;
}
 
.column8 {
  float: left;
  width: 66%;
  padding: 10px;
}

.column6 {
  float: left;
  width: 50%;
  padding: 10px;
}

.row:after {
  content: "";
  display: table;
  clear: both;
}
</style>
<div id="por-que-an√°lise-de-sentimentos" class="section level2">
<h2>Por que An√°lise de Sentimentos?</h2>
<p>Compreender os sentimentos por tr√°s de grandes volumes de texto tornou-se essencial, pois em um mundo cada vez mais digitalizado, a capacidade de compreender as respostas e emo√ß√µes em larga escala das pessoas diante de produtos, eventos ou t√≥picos espec√≠ficos n√£o √© apenas valiosa por fornecer insights, mas tamb√©m se tornou uma necessidade para alavancar neg√≥cios e tornar-se cada vez mais competitivo.</p>
<blockquote>
<p>An√°lise de sentimento, tamb√©m chamada de minera√ß√£o de opini√£o, √© o campo de estudo que analisa as opini√µes, sentimentos, avalia√ß√µes, aprecia√ß√µes, atitudes e emo√ß√µes das pessoas em rela√ß√£o a entidades como produtos, servi√ßos, organiza√ß√µes, indiv√≠duos, quest√µes, eventos, t√≥picos e seus atributos. <a href="https://www.cambridge.org/de/universitypress/subjects/computer-science/artificial-intelligence-and-natural-language-processing/sentiment-analysis-mining-opinions-sentiments-and-emotions-2nd-edition?format=HB&amp;isbn=9781108486378">Liu 2020</a></p>
</blockquote>
</div>
<div id="por-que-large-language-models" class="section level2">
<h2>Por que Large Language Models?</h2>
<p>A abordagem comum para resolver problemas de NLP envolviam a aplica√ß√£o de <em>text mining</em>, <em>embeddings</em> como <em>word2vec</em> e <em>GloVe (Global Vectors for Word Representation)</em> e t√©cnicas de Machine Learning, onde modelos como <em>Random Forest</em>, <em>SVM</em>, <em>Naive Bayes</em>, <em>KNN</em>, <em>Ensembles</em> e at√© mesmo Regress√£o eram frequentemente utilizados para classificar textos. Al√©m disso, o uso de redes neurais recorrentes (<em>RNNs</em>) sempre foi uma alternativa valiosa, especialmente em situa√ß√µes que demandavam o processamento de dados sequenciais, sendo a <em>LSTM (Long Short-Term Memory)</em> uma variante eficaz para lidar com o desafio conhecido como <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem"><em>vanishing gradient</em></a>.</p>
<p>J√° no cen√°rio atual de modelos pr√©-treinados, o <em>BERT (Bidirectional Encoder Representations from Transformers)</em> tamb√©m teve bastante destaque nesse dom√≠nio antes da ascens√£o do <em>ChatGPT</em>, demonstrando a viabilidade como um m√©todo gerador de texto e mostraram o poder que as redes neurais t√™m para gerar longas sequ√™ncias de texto que antes pareciam inating√≠veis.</p>
<center>
<p><img src="/post/2024-04-20-sentiment-analysis-llama2/parameters_transformer_based_language_models.png" style="width:70.0%" /></br>
<small><a href="https://www.techtarget.com/searchenterpriseai/definition/GPT-3">GPT-3 supera seus antecessores em termos de contagem de par√¢metros</a></small></p>
</center>
<p>Embora j√° existam h√° algum tempo, os <em>LLMs</em> ganharam a m√≠dia atrav√©s do <em>ChatGPT</em>, interface de chat da OpenAI para modelos LLM GPT-3 lan√ßado em 2020, com 175 milh√µes de par√¢metros, que j√° teve uma s√©rie de avan√ßos significativos nos √∫ltimos anos como seu irm√£o maior, o GPT-4 lan√ßado em 2023 conta com incr√≠veis 100 <strong>tr√≠lh√µes</strong> de par√¢metros.</p>
<center>
<p><img src="/post/2024-04-20-sentiment-analysis-llama2/comparison-between-GPT-3-and-GPT-4.png" style="width:50.0%" /></br>
<small><a href="https://www.techtarget.com/searchenterpriseai/definition/GPT-3">The comparison between GPT-3 and GPT-4 based on the number of parameters used in their architecture</a></small></p>
</center>
<p>Modelos com mais de 100 bilh√µes de par√¢metros j√° podem ser considerados muito grandes, com conhecimento mundial muito rico. Esses modelos maiores conseguem ‚Äúaprender‚Äù ainda mais informa√ß√µes sobre muitas coisas sobre fisica, filosofia, ci√™ncia, programa√ß√£o, etc sendo cada vez mais √∫teis para ajudar em tarefas que envolvam conhecimento profundo ou raciocinio complexo, sendo um bom ‚Äúparceiro‚Äù para brainstorming.</p>
<div class="w3-panel w3-pale-red w3-border">
<p>¬† <strong>‚ö†Ô∏è Aten√ß√£o!</strong> </br>
Afirmar que maiores modelos s√£o sempre melhores n√£o √© verdade. O tempo de processamento, lat√™ncia e o custo tamb√©m ir√£o aumentar, por isso <a href="https://medium.com/@masteringllm/mistral-7b-is-187x-cheaper-compared-to-gpt-4-b8e5ee1c9fc2">abordagens alternativas</a> tamb√©m devem ser consideradas.</p>
</div>
<div id="como-funcionam-os-llms" class="section level3">
<h3>Como funcionam os LLMs?</h3>
<p>Os <em>LLMs</em> s√£o modelos de <em>Machine Learning</em> que usam algoritmos de <em>Deep Learning</em> para processar e compreender a linguagem natural, gerando texto de maneira eficaz. Esses modelos s√£o treinados com grandes volumes de dados da internet, adquirindo a capacidade de identificar padr√µes na composi√ß√£o de palavras e frases. A id√©ia b√°sica por tr√°s desses modelos √© que s√£o capazes de gerar texto prevendo repetidamente a pr√≥xima palavra oferecendo resultados r√°pidos e diversas aplica√ß√µes pr√°ticas em v√°rias √°reas</p>
<div id="aplica√ß√µes" class="section level4">
<h4>Aplica√ß√µes</h4>
<p>Diferentemente de uma ferramenta de busca como o Google, o ChatGPT n√£o recupera informa√ß√µes, mas cria frases e textos completos em tempo real com base no processamento de um imenso volume de dados, veja alguns exemplos de uso para diferentes tarefas:</p>
<div class="row">
<div id="escrita" class="section level5 column4">
<h5>‚úçÔ∏è <strong>Escrita:</strong></h5>
<ul>
<li>Colabora√ß√£o em brainstorming, sugerindo nomes;</li>
<li>Elabora√ß√£o de templates para comunicados e e-mails;</li>
<li>Tradu√ß√£o autom√°tica.</li>
</ul>
</div>
<div id="leitura" class="section level5 column4">
<h5>üìñ <strong>Leitura</strong>:</h5>
<ul>
<li>Revis√£o de textos;</li>
<li>Sumariza√ß√£o de artigos extensos;</li>
<li>An√°lise de sentimentos, possibilitando a cria√ß√£o de dashboards para acompnhamento ao longo do tempo.</li>
</ul>
</div>
<div id="conversa" class="section level5 column4">
<h5>üí¨ <strong>Conversa</strong>:</h5>
<ul>
<li>Di√°logos e aconselhamentos;</li>
<li>Coaching de carreira;</li>
<li>Planejamento de viagens;
Sugest√µes de receitas;</li>
<li>Conversa√ß√£o interativa com documentos PDF;</li>
<li>Atendimento ao cliente;</li>
<li>Realiza√ß√£o de pedidos.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div id="o-que-faremos-aqui" class="section level2">
<h2>O que faremos aqui?</h2>
<p>Nosso objetivo aqui √© realizar uma an√°lise de sentimentos para classificar senten√ßas como positivas ou negativas utilizando algum LLM pr√©-treinado. Embora a OpenAI j√° tenha sido uma organiza√ß√£o sem fins lucrativos que lan√ßava seus projetos como c√≥digo aberto, desde o lan√ßamento do ChatGPT ela se tornou uma empresa que mant√©m a propriedade de seus c√≥digos fonte. Isso significa que apesar da facilidade de criar aplica√ß√µes, modelos mais poderosos e relativamente baratos, desenvolvedores de IA n√£o podem modificar o GPT-3 para atender √†s nossa necessidades espec√≠ficas ou incorpor√°-lo em seus pr√≥prios projetos de maneira livre e gratuita. Portanto teremos de recorrer √† alternativas n√£o t√£o(*) <em>open source</em> como o <a href="https://huggingface.co/meta-llama"><em>Llama 2</em> da Meta</a> que permite total controle sobre o modelo, rodar em nosso pr√≥prio computador/servidor e n√≥s d√° o controle sobre a privacidade dos nossos dados.</p>
<div class="w3-panel w3-pale-red w3-border">
<p>(*) ‚ÄúC√≥digo aberto‚Äù ü§î </br>
N√£o √© totalmente c√≥digo aberto pois por mais que a Meta tenha disponibilizado o modelo treinado para uso livre, ele n√£o compartilha os dados de treinamento do modelo ou o c√≥digo usado para trein√°-lo.</p>
</div>
</div>
<div id="m√£os-a-obra" class="section level1">
<h1>M√£os a obra!</h1>
<div id="iniciar-ambiente-de-trabalho" class="section level2">
<h2>Iniciar ambiente de trabalho</h2>
<p>Primeiramente vamos carregar todas as dependencias necess√°rias para executar os c√≥digos a seguir:</p>
<pre class="python"><code>import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from PIL import Image
from nltk.corpus import stopwords
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score
from llama_cpp import Llama
from tqdm.notebook import tqdm
tqdm.pandas()</code></pre>
</div>
<div id="carregar-dados" class="section level2">
<h2>Carregar dados</h2>
<p>Utilizaremos uma vers√£o <a href="https://www.kaggle.com/datasets/luisfredgs/imdb-ptbr">traduzida do dataset IMdb para o portugu√™s</a>, um conjunto de dados do Internet Movie Database (IMDB), que √© uma das maiores e mais abrangentes bases de dados online sobre filmes e programas de televis√£o.</p>
<pre class="python"><code> #Importar todo conjunto de dados
df = pd.read_csv(&#39;input/imdb-reviews-pt-br.csv&#39;, index_col=&#39;id&#39;)
# Obter amostra de tamanho 100
_, df = train_test_split(df, test_size=100, random_state=42, shuffle=True)</code></pre>
</div>
<div id="informa√ß√µes-gerais" class="section level2">
<h2>Informa√ß√µes gerais</h2>
<p>Esse dataset inclui avalia√ß√µes e cr√≠ticas de filmes feitas por usu√°rios do IMDB, bem como informa√ß√µes sobre os pr√≥prios filmes, como t√≠tulo, ano de lan√ßamento, g√™nero, etc. Para nossa finalidade para tarefa de an√°lise de sentimentos, utilizaremos os seguintes dados:</p>
<table>
<colgroup>
<col width="6%" />
<col width="41%" />
<col width="40%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">id</th>
<th align="left">text_en</th>
<th align="left">text_pt</th>
<th align="center">sentiment</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">12534</td>
<td align="left">This was unusual: a modern-day film which‚Ä¶</td>
<td align="left">Isso era incomum: um filme moderno que era‚Ä¶</td>
<td align="center">pos</td>
</tr>
<tr class="even">
<td align="center">35447</td>
<td align="left">Some of my old friends suggested me to wat‚Ä¶</td>
<td align="left">Alguns dos meus velhos amigos sugeriram qu‚Ä¶</td>
<td align="center">neg</td>
</tr>
<tr class="odd">
<td align="center">20281</td>
<td align="left">What a pleasure. This is really a parody. ‚Ä¶</td>
<td align="left">Que prazer. Isto √© realmente uma par√≥dia. S‚Ä¶</td>
<td align="center">pos</td>
</tr>
<tr class="even">
<td align="center">‚Ä¶</td>
<td align="left">‚Ä¶</td>
<td align="left">‚Ä¶</td>
<td align="center">‚Ä¶</td>
</tr>
<tr class="odd">
<td align="center">34241</td>
<td align="left">WOW!I just was given this film from a frie‚Ä¶</td>
<td align="left">WOW! Acabei de receber este filme de um am‚Ä¶</td>
<td align="center">neg</td>
</tr>
<tr class="even">
<td align="center">12896</td>
<td align="left">This film offers many delights and surprise‚Ä¶</td>
<td align="left">Este filme oferece muitas del√≠cias e surp‚Ä¶</td>
<td align="center">pos</td>
</tr>
<tr class="odd">
<td align="center">19748</td>
<td align="left">Over the years Ive watched this movie many‚Ä¶</td>
<td align="left">Ao longo dos anos, assisti a esse filme mu‚Ä¶</td>
<td align="center">pos</td>
</tr>
</tbody>
</table>
<p>Onde:</p>
<ul>
<li><code>id</code>: Identificador;</li>
<li><code>text_en</code>: texto em ingl√™s;</li>
<li><code>text_pt</code>: texto em portugu√™s;</li>
<li><code>sentiment</code>: r√≥tulo do texto, que pode ser <code>pos</code> ou <code>neg</code>.</li>
</ul>
</div>
<div id="an√°lise-explorat√≥ria" class="section level2">
<h2>An√°lise Explorat√≥ria</h2>
<hr />
<div id="distribui√ß√£o-dos-sentimentos-na-amostra" class="section level3">
<h3>Distribui√ß√£o dos sentimentos na amostra</h3>
<p>Primeiro vamos entender como ficou distribu√≠da a propor√ß√£o dos sentimentos na amostra coletada:</p>
<details>
<summary>
<em>Clique aqui para ver o c√≥digo do gr√°fico</em>
</summary>
<pre class="python"><code># Contagem absoluta
contagem_absoluta = df[&#39;sentiment&#39;].value_counts()

# Contagem relativa
contagem_relativa = df[&#39;sentiment&#39;].value_counts(normalize=True) * 100

# Criar gr√°fico de barras
fig, ax = plt.subplots(figsize=(6, 4))
barras = plt.bar(contagem_absoluta.index, contagem_absoluta, color=[&#39;green&#39;, &#39;red&#39;])

# Adicionar texto nas barras
for barra, abs_value, rel_value in zip(barras, contagem_absoluta, contagem_relativa):
    yval = barra.get_height()
    ax.text(barra.get_x() + barra.get_width()/2, yval, f&#39;{abs_value} ({rel_value:.1f}%)&#39;,
            ha=&#39;center&#39;, va=&#39;bottom&#39;, color=&#39;black&#39;, fontsize=12)

# Adicionar r√≥tulos e t√≠tulo
plt.xlabel(&#39;Sentimento&#39;, fontsize=14)
plt.ylabel(&#39;Frequ√™ncia absoluta&#39;, fontsize=14)
plt.title(&#39;Quantidade de textos de cada sentimento \nem uma amostra de tamanho 100&#39;, fontsize=16, x=0.5, y=1.1)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)

# Remover bordas da parte superior e direita
ax.spines[&#39;top&#39;].set_visible(False)
ax.spines[&#39;right&#39;].set_visible(False)

# Ajustar layout
plt.tight_layout()

# Salvar imagem
plt.savefig(f&quot;img/freq_sentiment.png&quot;, bbox_inches=&#39;tight&#39;)

# Exibir o gr√°fico
plt.show()</code></pre>
</details>
<p>¬†</p>
<center>
<p><img src="/post/2024-04-20-sentiment-analysis-llama2/freq_sentiment.png" /></p>
</center>
<div class="w3-panel w3-pale-blue w3-border">
<p>¬† üìå <strong>Interpreta√ß√£o:</strong>
Coletei uma amostra aleat√≥ria simples de tamanho n=100 de todas as reviews que cont√©m aproximadamente metade de cada sentimento para diminuir o tempo computacional de execu√ß√£o no meu computador.</p>
</div>
</div>
<div id="palavras-mais-frequentes-para-cada-sentimento" class="section level3 tabset">
<h3>Palavras mais frequentes para cada sentimento</h3>
<p>N√∫vens de palavras das resenhas dos filmes que foram anotadas como positivos e como negativos nas duas linguas dispon√≠veis no dataset:</p>
<details>
<summary>
<em>Clique aqui para ver o c√≥digo das Wordclouds</em>
</summary>
<pre class="python"><code>def generate_wordcloud(df, language=&#39;en&#39;):
    # Definir stopwords para o idioma escolhido
    if language == &#39;en&#39;:
        stop_words_pos = stop_words_neg = set(stopwords.words(&#39;english&#39;))
        stop_words_pos.update([&quot;film&quot;, &quot;movie&quot;, &quot;one&quot;])
        stop_words_neg.update([&quot;character&quot;, &quot;like&quot;, &quot;really&quot;, &quot;make&quot;, &quot;see&quot;])
    elif language == &#39;pt&#39;:
        stop_words_pos = stop_words_neg = set(stopwords.words(&#39;portuguese&#39;))
        stop_words_pos.update([&quot;filme&quot;, &quot;filmes&quot;, &quot;todo&quot;, &quot;t√£o&quot;, &quot;pode&quot;, &quot;todos&quot;])
        stop_words_neg.update([&quot;filme&quot;, &quot;filmes&quot;, &quot;todo&quot;, &quot;t√£o&quot;, &quot;filme&quot;, &quot;coisa&quot;, &quot;realmente&quot;])
    else:
        raise ValueError(&quot;Language must be &#39;en&#39; or &#39;pt&#39;.&quot;)

    # Concatenar textos positivos e negativos
    txt_pos = &quot; &quot;.join(review for review in df[df.sentiment == &#39;pos&#39;][f&#39;text_{language}&#39;])
    txt_neg = &quot; &quot;.join(review for review in df[df.sentiment == &#39;neg&#39;][f&#39;text_{language}&#39;])

    # Carregar m√°scaras de imagem
    mask_pos = np.array(Image.open(f&quot;img/pos.png&quot;))
    mask_neg = np.array(Image.open(f&quot;img/neg.png&quot;))

    # Gerar nuvens de palavras positivas e negativas
    wordcloud_positivo = WordCloud(
        stopwords=stop_words_pos,
        random_state=42,
        background_color=&quot;white&quot;,
        color_func=lambda *args, **kwargs: &quot;green&quot;,
        contour_color=&#39;black&#39;,
        contour_width=1,
        max_font_size=100,
        min_font_size=15,
        max_words=200,
        mask=mask_pos
    ).generate(txt_pos)

    wordcloud_negativo = WordCloud(
        stopwords=stop_words_neg,
        random_state=42,
        background_color=&quot;white&quot;,
        color_func=lambda *args, **kwargs: &quot;red&quot;,
        contour_color=&#39;black&#39;,
        contour_width=1,
        max_font_size=100,
        min_font_size=15,
        max_words=200,
        mask=mask_neg
    ).generate(txt_neg)

    # Configura√ß√µes do plot
    plt.figure(figsize=(7, 14))

    # Plotar nuvem de palavras positivas
    plt.subplot(1, 2, 1)
    plt.imshow(wordcloud_positivo, interpolation=&#39;bilinear&#39;)
    plt.axis(&#39;off&#39;)
    plt.title(&#39;Positivo&#39;, fontsize=20, color=&#39;green&#39;)

    # Plotar nuvem de palavras negativas
    plt.subplot(1, 2, 2)
    plt.imshow(wordcloud_negativo, interpolation=&#39;bilinear&#39;)
    plt.axis(&#39;off&#39;)
    plt.title(&#39;Negativo&#39;, fontsize=20, color=&#39;red&#39;)

    # Ajustar layout
    plt.tight_layout()

    # Salvar a nuvem de palavras como imagem
    plt.savefig(f&quot;img/wordcloud_{language}.png&quot;, bbox_inches=&#39;tight&#39;)

    # Exibir a nuvem de palavras
    plt.show()
    
# Exemplo de uso para o idioma ingl√™s
generate_wordcloud(df, language=&#39;en&#39;)

# Exemplo de uso para o idioma portugu√™s
generate_wordcloud(df, language=&#39;pt&#39;)</code></pre>
</details>
<p>¬†</p>
<center>
<p><img src="/post/2024-04-20-sentiment-analysis-llama2/wordcloud_en.png" /></p>
<p>N√∫vem de palavras mais frequentes das resenhas em <strong>üá∫üá≤ Ingl√™s</strong></p>
</center>
<center>
<p><img src="/post/2024-04-20-sentiment-analysis-llama2/wordcloud_pt.png" /></p>
<p>N√∫vem de palavras mais frequentes das resenhas em <strong>üáßüá∑ Portugu√™s</strong></p>
</center>
<div class="w3-panel w3-pale-blue w3-border">
<p>¬† <strong>üìå Interpreta√ß√£o:</strong>
Como esperado, mesmo com a mudan√ßa na l√≠ngua, a frequ√™ncia das palavras √© exibida de maneira muito similar de acordo com cada sentimento.</p>
</div>
</div>
</div>
<div id="an√°lise-de-sentimentos" class="section level2">
<h2>An√°lise de Sentimentos</h2>
<hr />
<div id="fam√≠lia-llama-2-de-large-language-models-llms" class="section level3">
<h3>Fam√≠lia <em>Llama 2</em> de <em>Large Language Models</em> (<em>LLMs</em>)</h3>
<p>Nesta se√ß√£o, exploraremos o <a href="https://llama.meta.com/"><em>Llama 2</em></a>, um modelo de c√≥digo aberto, e discutiremos as vantagens e desvantagens em rela√ß√£o aos LLMs de c√≥digo fechado ou remotos.</p>
<div id="tamanho-do-modelo" class="section level4">
<h4>Tamanho do modelo</h4>
<p>Para saber qual modelo utilizar, primeiramente precisamos ter em mente algumas no√ß√µes sobre a quantidade de par√¢metros e tamanhos dos LLM. No geral:</p>
<div class="row">
<div class="column4">
<p><big><strong>1 Bilh√£o</strong>:</big></p>
<p>Bons em correspond√™ncia de padr√µes e algum conhecimento b√°sico do mundo (como por exemplo classificar avalia√ß√µes por sentimento)</p>
</div>
<div class="column4">
<p><big><strong>10 Bilh√µes</strong>:</big></p>
<p>Maior conhecimento mundial, conhecem mais fatos esot√©ricos sobre o mundo e melhoram em seguir instru√ß√µes b√°sicas (bom para chatbot para pedidos de comida);</p>
</div>
<div class="column4">
<p><big><strong>100+ Bilh√µes</strong>: </big></p>
<p>Muito grandes, com conhecimento mundial muito rico, saber√£o coisas sobre f√≠sica, filosofia, ci√™ncia e assim por diante e ser√£o melhores em racioc√≠nios complexos (tarefas que envolvem conhecimento profundo ou racioc√≠nio complexo, parceiro para brainstorming)</p>
</div>
</div>
<p>Para uma an√°lise de sentimentos simples, n√£o √© necess√°rio um modelo com 100 bilh√µes de par√¢metros. Modelos menores, como os com 7 bilh√µes de par√¢metros, podem ser suficientes e menos computacionalmente exigentes.</p>
</div>
<div id="c√≥digo-aberto-ou-fechado" class="section level4">
<h4>C√≥digo aberto ou fechado</h4>
<p>Embora pr√≥ximos, os LLMs de c√≥digo aberto ainda n√£o conseguem igualar o poder e a precis√£o dos aplicativos de c√≥digo fechado dispon√≠veis comercialmente, como <a href="https://openai.com/gpt-4">GPT-4</a> e <a href="https://gemini.google.com/app">Bard (Gemini)</a>. Mesmo sendo menos poderosos, existem alguns pr√≥s e contras pelos quais podemos pesar na hora de escolher a melhor op√ß√£o:</p>
<div class="row">
<div class="column6">
<p><big><strong>Open Source</strong></big></p>
<ul>
<li>Total controle sobre o modelo</li>
<li>Pode rodar em nosso pr√≥prio computador/servidor</li>
<li>Controle sobre a privacidade dos dados</li>
</ul>
</div>
<div class="column6">
<p><big><strong>Closed</strong></big></p>
<ul>
<li>F√°cil de criar aplica√ß√µes</li>
<li>Maiores e mais poderosos</li>
<li>Relativamente barato</li>
<li>Existe um certo risco de depender do fornecedor</li>
</ul>
</div>
</div>
<p>Utilizaremos a abordagem de c√≥digo aberto por ser mais pr√°tica para fins de estudos, pois al√©m de gratuita, n√£o exige internet, registros ou chaves de API.</p>
</div>
<div id="uso-remoto-ou-local" class="section level4">
<h4>Uso remoto ou local</h4>
<p>Podemos interagir com o modelo de linguagem grande (LLM) do Llama 2 via API da <a href="https://huggingface.co/">Hugging Face</a>, seguindo as instru√ß√µes do <a href="https://huggingface.co/meta-llama">reposit√≥rio oficial da Meta</a> ou podemos baixar os arquivos do modelo em formato GGML para o <a href="https://huggingface.co/meta-llama/Llama-2-7b-chat-hf">Llama 2 7B Chat do Meta Llama 2</a>. Os formatos GGML s√£o utilizados para infer√™ncia de CPU + GPU usando o principamente o pacote <a href="https://pypi.org/project/llama-cpp-python/">llama-cpp-python</a>.</p>
<p>Para mais informa√ß√µes sobre como configurar o modelo consulte <a href="https://swharden.com/blog/2023-07-29-ai-chat-locally-with-python/">este link</a></p>
<pre class="python"><code>def load_llama_model(model_path=&quot;./input/llama-2-7b-chat.ggmlv3.q2_K.bin&quot;, language=&#39;en&#39;, seed=42):
    # Determinar o tamanho da janela de contexto com base no idioma
    if language == &#39;en&#39;:
        context_window = df.text_en.map(len).max()
    elif language == &#39;pt&#39;:
        context_window = df.text_pt.map(len).max()
    else:
        raise ValueError(&quot;Language must be &#39;en&#39; or &#39;pt&#39;.&quot;)

    # Carregar o modelo Llama
    return Llama(model_path=model_path,
                 verbose=False,
                 n_ctx=context_window,
                 seed=seed)</code></pre>
<p>Para obter os melhores resultados, devemos ser o mais claro e espec√≠ficos poss√≠vel nas intera√ß√µes. Por√©m devemos iniciar com um prompt simples e r√°pido para ir direcionando o modelo na dire√ß√£o desejada e avaliando os resultados obtidos e ajustando gradualmente o prompt para refinar e aprimorar a resposta desejada</p>
<pre class="python"><code>def classify_sentiment_llama(text, llama_model):
    # Construir a prompt para o modelo Llama
    prompt = f&#39;&#39;&#39; \
    Q: Answer with just one word, \
    does the following text express a \
    positive or negative feeling? \
    {text} \
    A:&#39;&#39;&#39;
    # Obter a sa√≠da do modelo Llama
    output = llama_model(prompt, max_tokens=3)
    return output[&quot;choices&quot;][0][&quot;text&quot;]</code></pre>
<p>Com nosso prompt definido, j√° podemos carregar o modelo:</p>
<pre class="python"><code># Carregar o modelo Llama para o idioma desejado
llama_model = load_llama_model(language=&#39;en&#39;)</code></pre>
<pre><code>## llama.cpp: loading model from ./llama-2-7b-chat.ggmlv3.q2_K.bin
## llama_model_load_internal: format     = ggjt v3 (latest)
## llama_model_load_internal: n_vocab    = 32000
## llama_model_load_internal: n_ctx      = 4320
## llama_model_load_internal: n_embd     = 4096
## llama_model_load_internal: n_mult     = 256
## llama_model_load_internal: n_head     = 32
## llama_model_load_internal: n_head_kv  = 32
## llama_model_load_internal: n_layer    = 32
## llama_model_load_internal: n_rot      = 128
## llama_model_load_internal: n_gqa      = 1
## llama_model_load_internal: rnorm_eps  = 5.0e-06
## llama_model_load_internal: n_ff       = 11008
## llama_model_load_internal: freq_base  = 10000.0
## llama_model_load_internal: freq_scale = 1
## llama_model_load_internal: ftype      = 10 (mostly Q2_K)
## llama_model_load_internal: model size = 7B
## llama_model_load_internal: ggml ctx size =    0.08 MB
## llama_model_load_internal: mem required  = 2733.66 MB (+ 2160.00 MB per state)
## llama_new_context_with_model: kv self size  = 2160.00 MB
## llama_new_context_with_model: compute buffer total size =  295.35 MB</code></pre>
<p>Ap√≥s instanciar o modelo, basta aplic√°-lo em nossa base de dados. (apliquei o mesmo modelo tanto para as reviews e portugu√™s quanto em ingl√™s).</p>
<pre class="python"><code>df[&#39;sentiment_llm_en&#39;] = df.text_en.progress_apply(lambda x: classify_sentiment_llama(x, llama_model))</code></pre>
<p><img src="/post/2024-04-20-sentiment-analysis-llama2/load_en.png" /></p>
<p>Como este modelo √© o mais b√°sico e n√£o alteramos nenhum par√¢metro (como por exemplo <code>temperature</code>, que determina se o output ser√° mais aleat√≥rio ou mais previs√≠vel) pode ser que a sa√≠da n√£o saia padronizada e necessite de algum p√≥s-processamento. Vejamos como foram os outputs do LLM:</p>
<details>
<summary>
<em>Clique aqui para ver o c√≥digo do gr√°fico</em>
</summary>
<pre class="python"><code># Contagem da frequ√™ncia das classifica√ß√µes
sentiment_llm_counts = df.groupby(&#39;sentiment&#39;).sentiment_llm_en.value_counts().reset_index(name=&#39;n&#39;)

# Organizar as categorias pela frequ√™ncia total
order = df.sentiment_llm_en.value_counts().reset_index(name=&#39;n&#39;)
order = order.sort_values(by=&#39;n&#39;, ascending=False)[&#39;index&#39;]

# Configura√ß√µes de estilo do seaborn
sns.set(style=&quot;whitegrid&quot;)

# Criar o gr√°fico de barras
plt.figure(figsize=(12, 4))
ax = sns.barplot(x=sentiment_llm_counts.sentiment_llm_en, y=sentiment_llm_counts.n, hue=sentiment_llm_counts.sentiment, order=order, palette=[&quot;red&quot;, &quot;green&quot;])

# Adicionar r√≥tulos e t√≠tulo
plt.ylim([0, 25])
plt.xticks(fontsize=12, rotation=90)
plt.yticks(fontsize=12)
ax.set_xlabel(&#39;Anota√ß√£o de sentimento das resenhas&#39;, fontsize=14)
ax.set_ylabel(&#39;Frequ√™ncia&#39;, fontsize=14)
ax.set_title(&#39;Frequ√™ncia dos sentimentos classificados pelo LLM em Ingl√™s\nem rela√ß√£o aos sentimentos j√° anotados da base&#39;, fontsize=20)

# Adicionar anota√ß√µes nas barras
for p in ax.patches:
    ax.annotate(f&#39;{p.get_height()}&#39;, (p.get_x() + p.get_width() / 2., p.get_height()),
                ha=&#39;center&#39;, va=&#39;baseline&#39;, fontsize=10, color=&#39;black&#39;, xytext=(0, 5),
                textcoords=&#39;offset points&#39;)

plt.legend(loc=&quot;upper right&quot;, title = &quot;Label real&quot;)

# Remover bordas da parte superior e direita
ax.spines[&#39;top&#39;].set_visible(False)
ax.spines[&#39;right&#39;].set_visible(False)
ax.grid(False)

# Salvar a nuvem de palavras como imagem
plt.savefig(f&quot;img/freq_class_llm_en.png&quot;, bbox_inches=&#39;tight&#39;)

# Exibir o gr√°fico
plt.show()</code></pre>
</details>
<!-- &nbsp; -->
<center>
<img src="/post/2024-04-20-sentiment-analysis-llama2/freq_class_llm_en.png" />
</center>
<div class="w3-panel w3-pale-blue w3-border">
<p>¬† <strong>üìå Interpreta√ß√£o:</strong>
√â poss√≠vel observar que o modelo pr√©-treinado conseguiu reconhecer de maneira bastante coerente o sentimento dos trechos para as categorias <code>pos</code> e <code>neg</code>, por√©m, n√£o vieram padronizadas exatamente como solicitamos ao modelo.</p>
</div>
<p>Como a sa√≠da n√£o foi padronizada, vamos realizar algum p√≥s-processamento para padronizar as classes como <code>pos</code> ou <code>neg</code> para possibilitar avaliar o desempenho do modelo com base em m√©tricas de classifica√ß√£o.</p>
<pre class="python"><code>conditions = [
    (df.sentiment_llm_en.str.contains(&#39;(?i)(?:pos|fun)&#39;)),
    (df.sentiment_llm_en.str.contains(&#39;(?i)(?:neg|horrible|melanchol)&#39;))
]
pd.crosstab(df.sentiment, np.select(conditions, [&#39;pos&#39;, &#39;neg&#39;], default=&#39;other&#39;))</code></pre>
<p>Com os outputs padronizados em duas classes, podemos verificar como foi a acur√°cia do modelo.</p>
</div>
<div id="desempenho" class="section level4">
<h4>Desempenho</h4>
<p>Como estamos diante de um problema de classifica√ß√£o, avaliaremos o desempenho do modelo com matrizes de confus√£o para entender a as taxas de acerto e calcular a acur√°cia pois o dataset √© balanceado.</p>
<details>
<summary>
<em>Clique aqui para ver o c√≥digo do gr√°fico</em>
</summary>
<pre class="python"><code># Matrizes de Confus√£o
conditions = [
    (df.sentiment_llm_en.str.contains(&#39;(?i)(?:pos|fun|good|comedy)&#39;)),
    (df.sentiment_llm_en.str.contains(&#39;(?i)(?:neg|melanchol|absurd|horrible)&#39;))
]
cm_llm_en = confusion_matrix(df.sentiment, np.select(conditions, [&#39;pos&#39;, &#39;neg&#39;], default=&#39;other&#39;))
accuracy_llm_en = accuracy_score(df.sentiment, np.select(conditions, [&#39;pos&#39;, &#39;neg&#39;], default=&#39;other&#39;))

conditions = [
    (df.sentiment_llm_pt.str.contains(&#39;(?i)(?:pos)&#39;)),
    (df.sentiment_llm_pt.str.contains(&#39;(?i)(?:neg|horr√≠vel)&#39;))
]
cm_llm_pt = confusion_matrix(df.sentiment, np.select(conditions, [&#39;pos&#39;, &#39;neg&#39;], default=&#39;other&#39;))
accuracy_llm_pt = accuracy_score(df.sentiment, np.select(conditions, [&#39;pos&#39;, &#39;neg&#39;], default=&#39;other&#39;))

# Configura√ß√µes de estilo do seaborn
sns.set(font_scale=1.2)
plt.figure(figsize=(12, 5))

# Plotar Matriz de Confus√£o para o modelo de LLM em ingl√™s
plt.subplot(1, 2, 1)
sns.heatmap(cm_llm_en, annot=True, fmt=&#39;d&#39;, cmap=&#39;binary&#39;, cbar=False, vmin=0, vmax=50,
            xticklabels=[&#39;Negativo&#39;, &#39;Neutro&#39;, &#39;Positivo&#39;], yticklabels=[&#39;Negativo&#39;, &#39;Neutro&#39;, &#39;Positivo&#39;])
plt.title(f&#39;Matriz de Confus√£o (Vader - Ingl√™s)\nAcur√°cia: {accuracy_llm_en:.0%}&#39;, fontsize=22)
plt.xlabel(&#39;Previsto&#39;, fontsize=14)
plt.ylabel(&#39;Real&#39;, fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)

# Plotar Matriz de Confus√£o para o modelo de LLM em portugu√™s
plt.subplot(1, 2, 2)
sns.heatmap(cm_llm_pt, annot=True, fmt=&#39;d&#39;, cmap=&#39;binary&#39;, cbar=False,vmin=0, vmax=50,
            xticklabels=[&#39;Negativo&#39;, &#39;Neutro&#39;, &#39;Positivo&#39;], yticklabels=[&#39;Negativo&#39;, &#39;Neutro&#39;, &#39;Positivo&#39;])
plt.title(f&#39;Matriz de Confus√£o (Vader - Portugu√™s)\nAcur√°cia: {accuracy_llm_pt:.0%}&#39;, fontsize=22)
plt.xlabel(&#39;Previsto&#39;, fontsize=14)
plt.ylabel(&#39;Real&#39;, fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)

# Ajustar layout
plt.tight_layout()

# Salvar a nuvem de palavras como imagem
plt.savefig(f&quot;img/cm_llm.png&quot;, bbox_inches=&#39;tight&#39;)

# Exibir o gr√°fico
plt.show()</code></pre>
</details>
<!-- &nbsp; -->
<center>
<img src="/post/2024-04-20-sentiment-analysis-llama2/cm_llm2.png" />
</center>
<div class="w3-panel w3-pale-blue w3-border">
<p>¬† <strong>üìå Interpreta√ß√£o:</strong>
A acur√°cia geral para a l√≠ngua Inglesa foi superior quando aplicado o mesmo modelo para a l√≠ngua portuguesa. Vale lembrar que este modelo foi treinado em Ingl√™s e estamos utilizado a menor das op√ß√µes.</p>
</div>
<p>O desempenho deste modelo √© muito interessante, principalmente por j√° ser pr√© treinado, n√£o sendo necess√°rio gastar tanto tempo na sua constru√ß√£o mas para afirmar que este modelo √© bom precisamos entender qual seria o resultado para resolver este problemas se utilizassemos a abordagem mais simples poss√≠vel.</p>
</div>
</div>
<div id="vader" class="section level3">
<h3>Vader</h3>
<!-- ## Baseline -->
<p>O <em><strong>VADER</strong> (Valence Aware Dictionary and sEntiment Reasoner)</em> √© uma abordagem mais simples e r√°pida em compara√ß√£o aos LLMs. N√£o requer o treinamento de um modelo, mas depende de l√©xicos de palavras relacionadas a sentimentos. Pode ser facilmente utilizado via bibliotecas de c√≥digo aberto em Python, como <a href="https://pypi.org/project/vaderSentiment/">vaderSentiment</a> para ingl√™s e <a href="https://github.com/rafjaa/LeIA">LeIA (L√©xico para Infer√™ncia Adaptada)</a> para portugu√™s.</p>
<p>A abordagem √© direta: no l√©xico (uma cole√ß√£o de palavras), cada palavra j√° possui uma nota atribu√≠da. Ao passar um documento (frase), retorna um dicion√°rio com o escore de polaridade com base no escore das palavras no texto. O dicion√°rio inclui o valor do sentimento geral normalizado (<code>compound</code>), variando de -1 (extremamente negativo) a +1 (extremamente positivo). Esse valor pode ser usado para descrever o sentimento predominante no texto, considerando os seguintes limites:</p>
<ul>
<li>Sentimento <span style="color: green;">positivo</span>: <code>compound</code> &gt;= 0.05</li>
<li>Sentimento <span style="color: red;">negativo</span>: <code>compound</code> &lt;= -0.05</li>
<li>Sentimento <span style="color: orange;">neutro</span>: (<code>compound</code> &gt; -0.05) e (<code>compound</code> &lt; 0.05)</li>
</ul>
<details>
<summary>
<em>Clique aqui para ver a fun√ß√£o utilizada para classificar o sentimento com base no escore <code>compound</code></em>
</summary>
<pre class="python"><code># Fun√ß√£o para classificar o sentimento com base no compound score
def classify_sentiment_vader(text, language=&#39;en&#39;):

    # Definir m√©todo que ser√° utilizado
    if language==&#39;en&#39;:
        from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
    elif language == &#39;pt&#39;:
        from leia import SentimentIntensityAnalyzer
    else:
        raise ValueError(&quot;Language must be &#39;en&#39; or &#39;pt&#39;.&quot;)

    # Instanciar a ferramenta para an√°lise de sentimentos
    analyzer = SentimentIntensityAnalyzer()
    # Realiza a an√°lise de sentimentos e obt√©m o compound score
    compound_score = analyzer.polarity_scores(text)[&#39;compound&#39;]
    # Classifica o sentimento com base no compound score
    if compound_score &gt;= 0.05:
        return &#39;pos&#39;
    elif compound_score &lt;= -0.05:
        return &#39;neg&#39;
    else:
        return &#39;neu&#39;

# Criando uma nova coluna &#39;sentimento_vader&#39;
df[&#39;sentiment_vader_en&#39;] = df.text_en.apply(lambda x: classify_sentiment_vader(x, &#39;en&#39;))
df[&#39;sentiment_vader_pt&#39;] = df.text_pt.apply(lambda x: classify_sentiment_vader(x, &#39;pt&#39;))</code></pre>
</details>
<!-- &nbsp; -->
<p>A execu√ß√£o do c√≥digo √© bem r√°pida, sendo √∫til para refer√™ncia como baseline ou em casos em que temos baixo recurso computacional e um grande volume de dados para classificar.</p>
<div id="desempenho-1" class="section level4">
<h4>Desempenho</h4>
<p>Como estamos diante de um problema de classifica√ß√£o, avaliaremos o desempenho do modelo com matrizes de confus√£o para entender a as taxas de acerto e calcular a acur√°cia pois o dataset √© balanceado.</p>
<details>
<summary>
<em>Clique aqui para ver o c√≥digo do gr√°fico</em>
</summary>
<pre class="python"><code># Matrizes de Confus√£o
cm_vader_en = confusion_matrix(df.sentiment, df.sentiment_vader_en)
cm_vader_pt = confusion_matrix(df.sentiment, df.sentiment_vader_pt)

# Acur√°cias
accuracy_vader_en = accuracy_score(df.sentiment, df.sentiment_vader_en)
accuracy_vader_pt = accuracy_score(df.sentiment, df.sentiment_vader_pt)

# Configura√ß√µes de estilo do seaborn
sns.set(font_scale=1.2)
plt.figure(figsize=(12, 5))

# Plotar Matriz de Confus√£o para o m√©todo Vader em ingl√™s
plt.subplot(1, 2, 1)
sns.heatmap(cm_vader_en, annot=True, fmt=&#39;d&#39;, cmap=&#39;binary&#39;, cbar=False,vmin=0, vmax=50,
            xticklabels=[&#39;Negativo&#39;, &#39;Neutro&#39;, &#39;Positivo&#39;], yticklabels=[&#39;Negativo&#39;, &#39;Neutro&#39;, &#39;Positivo&#39;])
plt.title(f&#39;Matriz de Confus√£o (Vader - Ingl√™s)\nAcur√°cia: {accuracy_vader_en:.0%}&#39;, fontsize=22)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.xlabel(&#39;Previsto&#39;, fontsize=14)
plt.ylabel(&#39;Real&#39;, fontsize=14)

# Plotar Matriz de Confus√£o para o m√©todo Vader em portugu√™s
plt.subplot(1, 2, 2)
sns.heatmap(cm_vader_pt, annot=True, fmt=&#39;d&#39;, cmap=&#39;binary&#39;, cbar=False,vmin=0, vmax=50,
            xticklabels=[&#39;Negativo&#39;, &#39;Neutro&#39;, &#39;Positivo&#39;], yticklabels=[&#39;Negativo&#39;, &#39;Neutro&#39;, &#39;Positivo&#39;])
plt.title(f&#39;Matriz de Confus√£o (Vader - Portugu√™s)\nAcur√°cia: {accuracy_vader_pt:.0%}&#39;, fontsize=22)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.xlabel(&#39;Previsto&#39;, fontsize=14)
plt.ylabel(&#39;Real&#39;, fontsize=14)

# Ajustar layout
plt.tight_layout()

# Salvar a nuvem de palavras como imagem
plt.savefig(f&quot;img/cm_vader.png&quot;, bbox_inches=&#39;tight&#39;)

# Exibir o gr√°fico
plt.show()</code></pre>
</details>
<!-- &nbsp; -->
<center>
<img src="/post/2024-04-20-sentiment-analysis-llama2/cm_vader.png" />
</center>
<div class="w3-panel w3-pale-blue w3-border">
<p>¬† <strong>üìå Interpreta√ß√£o:</strong> A acur√°cia geral do m√©todo foi praticamente o mesmo para ambas as linguas. Na lingua inglesa observamos mais casos de falsos positivos (22%), j√° na lingua portuguesa observamos mais casos de falsos negativos (14%).</p>
</div>
<p>Essa abordagem √© boa para ser utilizada como baseline pois quase todas as abordagens tradicionais de Machine Learning para a tarefa de an√°lise de sentimentos necessitam de tempo para desenvolvimento, treino, valida√ß√£o e sustenta√ß√£o de modelos.</p>
</div>
</div>
</div>
</div>
<div id="resultado-final" class="section level1">
<h1>Resultado Final</h1>
<hr />
<p>Avaliamos o desempenho de ambas as abordagens para determinar se o uso do LLM justificou-se em compara√ß√£o com a abordagem mais simples para a execu√ß√£o da tarefa de an√°lise de sentimentos.</p>
<details>
<summary>
<em>Clique aqui para ver o c√≥digo do gr√°fico</em>
</summary>
<pre class="python"><code>models = (
    &quot;Ingl√™s&quot;,
    &quot;Portugu√™s&quot;,
)
weight_counts = {
    &quot;Vader&quot;: np.array([accuracy_vader_en,
                       accuracy_vader_pt]),
    &quot;LLM&quot;: np.array([accuracy_llm_en-accuracy_vader_en,
                     accuracy_llm_pt-accuracy_vader_pt]),
}

fig, ax = plt.subplots()
bottom = np.zeros(2)
colors=[&quot;#b4dbe6&quot;, &quot;#024b7a&quot;]
for (boolean, weight_count), col in zip(weight_counts.items(), colors):
    p = ax.bar(models, weight_count, width=0.5, label=boolean, bottom=bottom, color=col)
    bottom += weight_count

# Formatar eixos
plt.ylim([0, 1.1])
plt.xlabel(&#39;Idioma das resenhas dos filmes&#39;, fontsize=14)
plt.ylabel(&#39;Ganho de Acur√°cia&#39;, fontsize=14)
plt.title(&quot;Compara√ß√£o do ganho de acur√°cia \ndo LLM em rela√ß√£o ao Vader&quot;, fontsize=16, x=0.5)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)

# Legenda
ax.legend(loc=&quot;upper right&quot;, title=&#39;M√©todo utilizado&#39;)
#specify order of items in legend
handles, labels = plt.gca().get_legend_handles_labels()
order = [1, 0]
plt.legend([handles[idx] for idx in order],[labels[idx] for idx in order])

accs=[x*100 for x in [accuracy_vader_en, accuracy_vader_pt, accuracy_llm_en, accuracy_llm_pt]]
for p, acc in zip(ax.patches, accs):
    width, height = p.get_width(), p.get_height()
    x, y = p.get_xy()
    ax.text(x+width/2,
            y+(height/2) - 0.01,
            &#39;{:.0f} %&#39;.format(acc),
            horizontalalignment=&#39;center&#39;,
            verticalalignment=&#39;center&#39;,
            color=&#39;white&#39;, fontsize=18)

# Adicionar setas e textos na figura
plt.arrow(0.3, 0.62, 0, 0.16,
          head_width = 0.05,
          width = 0.015,
          color=&#39;black&#39;)
plt.text(0.2, 0.9, &#39;+20,0%&#39;, fontsize = 20)

plt.arrow(0.7, 0.63, 0, 0.09,
          head_width = 0.05,
          width = 0.015,
          color=&#39;black&#39;)
plt.text(0.6, 0.84, &#39;+11,53%&#39;, fontsize = 20)

# Remover bordas da parte superior e direita
ax.spines[&#39;top&#39;].set_visible(False)
ax.spines[&#39;right&#39;].set_visible(False)
ax.spines[&#39;bottom&#39;].set_visible(True)
ax.spines[&#39;left&#39;].set_visible(True)
ax.grid(visible=None)
ax.set_facecolor(&#39;white&#39;)

# Ajustar layout
plt.tight_layout()

# Salvar a nuvem de palavras como imagem
plt.savefig(f&quot;img/acc_comparation.png&quot;, bbox_inches=&#39;tight&#39;)

# Exibir o gr√°fico
plt.show()</code></pre>
</details>
<!-- &nbsp; -->
<center>
<img src="/post/2024-04-20-sentiment-analysis-llama2/acc_comparation.png" />
</center>
<div class="w3-panel w3-pale-blue w3-border">
<p>¬† <strong>üìå Interpreta√ß√£o:</strong> A acur√°cia geral foi consideravelmente maior para o modelo Llama2 em ambas as l√≠nguas, mesmo sendo treinado principalmente em dados da l√≠ngua inglesa.</p>
</div>
</div>
<div id="conclus√£o-e-discuss√£o" class="section level1">
<h1>Conclus√£o e Discuss√£o</h1>
<hr />
<p>Os avan√ßos tecnol√≥gicos na √°rea s√£o verdadeiramente impressionantes e evidenciam a r√°pida evolu√ß√£o da intelig√™ncia artificial. √â importante estarmos sempre atentos a essas mudan√ßas, pois a √°rea de LLMs est√° em constante crescimento e melhorias significativas s√£o desenvolvidas diariamente.</p>
<p>Em meio a tantos avan√ßos, tamb√©m √© importante reconhecer as limita√ß√µes desses modelos. Um dos desafios √© o corte de conhecimento (knowledge cutoffs), o que significa que o modelo √© treinado at√© uma determinada data, como 2022, portanto n√£o possui conhecimento sobre eventos ou desenvolvimentos que ocorreram ap√≥s essa data. Al√©m disso, os LLMs est√£o sujeitos a ‚Äúhallucinations‚Äù, ou seja, podem inventar informa√ß√µes em um tom muito confiante, o que pode levar a resultados imprecisos ou at√© mesmo prejudiciais.</p>
<p>Outras limita√ß√µes incluem restri√ß√µes no input e output dos modelos, o que pode tornar dif√≠cil lidar com grandes volumes de dados ou fornecer resultados completos de uma s√≥ vez. Al√©m disso, os LLMs geralmente n√£o funcionam bem com dados estruturados, como tabelas, e podem reproduzir vieses e toxicidade presentes na sociedade, o que levanta preocupa√ß√µes √©ticas e sociais importantes.</p>
<p>Portanto, enquanto exploramos esse vasto campo das redes neurais, √© essencial abordar essas limita√ß√µes e desenvolver solu√ß√µes que permitam o uso √©tico e respons√°vel dessas poderosas ferramentas de IA.</p>
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias</h1>
<hr />
<ul>
<li><a href="https://medium.com/mapegy-tech/large-scale-language-models-for-innovation-and-technology-intelligence-sentiment-analysis-on-news-2c1ed1f6f2ad">Large-scale language models for innovation and technology intelligence: sentiment analysis on news articles</a></li>
<li><a href="https://medium.com/luisfredgs/an%C3%A1lise-de-sentimentos-com-redes-neurais-recorrentes-lstm-a5352b21e6aa">An√°lise de sentimentos com redes neurais recorrentes LSTM</a></li>
<li><a href="https://www.coursera.org/programs/applied-intelligence-workera-vshgt/learn/generative-ai-for-everyone?authProvider=accenture-main">Generative AI for Everyone - Andrew Ng - Coursera Course</a></li>
<li><a href="https://swharden.com/blog/2023-07-29-ai-chat-locally-with-python/">Run Llama 2 Locally with Python</a></li>
</ul>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2024-04-20-sentiment-analysis-llama2/">An√°lise de Sentimentos com um &#34;ChatGPT&#34; de C√≥digo Aberto</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category>Texto e NLP</category>
      <category domain="tag">chatgpt</category>
      <category domain="tag">data-science</category>
      <category domain="tag">gam</category>
      <category domain="tag">inteligencia-artificial</category>
      <category domain="tag">llama2</category>
      <category domain="tag">llm</category>
      <category domain="tag">python</category>
      <category domain="tag">redes-neurais</category>
      <category domain="tag">sentiment-analysis</category>
    </item>
    <item>
      <title>Solu√ß√£o Final - ML Olympiad [1¬∫ lugar]</title>
      <link>https://gomesfellipe.github.io/post/2023-05-30-solucao-final-ensure-healthy-lives-kaggle-competition/</link>
      <pubDate>Tue, 30 May 2023 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2023-05-30-solucao-final-ensure-healthy-lives-kaggle-competition/</guid>
      <description>Confira a estrat√©gia aplicada para esta competi√ß√£o</description>
      <content:encoded>&lt;![CDATA[
        
<link href="https://gomesfellipe.github.io/post/2023-05-30-solucao-final-ensure-healthy-lives-kaggle-competition/index_files/vembedr/css/vembedr.css" rel="stylesheet" />

<div id="TOC">
<ul>
<li><a href="#introdu%C3%A7%C3%A3o" id="toc-introdu√ß√£o">Introdu√ß√£o</a></li>
<li><a href="#defini%C3%A7%C3%A3o-do-problema-de-neg%C3%B3cio" id="toc-defini√ß√£o-do-problema-de-neg√≥cio">Defini√ß√£o do problema de neg√≥cio</a></li>
<li><a href="#solu%C3%A7%C3%B5es" id="toc-solu√ß√µes">Solu√ß√µes</a></li>
<li><a href="#estrat%C3%A9gia-anal%C3%ADtica" id="toc-estrat√©gia-anal√≠tica">Estrat√©gia anal√≠tica</a>
<ul>
<li><a href="#decis%C3%B5es-sobre-a-target" id="toc-decis√µes-sobre-a-target">Decis√µes sobre a target</a></li>
<li><a href="#processamento-dos-dados" id="toc-processamento-dos-dados">Processamento dos Dados</a></li>
<li><a href="#dados-externos" id="toc-dados-externos">Dados Externos</a></li>
<li><a href="#feature-engineering" id="toc-feature-engineering">Feature Engineering</a></li>
<li><a href="#modelos" id="toc-modelos">Modelos</a></li>
<li><a href="#ensemble" id="toc-ensemble">Ensemble</a></li>
<li><a href="#post-processing" id="toc-post-processing">Post Processing</a></li>
</ul></li>
<li><a href="#considera%C3%A7%C3%B5es-finais" id="toc-considera√ß√µes-finais">Considera√ß√µes Finais</a></li>
<li><a href="#sobre-o-autor" id="toc-sobre-o-autor">Sobre o Autor</a></li>
</ul>
</div>

<div id="introdu√ß√£o" class="section level1">
<h1>Introdu√ß√£o</h1>
<p>O <a href="https://www.meetup.com/TensorFlowSP/events/284607061/">TFUG - TensorFlow Users Group de S√£o Paulo</a> lan√ßou uma nova <a href="https://www.kaggle.com/competitions/ml-olympiad-ensure-healthy-lives">competi√ß√£o no Kaggle</a> onde o objetivo era desenvolver modelos para previs√£o de diagn√≥stico de s√≠ndromes respirat√≥rias, que √© um tema relacionado com um dos 17 t√≥picos de Desenvolvimento Sustent√°vel das Na√ß√µes Unidas - <em>Boa sa√∫de e bem-estar</em>.</p>
<p>Como um cientista de dados, acredito que seja muito importante continuarmos aprimorando nossas habilidades e conhecimentos. Competi√ß√µes como essa s√£o muito divertidas e possibilitam que testemos nossos limites em um ambiente competitivo e colaborativo, al√©m de ser uma grande oportunidade para nos desafiarmos e aprender uns com os outros.</p>
<p>Tive o enorme prazer de conquistar o primeiro lugar, dessa vez com meu grande amigo <a href="https://www.linkedin.com/in/kaike-wesley-reis">Kaike</a>, parceiro de competi√ß√µes de longa data que trouxe grande sinergia para a <a href="https://www.kaggle.com/code/gomes555/ml-olypiads-1-lugar-blending">solu√ß√£o final</a> com a contribui√ß√£o de seu modelo (compartilhado abertamente no Kaggle).</p>
<p>Aqui est√£o alguns dos pr√™mios recebidos:</p>
<center>
<img src="/post/2023-05-30-solucao-final-ensure-healthy-lives-kaggle-competition/premio2.png" style="width:80.0%" />
</center>
<p>Como nesta competi√ß√£o havia bastante trabalho a ser feito e tivemos apenas 1 m√™s para trabalhar na solu√ß√£o, foi preciso fazer uma boa gest√£o do c√≥digo e do tempo de desenvolvimento.</p>
</div>
<div id="defini√ß√£o-do-problema-de-neg√≥cio" class="section level1">
<h1>Defini√ß√£o do problema de neg√≥cio</h1>
<p>O objetivo desta competi√ß√£o consistiu em predizer qual o agente causador da s√≠ndrome respirat√≥ria aguda grave com base nos dados e sintomas dos pacientes.</p>
<p>Esta tarefa pode ser enquadrada como um problema supervisionado de classifica√ß√£o multinomial (com m√∫ltiplos outputs) na qual as previs√µes s√£o, de certa forma, dependentes da entrada umas das outras (o paciente s√≥ pode ter registrado uma das doen√ßas).</p>
<p>A valida√ß√£o da solu√ß√£o foi feita utilizando a m√©trica Macro (or Mean) F1-Score, que √© basicamente a m√©dia do F1 calculado sobre as previs√µes de cada nota.</p>
</div>
<div id="solu√ß√µes" class="section level1">
<h1>Solu√ß√µes</h1>
<p>Ambas solu√ß√µes (minha e do Kaike) foram compartilhadas no Kaggle:</p>
<ul>
<li><a href="https://www.kaggle.com/code/gomes555/ml-olympiad-1-lugar-catboost-pos-process">ML Olympiad - 1¬∫ Lugar - Catboost + Pos Process</a> (Fellipe)</li>
<li><a href="https://www.kaggle.com/code/kaikewreis/ml-olypiads-1-lugar-lightgbm-binary-ensemble">ML Olypiads - 1¬∫ Lugar - LightGBM Binary Ensemble</a> (Kaike)</li>
<li><a href="https://www.kaggle.com/code/gomes555/ml-olympiad-1-lugar-blending">ML Olympiad - 1¬∫ Lugar - Blending</a> (combina√ß√£o das solu√ß√µes em um emsemble)</li>
</ul>
<p>Disponibilizamos tamb√©m a solu√ß√£o em formato de v√≠deo, gravado em um meetup com dura√ß√£o de 1 hora e meia para o canal do <a href="https://www.youtube.com/@tensorflowugsp">TensorFlow UGSP</a> no Youtube no link: <a href="https://youtu.be/6HPJn38NF3w" class="uri">https://youtu.be/6HPJn38NF3w</a></p>
<center>
<div class="vembedr">
<div>
<iframe src="https://www.youtube.com/embed/6HPJn38NF3w" width="533" height="300" frameborder="0" allowfullscreen="" data-external="1"></iframe>
</div>
</div>
</center>
</div>
<div id="estrat√©gia-anal√≠tica" class="section level1">
<h1>Estrat√©gia anal√≠tica</h1>
<p>Nas se√ß√µes abaixo apresento o racional por tr√°s da minha solu√ß√£o, como chegamos nos 5 melhores modelos individuais (para cada doen√ßa respirat√≥ria) que utilizei em um ensemble para chegar ao primeiro lugar, bem como a estrat√©gia de p√≥s processamento que com que o score melhorasse significativamente.</p>
<div id="decis√µes-sobre-a-target" class="section level2">
<h2>Decis√µes sobre a target</h2>
<p>A primeira decis√£o importante era definir como enquadrar o problema; se utilizar√≠amos 1 modelo multiclasse ou diferentes modelos para cada classe.</p>
<p>Em todos os testes que fizemos, os modelos individuais superaram o F1-Score Macro de um modelo √∫nico. Como 3 das classes eram bastante desbalanceadas, acredito que modelos especializados nesses casos conseguiram captar melhor suas nuances.</p>
</div>
<div id="processamento-dos-dados" class="section level2">
<h2>Processamento dos Dados</h2>
<p>Como optamos por unificar os resultados apenas na reta final, meu pr√©-processamento foi muito diferente do feito pelo Kaike e isso foi fundamental para que as estimativas dos nossos modelos tivessem baixa correla√ß√£o. N√£o focarei aqui no meu pr√©-processamento, pois n√£o acho que foi o diferencial para atingir um score superior a 0.6 (quem tiver curiosidade est√° tudo bem documentado nos notebooks compartilhados).</p>
</div>
<div id="dados-externos" class="section level2">
<h2>Dados Externos</h2>
<p>O fato de n√£o termos as informa√ß√µes do ano em que esses dados foram coletados dificultou na busca de bases externas, pois indicadores socioecon√¥micos e de sa√∫de variam bastante ao longo do tempo.</p>
<p>Fizemos alguns testes utilizando o <a href="https://basedosdados.org/dataset/mundo-onu-adh">Atlas do Desenvolvimento Humano (ADH)</a>, mas n√£o tivemos muito sucesso, pois esses dados est√£o muito defasados (1991-2010). Tamb√©m tentamos acrescentar a informa√ß√£o de <a href="https://github.com/kelvins/Municipios-Brasileiros/">latitude e longitude de cada munic√≠pio</a>, mas isso n√£o trouxe uma melhora substancial no nosso score.</p>
</div>
<div id="feature-engineering" class="section level2">
<h2>Feature Engineering</h2>
<p>Outra etapa em que investimos bastante tempo foi para criar novas vari√°veis.</p>
<p>Novamente, nossa engenharia de recursos foi feita de maneira separada para que nossos modelos aprendessem aspectos diferentes dos dados. Abaixo, compartilho algumas das features que desenvolvi apenas para o meu modelo:</p>
<ul>
<li>Presen√ßa de sintomas relacionados √† Target;</li>
<li>Se tomografia era t√≠pica do COVID;</li>
<li>Intervalo de idade com mais casos;</li>
<li>Idade discretizada;</li>
<li>Diferen√ßa entre a semana de notifica√ß√£o e primeiros sintomas;</li>
<li>Novas features baseadas nas contagens de algumas features categ√≥ricas;</li>
<li>etc.</li>
</ul>
</div>
<div id="modelos" class="section level2">
<h2>Modelos</h2>
<p>Al√©m de pr√©-processamentos e feature engineering diferentes, tamb√©m utilizamos modelos e mecanismos de tunning diferentes, o que ajudou para que nossas estimativas tivessem baixa correla√ß√£o. Eu usei o Catboost como modelo final, j√° o Kaike optou por um LightGBM com tuning de hiperparametros.</p>
</div>
<div id="ensemble" class="section level2">
<h2>Ensemble</h2>
<p>Calculamos a m√©dia das probabilidades previstas de cada modelo para cada classe antes de selecionar a classe que tivesse a maior probabilidade.</p>
<p>Como nossas previs√µes tinham baixa correla√ß√£o, conseguimos ser bem sucedidos no ensemble combinando nossas submiss√µes com score ~0.6 alcan√ßando ~0.61 na tabela p√∫blica.</p>
</div>
<div id="post-processing" class="section level2">
<h2>Post Processing</h2>
<p>Acredito que o <strong>diferencial</strong> dessa competi√ß√£o estava no p√≥s processamento.</p>
<p>Quando avaliamos o score do modelo de cada classe, tamb√©m calculamos um threshold que maximizava os respectivos F1.</p>
<p>Observamos que nosso modelo para a classe 5 apresentava um F1 muito superior √†s demais classes com esse threshold otimizado, ent√£o fizemos o seguinte:</p>
<ol style="list-style-type: decimal">
<li>Calculamos as probabilidades individuais para cada classe;</li>
<li>Selecionamos a classe que tinha maior probabilidade estimada em cada inst√¢ncia;</li>
<li>Pegamos a classifica√ß√£o bin√°ria da classe 5 com o threshold otimizado e aplicamos a seguinte condi√ß√£o: Se o modelo da classe 5 estimou que y5[i]==1, ent√£o yfinal[i] √© 5, caso contr√°rio, use a classe de maior probabilidade entre as outras 4. (Em outras palavras: <code>np.where(y5_test_class==1, 5, sub.CLASSI_FIN)</code>)</li>
</ol>
</div>
</div>
<div id="considera√ß√µes-finais" class="section level1">
<h1>Considera√ß√µes Finais</h1>
<p>Foi uma competi√ß√£o muito interessante e desafiadora. Agrade√ßo imensamente ao <a href="https://www.meetup.com/TensorFlowSP/events/284607061/">TFUG</a> por organizar o evento e a todos os participantes que contribu√≠ram para o aprendizado coletivo.Foi uma √≥tima oportunidade de aprendizado e troca de experi√™ncias.</p>
<p>Espero que minha solu√ß√£o possa ser √∫til para outros projetos e desafios futuros.</p>
</div>
<div id="sobre-o-autor" class="section level1">
<h1>Sobre o Autor</h1>
<p>Me chamo Fellipe Gomes, sou cientista de dados e apaixonado por aprendizado de m√°quina. Compartilho meu conhecimento por meio de artigos, tutoriais e projetos de c√≥digo aberto. Se quiser saber mais sobre meu trabalho, sinta-se √† vontade para conferir meu <a href="https://www.linkedin.com/in/fellipe-gomes-06943264/">LinkedIn</a> e <a href="https://github.com/fellipe-gomes">GitHub</a>.</p>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2023-05-30-solucao-final-ensure-healthy-lives-kaggle-competition/">Solu√ß√£o Final - ML Olympiad [1¬∫ lugar]</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">catboost</category>
      <category domain="tag">classification</category>
      <category domain="tag">data-mining</category>
      <category domain="tag">data-science</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">machine-learning</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">pratica</category>
      <category domain="tag">r</category>
    </item>
    <item>
      <title>Solu√ß√£o Final - ML Olympiad [2¬∫ lugar]</title>
      <link>https://gomesfellipe.github.io/post/2022-04-20-solucao-final-education-quality-kaggle-competition/</link>
      <pubDate>Tue, 19 Apr 2022 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2022-04-20-solucao-final-education-quality-kaggle-competition/</guid>
      <description>Confira a estrat√©gia aplicada para esta competi√ß√£o</description>
      <content:encoded>&lt;![CDATA[
        

<div id="TOC">
<ul>
<li><a href="#introdu%C3%A7%C3%A3o" id="toc-introdu√ß√£o">Introdu√ß√£o</a></li>
<li><a href="#defini%C3%A7%C3%A3o-do-problema-de-neg%C3%B3cio" id="toc-defini√ß√£o-do-problema-de-neg√≥cio">Defini√ß√£o do problema de neg√≥cio</a></li>
<li><a href="#an%C3%A1lise-explorat%C3%B3ria-em-r" id="toc-an√°lise-explorat√≥ria-em-r">An√°lise Explorat√≥ria (em R)</a>
<ul>
<li><a href="#estrutura-da-base" id="toc-estrutura-da-base">Estrutura da base</a></li>
<li><a href="#ano-da-base-de-dados" id="toc-ano-da-base-de-dados">Ano da base de dados</a></li>
<li><a href="#target" id="toc-target">Target</a></li>
</ul></li>
<li><a href="#machine-learning-em-python" id="toc-machine-learning-em-python">Machine Learning (em Python)</a>
<ul>
<li><a href="#importar-dependencias" id="toc-importar-dependencias">Importar dependencias</a></li>
<li><a href="#carregar-dados" id="toc-carregar-dados">Carregar dados</a></li>
<li><a href="#modelagem" id="toc-modelagem">Modelagem</a></li>
</ul></li>
<li><a href="#submiss%C3%A3o" id="toc-submiss√£o">Submiss√£o</a></li>
<li><a href="#considera%C3%A7%C3%B5es-finais" id="toc-considera√ß√µes-finais">Considera√ß√µes Finais</a></li>
</ul>
</div>

<div id="introdu√ß√£o" class="section level1">
<h1>Introdu√ß√£o</h1>
<p>No final de Janeiro desde ano (2022) o <a href="https://www.meetup.com/TensorFlowSP/events/284607061/">TFUG - TensorFlow Users Group de S√£o Paulo</a> lan√ßou uma competi√ß√£o no Kaggle para prever as notas do enem que tem rela√ß√£o com um dos 17 t√≥picos de Desenvolvimento Sustent√°vel das Na√ß√µes Unidas - <em>Educa√ß√£o de Qualidade</em>.</p>
<p>Al√©m de divertido, o desafio foi repleto de possibilidades e bastante desafiador! Todos os competidores que trabalharam duro em pleno m√™s de carnaval est√£o de parab√©ns! üòÖ üòÇ</p>
<p>Aqui est√£o alguns dos pr√™mios recebidos:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/premio.png" style="width:80.0%" />
</center>
<p>Como nesta competi√ß√£o havia bastante trabalho a ser feito e tivemos apenas 1 m√™s para trabalhar na solu√ß√£o, foi preciso fazer uma boa gest√£o do c√≥digo e do tempo de desenvolvimento.</p>
<p>Nas se√ß√µes abaixo apresento o racional por tr√°s da minha solu√ß√£o bem como os 5 melhores modelos individuais (para cada nota) que utilizei em um ensemble para chegar ao segundo lugar.</p>
</div>
<div id="defini√ß√£o-do-problema-de-neg√≥cio" class="section level1">
<h1>Defini√ß√£o do problema de neg√≥cio</h1>
<p>O objetivo desta competi√ß√£o consistiu em prever as notas dos alunos(as) nas provas: Ci√™ncias da Natureza, Ci√™ncias Humanas, Linguagens e C√≥digos, Matem√°tica e Reda√ß√£o.</p>
<p>Apesar das notas serem calculadas de maneira independente, a partir de modelos de <a href="http://portal.mec.gov.br/ultimas-noticias/389-ensino-medio-2092297298/17319-teoria-de-resposta-ao-item-avalia-habilidade-e-minimiza-o-chute">TRI (Teoria de Resposta ao Item)</a> que levam em considera√ß√£o a performance em um caderno espec√≠fico e na dificuldade de cada quest√£o, o mesmo aluno realiza todas as provas em um curto per√≠odo de tempo.</p>
<p>Portanto, esta tarefa pode ser enquadrada como um problema supervisionado de regress√£o com m√∫ltiplos outputs na qual as previs√µes s√£o, de certa forma, dependentes da entrada umas das outras.</p>
<p>A valida√ß√£o da solu√ß√£o foi feita utilizando a m√©trica Mean Columnwise Root Mean Squared Error ‚Äì MCRMSE, que √© basicamente a m√©dia do RMSE calculado sobre as previs√µes de cada nota.</p>
</div>
<div id="an√°lise-explorat√≥ria-em-r" class="section level1">
<h1>An√°lise Explorat√≥ria (em R)</h1>
<p>Convido o leitor a conferir o <a href="https://gomesfellipe.github.io/post/2021-11-01-solucao-final-porto-seguro-data-challenge/">notebook publicado no Kaggle</a> com a an√°lise explorat√≥ria completa. Aqui irei trazer apenas alguns dos principais insights que encontrei durante a etapa de an√°lise explorat√≥ria.</p>
<div id="estrutura-da-base" class="section level2">
<h2>Estrutura da base</h2>
<p>Veja a seguir qual a estrutura geral da base de dados:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/02_df_status.png" style="width:95.0%" />
</center>
<p>√â not√≥rio que existem dados faltantes e que parece haver algum padr√£o. Vejamos com mais detalhse:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/03_missing.png" style="width:95.0%" />
</center>
<div class="w3-panel w3-pale-blue w3-border">
<p>¬† üí° Insights!</p>
<p>Existem dados <em>missing</em> nas 5 targets que queremos prever e note que existe uma rela√ß√£o tanto entre as provas de Matem√°tica e Ci√™ncias da Natuerza quanto nas de Ci√™ncias Humanas, Linguagens e C√≥digos e Reda√ß√£o, o que parece ocorrer devido a aus√™ncia do aluno incrito em comparecer a realiza√ß√£o da prova no respectivo dia.</p>
</div>
</div>
<div id="ano-da-base-de-dados" class="section level2">
<h2>Ano da base de dados</h2>
<p>Essa informa√ß√£o n√£o estava explicitamente dispon√≠vel, mas ap√≥s analisar a idade dos participantes em rela√ß√£o ao ano em que conclu√≠ram o ensino m√©dio, foi poss√≠vel identificar que tratavam-se dos dados de 2019, veja:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/05_ano_concluiu.png" style="width:95.0%" />
</center>
<p>Essa informa√ß√£o poderia ser √∫til na hora de buscar dados externos (permitido nesta competi√ß√£o).</p>
<div class="w3-panel w3-pale-blue w3-border">
<p>¬† üí° Insights!</p>
<p>‚Üí Aten√ß√£o aos outliers: √â no m√≠nimo estranho uma pessoa que formou em 2007 ter 17 anos;</p>
<p>‚Üí Como ningu√©m concluiu a escola no ano de 2019 e a m√©dia das idades vai diminuindo quanto mais pr√≥ximo de 2018, parece que estes dados s√£o de 2019. Essa inform√ß√£o poderia ser √∫til na hora de procurar por bases externas.</p>
</div>
</div>
<div id="target" class="section level2">
<h2>Target</h2>
<p>A primeira decis√£o importante era definir como enquadrar o problema; se seriam m√∫ltiplos modelos independentes ou modelos com sa√≠das dependentes.</p>
<p>Primeiramente vejamos como eram as distribui√ß√µes das notas por caderno:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/07_distribuicao_target.png" style="width:95.0%" />
</center>
<p>Ao olhar estas distribui√ß√µes foram surgindo v√°rias id√©ias! Cheguei at√© a tentar modelos estat√≠sticos GAM considerando a resposta como uma distribui√ß√£o Beta (transformando as targets no intervalo [0,1]) mas acabou n√£o apresentando bons resultados para a competi√ß√£o.. acho que seria necess√°rio um pouco mais de prepara√ß√£o nos dados.</p>
<p>Apesar das notas do enem serem calculadas via TRI (Teoria de Resposta ao Item) que considera as notas independentes, parece existir alguma correla√ß√£o entre as notas, veja:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/08_correlacao_notas.png" style="width:95.0%" />
</center>
<p>As targets da nota de L√≠nguas e C√≥digos e Ci√™ncias Humanas pareciam possuir uma correla√ß√£o ‚Äúinteressante‚Äù, mas, ap√≥s testar modelos de m√∫ltiplas respostas dependentes para cada dia (com e sem a nota da reda√ß√£o), em nenhum de meus testes superou (de maneira consistente) o desempenho de modelos que considerassem as sa√≠das independentes. Portanto foquei em criar 5 modelos independentes.</p>
</div>
</div>
<div id="machine-learning-em-python" class="section level1">
<h1>Machine Learning (em Python)</h1>
<p>Toda a rotina de pr√©-processamento dos dados, feature engineering, modelagem, ensamble e p√≥s-processamento foi realizada utilizando a linguagem Python para cada uma das 5 notas. Trouxe apenas o modelo final neste post mas, para chegar at√© aqui foram necess√°rio muitos testes!</p>
<div id="importar-dependencias" class="section level2">
<h2>Importar dependencias</h2>
<p>Carregar pacotes Python:</p>
<pre class="python"><code># data prep
import numpy as np 
import pandas as pd 
# pre process
from sklearn.preprocessing import MinMaxScaler
# modeling
from sklearn.model_selection import train_test_split
from catboost import CatBoostRegressor
# plots
import seaborn as sns
import matplotlib.pyplot as plt</code></pre>
<p>Confira a baixo as fun√ß√µes desenvolvidas para a solu√ß√£o deste problema</p>
<details>
<summary>
(<em>Clique aqui para expandir as fun√ß√µes</em>)
</summary>
<pre class="python"><code>def prep_data_questionarios(df):
  &#39;&#39;&#39;
  Converte dados de questionario para ordinal
  &#39;&#39;&#39;
    # escolaridade pai
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}
    df.loc[:, &#39;Q001&#39;] = df.loc[:, &#39;Q001&#39;].map(to_map).astype(int)

    # escolaridade mae
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}
    df.loc[:, &#39;Q002&#39;] = df.loc[:, &#39;Q002&#39;].map(to_map).astype(int) 

    # ocupacao pai
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: -1}
    df.loc[:, &#39;Q003&#39;] = df.loc[:, &#39;Q003&#39;].map(to_map).astype(int) 

    # ocupacao mae
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: -1}
    df.loc[:, &#39;Q004&#39;] = df.loc[:, &#39;Q004&#39;].map(to_map).astype(int) 

    # renda da familia
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;:5, &#39;G&#39;:6, &#39;H&#39;:7, &#39;I&#39;:8,
              &#39;J&#39;:9, &#39;K&#39;:10,&#39;L&#39;:11, &#39;M&#39;:12, &#39;N&#39;:13, &#39;O&#39;:14, &#39;P&#39;:15, &#39;Q&#39;:16}
    df.loc[:, &#39;Q006&#39;] = df.loc[:, &#39;Q006&#39;].map(to_map).astype(int) 

    # empregado domestico
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3}
    df.loc[:, &#39;Q007&#39;] = df.loc[:, &#39;Q007&#39;].map(to_map).astype(int) 

    # banheiro
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q008&#39;] = df.loc[:, &#39;Q008&#39;].map(to_map).astype(int) 

    # qnt de quartos
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q009&#39;] = df.loc[:, &#39;Q009&#39;].map(to_map).astype(int) 

    # qnt de carros
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q010&#39;] = df.loc[:, &#39;Q010&#39;].map(to_map).astype(int) 

    # qnt de motocicleta
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q011&#39;] = df.loc[:, &#39;Q011&#39;].map(to_map).astype(int) 

    # qnt de geladeira
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q012&#39;] = df.loc[:, &#39;Q012&#39;].map(to_map).astype(int) 

    # qnt de freezer
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q013&#39;] = df.loc[:, &#39;Q013&#39;].map(to_map).astype(int) 

    # qnt de maquina de lavar roupa
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q014&#39;] = df.loc[:, &#39;Q014&#39;].map(to_map).astype(int) 

    # qnt de maquina de secar roupa
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q015&#39;] = df.loc[:, &#39;Q015&#39;].map(to_map).astype(int) 

    # qnt de microondas
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q016&#39;] = df.loc[:, &#39;Q016&#39;].map(to_map).astype(int) 

    # qnt de maquina de lavar louca
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q017&#39;] = df.loc[:, &#39;Q017&#39;].map(to_map).astype(int) 

    # tem aspirador de po
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1}
    df.loc[:, &#39;Q018&#39;] = df.loc[:, &#39;Q018&#39;].map(to_map).astype(int) 

    # qtd tv colorida
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q019&#39;] = df.loc[:, &#39;Q019&#39;].map(to_map).astype(int) 

    # tem dvd
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1}
    df.loc[:, &#39;Q020&#39;] = df.loc[:, &#39;Q020&#39;].map(to_map).astype(int) 

    # tem tv por assinatura
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1}
    df.loc[:, &#39;Q021&#39;] = df.loc[:, &#39;Q021&#39;].map(to_map).astype(int) 

    # qtd telefone celular
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q022&#39;] = df.loc[:, &#39;Q022&#39;].map(to_map).astype(int) 

    # qtd telefone fixo
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1}
    df.loc[:, &#39;Q023&#39;] = df.loc[:, &#39;Q023&#39;].map(to_map).astype(int) 

    # qtd computador
    to_map =  {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q024&#39;] = df.loc[:, &#39;Q024&#39;].map(to_map).astype(int) 

    # tem acesso a internet
    to_map =  {&#39;A&#39;:0, &#39;B&#39;:1}
    df.loc[:, &#39;Q025&#39;] = df.loc[:, &#39;Q025&#39;].map(to_map).astype(int) 
    
    return(df)
  
def fe_questionario(df):
  &#39;&#39;&#39;
  Gerar novas features artificiais baseadas nos dados de questionario
  &#39;&#39;&#39;
    df.loc[:, &quot;Q021+Q006&quot;] = df[&quot;Q021&quot;] + df[&quot;Q006&quot;]
    df.loc[:, &quot;Q018+Q006&quot;] = df[&quot;Q018&quot;] + df[&quot;Q006&quot;]
    df.loc[:, &quot;Q018+Q008&quot;] = df[&quot;Q018&quot;] + df[&quot;Q008&quot;]
    df.loc[:, &quot;Q010+Q018&quot;] = df[&quot;Q010&quot;] + df[&quot;Q018&quot;]
    df.loc[:, &quot;Q018+Q024&quot;] = df[&quot;Q018&quot;] + df[&quot;Q024&quot;]
    
    df.loc[:, &quot;Q018*Q006&quot;] = df[&quot;Q018&quot;] * df[&quot;Q006&quot;]
    df.loc[:, &quot;Q010*Q018&quot;] = df[&quot;Q010&quot;] * df[&quot;Q018&quot;]
    
    return df
  
def fe_mun(data):
    &#39;&#39;&#39;
    Gerar novas features a partir das localizacoes de municipio
    &#39;&#39;&#39;
    for c in list(data.columns[data.dtypes==&#39;category&#39;]):
        data.loc[:, c] = data.loc[:, c].astype(&#39;object&#39;)
    
    data.loc[:, &#39;FE_MUNICIPIO_PROVA_x_MUNICIPIO_RESIDENCIA&#39;] = np.where(data.NO_MUNICIPIO_PROVA == data.NO_MUNICIPIO_RESIDENCIA , 1, 0)
    data.loc[:, &#39;FE_MUNICIPIO_PROVA_x_MUNICIPIO_NASCIMENTO&#39;] = np.where(data.NO_MUNICIPIO_PROVA == data.NO_MUNICIPIO_NASCIMENTO , 1, 0)
    data.loc[:, &#39;FE_MUNICIPIO_PROVA_x_MUNICIPIO_ESC&#39;] = np.where(data.NO_MUNICIPIO_PROVA == data.NO_MUNICIPIO_ESC , 1, 0)
    data.loc[:, &#39;FE_MUNICIPIO_RESIDENCIA_x_MUNICIPIO_NASCIMENTO&#39;] = np.where(data.NO_MUNICIPIO_RESIDENCIA == data.NO_MUNICIPIO_NASCIMENTO , 1, 0)
    data.loc[:, &#39;FE_MUNICIPIO_RESIDENCIA_x_MUNICIPIO_ESC&#39;] = np.where(data.NO_MUNICIPIO_RESIDENCIA == data.NO_MUNICIPIO_ESC , 1, 0)
    data.loc[:, &#39;FE_MUNICIPIO_NASCIMENTO_x_MUNICIPIO_ESC&#39;] = np.where(data.NO_MUNICIPIO_RESIDENCIA == data.NO_MUNICIPIO_ESC , 1, 0)
    
    for c in list(data.columns[data.dtypes==&#39;object&#39;]):
        data.loc[:, c] = data.loc[:, c].astype(&#39;category&#39;)
    
    return data
  
def fe_in(df):
    &#39;&#39;&#39;
    Gerar features a partir das indicadoras
    &#39;&#39;&#39;
    df.loc[:, &#39;IN_DEFICIT_ATENCAO+IN_TEMPO_ADICIONAL&#39;] = df[&quot;IN_DEFICIT_ATENCAO&quot;] + df[&quot;IN_TEMPO_ADICIONAL&quot;]
    df.loc[:, &#39;IN_LEDOR+IN_TRANSCRICAO&#39;] = df[&quot;IN_LEDOR&quot;] + df[&quot;IN_TRANSCRICAO&quot;]

    return df
  
def prep_co_escola(df):
    &#39;&#39;&#39;
    Converter codigo da escola para categorico
    &#39;&#39;&#39;
    df.loc[:, &#39;CO_ESCOLA&#39;] = [str(x) for x in df.CO_ESCOLA]
    df.loc[:, &#39;CO_ESCOLA&#39;] = np.where(df[&#39;CO_ESCOLA&#39;]==&#39;nan&#39;, np.nan, df[&#39;CO_ESCOLA&#39;])
    df.loc[:, &#39;CO_ESCOLA&#39;] = df.loc[:, &#39;CO_ESCOLA&#39;].astype(&#39;category&#39;)
    
    return df
  
def fe_extra(df):
    &#39;&#39;&#39;
    Gerar novas features 
    &#39;&#39;&#39;
    df.loc[:, &quot;FE_IDADE_DISCRETA&quot;] = pd.cut(df.NU_IDADE, (0, 15, 18, 23, 36, 60, 120), labels=[&#39;ADOLESCENTE&#39;,&#39;ADOLESCENTE_2&#39;, &#39;JOVEM&#39;,&#39;JOVEM_2&#39;, &#39;ADULTO&#39;, &#39;IDOSO&#39;]).astype(&#39;category&#39;)
    df.loc[:, &#39;FE_OCUPACAO_PAIS&#39;] = df.Q003 + df.Q004
    df.loc[:, &#39;FE_ESCOLARIDADE_PAIS&#39;] = df.Q001 + df.Q002
    df.loc[:, &#39;FE_RENDA_POR_PESSOA&#39;] = df.Q006 / df.Q005
    df.loc[:, &#39;FE_CELULAR_POR_PESSOA&#39;] = df.Q022 / df.Q005
    df.loc[:, &#39;FE_COMPUTADOR_POR_PESSOA&#39;] = df.Q024 / df.Q005
    df.loc[:, &#39;FE_VISAO_RUIM&#39;] = df[[&#39;IN_BAIXA_VISAO&#39;, &#39;IN_CEGUEIRA&#39;, &#39;IN_VISAO_MONOCULAR&#39;, &#39;IN_SURDO_CEGUEIRA&#39;]].max(axis=1)
    df.loc[:, &#39;FE_AUDICAO_RUIM&#39;] = df[[&#39;IN_SURDEZ&#39;, &#39;IN_DEFICIENCIA_AUDITIVA&#39;, &#39;IN_SURDO_CEGUEIRA&#39;]].max(axis=1)
    df.loc[:, &#39;FE_TDAH_MAIS_TEMPO&#39;] = df.IN_TEMPO_ADICIONAL + df.IN_DEFICIT_ATENCAO
    df.loc[:, &#39;FE_TDAH_MEDICADO&#39;] = np.where((df.IN_DEFICIT_ATENCAO==1)&amp;(df.IN_MEDICAMENTOS==1), 1, 0)
    df.loc[:, &#39;FE_RECURSO_VISAO&#39;] =  df[[&#39;IN_BRAILLE&#39;, &#39;IN_AMPLIADA_24&#39;, &#39;IN_AMPLIADA_18&#39;, &#39;IN_LEDOR&#39;, &#39;IN_MAQUINA_BRAILE&#39;, &#39;IN_LAMINA_OVERLAY&#39;]].max(axis=1)
    df.loc[:, &#39;FE_RECURSO_SURDEZ&#39;] =  df[[&#39;IN_LIBRAS&#39;, &#39;IN_LEITURA_LABIAL&#39;, &#39;IN_TRANSCRICAO&#39;]].max(axis=1)
    acess = [&#39;IN_ACESSO&#39;, &#39;IN_MESA_CADEIRA_RODAS&#39;, &#39;IN_MESA_CADEIRA_SEPARADA&#39;, &#39;IN_APOIO_PERNA&#39;, &#39;IN_CADEIRA_ESPECIAL&#39;, &#39;IN_CADEIRA_CANHOTO&#39;, &#39;IN_CADEIRA_ACOLCHOADA&#39;, &#39;IN_MOBILIARIO_OBESO&#39;, &#39;IN_SALA_INDIVIDUAL&#39;, &#39;IN_SALA_ESPECIAL&#39;, &#39;IN_SALA_ACOMPANHANTE&#39;, &#39;IN_MOBILIARIO_ESPECIFICO&#39;, &#39;IN_MATERIAL_ESPECIFICO&#39;]
    df.loc[:, &#39;FE_ACESSIBILIDADE&#39;] =  df[acess].max(axis=1)

    return df</code></pre>
</details>
<p>¬†</p>
<p>Carregar features artificiais extra√≠das atrav√©s de um modelo KNN. N√£o apresentarei o c√≥digo aqui (talvez fique para um pr√≥ximo post) mas a id√©ia √© basicamente a seguinte:</p>
<div class="w3-panel w3-pale-yellow w3-border">
<p>¬† üß™ Feature Extraction com KNN</p>
<p>Ajuste um <code>KNeighborsRegressor</code> encontrando os K-vizinhos mais pr√≥ximos de cada inst√¢ncia out-of-fold via valida√ß√£o cruzada (para evitar data leak) nos dados de treino e depois ajuste um modelo em todos os dados de treino para obter os K-vizinhos mais pr√≥ximos nos dados de teste.</p>
</div>
<p>Quem sabe no futuro fa√ßo um post compartilhando esta estrat√©gia com mais detalhes.</p>
<pre class="python"><code>knn_train = pd.read_csv(&quot;../input/knn/KNN_feat_train_CH_LC.csv&quot;)
knn_test = pd.read_csv(&quot;../input/knn/KNN_feat_test_CH_LC.csv&quot;)

knn_train_cn_mt = pd.read_csv(&quot;../input/knn/KNN_feat_train_CN_MT.csv&quot;)
knn_test_cn_mt = pd.read_csv(&quot;../input/knn/KNN_feat_test_CN_MT.csv&quot;)

knn_train_rd = pd.read_csv(&quot;../input/knn/KNN_feat_train_RD.csv&quot;)
knn_test_rd = pd.read_csv(&quot;../input/knn/KNN_feat_test_RD.csv&quot;)</code></pre>
</div>
<div id="carregar-dados" class="section level2">
<h2>Carregar dados</h2>
<p>Importar uma vers√£o do dataset no formato <code>.parquet</code> que foi compactada com um truque para otimizar o consumo de mem√≥ria disponibilizada pelos organizadores <a href="https://www.kaggle.com/code/caneiro/mlo-make-parquet">neste notebook</a>.</p>
<pre class="python"><code>train = pd.read_parquet(&#39;train.parquet&#39;)
test = pd.read_parquet(&#39;test.parquet&#39;)
sub = pd.read_csv(&#39;../input/qualityeducation/sample_submission.csv&#39;)</code></pre>
<p>Definir objetos com targets</p>
<pre class="python"><code>targets = [&#39;NU_NOTA_LC&#39;, &#39;NU_NOTA_CH&#39;, &#39;NU_NOTA_CN&#39;,  &#39;NU_NOTA_MT&#39;, &#39;NU_NOTA_REDACAO&#39;]
presencas = [&#39;TP_PRESENCA_LC&#39;, &#39;TP_PRESENCA_CH&#39;, &#39;TP_PRESENCA_CN&#39;, &#39;TP_PRESENCA_MT&#39;, &#39;TP_STATUS_REDACAO&#39;]</code></pre>
<div class="w3-panel w3-pale-yellow w3-border">
<p>¬† ‚ö†Ô∏è Aten√ß√£o:</p>
<p>A feature de presen√ßa √© muito importante no p√≥s-processamento para atribuir nota zero aos alunos que n√£o foram realizar a prova mas n√£o faz sentido mant√™-la nos dados de treino pois ser√° sempre constante.</p>
</div>
<div id="dados-externos" class="section level3">
<h3>Dados externos</h3>
<p>Dados Externos utilizados:</p>
<ol style="list-style-type: decimal">
<li><a href="https://basedosdados.org/dataset/mundo-onu-adh">Atlas do Desenvolvimento Humano (ADH)</a></li>
</ol>
<p>Esta base tinha muita informa√ß√£o legal mas sua cobertura temporal estava bastante defasada (1991 - 2010) o que pode adicionar algum ru√≠do ao modelo.</p>
<p>As features selecionadas (sem muito crit√©rio) desta base foram:</p>
<pre class="python"><code>extra1 = pd.read_csv(&quot;municipio.csv&quot;)

extra1 = extra1[extra1.ano==2010]

features_extra1 = [&#39;expectativa_vida&#39;, &#39;razao_dependencia&#39;, &#39;expectativa_anos_estudo&#39;,
&#39;taxa_analfabetismo_11_a_14&#39;, &#39;taxa_analfabetismo_15_a_17&#39;, &#39;taxa_analfabetismo_18_mais&#39;,
&#39;taxa_atraso_0_basico&#39;, &#39;taxa_atraso_0_fundamental&#39;, &#39;taxa_atraso_0_medio&#39;,
&#39;taxa_freq_bruta_medio&#39;, &#39;taxa_freq_liquida_medio&#39;,
&#39;taxa_freq_medio_18_24&#39;, &#39;taxa_freq_medio_6_14&#39;, &#39;indice_gini&#39;,&#39;prop_pobreza_extrema&#39;, &#39;prop_pobreza&#39;,
&#39;prop_renda_10_ricos&#39;, &#39;prop_renda_20_pobres&#39;, &#39;razao_10_ricos_40_pobres&#39;,&#39;renda_pc&#39; , &#39;renda_pc_quintil_1&#39;,
&#39;indice_theil&#39;, &#39;prop_trabalhadores_conta_proria&#39;, 
&#39;prop_empregadores&#39;, &#39;prop_ocupados_agropecuaria&#39;, &#39;prop_ocupados_comercio&#39;,
&#39;prop_ocupados_construcao&#39;, &#39;prop_ocupados_formalizacao&#39;, &#39;prop_ocupados_medio&#39;,
&#39;prop_ocupados_servicos&#39;, &#39;prop_ocupados_superior&#39;,
&#39;prop_ocupados_renda_0&#39;, &#39;renda_media_ocupados&#39;, &#39;indice_treil_trabalho&#39;,
&#39;taxa_ocupados_carteira&#39;, &#39;taxa_agua_encanada&#39;, 
&#39;taxa_banheiro_agua_encanada&#39;, &#39;taxa_coleta_lixo&#39;, &#39;taxa_energia_eletrica&#39;,
&#39;taxa_agua_esgoto_inadequados&#39;, &#39;taxa_criancas_dom_sem_fund&#39;,
&#39;pea&#39;, &#39;indice_escolaridade&#39;, &#39;indice_frequencia_escolar&#39;, 
&#39;idhm&#39;, &#39;idhm_e&#39;, &#39;idhm_l&#39;, &#39;idhm_r&#39;]
extra1 = extra1[[&#39;id_municipio&#39;]+features_extra1]

train = pd.merge(train, extra1, how=&#39;left&#39;, left_on=&#39;CO_MUNICIPIO_RESIDENCIA&#39;, right_on=&#39;id_municipio&#39;)
test = pd.merge(test, extra1, how=&#39;left&#39;, left_on=&#39;CO_MUNICIPIO_RESIDENCIA&#39;, right_on=&#39;id_municipio&#39;)</code></pre>
<ol start="2" style="list-style-type: decimal">
<li><a href="https://www.gov.br/inep/pt-br/acesso-a-informacao/dados-abertos/microdados/censo-escolar">Microdados do Censo Escolar da Educaca√ß√£o B√°sica</a></li>
</ol>
<p>Base dispon√≠vel no mesmo site dos dados da competi√ß√£o e que tr√°s informa√ß√µes muito ricas das escolas do Brasil. Infelizmente quase 75% da informa√ß√£o da escola do aluno era missing ent√£o esta base n√£o conseguiu alavancar os ganhos do modelo de maneira consider√°vel.</p>
<p>Nesta base foquei principalmente nas features utilizadas para calcular o IIE (√çndice de Estrutura da Escola) que se baseia nos seguintes componentes:</p>
<table>
<colgroup>
<col width="32%" />
<col width="24%" />
<col width="42%" />
</colgroup>
<thead>
<tr class="header">
<th>Componente 1: Pedag√≥gica (IEE_Pedag√≥gico):</th>
<th>Componente 2: B√°sica (IEE_B√°sico):</th>
<th>Componente 3: Tecnol√≥gica (IEE_Tecnol√≥gico):</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Qualifica√ß√£o do docente (forma√ß√£o acad√™mica dos professores)</td>
<td>√Ågua filtrada (bin√°ria)</td>
<td>N√∫mero de computadores por aluno (computadores dispon√≠veis para uso dos alunos)</td>
</tr>
<tr class="even">
<td>N√∫mero de alunos por sala</td>
<td>Acesso √† rede p√∫blica de energia (bin√°ria)</td>
<td>N√∫mero de equipamentos multim√≠dia por aluno</td>
</tr>
<tr class="odd">
<td>N√∫mero de funcion√°rios por aluno</td>
<td>Acesso √† rede p√∫blica de esgoto (bin√°ria)</td>
<td>Acesso a internet (bin√°ria)</td>
</tr>
<tr class="even">
<td>Quadra de esportes coberta (bin√°ria)</td>
<td>Coleta peri√≥dica de lixo (bin√°ria)</td>
<td>Laborat√≥rio de Ci√™ncias (bin√°ria)</td>
</tr>
<tr class="odd">
<td>Biblioteca (bin√°ria)</td>
<td>Banheiro dentro do pr√©dio (bin√°ria)</td>
<td>Laborat√≥rio de Inform√°tica (bin√°ria)</td>
</tr>
</tbody>
</table>
<ul>
<li><a href="https://leosalesblog.wordpress.com/2018/02/03/escola-ruim-aluno-ruim-entendendo-a-relacao-entre-estrutura-escolar-e-desempenho-no-enem/">Fonte</a></li>
</ul>
<pre class="python"><code># Importar dados
extra2 = pd.read_csv(&#39;microdados_ed_basica_2021.csv&#39;, error_bad_lines=False, sep=&#39;;&#39;, encoding=&#39;latin1&#39;, dtype={&#39;CO_ORGAO_REGIONAL&#39;: &#39;str&#39;})
extra2 = extra2[extra2.isnull().sum(axis=1) / extra2.shape[1] &lt; .9]

# Tratamento nas features
extra2.loc[:, &#39;QT_TOTAL_ALUNOS&#39;] = extra2[[&#39;QT_MAT_BAS_ND&#39;, &#39;QT_MAT_BAS_BRANCA&#39;, &#39;QT_MAT_BAS_PRETA&#39;, &#39;QT_MAT_BAS_PARDA&#39;, &#39;QT_MAT_BAS_AMARELA&#39;, &#39;QT_MAT_BAS_INDIGENA&#39;]].sum(axis=1).fillna(0)
extra2.loc[:, &#39;QT_TOTAL_PROFESSORES&#39;] = (extra2.QT_DOC_BAS + extra2.QT_DOC_INF + extra2.QT_DOC_INF_CRE + extra2.QT_DOC_INF_PRE + extra2.QT_DOC_FUND + extra2.QT_DOC_FUND_AI + extra2.QT_DOC_FUND_AF + extra2.QT_DOC_MED + extra2.QT_DOC_PROF + extra2.QT_DOC_PROF_TEC + extra2.QT_DOC_EJA + extra2.QT_DOC_EJA_FUND + extra2.QT_DOC_EJA_MED + extra2.QT_DOC_ESP + extra2.QT_DOC_ESP_CC + extra2.QT_DOC_ESP_CE).fillna(0)
extra2.loc[:, &#39;QT_SALAS_UTILIZADAS&#39;] = (extra2.loc[:, &#39;QT_TOTAL_ALUNOS&#39;] / extra2.QT_SALAS_UTILIZADAS).fillna(0)
extra2.loc[:, &#39;QT_COMP_DISP_ALUNO&#39;] = extra2.QT_DESKTOP_ALUNO + extra2.QT_COMP_PORTATIL_ALUNO + extra2.QT_TABLET_ALUNO

# Selecao de faetures importantes
features_extra2 = [&#39;CO_ENTIDADE&#39;, &#39;QT_SALAS_UTILIZADAS&#39;, &#39;QT_TOTAL_PROFESSORES&#39;, &#39;IN_QUADRA_ESPORTES_COBERTA&#39;, &#39;IN_BIBLIOTECA&#39;,
       &#39;IN_AGUA_POTAVEL&#39;, &#39;IN_ENERGIA_REDE_PUBLICA&#39;, &#39;IN_ESGOTO_REDE_PUBLICA&#39;, &#39;IN_LIXO_SERVICO_COLETA&#39;, &#39;IN_BANHEIRO&#39;,
       &#39;QT_COMP_DISP_ALUNO&#39;, &#39;QT_EQUIP_MULTIMIDIA&#39;, &#39;IN_INTERNET&#39;, &#39;IN_LABORATORIO_CIENCIAS&#39;, &#39;IN_LABORATORIO_INFORMATICA&#39;]
extra2 = extra2[features_extra2]

# Remover outliers
for c in list(extra2.iloc[:, 1:].columns):
    trs = extra2.loc[extra2[c]!=88888, c].quantile(.99)
    extra2.loc[(extra2[c]==88888)|(extra2[c]&gt;trs), c] = trs
    
#Normalizar para calcular IEE
scaler = MinMaxScaler()
to_iee = scaler.fit_transform(extra2.iloc[:, 1:])
to_iee = pd.DataFrame(to_iee, columns=extra2.iloc[:, 1:].columns)

# Calcular IEE e componentes
extra2.loc[:, &#39;COMP1&#39;] = to_iee[[&#39;QT_SALAS_UTILIZADAS&#39;, &#39;QT_TOTAL_PROFESSORES&#39;, &#39;IN_QUADRA_ESPORTES_COBERTA&#39;, &#39;IN_BIBLIOTECA&#39;]].sum(axis=1)
extra2.loc[:, &#39;COMP2&#39;] = to_iee[[&#39;IN_AGUA_POTAVEL&#39;, &#39;IN_ENERGIA_REDE_PUBLICA&#39;, &#39;IN_ESGOTO_REDE_PUBLICA&#39;, &#39;IN_LIXO_SERVICO_COLETA&#39;, &#39;IN_BANHEIRO&#39;]].sum(axis=1)
extra2.loc[:, &#39;COMP3&#39;] = to_iee[[&#39;QT_COMP_DISP_ALUNO&#39;, &#39;QT_EQUIP_MULTIMIDIA&#39;, &#39;IN_INTERNET&#39;, &#39;IN_LABORATORIO_CIENCIAS&#39;, &#39;IN_LABORATORIO_INFORMATICA&#39;]].sum(axis=1)
extra2.loc[:, &#39;IEE&#39;] = extra2.COMP1 + extra2.COMP2 + extra2.COMP3

train = pd.merge(train, extra2, how=&#39;left&#39;, left_on=&#39;CO_ESCOLA&#39;, right_on=&#39;CO_ENTIDADE&#39;).drop(&#39;CO_ENTIDADE&#39;, axis=1)
test = pd.merge(test, extra2, how=&#39;left&#39;, left_on=&#39;CO_ESCOLA&#39;, right_on=&#39;CO_ENTIDADE&#39;).drop(&#39;CO_ENTIDADE&#39;, axis=1)</code></pre>
</div>
</div>
<div id="modelagem" class="section level2">
<h2>Modelagem</h2>
<p>Testei muitos modelos e muitas abordagens (inclusive com finalidade de estudo). Foram modelos estat√≠sticos (GAM considerando a distribui√ß√£o Beta(0,1)), redes neurais (TabNet) e √°rvores mas no final das contas os que tiveram melhor custo/benef√≠cio foram o LightGBM e o CatBoost.</p>
<p>Sobre o tuning, tomei a decis√£o de n√£o investir muito em otimiza√ß√£o autom√°tica de hiperpar√¢metros pois o tempo era curto e os ganhos seriam pequenos comparados com o potencial ganho com a variedade de features que poderiam ser geradas, ent√£o fiz apenas alguns testes manuais conforme via necessidade.</p>
<div id="pre-processing" class="section level4">
<h4>Pre processing</h4>
<p>A etapa que investi bastante tempo foi para criar novas vari√°veis. A seguir trago algumas features constru√≠das que foram utilizadas em determinados modelos, a partir dos dados dispon√≠veis:</p>
<ul>
<li>Renda somada dos pais;</li>
<li>N√≠vel de ocupa√ß√£o somado dos pais;</li>
<li>Renda dividido pelo n√∫mero de pessoas na casa;</li>
<li>Quantidade de celulares por pessoa na casa;</li>
<li>Quantidade de computadores por pessoa na casa;</li>
<li>Se a pessoa possui vis√£o ruim (se possui baixa vis√£o, cegueira ou monocular);</li>
<li>Se a pessoa possui audi√ß√£o ruim (Surdez, defici√™ncia auditiva);</li>
<li>Se o aluno possui TDAH e toma medicamento controlado;</li>
<li>Se o aluno possui TDAH e teve mais tempo de prova;</li>
<li>Se precisou de recurso de vis√£o ou audi√ß√£o (libras, baile, etc);</li>
<li>Se o munic√≠pio que nasceu √© o mesmo da escola;</li>
<li>Se o munic√≠pio que fez a prova √© o mesmo da escola;</li>
<li>Se o munic√≠pio da prova √© o mesmo da resid√™ncia;</li>
<li>Nota m√©dia dos alunos da respectiva escola nas outras provas (*);</li>
<li>Renda m√©dia dos alunos da respectiva escola (*).</li>
</ul>
<p>(*) Estas features precisaram ser calculadas de maneira muito cuidadosa para n√£o causar algum tipo de data leak!</p>
</div>
<div id="post-processing" class="section level4">
<h4>Post Processing</h4>
<p>Essa base tinha uma pegadinha que fazia muita diferen√ßa no resultado final. Existem duas possibilidades de um aluno tirar zero em uma prova: errar tudo ou n√£o comparecer.</p>
<p>Como temos a informa√ß√£o da presen√ßa do aluno na prova (o que na pr√°tica seria meio estranho) bastava dar zero para os alunos faltantes na hora de prever nos dados de teste para submeter.</p>
</div>
<div id="linguagens-e-c√≥digos" class="section level3">
<h3>Linguagens e C√≥digos</h3>
<p>Definir finalidade de algumas colunas:</p>
<pre class="python"><code># colunas que serao dropadas
to_drop = [&#39;IN_PROVA_DEITADO&#39;,
            &#39;NU_INSCRICAO&#39;,
            &#39;CO_MUNICIPIO_ESC&#39;,
            &#39;CO_UF_NASCIMENTO&#39;,
            &#39;CO_UF_RESIDENCIA&#39;,
            &#39;CO_UF_ESC&#39;,
            &#39;CO_UF_PROVA&#39;,
            &#39;CO_MUNICIPIO_PROVA&#39;,
            &#39;CO_MUNICIPIO_RESIDENCIA&#39;,
            &#39;CO_MUNICIPIO_NASCIMENTO&#39;]

# definir target e presenca
target = &quot;NU_NOTA_LC&quot;
presenca = &quot;TP_PRESENCA_LC&quot;

# demais notas para dropar (menos ch)
notas = list(set(targets)-set([target, &#39;NU_NOTA_CH&#39;]))</code></pre>
<p>Pr√©-processamento nos dados de treino</p>
<pre class="python"><code>X = train.copy()
X.loc[:, &#39;knn_feature&#39;] = knn_train.knn_oof
X = X.drop(to_drop, axis=1) 
X = X[X[presenca]==1]
X = X[~X[target].isnull()]

X = X.loc[:, ~X.columns.isin([target]+[presenca]+notas)]
X.loc[:, &#39;FE_RENDA&#39;] = X.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000, &#39;C&#39;:1500, &#39;D&#39;:2000,
&#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, &#39;I&#39;:6000, &#39;J&#39;:7000,&#39;K&#39;:8000,&#39;L&#39;:9000,
&#39;M&#39;:10000, &#39;N&#39;:12000, &#39;O&#39;:15000, &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
X = prep_data_questionarios(X)
X = fe_mun(X)
X = fe_questionario(X)
X = fe_in(X)
X = prep_co_escola(X)
X = fe_extra(X)

y = train.loc[(train[presenca]==1)&amp;(~train[target].isnull()), target].astype(np.float64)</code></pre>
<p>Pr√©-processamento nos dados de teste</p>
<pre class="python"><code>X_test = test.copy()
X_test.loc[:, &#39;knn_feature&#39;] = knn_test.knn_test
X_test = X_test.drop(to_drop, axis=1) 

X_test = X_test.loc[:, ~X_test.columns.isin([presenca])]
X_test.loc[:, &#39;FE_RENDA&#39;] = X_test.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000, &#39;C&#39;:1500, &#39;D&#39;:2000,
&#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, &#39;I&#39;:6000, &#39;J&#39;:7000, &#39;K&#39;:8000,&#39;L&#39;:9000,
&#39;M&#39;:10000, &#39;N&#39;:12000, &#39;O&#39;:15000, &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
X_test = prep_data_questionarios(X_test)
X_test = fe_mun(X_test)
X_test = fe_questionario(X_test)
X_test = fe_in(X_test)
X_test = prep_co_escola(X_test)
X_test = fe_extra(X_test)</code></pre>
<p>Feature engineering separada para evitar data leak:</p>
<pre class="python"><code># calcular estatisticas nos dados de treino
co_escola_renda_media = X.groupby(&#39;CO_ESCOLA&#39;).FE_RENDA.mean()
co_escola_idade_media = X.groupby(&#39;CO_ESCOLA&#39;).NU_IDADE.mean()
co_escola_nota_ch = X.groupby(&#39;CO_ESCOLA&#39;).NU_NOTA_CH.mean()
X = X.drop(&#39;NU_NOTA_CH&#39;, axis=1)

# instanciar objeto com as estatisticas por escola
co_escola_aux = pd.DataFrame({
    &#39;CO_ESCOLA&#39;: co_escola_renda_media.index,
    &#39;FE_ESCOLA_RENDA_MEDIA&#39;: co_escola_renda_media,
    &#39;FE_IDADE_MEDIA&#39;: co_escola_idade_media,
    &#39;FE_NOTA_CH&#39;: co_escola_nota_ch
}).reset_index(drop=True)

# Concatenar estatisticas nas bases de treino e teste
X = pd.merge(X, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)
X_test = pd.merge(X_test, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)

# Codigo da escola para categorico
X.loc[:, &#39;CO_ESCOLA&#39;] = X.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)
X_test.loc[:, &#39;CO_ESCOLA&#39;] = X_test.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)

# Features de contagem
X.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X_test.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X.NO_MUNICIPIO_RESIDENCIA.map({x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X_test.NO_MUNICIPIO_RESIDENCIA.map({ x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X.NO_MUNICIPIO_NASCIMENTO.map({ x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X_test.NO_MUNICIPIO_NASCIMENTO.map({x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})

X.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X_test.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})</code></pre>
<p>Ajustar modelo:</p>
<pre class="python"><code>cat_feat = X.columns[X.dtypes==&#39;category&#39;]
cat_indices = [X.columns.get_loc(x) for x in cat_feat]

for c in list(cat_feat):
    X.loc[:, c] = X.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)
    X_test.loc[:, c] = X_test.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)

X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.1, random_state=SEED)
    
clf = CatBoostRegressor(random_state=314,
                            cat_features=cat_indices,
                            verbose=0,
                            loss_function = &quot;RMSE&quot;,
                            od_type = &quot;Iter&quot;,
                            od_wait = 100,
                            iterations=3000,
                            use_best_model=True)

clf.fit(X, y, eval_set = (X_eval, y_eval), verbose=False, plot=True)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/lc_catboost.png" style="width:95.0%" />
</center>
<p>Salvar previs√µes:</p>
<pre class="python"><code>sub.loc[:, &#39;NU_NOTA_LC&#39;] = clf.predict(X_test)
# alunos que nao foram fazer a prova tiraram zero
sub.loc[test.TP_PRESENCA_LC!=1, &#39;NU_NOTA_LC&#39;] = 0</code></pre>
<p>Comparar distribui√ß√£o da target nos dados de treino com rela√ß√£o √†s previs√µes do modelo:</p>
<pre class="python"><code>sns.kdeplot(train.loc[:, target], shade=True, color=&#39;r&#39;, clip=[0,1000])
sns.kdeplot(sub.loc[:, target], shade=True, color=&#39;b&#39;, clip=[0,1000])
plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
plt.title(target)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/lc_pred.png" style="width:50.0%" />
</center>
</div>
<div id="ci√™ncias-humanas" class="section level3">
<h3>Ci√™ncias Humanas</h3>
<p>Novas features desenvolvidas especificamente para este modelo:</p>
<pre class="python"><code>def fe_ch(df):
    
    df.loc[:, &#39;FE_RENDA&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000,
    &#39;C&#39;:1500, &#39;D&#39;:2000, &#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, &#39;I&#39;:6000,
    &#39;J&#39;:7000, &#39;K&#39;:8000,&#39;L&#39;:9000, &#39;M&#39;:10000, &#39;N&#39;:12000, &#39;O&#39;:15000, 
    &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
    df.loc[:, &#39;FE_NU_IDADE*TP_ANO_CONCLUIU&#39;] = df.TP_ANO_CONCLUIU * df.NU_IDADE
    df.loc[:, &#39;FE_Q002+Q024&#39;] = df.loc[:, &#39;Q002&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, 
    &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}).astype(int) + 
    df.loc[:, &#39;Q024&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}).astype(int) 
    df.loc[:, &#39;FE_SCORE&#39;] = (1/df.TP_ANO_CONCLUIU) + np.sqrt(df.NU_IDADE) +
    np.where(df.TP_ESCOLA==3, 1, 0)
    
    return df</code></pre>
<p>Definir finalidade de algumas colunas:</p>
<pre class="python"><code># colunas que serao dropadas
to_drop = [&#39;IN_PROVA_DEITADO&#39;,
           &#39;NU_INSCRICAO&#39;,
           &#39;CO_MUNICIPIO_ESC&#39;,
           &#39;CO_UF_NASCIMENTO&#39;,
           &#39;CO_UF_RESIDENCIA&#39;,
           &#39;CO_UF_ESC&#39;,
           &#39;CO_UF_PROVA&#39;,
           &#39;CO_MUNICIPIO_PROVA&#39;,
           &#39;CO_MUNICIPIO_RESIDENCIA&#39;,
          &#39;CO_MUNICIPIO_NASCIMENTO&#39;]

# definir target e presenca
target = &quot;NU_NOTA_CH&quot;
presenca = &quot;TP_PRESENCA_CH&quot;

# demais notas para dropar (menos lc)
notas = list(set(targets)-set([target, &#39;NU_NOTA_LC&#39;]))</code></pre>
<p>Pr√©-processamento nos dados de treino</p>
<pre class="python"><code>X = train.copy()
X.loc[:, &#39;knn_feature&#39;] = knn_train.knn_oof
X = X.drop(to_drop, axis=1)
X = X[X[presenca]==1]
X = X[~X[target].isnull()]

X = X.loc[:, ~X.columns.isin([target]+[presenca]+notas)]
X = fe_ch(X)
X = prep_data_questionarios(X)
X = fe_mun(X)
X = fe_questionario(X)
#X = fe_in(X)
X = prep_co_escola(X)
X = fe_extra(X)

y = train.loc[(train[presenca]==1)&amp;(~train[target].isnull()), target].astype(np.float64)</code></pre>
<p>Pr√©-processamento nos dados de teste</p>
<pre class="python"><code>X_test = test.copy()
X_test.loc[:, &#39;knn_feature&#39;] = knn_test.knn_test
X_test = X_test.drop(to_drop, axis=1) 

X_test = X_test.loc[:, ~X_test.columns.isin([presenca])]
X_test = fe_ch(X_test)
X_test = prep_data_questionarios(X_test)
X_test = fe_mun(X_test)
X_test = fe_questionario(X_test)
#X_test = fe_in(X_test)
X_test = prep_co_escola(X_test)
X_test = fe_extra(X_test)</code></pre>
<p>Feature engineering separada para evitar data leak:</p>
<pre class="python"><code># calcular estatisticas nos dados de treino
co_escola_renda_media = X.groupby(&#39;CO_ESCOLA&#39;).FE_RENDA.mean()
co_escola_idade_media = X.groupby(&#39;CO_ESCOLA&#39;).NU_IDADE.mean()
co_escola_nota_lc = X.groupby(&#39;CO_ESCOLA&#39;).NU_NOTA_LC.mean()
X = X.drop(&#39;NU_NOTA_LC&#39;, axis=1)

# instanciar objeto com as estatisticas por escola
co_escola_aux = pd.DataFrame({
    &#39;CO_ESCOLA&#39;: co_escola_renda_media.index,
    &#39;FE_ESCOLA_RENDA_MEDIA&#39;: co_escola_renda_media,
    &#39;FE_IDADE_MEDIA&#39;: co_escola_idade_media,
    &#39;FE_NOTA_LC&#39;: co_escola_nota_lc
}).reset_index(drop=True)

# Concatenar estatisticas nas bases de treino e teste
X = pd.merge(X, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)
X_test = pd.merge(X_test, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)

# Codigo da escola para categorico
X.loc[:, &#39;CO_ESCOLA&#39;] = X.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)
X_test.loc[:, &#39;CO_ESCOLA&#39;] = X_test.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)

# Features de contagem
X.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X_test.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X.NO_MUNICIPIO_RESIDENCIA.map({x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X_test.NO_MUNICIPIO_RESIDENCIA.map({ x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X.NO_MUNICIPIO_NASCIMENTO.map({ x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X_test.NO_MUNICIPIO_NASCIMENTO.map({x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})

X.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X_test.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})</code></pre>
<p>Ajustar modelo:</p>
<pre class="python"><code>%%time

cat_feat = X.columns[X.dtypes==&#39;category&#39;]
cat_indices = [X.columns.get_loc(x) for x in cat_feat]

for c in list(cat_feat):
    X.loc[:, c] = X.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)
    X_test.loc[:, c] = X_test.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)

X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.1, random_state=SEED)

clf = CatBoostRegressor(random_state=314,
                            cat_features=cat_indices,
                            verbose=0,
                            loss_function = &quot;RMSE&quot;,
                            od_type = &quot;Iter&quot;,
                            od_wait = 100,iterations=3000,
                            use_best_model=True)

clf.fit(X, y, eval_set = (X_eval, y_eval), verbose=False, plot=True)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/ch_catboost.png" style="width:95.0%" />
</center>
<p>Salvar previs√µes:</p>
<pre class="python"><code>sub.loc[:, &#39;NU_NOTA_CH&#39;] = clf.predict(X_test)
# alunos que nao foram fazer a prova tiraram zero
sub.loc[test.TP_PRESENCA_CH!=1, &#39;NU_NOTA_CH&#39;] = 0</code></pre>
<p>Comparar distribui√ß√£o da target nos dados de treino com rela√ß√£o √†s previs√µes do modelo:</p>
<pre class="python"><code>sns.kdeplot(train.loc[:, target], shade=True, color=&#39;r&#39;, clip=[0,1000])
sns.kdeplot(sub.loc[:, target], shade=True, color=&#39;b&#39;, clip=[0,1000])
plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
plt.title(target)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/ch_pred.png" style="width:50.0%" />
</center>
</div>
<div id="ci√™ncias-da-natureza" class="section level3">
<h3>Ci√™ncias da Natureza</h3>
<p>Novas features desenvolvidas especificamente para este modelo:</p>
<pre class="python"><code>def fe_cn(df):
    df.loc[:, &#39;FE_RENDA&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000,
    &#39;C&#39;:1500, &#39;D&#39;:2000, &#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, 
    &#39;I&#39;:6000, &#39;J&#39;:7000, &#39;K&#39;:8000,&#39;L&#39;:9000, &#39;M&#39;:10000, &#39;N&#39;:12000, 
    &#39;O&#39;:15000, &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
    df.loc[:, &#39;FE_NU_IDADE*TP_ANO_CONCLUIU&#39;] = df.TP_ANO_CONCLUIU * df.NU_IDADE
    df.loc[:, &#39;FE_Q002+Q024&#39;] = df.loc[:, &#39;Q002&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2,
    &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}).astype(int) + 
    df.loc[:, &#39;Q024&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}).astype(int) 
    df.loc[:, &#39;FE_SCORE&#39;] = (1/df.TP_ANO_CONCLUIU) + np.sqrt(df.NU_IDADE) + np.where(df.TP_ESCOLA==3, 1, 0)
    
    df.loc[:, &#39;FE_UF_ESCOLA&#39;] = df.SG_UF_ESC.map({
      &#39;AM&#39;:&#39;Norte&#39;, &#39;RR&#39;:&#39;Norte&#39;, &#39;AP&#39;:&#39;Norte&#39;, &#39;PA&#39;:&#39;Norte&#39;, &#39;TO&#39;:&#39;Norte&#39;, &#39;RO&#39;:&#39;Norte&#39;, &#39;AC&#39;:&#39;Norte&#39;,
      &#39;MA&#39;:&#39;Nordeste&#39;, &#39;PI&#39;:&#39;Nordeste&#39;, &#39;CE&#39;:&#39;Nordeste&#39;, &#39;RN&#39;:&#39;Nordeste&#39;, &#39;PE&#39;:&#39;Nordeste&#39;, &#39;PB&#39;:&#39;Nordeste&#39;, &#39;SE&#39;:&#39;Nordeste&#39;, &#39;AL&#39;:&#39;Nordeste&#39;, &#39;BA&#39;:&#39;Nordeste&#39;,
      &#39;MT&#39;: &#39;CentroOeste&#39;, &#39;MS&#39;: &#39;CentroOeste&#39;, &#39;GO&#39;: &#39;CentroOeste&#39;,
      &#39;SP&#39;: &#39;Sudeste&#39;, &#39;RJ&#39;: &#39;Sudeste&#39;, &#39;ES&#39;: &#39;Sudeste&#39;, &#39;MG&#39;: &#39;Sudeste&#39;,
      &#39;PR&#39;: &#39;Sul&#39;, &#39;RS&#39;: &#39;Sul&#39;, &#39;SC&#39;: &#39;Sul&#39;}).astype(&#39;category&#39;)
    return df</code></pre>
<p>Definir finalidade de algumas colunas:</p>
<pre class="python"><code># colunas que serao dropadas
to_drop = [&#39;IN_PROVA_DEITADO&#39;,
           &#39;NU_INSCRICAO&#39;,
           &#39;CO_MUNICIPIO_ESC&#39;,
           &#39;CO_UF_NASCIMENTO&#39;,
           &#39;CO_UF_RESIDENCIA&#39;,
           &#39;CO_UF_ESC&#39;,
           &#39;CO_UF_PROVA&#39;,
           &#39;CO_MUNICIPIO_PROVA&#39;,
           &#39;CO_MUNICIPIO_RESIDENCIA&#39;,
          &#39;CO_MUNICIPIO_NASCIMENTO&#39;]

# definir target e presenca
target = &quot;NU_NOTA_CN&quot;
presenca = &quot;TP_PRESENCA_CN&quot;

# demais notas para dropar (menos mt)
notas = list(set(targets)-set([target, &#39;NU_NOTA_MT&#39;]))</code></pre>
<p>Pr√©-processamento nos dados de treino</p>
<pre class="python"><code>X = train.copy()
X = X.drop(to_drop, axis=1) 
X = X[X[presenca]==1]
X = X[~X[target].isnull()]

X = X.loc[:, ~X.columns.isin([target]+[presenca]+notas)]
X = fe_cn(X)
X = prep_data_questionarios(X)
X = fe_mun(X)
X = fe_questionario(X)
X = fe_in(X)
X = prep_co_escola(X)
X = fe_extra(X)

y = train.loc[(train[presenca]==1)&amp;(~train[target].isnull()), target].astype(np.float64)</code></pre>
<p>Pr√©-processamento nos dados de teste</p>
<pre class="python"><code>X_test = test.copy()
X_test = X_test.drop(to_drop, axis=1) 

X_test = X_test.loc[:, ~X_test.columns.isin([presenca])]
X_test = fe_cn(X_test)
X_test = prep_data_questionarios(X_test)
X_test = fe_mun(X_test)
X_test = fe_questionario(X_test)
X_test = fe_in(X_test)
X_test = prep_co_escola(X_test)
X_test = fe_extra(X_test)</code></pre>
<p>Feature engineering separada para evitar data leak:</p>
<pre class="python"><code># calcular estatisticas nos dados de treino
co_escola_renda_media = X.groupby(&#39;CO_ESCOLA&#39;).FE_RENDA.mean()
co_escola_idade_media = X.groupby(&#39;CO_ESCOLA&#39;).NU_IDADE.mean()
co_escola_nota_mt = X.groupby(&#39;CO_ESCOLA&#39;).NU_NOTA_MT.mean()
X = X.drop(&#39;NU_NOTA_MT&#39;, axis=1)

# instanciar objeto com as estatisticas por escola
co_escola_aux = pd.DataFrame({
    &#39;CO_ESCOLA&#39;: co_escola_renda_media.index,
    &#39;FE_ESCOLA_RENDA_MEDIA&#39;: co_escola_renda_media,
    &#39;FE_IDADE_MEDIA&#39;: co_escola_idade_media,
    &#39;FE_NOTA_MT&#39;: co_escola_nota_mt
}).reset_index(drop=True)

# Concatenar estatisticas nas bases de treino e teste
X = pd.merge(X, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)
X_test = pd.merge(X_test, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)

# Codigo da escola para categorico
X.loc[:, &#39;CO_ESCOLA&#39;] = X.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)
X_test.loc[:, &#39;CO_ESCOLA&#39;] = X_test.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)

# Features de contagem
X.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X_test.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X.NO_MUNICIPIO_RESIDENCIA.map({x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X_test.NO_MUNICIPIO_RESIDENCIA.map({ x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X.NO_MUNICIPIO_NASCIMENTO.map({ x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X_test.NO_MUNICIPIO_NASCIMENTO.map({x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})

X.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X_test.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})</code></pre>
<p>Ajustar modelo:</p>
<pre class="python"><code>%%time

cat_feat = X.columns[X.dtypes==&#39;category&#39;]
cat_indices = [X.columns.get_loc(x) for x in cat_feat]

for c in list(cat_feat):
    X.loc[:, c] = X.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)
    X_test.loc[:, c] = X_test.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)

X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.1, random_state=SEED)

clf = CatBoostRegressor(random_state=314,
                            cat_features=cat_indices,
                            verbose=0,
                            loss_function = &quot;RMSE&quot;,
                            od_type = &quot;Iter&quot;,
                            od_wait = 100,iterations=3000,
                            use_best_model=True)

clf.fit(X, y, eval_set = (X_eval, y_eval), verbose=False, plot=True)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/cn_catboost.png" style="width:95.0%" />
</center>
<p>Salvar previs√µes:</p>
<pre class="python"><code>sub.loc[:, &#39;NU_NOTA_CN&#39;] = clf.predict(X_test)
# alunos que nao foram fazer a prova tiraram zero
sub.loc[test.TP_PRESENCA_CN!=1, &#39;NU_NOTA_CN&#39;] = 0</code></pre>
<p>Comparar distribui√ß√£o da target nos dados de treino com rela√ß√£o √†s previs√µes do modelo:</p>
<pre class="python"><code>sns.kdeplot(train.loc[:, target], shade=True, color=&#39;r&#39;, clip=[0,1000])
sns.kdeplot(sub.loc[:, target], shade=True, color=&#39;b&#39;, clip=[0,1000])
plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
plt.title(target)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/cn_pred.png" style="width:50.0%" />
</center>
</div>
<div id="matem√°tica" class="section level3">
<h3>Matem√°tica</h3>
<p>Novas features desenvolvidas especificamente para este modelo:</p>
<pre class="python"><code>def fe_mt(df):
    
    df.loc[:, &#39;FE_RENDA&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000, &#39;C&#39;:1500, &#39;D&#39;:2000, &#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, &#39;I&#39;:6000, &#39;J&#39;:7000, &#39;K&#39;:8000,&#39;L&#39;:9000, &#39;M&#39;:10000, &#39;N&#39;:12000, &#39;O&#39;:15000, &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
    df.loc[:, &#39;FE_NU_IDADE*TP_ANO_CONCLUIU&#39;] = df.TP_ANO_CONCLUIU * df.NU_IDADE
    df.loc[:, &#39;FE_Q002+Q024&#39;] = df.loc[:, &#39;Q002&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}).astype(int) + df.loc[:, &#39;Q024&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}).astype(int) 
    df.loc[:, &#39;FE_SCORE&#39;] = (1/df.TP_ANO_CONCLUIU) + np.sqrt(df.NU_IDADE) + np.where(df.TP_ESCOLA==3, 1, 0)
    
    df.loc[:, &#39;FE_UF_ESCOLA&#39;] = df.SG_UF_ESC.map({&#39;AM&#39;:&#39;Norte&#39;, &#39;RR&#39;:&#39;Norte&#39;, &#39;AP&#39;:&#39;Norte&#39;, &#39;PA&#39;:&#39;Norte&#39;, &#39;TO&#39;:&#39;Norte&#39;, &#39;RO&#39;:&#39;Norte&#39;, &#39;AC&#39;:&#39;Norte&#39;,
                &#39;MA&#39;:&#39;Nordeste&#39;, &#39;PI&#39;:&#39;Nordeste&#39;, &#39;CE&#39;:&#39;Nordeste&#39;, &#39;RN&#39;:&#39;Nordeste&#39;, &#39;PE&#39;:&#39;Nordeste&#39;, &#39;PB&#39;:&#39;Nordeste&#39;, &#39;SE&#39;:&#39;Nordeste&#39;, &#39;AL&#39;:&#39;Nordeste&#39;, &#39;BA&#39;:&#39;Nordeste&#39;,
                &#39;MT&#39;: &#39;CentroOeste&#39;, &#39;MS&#39;: &#39;CentroOeste&#39;, &#39;GO&#39;: &#39;CentroOeste&#39;,
                &#39;SP&#39;: &#39;Sudeste&#39;, &#39;RJ&#39;: &#39;Sudeste&#39;, &#39;ES&#39;: &#39;Sudeste&#39;, &#39;MG&#39;: &#39;Sudeste&#39;,
                &#39;PR&#39;: &#39;Sul&#39;, &#39;RS&#39;: &#39;Sul&#39;, &#39;SC&#39;: &#39;Sul&#39;}).astype(&#39;category&#39;)
    
    
    return df</code></pre>
<p>Definir finalidade de algumas colunas:</p>
<pre class="python"><code># colunas que serao dropadas
to_drop = [&#39;IN_PROVA_DEITADO&#39;,
           &#39;NU_INSCRICAO&#39;,
           &#39;CO_MUNICIPIO_ESC&#39;,
           &#39;CO_UF_NASCIMENTO&#39;,
           &#39;CO_UF_RESIDENCIA&#39;,
           &#39;CO_UF_ESC&#39;,
           &#39;CO_UF_PROVA&#39;,
           &#39;CO_MUNICIPIO_PROVA&#39;,
           &#39;CO_MUNICIPIO_RESIDENCIA&#39;,
          &#39;CO_MUNICIPIO_NASCIMENTO&#39;]

# definir target e presenca
target = &quot;NU_NOTA_MT&quot;
presenca = &quot;TP_PRESENCA_MT&quot;

# demais notas para dropar (menos cn)
notas = list(set(targets)-set([target, &#39;NU_NOTA_CN&#39;]))</code></pre>
<p>Pr√©-processamento nos dados de treino</p>
<pre class="python"><code>X = train.copy()
X = X.drop(to_drop, axis=1) 
X = X[X[presenca]==1]
X = X[~X[target].isnull()]

X = X.loc[:, ~X.columns.isin([target]+[presenca]+notas)]
X = fe_mt(X)
X = prep_data_questionarios(X)
X = fe_mun(X)
#X = fe_questionario(X)
#X = fe_in(X)
X = prep_co_escola(X)
X = fe_extra(X)

y = train.loc[(train[presenca]==1)&amp;(~train[target].isnull()), target].astype(np.float64)</code></pre>
<p>Pr√©-processamento nos dados de teste</p>
<pre class="python"><code>X_test = test.copy()
X_test = X_test.drop(to_drop, axis=1) 

X_test = X_test.loc[:, ~X_test.columns.isin([presenca])]
X_test = fe_mt(X_test)
X_test = prep_data_questionarios(X_test)
X_test = fe_mun(X_test)
#X_test = fe_questionario(X_test)
#X_test = fe_in(X_test)
X_test = prep_co_escola(X_test)
X_test = fe_extra(X_test)</code></pre>
<p>Feature engineering separada para evitar data leak:</p>
<pre class="python"><code># calcular estatisticas nos dados de treino
co_escola_renda_media = X.groupby(&#39;CO_ESCOLA&#39;).FE_RENDA.mean()
co_escola_idade_media = X.groupby(&#39;CO_ESCOLA&#39;).NU_IDADE.mean()
co_escola_nota_cn = X.groupby(&#39;CO_ESCOLA&#39;).NU_NOTA_CN.mean()
X = X.drop(&#39;NU_NOTA_CN&#39;, axis=1)

# instanciar objeto com as estatisticas por escola
co_escola_aux = pd.DataFrame({
    &#39;CO_ESCOLA&#39;: co_escola_renda_media.index,
    &#39;FE_ESCOLA_RENDA_MEDIA&#39;: co_escola_renda_media,
    &#39;FE_IDADE_MEDIA&#39;: co_escola_idade_media,
    &#39;FE_NOTA_CN&#39;: co_escola_nota_cn
}).reset_index(drop=True)

# Concatenar estatisticas nas bases de treino e teste
X = pd.merge(X, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)
X_test = pd.merge(X_test, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)

# Codigo da escola para categorico
X.loc[:, &#39;CO_ESCOLA&#39;] = X.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)
X_test.loc[:, &#39;CO_ESCOLA&#39;] = X_test.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)

# Features de contagem
X.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X_test.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X.NO_MUNICIPIO_RESIDENCIA.map({x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X_test.NO_MUNICIPIO_RESIDENCIA.map({ x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X.NO_MUNICIPIO_NASCIMENTO.map({ x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X_test.NO_MUNICIPIO_NASCIMENTO.map({x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})

X.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X_test.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})</code></pre>
<p>Ajustar modelo:</p>
<pre class="python"><code>%%time

cat_feat = X.columns[X.dtypes==&#39;category&#39;]
cat_indices = [X.columns.get_loc(x) for x in cat_feat]

for c in list(cat_feat):
    X.loc[:, c] = X.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)
    X_test.loc[:, c] = X_test.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)

X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.1, random_state=SEED)
    
clf = CatBoostRegressor(random_state=314,
                            cat_features=cat_indices,
                            verbose=0,
                            loss_function = &quot;RMSE&quot;,
                            od_type = &quot;Iter&quot;,
                            od_wait = 100,iterations=3000,
                            use_best_model=True)

clf.fit(X, y, eval_set = (X_eval, y_eval), verbose=False, plot=True)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/mt_catboost.png" style="width:95.0%" />
</center>
<p>Salvar previs√µes:</p>
<pre class="python"><code>sub.loc[:, &#39;NU_NOTA_MT&#39;] = clf.predict(X_test)
# alunos que nao foram fazer a prova tiraram zero
sub.loc[test.TP_PRESENCA_CN!=1, &#39;NU_NOTA_MT&#39;] = 0</code></pre>
<p>Comparar distribui√ß√£o da target nos dados de treino com rela√ß√£o √†s previs√µes do modelo:</p>
<pre class="python"><code>sns.kdeplot(train.loc[:, target], shade=True, color=&#39;r&#39;, clip=[0,1000])
sns.kdeplot(sub.loc[:, target], shade=True, color=&#39;b&#39;, clip=[0,1000])
plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
plt.title(target)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/mt_pred.png" style="width:50.0%" />
</center>
</div>
<div id="reda√ß√£o" class="section level3">
<h3>Reda√ß√£o</h3>
<p>Novas features desenvolvidas especificamente para este modelo:</p>
<pre class="python"><code>def fe_rd(df):
    
    df.loc[:, &#39;FE_RENDA&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000, &#39;C&#39;:1500, &#39;D&#39;:2000, &#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, &#39;I&#39;:6000, &#39;J&#39;:7000, &#39;K&#39;:8000,&#39;L&#39;:9000, &#39;M&#39;:10000, &#39;N&#39;:12000, &#39;O&#39;:15000, &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
    df.loc[:, &#39;FE_NU_IDADE*TP_ANO_CONCLUIU&#39;] = df.TP_ANO_CONCLUIU * df.NU_IDADE
    df.loc[:, &#39;FE_Q002+Q024&#39;] = df.loc[:, &#39;Q002&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}).astype(int) + df.loc[:, &#39;Q024&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}).astype(int) 
    df.loc[:, &#39;FE_SCORE&#39;] = (1/df.TP_ANO_CONCLUIU) + np.sqrt(df.NU_IDADE) + np.where(df.TP_ESCOLA==3, 1, 0)
    
    df.loc[:, &#39;FE_RENDA_FAMILIA_+_IDADE&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;:5, &#39;G&#39;:6, &#39;H&#39;:7, &#39;I&#39;:8, &#39;J&#39;:9, &#39;K&#39;:10,&#39;L&#39;:11, &#39;M&#39;:12, &#39;N&#39;:13, &#39;O&#39;:14, &#39;P&#39;:15, &#39;Q&#39;:16}).astype(int) + df.NU_IDADE        
    df.loc[:, &#39;FE_RENDA_FAMILIA_+_ANO_CONCLUIU&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;:5, &#39;G&#39;:6, &#39;H&#39;:7, &#39;I&#39;:8, &#39;J&#39;:9, &#39;K&#39;:10,&#39;L&#39;:11, &#39;M&#39;:12, &#39;N&#39;:13, &#39;O&#39;:14, &#39;P&#39;:15, &#39;Q&#39;:16}).astype(int)+ df.TP_ANO_CONCLUIU  
    
    return df</code></pre>
<p>Definir finalidade de algumas colunas:</p>
<pre class="python"><code># colunas que serao dropadas
to_drop = [&#39;IN_PROVA_DEITADO&#39;,
           &#39;NU_INSCRICAO&#39;,
           &#39;CO_MUNICIPIO_ESC&#39;,
           &#39;CO_UF_NASCIMENTO&#39;,
           &#39;CO_UF_RESIDENCIA&#39;,
           &#39;CO_UF_ESC&#39;,
           &#39;CO_UF_PROVA&#39;,
           &#39;CO_MUNICIPIO_PROVA&#39;,
           &#39;CO_MUNICIPIO_RESIDENCIA&#39;,
          &#39;CO_MUNICIPIO_NASCIMENTO&#39;]

# definir target e presenca
target = &quot;NU_NOTA_REDACAO&quot;
presenca = &quot;TP_STATUS_REDACAO&quot;

# demais notas para dropar 
notas = list(set(targets)-set([target]))</code></pre>
<p>Pr√©-processamento nos dados de treino</p>
<pre class="python"><code>X = train.copy()
X = X.drop(to_drop, axis=1) 
X = X[X[presenca]==1]
X = X[~X[target].isnull()]


X = X.loc[:, ~X.columns.isin([target]+[presenca]+notas)]
X = fe_rd(X)
X = prep_data_questionarios(X)
X = fe_mun(X)
#X = fe_questionario(X)
X = fe_in(X)
X = prep_co_escola(X)
X = fe_extra(X)

y = train.loc[(train[presenca]==1)&amp;(~train[target].isnull()), target].astype(np.float64)</code></pre>
<p>Pr√©-processamento nos dados de teste</p>
<pre class="python"><code>X_test = test.copy()
X_test = X_test.drop(to_drop, axis=1) 

X_test = X_test.loc[:, ~X_test.columns.isin([presenca])]
X_test = fe_rd(X_test)
X_test = prep_data_questionarios(X_test)
X_test = fe_mun(X_test)
#X_test = fe_questionario(X_test)
X_test = fe_in(X_test)
X_test = prep_co_escola(X_test)
X_test = fe_extra(X_test)</code></pre>
<p>Feature engineering separada para evitar data leak:</p>
<pre class="python"><code># calcular estatisticas nos dados de treino
co_escola_renda_media = X.groupby(&#39;CO_ESCOLA&#39;).FE_RENDA.mean()
co_escola_idade_media = X.groupby(&#39;CO_ESCOLA&#39;).NU_IDADE.mean()

# instanciar objeto com as estatisticas por escola
co_escola_aux = pd.DataFrame({
    &#39;CO_ESCOLA&#39;: co_escola_renda_media.index,
    &#39;FE_ESCOLA_RENDA_MEDIA&#39;: co_escola_renda_media,
    &#39;FE_IDADE_MEDIA&#39;: co_escola_idade_media
}).reset_index(drop=True)

# Concatenar estatisticas nas bases de treino e teste
X = pd.merge(X, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)
X_test = pd.merge(X_test, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)

# Codigo da escola para categorico
X.loc[:, &#39;CO_ESCOLA&#39;] = X.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)
X_test.loc[:, &#39;CO_ESCOLA&#39;] = X_test.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)

# Features de contagem
X.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X_test.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X.NO_MUNICIPIO_RESIDENCIA.map({x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X_test.NO_MUNICIPIO_RESIDENCIA.map({ x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X.NO_MUNICIPIO_NASCIMENTO.map({ x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X_test.NO_MUNICIPIO_NASCIMENTO.map({x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})

X.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X_test.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})</code></pre>
<p>Ajustar modelo:</p>
<pre class="python"><code>%%time

cat_feat = X.columns[X.dtypes==&#39;category&#39;]
cat_indices = [X.columns.get_loc(x) for x in cat_feat]

for c in list(cat_feat):
    X.loc[:, c] = X.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)
    X_test.loc[:, c] = X_test.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)

X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.1, random_state=SEED)

clf = CatBoostRegressor(random_state=314,
                            cat_features=cat_indices,
                            verbose=0,
                            loss_function = &quot;RMSE&quot;,
                            od_type = &quot;Iter&quot;,
                            od_wait = 100,iterations=3000,
                            use_best_model=True)

clf.fit(X, y, eval_set = (X_eval, y_eval), verbose=False, plot=True)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/redacao_catboost.png" style="width:95.0%" />
</center>
<p>Salvar previs√µes:</p>
<pre class="python"><code>sub.loc[:, &#39;NU_NOTA_REDACAO&#39;] = clf.predict(X_test)
# alunos que nao foram fazer a prova tiraram zero
sub.loc[test.TP_STATUS_REDACAO!=1, &#39;NU_NOTA_REDACAO&#39;] = 0</code></pre>
<p>Comparar distribui√ß√£o da target nos dados de treino com rela√ß√£o √†s previs√µes do modelo:</p>
<pre class="python"><code>sns.kdeplot(train.loc[:, target], shade=True, color=&#39;r&#39;, clip=[0,1000])
sns.kdeplot(sub.loc[:, target], shade=True, color=&#39;b&#39;, clip=[0,1000])
plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
plt.title(target)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/redacao_pred.png" style="width:50.0%" />
</center>
</div>
</div>
</div>
<div id="submiss√£o" class="section level1">
<h1>Submiss√£o</h1>
<p>Veja a seguir como ficou a distribui√ß√£o das previs√µes comparada √† distribui√ß√£o da target nos dados de treino:</p>
<pre class="python"><code>plt.figure(figsize=(16, 5))

notas = [&#39;NU_NOTA_CH&#39;, &#39;NU_NOTA_CN&#39;, &#39;NU_NOTA_MT&#39;, &#39;NU_NOTA_LC&#39;, &#39;NU_NOTA_REDACAO&#39;]

for i in range(len(notas)):

    plt.subplot(1, 5, i+1)
    sns.kdeplot(train.loc[:, notas[i]], shade=True, color=&#39;r&#39;, clip=[0,1000])
    sns.kdeplot(sub.loc[:, notas[i]], shade=True, color=&#39;b&#39;, clip=[0,1000])
    plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
    plt.title(notas[i])
plt.tight_layout()
plt.show()</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/all_pred.png" style="width:95.0%" />
</center>
<p>Acredito que talvez um tuning do modelo poderia trazer mais qualidade √†s previs√µes mas com o tempo limitado n√£o pude investir muito nesta etapa.</p>
</div>
<div id="considera√ß√µes-finais" class="section level1">
<h1>Considera√ß√µes Finais</h1>
<p>Em resumo, essas foram as principais id√©ias para a solu√ß√£o da competi√ß√£o e acredito que um dos segredos era focar em feature engineering por 2 motivos:</p>
<ul>
<li>A base era muito grande e o processo de tuning seria muito custoso (a n√£o ser que tenha um √≥timo computador a disposi√ß√£o);</li>
<li>Os atributos n√£o eram an√¥nimos, o que d√° muita informa√ß√£o de contexto.</li>
</ul>
<p>Agrade√ßo aos organizadores e √† todos os participantes que tornaram esta competi√ß√£o t√£o divertida! Por mais competi√ß√µes como esta, que valorizam a comunidade brasileira de Data Science!</p>
<p>Espero que tenham gostado e at√© logo!</p>
<p>Abra√ßos!</p>
<p>Fellipe Gomes</p>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2022-04-20-solucao-final-education-quality-kaggle-competition/">Solu√ß√£o Final - ML Olympiad [2¬∫ lugar]</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">catboost</category>
      <category domain="tag">data-mining</category>
      <category domain="tag">data-science</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">machine-learning</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">pratica</category>
      <category domain="tag">r</category>
      <category domain="tag">regressao</category>
    </item>
    <item>
      <title>Solu√ß√£o Final - Porto Seguro Data Challenge [3¬∫ lugar]</title>
      <link>https://gomesfellipe.github.io/post/2021-11-01-solucao-final-porto-seguro-data-challenge/</link>
      <pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2021-11-01-solucao-final-porto-seguro-data-challenge/</guid>
      <description>Confira a estrat√©gia aplicada para a competi√ß√£o de machine learning do Porto Seguro hospedada no Kaggle</description>
      <content:encoded>&lt;![CDATA[
        

<div id="TOC">
<ul>
<li><a href="#introdu%C3%A7%C3%A3o" id="toc-introdu√ß√£o">Introdu√ß√£o</a></li>
<li><a href="#defini%C3%A7%C3%A3o-do-problema-de-neg%C3%B3cio" id="toc-defini√ß√£o-do-problema-de-neg√≥cio">Defini√ß√£o do problema de neg√≥cio</a></li>
<li><a href="#an%C3%A1lise-explorat%C3%B3ria-em-r" id="toc-an√°lise-explorat√≥ria-em-r">An√°lise Explorat√≥ria (em R)</a></li>
<li><a href="#machine-learning-em-python" id="toc-machine-learning-em-python">Machine Learning (em Python)</a>
<ul>
<li><a href="#importar-depend%C3%AAncias" id="toc-importar-depend√™ncias">Importar depend√™ncias</a></li>
<li><a href="#stage-0-feature-extraction-com-knn" id="toc-stage-0-feature-extraction-com-knn">Stage 0: Feature Extraction com KNN</a></li>
<li><a href="#stage-1-tuning-xgboost-com-optuna" id="toc-stage-1-tuning-xgboost-com-optuna">Stage 1: Tuning XGBoost com Optuna</a></li>
<li><a href="#stage-2-calcular-out-of-fold-shap-values" id="toc-stage-2-calcular-out-of-fold-shap-values">Stage 2: Calcular Out-Of-Fold SHAP values</a></li>
<li><a href="#stage-3-modelo-final-com-autogluon" id="toc-stage-3-modelo-final-com-autogluon">Stage 3: Modelo Final com AutoGluon</a></li>
</ul></li>
<li><a href="#conclus%C3%A3o" id="toc-conclus√£o">Conclus√£o</a></li>
<li><a href="#refer%C3%AAncias" id="toc-refer√™ncias">Refer√™ncias</a></li>
</ul>
</div>

<style>
.column {
float: left;
width: 50%;
padding: 10px;
}

.column4 {
float: left;
width: 20%;
padding: 10px;
}

.column8 {
float: left;
width: 80%;
padding: 10px;
}

.row:after {
content: "";
display: table;
clear: both;
}

.center {
display: flex;
justify-content: center;
align-items: center;
height: 200px;
}
</style>
<hr />
<div id="introdu√ß√£o" class="section level1">
<h1>Introdu√ß√£o</h1>
<div class="row">
<div class="column8">
<p>Em Agosto e 2021 a Porto Seguro lan√ßou um desafio no Kaggle que consistia em estimar a propens√£o de aquisi√ß√£o de novos produtos. Tratava-se de um problema de classifica√ß√£o e foi bem desafiador principalmente por 2 motivos:</p>
<ol style="list-style-type: decimal">
<li>Todas as features da base de ddos eram anonimas;</li>
<li>A m√©trica de avalia√ß√£o foi a F1 Score (sens√≠vel √† um ponto de corte)</li>
</ol>
</div>
<div class="column4">
<div class="float">
<img src="https://media.giphy.com/media/Ie2Hs3A0uJRtK/giphy.gif" alt="Via Giphy" />
<div class="figcaption"><a href="https://media.giphy.com/media/Ie2Hs3A0uJRtK/giphy.gif">Via Giphy</a></div>
</div>
</div>
</div>
<p>Depois de 2 longos meses e dezenas de notebooks desenvolvidos, muitas submiss√µes frustradas e muitas horas a menos de sono, cheguei em uma solu√ß√£o final que envole um <em>blending</em> de modelos e <em>pseudo-labels</em> e quando a competi√ß√£o acabou, percebi que uma solu√ß√£o mais simples de implementar teria um resultado privado ainda maior do que o notebook que selecionei. üòÖ</p>
<div class="w3-panel w3-sand w3-border">
<p>‚ö†Ô∏è Aten√ß√£o!</p>
<p>Neste post abordarei uma solu√ß√£o mais simples e eficiente mas caso tenha interesse em conferir a solu√ß√£o final completa (um grande frankstein), j√° est√° <a href="https://github.com/gomesfellipe/porto_seguro_data_challenge">publica la no github</a>.</p>
</div>
<p>Este notebook √© uma <a href="https://www.kaggle.com/gomes555/3st-place-simplified-solution-0-6967-private">reescritura do meu notebook publicado no Kaggle em linguagem Python</a>. Para quem acompanha meus posts de R pode achar meio estranho este notebook mas convido-o a tentar entender a solu√ß√£o pois foi desenvolvida pela perspectiva de um usu√°rio nativo de R.</p>
<p>Espero que gostem! ü§ò</p>
</div>
<div id="defini√ß√£o-do-problema-de-neg√≥cio" class="section level1">
<h1>Defini√ß√£o do problema de neg√≥cio</h1>
<p>Segundo a descri√ß√£o da competi√ß√£o:</p>
<blockquote>
<p>Voc√™ provavelmente j√° recebeu uma liga√ß√£o de telemarketing oferecendo um produto que voc√™ n√£o precisa. Essa situa√ß√£o de estresse √© minimizada quando voc√™ oferece um produto que o cliente realmente precisa. <br /><br /> Nessa competi√ß√£o voc√™ ser√° desafiado a construir um modelo que prediz a probabilidade de aquisi√ß√£o de um produto.</p>
</blockquote>
<p>Sobre a m√©trica de avalia√ß√£o:</p>
<p>O crit√©rio utilizado para defini√ß√£o da melhor solu√ß√£o ser√° o F1-Score, veja sua formula:</p>
<p><span class="math display">\[
F_1 = 2 \times \frac{precision \times recall}{precision + recall}  
\]</span></p>
<p>Note que tanto a <em>Precision</em> quanto a <em>Recall</em> precisam de um ponto de corte para obter as classes e por isso busquei otmizar as m√©tricas <em>ROC-AUC</em> e <em>Log Loss</em> para obter estimativas de probabilidades com qualidade para finalmente calcular os pontos de corte que maximizam a <em>F1</em>.</p>
</div>
<div id="an√°lise-explorat√≥ria-em-r" class="section level1">
<h1>An√°lise Explorat√≥ria (em R)</h1>
<p>Antes de partir para modelagem fiz uma an√°lise explorat√≥ria utilizando a linguagem R. Neste post tratarei de maneira bem breve e quem tiver interesse em conferir mais detalhes bem como os c√≥digos dos gr√°ficos basta acessar o <a href="https://www.kaggle.com/gomes555/porto-seguro-r-an-lise-explorat-ria-dos-dados">notebook que deixei aberto no Kaggle</a>.</p>
<p>Veja alguns gr√°ficos:</p>
<center>
<img src="/post/2021-11-01-solucao-final-porto-seguro-data-challenge/aed.png" style="width:90.0%" />
</center>
<!-- <div class="w3-panel w3-light-blue w3-border"> -->
<p><strong>üìå Interpreta√ß√£o:</strong> <br></p>
<ul>
<li><strong>Categ√≥ricas</strong>:
<div style="color: rgb(0, 0, 0);">
<ul>
<li>
<strong>Qualitativo nominal</strong>: Possuem muitas classes, poderia ser o nome do produto, regi√£o, um texto o que torna o desafio ainda maior para criar novas features;
</li>
<li>
<strong>Qualitativo ordinal</strong>: Basicamente deixei como veio pois j√° tava como numerico;
</li>
</ul>
</div></li>
<li><strong>Num√©ricas</strong>:
<div style="color: rgb(0, 0, 0);">
<ul>
<li>
<strong>Quantitativo continua</strong>: Todas est√£o normalizadas (0, 1), algumas s√£o bimodais, algumas assim√©tricas a direita (pode ser tempo ate alguma coisa);
</li>
<li>
<strong>Quantitativo discreto</strong>: Sem muito o que fazer, observa√ß√£o apenas a feature <code>var52</code> que parece idade
</li>
</ul>
</div></li>
<li><strong>Dados missing</strong>: Parece haver algum padr√£o na maneira como os dados missing ocorrem e tentei substituir os <code>-999</code> por <code>NaN</code>, imputar a m√©dia, a mediana e via outros modelos
<!-- </div> --></li>
</ul>
<p>N√£o achei que seria muito produtivo ficar adivinhando o que poderia ser cada feature pois praticamente todos as transforma√ß√µes e novas features que gerei n√£o superavam o resultado do modelo ajustado nos dados da maneira que vinham portanto procurei investir mais tempo na modelagem mesmo.</p>
</div>
<div id="machine-learning-em-python" class="section level1">
<h1>Machine Learning (em Python)</h1>
<p>Veja a estrat√©gia de modelagem de maneira visual:</p>
</br>
<center>
<img src="/post/2021-11-01-solucao-final-porto-seguro-data-challenge/final_pipeline.png" style="width:60.0%" />
</center>
<p></br></p>
<div id="importar-depend√™ncias" class="section level2">
<h2>Importar depend√™ncias</h2>
<p>Carregar pacotes do Python</p>
<pre class="python"><code># general packages
import pandas as pd
import numpy as np
import time
# knn features
from gokinjo import knn_kfold_extract
from gokinjo import knn_extract
# ml tools
from sklearn.model_selection import StratifiedKFold, KFold
from sklearn.metrics import f1_score, log_loss, roc_auc_score
# models
from catboost import CatBoostClassifier
from xgboost import XGBClassifier
# optimization
import optuna
# interpretable ml
import shap
# automl
from autogluon.tabular import TabularPredictor
# ignore specific warnings
import warnings
warnings.filterwarnings(&quot;ignore&quot;, message=&quot;ntree_limit is deprecated, use `iteration_range` or model slicing instead.&quot;)</code></pre>
<p>Definir fun√ß√µes auxiliares para calcular o ponto de corte que maximiza a F1:</p>
<pre class="python"><code>def get_threshold(y_true, y_pred):
    thresholds = np.arange(0.0, 1.0, 0.01)
    f1_scores = []
    for thresh in thresholds:
        f1_scores.append(
            f1_score(y_true, [1 if m&gt;thresh else 0 for m in y_pred]))
    f1s = np.array(f1_scores)
    return thresholds[f1s.argmax()]
    
def custom_f1(y_true, y_pred):
    max_f1_threshold =  get_threshold(y_true, y_pred)
    y_pred = np.where(y_pred&gt;max_f1_threshold, 1, 0)
    return f1_score(y_true, y_pred) </code></pre>
<p>Carregar <a href="https://www.kaggle.com/c/porto-seguro-data-challenge/data">dados da competi√ß√£o</a>:</p>
<pre class="python"><code># load data
train = pd.read_csv(&#39;../input/porto-seguro-data-challenge/train.csv&#39;).drop(&#39;id&#39;, axis=1)
test = pd.read_csv(&#39;../input/porto-seguro-data-challenge/test.csv&#39;).drop(&#39;id&#39;, axis=1)
sample_submission = pd.read_csv(&#39;../input/porto-seguro-data-challenge/submission_sample.csv&#39;)
meta = pd.read_csv(&#39;../input/porto-seguro-data-challenge/metadata.csv&#39;)

# get data types
cat_nom = [x for x in meta.iloc[1:-1, :].loc[(meta.iloc[:,1]==&quot;Qualitativo nominal&quot;)].iloc[:,0]] 
cat_ord = [x for x in meta.iloc[1:-1, :].loc[(meta.iloc[:,1]==&quot;Qualitativo ordinal&quot;)].iloc[:,0]] 
num_dis = [x for x in meta.iloc[1:-1, :].loc[(meta.iloc[:,1]==&quot;Quantitativo discreto&quot;)].iloc[:,0]] 
num_con = [x for x in meta.iloc[1:-1, :].loc[(meta.iloc[:,1]==&quot;Quantitativo continua&quot;)].iloc[:,0]] </code></pre>
</div>
<div id="stage-0-feature-extraction-com-knn" class="section level2">
<h2>Stage 0: Feature Extraction com KNN</h2>
<p>Esta t√©cnica gera <span class="math inline">\(k \times c\)</span> novas features, onde <span class="math inline">\(c\)</span> √© o n√∫mero de classes da target. As novas features s√£o calculadas a partir das dist√¢ncias entre as observa√ß√µes e seus k vizinhos mais pr√≥ximos dentro de cada classe;</p>
<p>O valor para os <span class="math inline">\(K\)</span> vizinhos mais pr√≥ximos selecionado foi <span class="math inline">\(K=1\)</span> e para isso utilizei a biblioteca
<a href="https://github.com/momijiame/gokinjo"><code>gokinjo</code></a> que foi <a href="https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335">inspirada nas id√©ias apresentadas na solu√ß√£o vencedora do Otto Group Product Classification Challenge.</a></p>
<pre class="python"><code># convert to numpy because gokinjo expects np arrays
X = train[cat_nom+cat_ord+num_dis+num_con].to_numpy()
y = train.y.to_numpy()
X_test = test[cat_nom+cat_ord+num_dis+num_con].to_numpy()

# extract on train data
KNN_feat_train = knn_kfold_extract(X, y, k=1, normalize=&#39;standard&#39;)
print(&quot;KNN features for training set, shape: &quot;, np.shape(KNN_feat_train))

# extract on test data
KNN_feat_test = knn_extract(X, y, X_test, k=1, normalize=&#39;standard&#39;)
print(&quot;KNN features for test set, shape: &quot;, np.shape(KNN_feat_test))

# convert to dataframe
knn_feat_train = pd.DataFrame(KNN_feat_train, columns=[&quot;knn&quot;+str(x) for x in range(knn_feat_train.shape[1])])
knn_feat_test = pd.DataFrame(KNN_feat_test, columns=[&quot;knn&quot;+str(x) for x in range(knn_feat_test.shape[1])])</code></pre>
<pre><code>## KNN features for training set, shape:  (14123, 2)
## KNN features for test set, shape:  (21183, 2)</code></pre>
</div>
<div id="stage-1-tuning-xgboost-com-optuna" class="section level2">
<h2>Stage 1: Tuning XGBoost com Optuna</h2>
<p>Testei e otimizei muitos modelos como XGBoost, NGBoost, LightGBM, CatBoost, TabNet, HistGradientBoosting e algumas DNNs e em todos os casos (exceto DNNs) utilizei o Optuna para a sele√ß√£o dos hiperpar√¢metros.</p>
<p>Tamb√©m inclui nas tentativas iniciais de otimiza√ß√£o alguns m√©todos de remostrarem como Random Under Sampling, Smote, Tomek, Adasyn dentre outros mas n√£o tive muito sucesso.. apenas a combina√ß√£o Tomek + CatBoost pareceu trazer algum ganho.</p>
<p>Claro que minhas tentativas n√£o foram exautivas e devido ao tempo limitado acabei selecionando o XGBoost que foi o que apresentou as melhores m√©tricas depois de otimizado e tamb√©m o CatBoost com alguns hiperpar√¢metros fixos para serem a base deste pipeline.</p>
<p>Principais Informa√ß√µes üìå :</p>
<ul>
<li>Nenhum pr√©-processamento;</li>
<li>KFold K=10;</li>
<li>Otimiza√ß√£o de hiperpar√¢metros com Optuna;</li>
<li>Loss do XGBoost: Log Loss;</li>
<li>Loss do Otimizador: Log Loss;</li>
<li>Sem resampling;</li>
<li>Previs√£o final com a probabilidade m√©dia de 10 seeds diferentes</li>
</ul>
<pre class="python"><code>X_test = test[cat_nom+cat_ord+num_dis+num_con]
X = train[cat_nom+cat_ord+num_dis+num_con]
y = train.y

K=10
SEED=314
kf = KFold(n_splits=K, random_state=SEED, shuffle=True)</code></pre>
<pre class="python"><code>fixed_params = {
    &#39;random_state&#39;: 9,
    &quot;objective&quot;: &quot;binary:logistic&quot;,
    &quot;eval_metric&quot;: &#39;logloss&#39;,
    &#39;use_label_encoder&#39;:False,
    &#39;n_estimators&#39;:10000,
}

def objective(trial):
    
    hyperparams = {
        &#39;clf&#39;:{
        &quot;booster&quot;: trial.suggest_categorical(&quot;booster&quot;, [&quot;gbtree&quot;]),
        &quot;lambda&quot;: trial.suggest_float(&quot;lambda&quot;, 1e-8, 5.0, log=True),
        &quot;alpha&quot;: trial.suggest_float(&quot;alpha&quot;, 1e-8, 5.0, log=True)
        }
    }
    
    if hyperparams[&#39;clf&#39;][&quot;booster&quot;] == &quot;gbtree&quot; or hyperparams[&#39;clf&#39;][&quot;booster&quot;] == &quot;dart&quot;:
        hyperparams[&#39;clf&#39;][&quot;max_depth&quot;] = trial.suggest_int(&quot;max_depth&quot;, 1, 9)
        hyperparams[&#39;clf&#39;][&quot;eta&quot;] = trial.suggest_float(&quot;eta&quot;, 0.01, 0.1, log=True)
        hyperparams[&#39;clf&#39;][&quot;gamma&quot;] = trial.suggest_float(&quot;gamma&quot;, 1e-8, 1.0, log=True)
        hyperparams[&#39;clf&#39;][&quot;grow_policy&quot;] = trial.suggest_categorical(&quot;grow_policy&quot;, [&quot;depthwise&quot;, &quot;lossguide&quot;])
        hyperparams[&#39;clf&#39;][&#39;min_child_weight&#39;] = trial.suggest_int(&#39;min_child_weight&#39;, 5, 20)
        hyperparams[&#39;clf&#39;][&quot;subsample&quot;] = trial.suggest_float(&quot;subsample&quot;, 0.03, 1)
        hyperparams[&#39;clf&#39;][&quot;colsample_bytree&quot;] = trial.suggest_float(&quot;colsample_bytree&quot;, 0.03, 1)
        hyperparams[&#39;clf&#39;][&#39;max_delta_step&#39;] = trial.suggest_float(&#39;max_delta_step&#39;, 0, 10)
        
    if hyperparams[&#39;clf&#39;][&quot;booster&quot;] == &quot;dart&quot;:
        hyperparams[&#39;clf&#39;][&quot;sample_type&quot;] = trial.suggest_categorical(&quot;sample_type&quot;, [&quot;uniform&quot;, &quot;weighted&quot;])
        hyperparams[&#39;clf&#39;][&quot;normalize_type&quot;] = trial.suggest_categorical(&quot;normalize_type&quot;, [&quot;tree&quot;, &quot;forest&quot;])
        hyperparams[&#39;clf&#39;][&quot;rate_drop&quot;] = trial.suggest_float(&quot;rate_drop&quot;, 1e-8, 1.0, log=True)
        hyperparams[&#39;clf&#39;][&quot;skip_drop&quot;] = trial.suggest_float(&quot;skip_drop&quot;, 1e-8, 1.0, log=True)
    
    params = dict(**fixed_params, **hyperparams[&#39;clf&#39;])
    xgb_oof = np.zeros(X.shape[0])

    for fold, (train_idx, val_idx) in enumerate(kf.split(X=X, y=y)):
        X_train = X.iloc[train_idx]
        y_train = y.iloc[train_idx]
        X_val = X.iloc[val_idx]
        y_val = y.iloc[val_idx]
        
        model = XGBClassifier(**params)
        
        model.fit(X_train, y_train,
                  eval_set=[(X_val, y_val)],
                  early_stopping_rounds=150,
                  verbose=False)
    
        xgb_oof[val_idx] = model.predict_proba(X_val)[:,1]

        del model

    return log_loss(y, xgb_oof)</code></pre>
<p>Como no Kaggle existe o limite de aproximadamente 8h para executar um notebook, coloquei um limite de 7.5 horas para a busca de hiperpar√¢metros:</p>
<pre class="python"><code>study_xgb = optuna.create_study(direction=&#39;minimize&#39;)

study_xgb.optimize(objective, 
               timeout=60*60*7.5, 
               gc_after_trial=True)</code></pre>
<p>Resultados da busca:</p>
<pre class="python"><code>print(&#39;-&gt; Number of finished trials: &#39;, len(study_xgb.trials))
print(&#39;-&gt; Best trial:&#39;)
trial = study_xgb.best_trial
print(&#39;\tValue: {}&#39;.format(trial.value))
print(&#39;-&gt; Params: &#39;)
trial.params</code></pre>
<pre><code>## -&gt; Number of finished trials:  197
## -&gt; Best trial:
## 	Value: 0.3028443879614926
## -&gt; Params: 
## {&#39;booster&#39;: &#39;gbtree&#39;,
##  &#39;lambda&#39;: 9.012384508756378e-07,
##  &#39;alpha&#39;: 0.7472040331088792,
##  &#39;max_depth&#39;: 5,
##  &#39;eta&#39;: 0.01507605562231303,
##  &#39;gamma&#39;: 1.0214961302342215e-08,
##  &#39;grow_policy&#39;: &#39;lossguide&#39;,
##  &#39;min_child_weight&#39;: 5,
##  &#39;subsample&#39;: 0.9331005225916879,
##  &#39;colsample_bytree&#39;: 0.25392142363325004,
##  &#39;max_delta_step&#39;: 5.685109389498008}</code></pre>
<p>Acompanhar o hist√≥rico de cada etapa da otimiza√ß√£o:</p>
<pre class="python"><code>plot_optimization_history(study_xgb)</code></pre>
<center>
<img src="/post/2021-11-01-solucao-final-porto-seguro-data-challenge/optimization_hist.png" style="width:90.0%" />
</center>
<p>Avaliar as combina√ß√µes de hiperpar√¢metros mais bem sucedidas:</p>
<pre class="python"><code>optuna.visualization.plot_parallel_coordinate(study_xgb)</code></pre>
<center>
<img src="/post/2021-11-01-solucao-final-porto-seguro-data-challenge/parallel_plot.png" style="width:90.0%" />
</center>
<p>Quais hiperpar√¢metros tiveram mais impacto na modelagem:</p>
<pre class="python"><code>plot_param_importances(study_xgb)</code></pre>
<center>
<img src="/post/2021-11-01-solucao-final-porto-seguro-data-challenge/param_imp.png" style="width:90.0%" />
</center>
<p>Ap√≥s as 7.5 horas de busca, a melhor combina√ß√£o encontrada para o XGBoost foi a seguinte:</p>
<pre class="python"><code># After 7.5 hours...
study_xgb = {&#39;booster&#39;: &#39;gbtree&#39;,
 &#39;lambda&#39;: 9.012384508756378e-07,
 &#39;alpha&#39;: 0.7472040331088792,
 &#39;max_depth&#39;: 5,
 &#39;eta&#39;: 0.01507605562231303,
 &#39;gamma&#39;: 1.0214961302342215e-08,
 &#39;grow_policy&#39;: &#39;lossguide&#39;,
 &#39;min_child_weight&#39;: 5,
 &#39;subsample&#39;: 0.9331005225916879,
 &#39;colsample_bytree&#39;: 0.25392142363325004,
 &#39;max_delta_step&#39;: 5.685109389498008}</code></pre>
<p>Preparar lista de hiperpar√¢metros do XGBoost:</p>
<pre class="python"><code>final_params_xgb = dict()
final_params_xgb[&#39;clf&#39;]=dict(**fixed_params, **study_xgb)</code></pre>
</div>
<div id="stage-2-calcular-out-of-fold-shap-values" class="section level2">
<h2>Stage 2: Calcular Out-Of-Fold SHAP values</h2>
<p>Ap√≥s obter a melhor combina√ß√£o de hiperpar√¢metros para o XGBoost e encontrar resultados formid√°veis com o CatBoost modificando apenas alguns hiperpar√¢metros, resolvi tentar utilizar a informa√ß√£o adquirida pelo <em>SHAP values</em> desses modelos como entrada para novos modelos.</p>
<p>Algumas vantagens de se usar o shap values como um m√©todo de encoder dos dados, <a href="https://www.kaggle.com/pavelvod/gbm-supervised-pretraining">segundo este notebook publicado no Kaggle</a> (muito interessante por sinal):</p>
<ul>
<li>Normaliza os dados;</li>
<li>Mais ou menos Linearizado pois as <em>features</em> s√£o transformadas em suas import√¢ncias;</li>
<li>Recursos categ√≥ricos codificados de maneira mais inteligente (A codifica√ß√£o n√£o √© linear e depende de outros recursos da amostra);</li>
<li>Tratamento mais inteligente para valores <em>missing</em>.</li>
</ul>
<p>Para evitar <em>data leak</em>, o <em>SHAP values</em> foi calculado em cima dos dados <em>out-of-fold</em> para os dados de treino e a m√©dia da previs√£o de todos os <em>fold</em> nos dados de teste.</p>
<p>Definir estrat√©gia de valida√ß√£o cruzada:</p>
<pre class="python"><code>X_test = test[cat_nom+cat_ord+num_dis+num_con]
X = train[cat_nom+cat_ord+num_dis+num_con]
y = train.y

K=15 # number of bins with Sturge‚Äôs rule
SEED=123
kf = StratifiedKFold(n_splits=K, random_state=SEED, shuffle=True)</code></pre>
<div id="xgboost" class="section level3">
<h3>XGBoost</h3>
<p>Obter <em>out-of-fold</em> SHAP do modelo XGBoost tunado:</p>
<pre class="python"><code>shap1_oof = np.zeros((X.shape[0], X.shape[1]))
shap1_test = np.zeros((X_test.shape[0], X_test.shape[1]))
model_shap1_oof = np.zeros(X.shape[0])

for fold, (train_idx, val_idx) in enumerate(kf.split(X=X, y=y)):
    print(f&quot;‚ûú FOLD :{fold}&quot;)
    X_train = X.iloc[train_idx]
    y_train = y.iloc[train_idx]
    X_val = X.iloc[val_idx]
    y_val = y.iloc[val_idx]
    
    start = time.time()
    
    model = XGBClassifier(**final_params_xgb[&#39;clf&#39;])
    
    model.fit(X_train, y_train,
              eval_set=[(X_val, y_val)],
              early_stopping_rounds=150,
              verbose=False)
    
    model_shap1_oof[val_idx] += model.predict_proba(X_val)[:,1]
    
    print(&quot;Final F1     :&quot;, custom_f1(y_val, model_shap1_oof[val_idx]))
    print(&quot;Final AUC    :&quot;, roc_auc_score(y_val, model_shap1_oof[val_idx]))
    print(&quot;Final LogLoss:&quot;, log_loss(y_val, model_shap1_oof[val_idx]))

    explainer = shap.TreeExplainer(model)
    shap1_oof[val_idx] = explainer.shap_values(X_val)
    shap1_test += explainer.shap_values(X_test) / K

    print(f&quot;elapsed: {time.time()-start:.2f} sec\n&quot;)
    
shap1_oof = pd.DataFrame(shap1_oof, columns = [x+&quot;_shap1&quot; for x in X.columns])
shap1_test = pd.DataFrame(shap1_test, columns = [x+&quot;_shap1&quot; for x in X_test.columns])

print(&quot;Final F1     :&quot;, custom_f1(y, model_shap1_oof))
print(&quot;Final AUC    :&quot;, roc_auc_score(y, model_shap1_oof))
print(&quot;Final LogLoss:&quot;, log_loss(y, model_shap1_oof))</code></pre>
<pre><code>## ‚ûú FOLD :0
## Final F1     : 0.7032967032967034
## Final AUC    : 0.902330627099664
## Final LogLoss: 0.2953604946129216
## elapsed: 62.58 sec
## 
## ‚ûú FOLD :1
## Final F1     : 0.6193853427895981
## Final AUC    : 0.8613101903695408
## Final LogLoss: 0.34227429854659686
## elapsed: 45.96 sec
## 
## ‚ûú FOLD :2
## Final F1     : 0.6793478260869567
## Final AUC    : 0.8945898656215007
## Final LogLoss: 0.3085819148842589
## elapsed: 58.84 sec
## 
## ‚ûú FOLD :3
## Final F1     : 0.7073791348600509
## Final AUC    : 0.9058020716685331
## Final LogLoss: 0.2881665477053405
## elapsed: 62.24 sec
## 
## ‚ûú FOLD :4
## Final F1     : 0.7239583333333334
## Final AUC    : 0.9053121500559911
## Final LogLoss: 0.29320601468396107
## elapsed: 93.74 sec
## 
## ‚ûú FOLD :5
## Final F1     : 0.7009803921568627
## Final AUC    : 0.9076567749160134
## Final LogLoss: 0.2872539995859452
## elapsed: 73.34 sec
## 
## ‚ûú FOLD :6
## Final F1     : 0.6736292428198434
## Final AUC    : 0.8822788353863381
## Final LogLoss: 0.320014158050091
## elapsed: 55.16 sec
## 
## ‚ûú FOLD :7
## Final F1     : 0.7135416666666666
## Final AUC    : 0.9016657334826428
## Final LogLoss: 0.29617989833438774
## elapsed: 74.49 sec
## 
## ‚ûú FOLD :8
## Final F1     : 0.7135135135135134
## Final AUC    : 0.8893825776158104
## Final LogLoss: 0.29351621553572266
## elapsed: 93.71 sec
## 
## ‚ûú FOLD :9
## Final F1     : 0.7391304347826086
## Final AUC    : 0.9064054944284814
## Final LogLoss: 0.28033187155768635
## elapsed: 95.65 sec
## 
## ‚ûú FOLD :10
## Final F1     : 0.684863523573201
## Final AUC    : 0.9031046324199313
## Final LogLoss: 0.29823173886367804
## elapsed: 64.70 sec
## 
## ‚ûú FOLD :11
## Final F1     : 0.704225352112676
## Final AUC    : 0.8882052000840984
## Final LogLoss: 0.30525241732057884
## elapsed: 50.06 sec
## 
## ‚ûú FOLD :12
## Final F1     : 0.6666666666666666
## Final AUC    : 0.8905529469479291
## Final LogLoss: 0.313654842143217
## elapsed: 78.45 sec
## 
## ‚ûú FOLD :13
## Final F1     : 0.6500000000000001
## Final AUC    : 0.8745111780783517
## Final LogLoss: 0.3300786509821235
## elapsed: 59.54 sec
## 
## ‚ûú FOLD :14
## Final F1     : 0.7135416666666666
## Final AUC    : 0.9063284042329526
## Final LogLoss: 0.29314716930177404
## elapsed: 70.28 sec
## 
## Final F1     : 0.6822461331540014
## Final AUC    : 0.8945288307257988
## Final LogLoss: 0.30301717097927483</code></pre>
</div>
<div id="catboost" class="section level3">
<h3>CatBoost</h3>
<p>Obter <em>out-of-fold</em> SHAP do modelo CatBoost + features extrat√≠das via KNN:</p>
<pre class="python"><code>X = pd.concat([X, knn_feat_train], axis=1)
X_test = pd.concat([X_test, knn_feat_test], axis=1)</code></pre>
<pre class="python"><code>shap2_oof = np.zeros((X.shape[0], X.shape[1]))
shap2_test = np.zeros((X_test.shape[0], X_test.shape[1]))
model_shap2_oof = np.zeros(X.shape[0])

for fold, (train_idx, val_idx) in enumerate(kf.split(X=X, y=y)):
    print(f&quot;‚ûú FOLD :{fold}&quot;)
    X_train = X.iloc[train_idx]
    y_train = y.iloc[train_idx]
    X_val = X.iloc[val_idx]
    y_val = y.iloc[val_idx]
    
    start = time.time()
    
    model = CatBoostClassifier(random_seed=SEED,
                               verbose = 0,
                               n_estimators=10000,
                               loss_function= &#39;Logloss&#39;,
                               use_best_model=True,
                               eval_metric= &#39;Logloss&#39;)
    
    model.fit(X_train, y_train, 
              eval_set = [(X_val,y_val)], 
              early_stopping_rounds = 100,
              verbose = False)
    
    model_shap2_oof[val_idx] += model.predict_proba(X_val)[:,1]
    
    print(&quot;Final F1     :&quot;, custom_f1(y_val, model_shap2_oof[val_idx]))
    print(&quot;Final AUC    :&quot;, roc_auc_score(y_val, model_shap2_oof[val_idx]))
    print(&quot;Final LogLoss:&quot;, log_loss(y_val, model_shap2_oof[val_idx]))

    explainer = shap.TreeExplainer(model)
    shap2_oof[val_idx] = explainer.shap_values(X_val)
    shap2_test += explainer.shap_values(X_test) / K

    print(f&quot;elapsed: {time.time()-start:.2f} sec\n&quot;)
    
shap2_oof = pd.DataFrame(shap2_oof, columns = [x+&quot;_shap&quot; for x in X.columns])
shap2_test = pd.DataFrame(shap2_test, columns = [x+&quot;_shap&quot; for x in X_test.columns])

print(&quot;Final F1     :&quot;, custom_f1(y, model_shap2_oof))
print(&quot;Final AUC    :&quot;, roc_auc_score(y, model_shap2_oof))
print(&quot;Final LogLoss:&quot;, log_loss(y, model_shap2_oof))</code></pre>
<pre><code>## ‚ûú FOLD :0
## Final F1     : 0.6972010178117048
## Final AUC    : 0.8954157334826428
## Final LogLoss: 0.29952314366911725
## elapsed: 22.84 sec
## 
## ‚ûú FOLD :1
## Final F1     : 0.6348448687350835
## Final AUC    : 0.8628429451287795
## Final LogLoss: 0.3407490151943705
## elapsed: 12.59 sec
## 
## ‚ûú FOLD :2
## Final F1     : 0.6809651474530831
## Final AUC    : 0.8949538073908175
## Final LogLoss: 0.3066089330852162
## elapsed: 18.03 sec
## 
## ‚ûú FOLD :3
## Final F1     : 0.702247191011236
## Final AUC    : 0.9107992721164613
## Final LogLoss: 0.2877216893570601
## elapsed: 15.66 sec
## 
## ‚ûú FOLD :4
## Final F1     : 0.7131367292225201
## Final AUC    : 0.9018687010078387
## Final LogLoss: 0.2976481761596595
## elapsed: 29.35 sec
## 
## ‚ûú FOLD :5
## Final F1     : 0.7055837563451777
## Final AUC    : 0.909231522956327
## Final LogLoss: 0.28834373773423566
## elapsed: 15.35 sec
## 
## ‚ûú FOLD :6
## Final F1     : 0.6631578947368421
## Final AUC    : 0.8796402575587906
## Final LogLoss: 0.32303153676573987
## elapsed: 19.13 sec
## 
## ‚ûú FOLD :7
## Final F1     : 0.6997389033942559
## Final AUC    : 0.901637737961926
## Final LogLoss: 0.2985978485411335
## elapsed: 23.30 sec
## 
## ‚ûú FOLD :8
## Final F1     : 0.6965699208443271
## Final AUC    : 0.8825565912117177
## Final LogLoss: 0.3009859242847037
## elapsed: 20.19 sec
## 
## ‚ûú FOLD :9
## Final F1     : 0.7435897435897436
## Final AUC    : 0.9042469689536757
## Final LogLoss: 0.28276851015512977
## elapsed: 24.39 sec
## 
## ‚ûú FOLD :10
## Final F1     : 0.6767676767676767
## Final AUC    : 0.902712173242694
## Final LogLoss: 0.29999812838692497
## elapsed: 16.14 sec
## 
## ‚ûú FOLD :11
## Final F1     : 0.7013698630136986
## Final AUC    : 0.8865022075828719
## Final LogLoss: 0.3081393413008847
## elapsed: 13.50 sec
## 
## ‚ûú FOLD :12
## Final F1     : 0.6630434782608696
## Final AUC    : 0.8920456934613498
## Final LogLoss: 0.31338640296724246
## elapsed: 24.48 sec
## 
## ‚ûú FOLD :13
## Final F1     : 0.6485148514851485
## Final AUC    : 0.8689887167986544
## Final LogLoss: 0.3369797070301582
## elapsed: 17.17 sec
## 
## ‚ûú FOLD :14
## Final F1     : 0.7108753315649867
## Final AUC    : 0.8994743850304856
## Final LogLoss: 0.301420230674656
## elapsed: 16.51 sec
## 
## Final F1     : 0.6823234134098244
## Final AUC    : 0.892656043550729
## Final LogLoss: 0.305726567456891</code></pre>
<pre class="python"><code>train = pd.concat([train, shap1_oof], axis=1)
test = pd.concat([test, shap1_test], axis=1)

train = pd.concat([train, shap2_oof], axis=1)
test = pd.concat([test, shap2_test], axis=1)</code></pre>
</div>
</div>
<div id="stage-3-modelo-final-com-autogluon" class="section level2">
<h2>Stage 3: Modelo Final com AutoGluon</h2>
<p>AutoGluon √© um <a href="https://github.com/awslabs/autogluon">AutoML desenvolvido pela Amazon</a> muito f√°cil de utilizar (no melhor estilo <code>sklearn</code> com m√©todos <code>.fit()</code> e <code>.predict()</code>).</p>
<p>Principais Informa√ß√µes üìå :</p>
<ul>
<li>Inputs: Dataset original + knn features + Shapt values do XGBoost tunado e do CatBoost;</li>
<li>Loss do XGBoost: Log Loss;</li>
<li>Loss do CatBoost: AUC;</li>
<li>Loss do AutoGluon: Log Loss;</li>
<li>Tempo de processamento: 7h30m</li>
</ul>
<div class="w3-panel w3-pale-green w3-border">
<p><strong>üí° Insight</strong> <br></p>
<p>Um recurso muito √∫til do AutoGluon √© poder acessar as previs√µes out-of-folds, o que facilita no c√°lculo do <em>threshold</em> que maximiza a <em>F1 Score</em>.</p>
</div>
<pre class="python"><code>predictor = TabularPredictor(label=&quot;y&quot;,
                             problem_type=&#39;binary&#39;,
                             eval_metric=&quot;log_loss&quot;,
                             path=&#39;./AutoGlon/&#39;,
                             verbosity=1)

predictor.fit(train, presets=&#39;best_quality&#39;, time_limit=60*60*7.5) 

results = predictor.fit_summary()</code></pre>
<pre><code>## *** Summary of fit() ***
## Estimated performance of each model:
##                       model  score_val  pred_time_val      fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order
## 0       WeightedEnsemble_L2  -0.299310      30.410467   8888.826963                0.001654           2.456810            2       True         14
## 1           CatBoost_BAG_L1  -0.301038       3.051793   2376.887100                3.051793        2376.887100            1       True          7
## 2       WeightedEnsemble_L3  -0.301722     194.034947  22907.669139                0.001541           2.008858            3       True         26
## 3         LightGBMXT_BAG_L2  -0.302135     131.534432  17201.299530                1.378576         389.400378            2       True         15
## 4         LightGBMXT_BAG_L1  -0.302562       3.570399    969.385833                3.570399         969.385833            1       True          3
## 5           CatBoost_BAG_L2  -0.302646     131.912474  17619.939451                1.756617         808.040299            2       True         19
## 6           LightGBM_BAG_L2  -0.303002     131.422007  17281.518763                1.266150         469.619612            2       True         16
## 7           LightGBM_BAG_L1  -0.303264       2.964433   1038.037160                2.964433        1038.037160            1       True          4
## 8            XGBoost_BAG_L1  -0.303471       4.475003   2036.551052                4.475003        2036.551052            1       True         11
## 9    NeuralNetFastAI_BAG_L1  -0.304455      19.917584   3434.894841               19.917584        3434.894841            1       True         10
## 10           XGBoost_BAG_L2  -0.304499     132.757505  17834.135370                2.601648        1022.236218            2       True         23
## 11   NeuralNetFastAI_BAG_L2  -0.306339     142.018741  18777.287244               11.862885        1965.388093            2       True         22
## 12     LightGBMLarge_BAG_L2  -0.306606     131.701429  18260.504603                1.545573        1448.605452            2       True         25
## 13    NeuralNetMXNet_BAG_L2  -0.308237     177.769179  19273.211899               47.613322        2461.312748            2       True         24
## 14     LightGBMLarge_BAG_L1  -0.309686       3.042399   2629.185346                3.042399        2629.185346            1       True         13
## 15    ExtraTreesEntr_BAG_L2  -0.314045     132.017535  16815.886061                1.861679           3.986910            2       True         21
## 16  RandomForestEntr_BAG_L2  -0.314454     132.061970  16843.769642                1.906114          31.870490            2       True         18
## 17    ExtraTreesGini_BAG_L2  -0.314960     132.123651  16816.087081                1.967794           4.187930            2       True         20
## 18    NeuralNetMXNet_BAG_L1  -0.317156      81.677096   4258.886806               81.677096        4258.886806            1       True         12
## 19  RandomForestGini_BAG_L2  -0.321702     132.035970  16835.326491                1.880114          23.427339            2       True         17
## 20    ExtraTreesEntr_BAG_L1  -0.323283       1.794093      4.051307                1.794093           4.051307            1       True          9
## 21  RandomForestEntr_BAG_L1  -0.324296       1.966043     33.380685                1.966043          33.380685            1       True          6
## 22    ExtraTreesGini_BAG_L1  -0.325897       1.796291      3.748723                1.796291           3.748723            1       True          8
## 23  RandomForestGini_BAG_L1  -0.328218       1.778995     22.705248                1.778995          22.705248            1       True          5
## 24    KNeighborsDist_BAG_L1  -1.070156       2.010938      2.075571                2.010938           2.075571            1       True          2
## 25    KNeighborsUnif_BAG_L1  -1.071373       2.110790      2.109480                2.110790           2.109480            1       True          1
## Number of models trained: 26
## Types of models trained:
## {&#39;StackerEnsembleModel_RF&#39;, &#39;StackerEnsembleModel_NNFastAiTabular&#39;, &#39;WeightedEnsembleModel&#39;, &#39;StackerEnsembleModel_XGBoost&#39;, &#39;StackerEnsembleModel_CatBoost&#39;, &#39;StackerEnsembleModel_KNN&#39;, &#39;StackerEnsembleModel_LGB&#39;, &#39;StackerEnsembleModel_XT&#39;, &#39;StackerEnsembleModel_TabularNeuralNet&#39;}
## Bagging used: True  (with 10 folds)
## Multi-layer stack-ensembling used: True  (with 3 levels)
## Feature Metadata (Processed):
## (raw dtype, special dtypes):
## (&#39;float&#39;, [])     : 152 | [&#39;var55&#39;, &#39;var56&#39;, &#39;var57&#39;, &#39;var58&#39;, &#39;var59&#39;, ...]
## (&#39;int&#39;, [])       :  48 | [&#39;var1&#39;, &#39;var2&#39;, &#39;var3&#39;, &#39;var4&#39;, &#39;var5&#39;, ...]
## (&#39;int&#39;, [&#39;bool&#39;]) :   6 | [&#39;var27&#39;, &#39;var31&#39;, &#39;var44&#39;, &#39;var49&#39;, &#39;var50&#39;, ...]
## Plot summary of models saved to file: ./AutoGlon/SummaryOfModels.html
## *** End of fit() summary ***</code></pre>
<p>Nota: Os resultados podem variar devido √† natureza estoc√°stica do algoritmo ou procedimento de avalia√ß√£o.</p>
<pre class="python"><code># get final predictions
y_oof = predictor.get_oof_pred_proba().iloc[:,1]
y_pred = predictor.predict_proba(test).iloc[:,1]</code></pre>
<pre class="python"><code>final_threshold = get_threshold(train.y, y_oof)
final_threshold</code></pre>
<pre><code>## 0.31</code></pre>
<pre class="python"><code>print(&quot;Final F1     :&quot;, custom_f1(y, y_oof))
print(&quot;Final AUC    :&quot;, roc_auc_score(y, y_oof))
print(&quot;Final LogLoss:&quot;, log_loss(y, y_oof))</code></pre>
<pre><code>## Final F1     : 0.6846193682030037
## Final AUC    : 0.8961328807692966
## Final LogLoss: 0.2993098559321765</code></pre>
<p>Ap√≥s submiss√£o:</p>
<center>
<img src="/post/2021-11-01-solucao-final-porto-seguro-data-challenge/final_sub.png" style="width:90.0%" />
</center>
</div>
</div>
<div id="conclus√£o" class="section level1">
<h1>Conclus√£o</h1>
<p>Gostaria de agradecer imensamente ao time do Porto Seguro pela iniciativa, pois esse tipo de competi√ß√£o (t√£o detalhada e desafiadora) n√£o tem sido muito comum no Brasil e √© muito importante para fomentar a comunidade brasileira de ci√™ncia de dados!</p>
<p>Sabemos que o ‚Äúmundo real‚Äù √© diferente do mundo das competi√ß√µes (onde buscamos o melhor score a todo custo) por√©m, na minha vis√£o, n√£o deixa de ser um √≥timo exerc√≠cio para treinar o racioc√≠nio anal√≠tico.. al√©m de ser muito empolgante e divertido!</p>
<p>Tive o enorme prazer de trocar id√©ias e conhecer pessoas fora da curva bem como me tornar f√£ de alguns competidores! A cada semana q passava o n√≠vel estava cada vez mais alto!</p>
<p>Com certeza este pipeline poderia ser muito melhor, sinto que poderia ter gasto mais tempo com <em>feature engineering</em> e tido mais paciencia com alguns modelos. Tentei fazer o melhor que pude com o tempo dispon√≠vel e me sinto muito grato pela experi√™ncia de apresentar os resultados e aprender bastante com a solu√ß√£o dos top colocados.</p>
<p>N√£o acaba por aqui! Agora √© hora de voltar aos estudos, continuar praticando com as <a href="https://www.kaggle.com/c/tabular-playground-series-nov-2021/overview">TPS‚Äôs do Kaggle</a> e, quem sabe, ir melhor na pr√≥xima!</p>
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias</h1>
<ul>
<li><a href="https://github.com/momijiame/gokinjo" class="uri">https://github.com/momijiame/gokinjo</a></li>
<li><a href="https://www.kaggle.com/melanie7744/tps6-boost-your-score-with-knn-features" class="uri">https://www.kaggle.com/melanie7744/tps6-boost-your-score-with-knn-features</a></li>
<li><a href="https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335" class="uri">https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335</a></li>
<li><a href="https://www.kaggle.com/pavelvod/gbm-supervised-pretraining" class="uri">https://www.kaggle.com/pavelvod/gbm-supervised-pretraining</a></li>
</ul>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2021-11-01-solucao-final-porto-seguro-data-challenge/">Solu√ß√£o Final - Porto Seguro Data Challenge [3¬∫ lugar]</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">catboost</category>
      <category domain="tag">data-science</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">knn</category>
      <category domain="tag">machine-learning</category>
      <category domain="tag">optuna</category>
      <category domain="tag">pratica</category>
      <category domain="tag">python</category>
      <category domain="tag">shap</category>
      <category domain="tag">threshold-movel</category>
    </item>
    <item>
      <title>Otimizando pipelines que envolvem dados desbalanceados</title>
      <link>https://gomesfellipe.github.io/post/2021-06-28-imbalanced-workflowsets/</link>
      <pubDate>Mon, 28 Jun 2021 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2021-06-28-imbalanced-workflowsets/</guid>
      <description>Utilizaremos o framework tidymodels para machine learning em R com o aux√≠lio do pacote workflowsets para otimizar pipelines de dados desbalanceados</description>
      <content:encoded>&lt;![CDATA[
        

<div id="TOC">
<ul>
<li><a href="#o-problema-envolvendo-dados-desbalanceados" id="toc-o-problema-envolvendo-dados-desbalanceados">O problema envolvendo dados desbalanceados</a></li>
<li><a href="#objetivo" id="toc-objetivo">Objetivo</a></li>
<li><a href="#depend%C3%AAncias" id="toc-depend√™ncias">Depend√™ncias</a></li>
<li><a href="#preparar-dados" id="toc-preparar-dados">Preparar dados</a></li>
<li><a href="#breve-an%C3%A1lise-explorat%C3%B3ria" id="toc-breve-an√°lise-explorat√≥ria">Breve an√°lise explorat√≥ria</a></li>
<li><a href="#modelagem" id="toc-modelagem">Modelagem</a>
<ul>
<li><a href="#baselines" id="toc-baselines">Baselines</a></li>
<li><a href="#preparar-pipeline-de-dados-com-workflowsets" id="toc-preparar-pipeline-de-dados-com-workflowsets">Preparar Pipeline de dados com <code>workflowsets</code></a></li>
<li><a href="#benchmark" id="toc-benchmark">Benchmark</a></li>
</ul></li>
<li><a href="#conclus%C3%A3o" id="toc-conclus√£o">Conclus√£o</a></li>
<li><a href="#refer%C3%AAncias" id="toc-refer√™ncias">Refer√™ncias</a></li>
</ul>
</div>

<style>
.column {
float: left;
width: 50%;
padding: 10px;
}

.column4 {
float: left;
width: 33%;
padding: 10px;
}

.column8 {
float: left;
width: 66%;
padding: 10px;
}

.row:after {
content: "";
display: table;
clear: both;
}

.center {
display: flex;
justify-content: center;
align-items: center;
height: 200px;
}
</style>
<div id="o-problema-envolvendo-dados-desbalanceados" class="section level1">
<h1>O problema envolvendo dados desbalanceados</h1>
<p>A tarefa de classifica√ß√£o com dados desbalanceados √© muito comum na vida real podendo variar desde um leve vi√©s at√© um enorme desequil√≠brio na distribui√ß√£o da classe de interesse. Problemas mais comuns envolvem:</p>
<ul>
<li>Detec√ß√£o de fraude;</li>
<li>Previs√£o de inadimpl√™ncia;</li>
<li>Identificador de <em>spam</em>;</li>
<li>Busca por anomalias/outliers;</li>
<li>Detec√ß√£o de poss√≠veis roubos/furtos/vulnerabilidades;</li>
<li>Previs√£o de <em>churn</em>;</li>
<li>etc</li>
</ul>
<div class="row">
<div class="column8">
<p>Este tipo de tarefa representa um enorme desafio para modelagem preditiva pois a maioria dos algoritmos de machine learning foram projetados sob suposi√ß√£o de haver um n√∫mero igual de exemplos para cada classe de interesse.</p>
<p>E isso √© um grande problema pois normalmente estamos interessados em prever a classe minorit√°ria e para isso √© preciso tomar uma s√©rie de decis√µes, como por exemplo: m√©trica utilizada, m√©todo para valida√ß√£o cruzada, ado√ß√£o (ou n√£o) do uso de m√©todos de reamostragem, quais algoritmos utilizar, qual ser√° o threshold, etc</p>
</div>
<div class="column4">
<p></br>
<img src="https://media.giphy.com/media/JPV8lNtI59zaWyL4pf/giphy.gif" alt="Via Giphy" /></p>
</div>
</div>
<p>Lidar com dados desbalanceados √© um assunto longo portanto tentarei dar mais aten√ß√£o apenas em um <em>hack</em> para encontrar a melhor forma de se aplicar o balanceamento dos dados. N√£o pretendo me aprofundar na teoria envolvida na escolha das m√©tricas neste post, caso o leitor deseje se aprofundar sobre a teoria envolvida com classifica√ß√£o que envolve dados desbalanceados, sugiro a leitura do livro: <a href="https://machinelearningmastery.com/imbalanced-classification-with-python/">Imbalanced Classification with Python - Choose Better Metrics, Balance Skewed Classes and Apply Cost-Sensitive Learning</a> e consultar os links de refer√™ncia no final do post).</p>
</div>
<div id="objetivo" class="section level1">
<h1>Objetivo</h1>
<p>Utilizaremos neste post o pacote <code>workflowsets</code> a fim de otimizar o pipeline de reamostragem da base para lidar com o desbalanceamento dos dados.</p>
<p>Para efeitos de compara√ß√£o, utilizarei como refer√™ncia o (excelente) <a href="https://juliasilge.com/blog/sliced-aircraft/">post escrito recentemente pela Julia Silge</a> em seu blog que tamb√©m aborda o problema de dados desbalanceados utilizando um conjunto de dados de uma <a href="https://www.kaggle.com/c/sliced-s01e02-xunyc5">competi√ß√£o do Kaggle</a>. Utilizarei a mesma configura√ß√£o de pr√©-processamento adotado em seu post para que a compara√ß√£o seja justa.</p>
<p>Portanto, nosso objetivo de modelagem ser√° prever se uma colis√£o com animais selvagens resultou em danos a aeronave.</p>
<div class="w3-panel w3-pale-green w3-border">
<p>‚ö†Ô∏è Este dataset √© rico em possibilidades para diferentes tipos de pr√© processamentos e por isso convido o leitor a analis√°-lo com maior profundidade e tamb√©m a compartilhar seus resultados!</p>
</div>
</div>
<div id="depend√™ncias" class="section level1">
<h1>Depend√™ncias</h1>
<p>Primeiro vamos carregar as bibliotecas necess√°rias e algumas fun√ß√µes desenvolvidas para o post</p>
<pre class="r"><code>library(tidyverse)    # ds toolkit
library(tidymodels)   # ml toolkit
library(baguette)     # bag_tree
library(themis)       # imbalanced
library(workflowsets) # opt pipelines
library(patchwork)    # arrange plots 

doParallel::registerDoParallel()
theme_set(theme_bw())</code></pre>
<details>
<summary>
(<em>Clique aqui para ver as fun√ß√µes</em> <code>print_table</code> <em>e</em> <code>conf_mat_plot</code> <em>importadas</em>)
</summary>
<pre class="r"><code># Para o print de tabelas
print_table &lt;- function(x, round=0, cv=F, wf=F, bm=F, ...){ 
  
  if(round&gt;0) x &lt;- x %&gt;% mutate_if(is.numeric, ~round(.x, round))
  
  if(cv==T){
    columns_spec = list(
      .metric = reactable::colDef(minWidth = 75),
      .estimator = reactable::colDef(minWidth = 70),
      .config = reactable::colDef(minWidth = 120)
    )
  } else if(wf==T){
    columns_spec = list(
      wflow_id = reactable::colDef(minWidth = 100),
      .metric = reactable::colDef(minWidth = 100),
      preprocessor = reactable::colDef(minWidth = 110),
      rank = reactable::colDef(minWidth = 50),
      n = reactable::colDef(minWidth = 50)
    )
  }else if (bm==T){
    columns_spec = list(
      wflow_id = reactable::colDef(minWidth = 130),
      model = reactable::colDef(minWidth = 80)
    )
  }else{
    columns_spec = NULL
  }
  
  reactable::reactable(x, striped = T, bordered = T,
                       highlight = T, pagination = F, resizable = T, 
                       columns = columns_spec, ...)
  
}

# Para plot da matriz de confusao e distribuicoes de probabilidade
conf_mat_plot &lt;- function(x, null_model = FALSE){
  p1 &lt;- 
    x %&gt;%
    select(.pred_class, damaged) %&gt;%
    table() %&gt;% 
    conf_mat() %&gt;% 
    autoplot(type = &quot;heatmap&quot;)+
    labs(title = &quot;Matriz de confus√£o&quot;)
  
  p2 &lt;- 
    x  %&gt;%
    ggplot() +
    geom_density(aes(x = .pred_damage, fill = damaged), 
                 alpha = 0.5)+
    labs(title = &quot;Distribui√ß√µes de probabilidade previstas&quot;,
         subtitle = &quot;por classe&quot;)+ 
    scale_x_continuous(limits = 0:1)+
    scale_fill_brewer(palette=&quot;Set1&quot;)
  
  p1 | p2
} </code></pre>
</details>
<p>¬†</p>
<p>Em seguida vamos importar os dados provenientes da competi√ß√£o Inclass do Kaggle <a href="https://www.kaggle.com/c/sliced-s01e02-xunyc5">SLICED s01e02 - Predict whether an aircraft strike with wildlife causes damage</a>. Para mais informa√ß√µes consulte a <a href="https://www.kaggle.com/c/sliced-s01e02-xunyc5/data">documenta√ß√£o e dicion√°rio dos dados</a>.</p>
<pre class="r"><code>df &lt;- read_csv(&quot;train.csv&quot;)</code></pre>
<p>Note que carregamos apenas os dados de treino pois os dados de teste n√£o possuem a target.</p>
</div>
<div id="preparar-dados" class="section level1">
<h1>Preparar dados</h1>
<p>Tratar a vari√°vel target <code>damaged</code> e avaliar sua distribui√ß√£o:</p>
<pre class="r"><code>df &lt;- df %&gt;% 
  mutate(damaged = if_else(damaged==1, &quot;damage&quot;, &quot;not_damage&quot;) %&gt;% 
           factor(levels = c(&quot;damage&quot;, &quot;not_damage&quot;)))</code></pre>
<details>
<summary>
(<em>Clique aqui para ver o c√≥digo do gr√°fico abaixo</em>)
</summary>
<pre class="r"><code>p1 &lt;- df %&gt;% 
  count(damaged) %&gt;% 
  ggplot(aes(x=rev(damaged), y=n, fill=damaged))+
  geom_bar(stat = &quot;identity&quot;)+
  scale_fill_brewer(palette=&quot;Set1&quot;)+
  theme(legend.position = &quot;bottom&quot;)+
  labs(y=&quot;N√∫mero de inst√¢ncias&quot;, x = &quot;&quot;)

p2 &lt;- df %&gt;% 
  count(damaged) %&gt;% 
  arrange(desc(damaged)) %&gt;%
  mutate(prop = n / sum(n)) %&gt;%
  mutate(ypos = cumsum(prop)- 0.5*prop )%&gt;% 
  ggplot(aes(x=&quot;&quot;, y=prop, fill=damaged)) +
  geom_bar(stat=&quot;identity&quot;, width=1) +
  coord_polar(&quot;y&quot;, start=0) +
  theme_void() + 
  theme(legend.position=&quot;none&quot;) +
  geom_text(aes(y = ypos,
                label = paste(scales::comma(n, big.mark = &quot;.&quot;),
                              scales::comma(n/sum(n), big.mark = &quot;.&quot;, 
                                            suffix = &quot;%&quot; ),sep = &quot;\n&quot;)
                
  ), 
  color = &quot;white&quot;, size=6) +
  scale_fill_brewer(palette=&quot;Set1&quot;)</code></pre>
</details>
<p>¬†</p>
<pre class="r"><code>p1 + p2 </code></pre>
<center>
<img src="/post/2021-06-28-imbalanced-workflowsets/unnamed-chunk-6-1.png" style="width:80.0%" />
</center>
<p>Veja que estamos diante de um problema que existem aproximadamente 9 casos de dano para cada 100 eventos observados.</p>
</div>
<div id="breve-an√°lise-explorat√≥ria" class="section level1">
<h1>Breve an√°lise explorat√≥ria</h1>
<p>Vamos iniciar a explorat√≥ria com uma avalia√ß√£o geral dos dados brutos</p>
<pre class="r"><code>DataExplorer::plot_intro(df, ggtheme = theme_bw(), 
                         theme_config = list(legend.position = &quot;bottom&quot;))</code></pre>
<center>
<img src="/post/2021-06-28-imbalanced-workflowsets/unnamed-chunk-7-1.png" style="width:80.0%" />
</center>
<p>Primeira informa√ß√£o que chama aten√ß√£o √© que quase 1/4 desses dados √© faltante. Vamos olhar a estrutura dessa base de maneira mais aprofundada:</p>
<pre class="r"><code>df %&gt;% 
  sample_frac(0.01) %&gt;% 
  visdat::vis_dat()</code></pre>
<center>
<img src="/post/2021-06-28-imbalanced-workflowsets/unnamed-chunk-8-1.png" style="width:80.0%" />
</center>
<p>Parece existir algum padr√£o nos dados faltantes (que coocorrem em diveros atributos). Al√©m disso algumas colunas est√£o quase inteiramente vazias e ser√£o descartadas no processo de modelagem.</p>
<p>Uma vis√£o geral das classes das features categ√≥ricas:</p>
<pre class="r"><code>df %&gt;%
  select(-damaged, -id)%&gt;%
  mutate_all(as.factor) %&gt;%
  inspectdf::inspect_cat() %&gt;% 
  inspectdf::show_plot()</code></pre>
<center>
<img src="/post/2021-06-28-imbalanced-workflowsets/unnamed-chunk-9-1.png" style="width:80.0%" />
</center>
<p>Algumas features possuem muitas classes e caso seja feita a transforma√ß√£o <em>one-hot-encoding</em> (estrat√©gia amplamente utilizada para lidar com features categ√≥ricas) sem algum cuidado, o desempenho da maioria dos modelos de machine learning pode ser prejudicado por tornar a base anal√≠tica muito esparsa.</p>
<p>Uma vis√£o geral das classes das features num√©ricas em rela√ß√£o a target:</p>
<pre class="r"><code>num_columns &lt;- c(df %&gt;% select_if(is.numeric) %&gt;% colnames(), &#39;damaged&#39;)
df%&gt;% 
  select_at(num_columns) %&gt;% 
  select(-id) %&gt;%
  gather(key, value, -damaged) %&gt;%
  ggplot(aes(y=damaged, x=value))+
  geom_boxplot()+
  facet_wrap(~key, ncol=5, scales = &quot;free_x&quot;)+
  labs(x = &quot;&quot;, y=&quot;&quot;)</code></pre>
<center>
<img src="/post/2021-06-28-imbalanced-workflowsets/unnamed-chunk-10-1.png" style="width:80.0%" />
</center>
<p>Parece que algumas features possuem comportamentos diferentes quando avaliados segundo a target. Al√©m disso √© poss√≠vel notar que as features <code>aircraft_mass</code>, <code>distance</code>, <code>engine4_position</code>, <code>engines</code>, <code>height</code> e <code>speed</code> apresentam outliers.</p>
</div>
<div id="modelagem" class="section level1">
<h1>Modelagem</h1>
<p>Finalmente chegamos a modelagem!</p>
<p>Primeiro vamos definir um esquema de reamostragem (com estratifica√ß√£o) que ser√° utilizado para avaliar os modelos e as m√©tricas de qualidade.</p>
<pre class="r"><code>set.seed(123)

bird_folds &lt;- vfold_cv(df, v = 5, strata = damaged)
bird_metrics &lt;- metric_set(mn_log_loss, accuracy, sensitivity, specificity)</code></pre>
<p>Nossos conjuntos de pipelines necessitar√£o de um pr√©-processador base que ser√° comum a todos como camada inicial. Para isso utilizaremos o mesmo definido no post de refer√™ncia.</p>
<pre class="r"><code>base_rec &lt;- recipe(damaged ~ ., data = df) %&gt;%
  step_select( damaged, flight_impact, precipitation,
               visibility, flight_phase, engines, incident_year,
               incident_month, species_id, engine_type,
               aircraft_model, species_quantity, height, speed) %&gt;% 
  step_novel(all_nominal_predictors()) %&gt;%
  step_other(all_nominal_predictors(), threshold = 0.01) %&gt;%
  step_unknown(all_nominal_predictors()) %&gt;%
  step_impute_median(all_numeric_predictors()) %&gt;%
  step_zv(all_predictors())</code></pre>
<div id="baselines" class="section level2">
<h2>Baselines</h2>
<p>Para efeitos de compara√ß√£o, vamos ajustar 2 modelos que ser√£o utilizados como baselines para saber se a complexidade que estamos adicionando no modelo est√° realmente trazendo algum ganho na performance do modelo. Os modelos ser√£o:</p>
<ul>
<li>Modelo nulo: um modelo que sempre prev√™ a classe majorit√°ria;</li>
<li>Modelo de base: <a href="https://bradleyboehmke.github.io/HOML/bagging.html">Bagged Decision Tree</a> sem adicionar pr√©-processamento para compensar o desequil√≠brio de classe.</li>
</ul>
<div id="modelo-nulo" class="section level3">
<h3>Modelo nulo</h3>
<p>Avaliando modelo nulo via valida√ß√£o cruzada:</p>
<pre class="r"><code>null_spec &lt;- null_model(mode = &quot;classification&quot;) %&gt;% 
  set_engine(&quot;parsnip&quot;)

null_wf &lt;-
  workflow() %&gt;%
  add_recipe(base_rec) %&gt;%
  add_model(null_spec)

null_rs &lt;-
  fit_resamples(
    object = null_wf,
    resamples = bird_folds,
    metrics = bird_metrics,
    control = control_resamples(save_pred = TRUE)
  ) 

collect_metrics(null_rs) %&gt;% print_table(round = 5, cv = T) </code></pre>
<p><img src="/post/2021-06-28-imbalanced-workflowsets/tab1.png" /></p>
<p>Qualquer modelo com desempenho pior do que este deve ser descartado. Vejamos a matriz de confus√£o:</p>
<pre class="r"><code>collect_predictions(null_rs) %&gt;% 
  conf_mat_plot()</code></pre>
<center>
<img src="/post/2021-06-28-imbalanced-workflowsets/unnamed-chunk-14-1.png" style="width:80.0%" />
</center>
</div>
<div id="modelo-de-base" class="section level3">
<h3>Modelo de base</h3>
<p>Agora vamos ajusta o modelo <em>Bagged Decision Tree</em> sem o pr√©-processamento para compensar o desequil√≠brio de classe:</p>
<pre class="r"><code>bag_spec &lt;-
  bag_tree(min_n = 10) %&gt;%
  set_engine(&quot;rpart&quot;, times = 25) %&gt;%
  set_mode(&quot;classification&quot;)

imb_wf &lt;-
  workflow() %&gt;%
  add_recipe(base_rec) %&gt;%
  add_model(bag_spec)

set.seed(123)
imb_rs &lt;-
  fit_resamples(
    imb_wf,
    resamples = bird_folds,
    metrics = bird_metrics,
    control = control_resamples(save_pred = TRUE)
  )

collect_metrics(imb_rs) %&gt;% print_table(round = 5, cv = T)</code></pre>
<p><img src="/post/2021-06-28-imbalanced-workflowsets/tab2.png" /></p>
<p>Apesar do elevado n√∫mero de falsos negativos, este modelo j√° esta com um desempenho razo√°vel em compara√ß√£ao ao modelo nulo e o n√∫mero de verdadeiros positivos j√° √© quase o dobro do n√∫mero de falsos positivos. Veja na matriz de confus√£o abaixo:</p>
<pre class="r"><code>collect_predictions(imb_rs) %&gt;% 
  conf_mat_plot()</code></pre>
<center>
<img src="/post/2021-06-28-imbalanced-workflowsets/unnamed-chunk-16-1.png" style="width:80.0%" />
</center>
</div>
</div>
<div id="preparar-pipeline-de-dados-com-workflowsets" class="section level2">
<h2>Preparar Pipeline de dados com <code>workflowsets</code></h2>
<p>A escolha do m√©todo de amostragem dos dados √© t√£o importante quanto a escolha do modelo preditivo que ser√° utilizado pois o desempenho pode ser enganosamente otimista visto que o algoritmo de bagging n√£o esta usando nenhuma estrat√©gia de subamostragem aleat√≥ria da classe majorit√°ria em cada amostra de bootstrap para equilibrar as duas classes.</p>
<p>Existem muitos m√©todos para amostragem de dados e n√£o h√° um m√©todo √∫nico que seja melhor em todos os problemas de classifica√ß√£o (assim como n√£o existe o ‚Äúmelhor modelo‚Äù) portanto, utilizaremos este pacote para testar diferentes m√©todos e tamb√©m tunar seus hiperpar√¢metros.</p>
<div id="oversampling" class="section level3">
<h3>Oversampling</h3>
<p>Estes m√©todos duplicam ou sintetizam novos dados da classe minorit√°ria. Deve ser usado com cautela pois na vida real pode gerar alguns dados que n√£o condizem com a relidade ou criar tantas inst√¢ncias que acaba consumindo muito mais tempo de processamento.</p>
<div id="random-oversampling" class="section level4">
<h4>Random Oversampling</h4>
<p>Este m√©todo simplesmente duplica aleat√≥riamente exemplos da classe minorit√°ria. Vamos tunar esta propor√ß√£o buscando n√∫meros reais no intervalo [0.5,1].</p>
<pre class="r"><code>rec_up &lt;- base_rec %&gt;% 
  step_upsample(damaged, over_ratio = tune())

params_up &lt;- rec_up %&gt;% 
  parameters() %&gt;% update(over_ratio = mixture(c(0.5, 1)))</code></pre>
</div>
<div id="smote---synthetic-minority-oversampling-technique" class="section level4">
<h4>SMOTE - Synthetic Minority Oversampling Technique</h4>
<p>O SMOTE funciona gerando novos dados sint√©tios baseados em exemplos selecionando que est√£o ‚Äúpr√≥ximos‚Äù. Vamos tunar tanto a propor√ß√£o de dados que ser√£o gerados quanto a quantidade de vizinhos selecionados, buscando n√∫meros reais e inteiros no intervalo [0.5,1] e [1, 10], respectivamente.</p>
<pre class="r"><code>rec_smote &lt;- base_rec %&gt;%
  step_dummy(all_nominal_predictors()) %&gt;%
  step_smote(damaged, over_ratio = tune(), 
             neighbors = tune())

params_smote &lt;- rec_smote %&gt;% 
  parameters() %&gt;% update(over_ratio = mixture(c(0.5, 1)),
                          neighbors = neighbors())</code></pre>
</div>
<div id="adasyn---adaptive-synthetic-sampling" class="section level4">
<h4>ADASYN - Adaptive Synthetic Sampling</h4>
<p>O ADASYN √© uma extens√£o do SMOTE que busca propor melhorias. Vamos tunar os mesmos par√¢metros definidos no SMOTE.</p>
<pre class="r"><code>rec_adasyn &lt;- base_rec %&gt;%
  step_dummy(all_nominal_predictors()) %&gt;%
  step_adasyn(damaged, 
              over_ratio = tune(), 
              neighbors = tune())

params_adasyn &lt;- rec_adasyn %&gt;% 
  parameters() %&gt;% update(over_ratio = mixture(c(0.5, 1)),
                          neighbors = neighbors())</code></pre>
</div>
</div>
<div id="undersampling" class="section level3">
<h3>Undersampling</h3>
<p>S√£o t√©cnicas que excluem ou selecionam um subconjunto de exemplos da classe majorit√°ria e existem dezenas (se n√£o centenas) desses m√©todos. Neste post utilizaremos s√≥ 3 mas existem outros implementados em outras bibliotecas (em R e em Python).</p>
<div id="random-undersampling" class="section level4">
<h4>Random Undersampling</h4>
<p>Este √© o m√©todo mais simples e envolve a exclus√£o aleat√≥ria de algumas inst√¢ncias da classe majorit√°ria. Vamos tunar esta propor√ß√£o de frequ√™ncias da minorit√°ria para a majorit√°ria.</p>
<pre class="r"><code>rec_down &lt;- base_rec %&gt;% 
  step_downsample(damaged, under_ratio = tune())

params_down &lt;- rec_down %&gt;% 
  parameters() %&gt;% update(under_ratio = deg_free())</code></pre>
</div>
<div id="near-miss-undersampling" class="section level4">
<h4>Near Miss Undersampling</h4>
<p>Este algoritmo se baseia em m√©todos de KNN selecionando exemplos da classe majorit√°ria que tem menor dist√¢ncia m√©dia dos k exemplos mais pr√≥ximos. Vamos tunar tanto a propor√ß√£o quanto o n√∫mero de vizinhos utilizados.</p>
<pre class="r"><code>rec_nearmiss &lt;- base_rec %&gt;%
  step_dummy(all_nominal_predictors()) %&gt;%
  step_nearmiss(damaged, 
                under_ratio = tune(), 
                neighbors = tune())

params_nearmiss &lt;- rec_nearmiss %&gt;% 
  parameters() %&gt;% update(under_ratio = deg_free(),
                          neighbors = neighbors())</code></pre>
</div>
<div id="tomek-links-undersampling" class="section level4">
<h4>Tomek Links Undersampling</h4>
<p>Este algoritmo que tenta excluir inst√¢ncias que sejam pr√≥ximas e que possuam classes diferentes, buscando diminuir a ambiguidade dos dados. N√£o vamos tunar nenhum hiperpar√¢metro aqui.</p>
<pre class="r"><code>rec_tomek &lt;- base_rec %&gt;%
  step_dummy(all_nominal_predictors()) %&gt;%
  step_tomek(damaged)</code></pre>
</div>
</div>
<div id="preparar-pipeline-de-dados" class="section level3">
<h3>Preparar pipeline de dados</h3>
<p>Agora que todos pipelines de dados candidatos est√£o definidos, vamos combinar tudo em um √∫nico objeto com <code>workflow_set</code>:</p>
<pre class="r"><code>chi_models &lt;- 
  workflow_set(
    preproc = list(upsample = rec_up,
                   smote = rec_smote,
                   adasyn = rec_adasyn,
                   downsample = rec_down,
                   nearmiss = rec_nearmiss,
                   tomek = rec_tomek),
    models = list(bag_spec = bag_spec),
    cross = TRUE
  )</code></pre>
<p>Utilizar a fun√ß√£o <code>option_add</code> para adicionar as informa√ß√µes dos intervalos definidos para cada hiperpar√¢metro:</p>
<pre class="r"><code>chi_models &lt;- chi_models %&gt;% 
  option_add(param_info = params_up, id = &quot;upsample_bag_spec&quot;)  %&gt;% 
  option_add(param_info = params_smote, id = &quot;smote_bag_spec&quot;) %&gt;% 
  option_add(param_info = params_adasyn, id = &quot;adasyn_bag_spec&quot;) %&gt;% 
  option_add(param_info = params_down, id = &quot;downsample_bag_spec&quot;) %&gt;% 
  option_add(param_info = params_nearmiss, id = &quot;nearmiss_bag_spec&quot;)</code></pre>
<p>Finalmente, vamos ajustar todos os modelos utilizando o m√©todo simples para fazer a busca dos melhores hiperpar√¢metros em grids de 20 valores aleat√≥rios e calcular os scores via valida√ß√£o cruzada (esta parte pode demorar bastante tempo):</p>
<pre class="r"><code>set.seed(123)
chi_models &lt;- 
  chi_models %&gt;% 
  workflow_map(&quot;tune_grid&quot;,
               resamples = bird_folds, 
               grid = 20, 
               metrics = bird_metrics,
               control = control_resamples(save_pred = TRUE),
               verbose = TRUE)</code></pre>
<p>Vejamos os resultados:</p>
<pre class="r"><code>rank_results(chi_models, rank_metric = &quot;mn_log_loss&quot;, select_best = TRUE) %&gt;% 
  select(-.config) %&gt;%
  mutate(wflow_id = str_remove(wflow_id, &quot;_bag_spec&quot;)) %&gt;% 
  print_table(round = 5, wf=T, height = 300, filterable = T)</code></pre>
<p>Matriz de confus√£o do modelo com menor <em>logloss</em>:</p>
<pre class="r"><code>collect_predictions(chi_models) %&gt;% 
  filter(wflow_id == &quot;tomek_bag_spec&quot;) %&gt;% 
  conf_mat_plot()</code></pre>
<center>
<img src="/post/2021-06-28-imbalanced-workflowsets/unnamed-chunk-29-1.png" style="width:80.0%" />
</center>
</div>
</div>
<div id="benchmark" class="section level2">
<h2>Benchmark</h2>
<p>Comparando os resultados dos modelos ajustados:</p>
<details>
<summary>
(<em>Clique aqui para ver o c√≥digo que cria o objeto</em> <code>benchmark</code>)
</summary>
<pre class="r"><code>benchmark &lt;- bind_rows(
  mutate(collect_metrics(null_rs), wflow_id = &quot;default&quot;, model = &quot;null_model&quot;) %&gt;% 
    select(.metric, mean, wflow_id, model) %&gt;% 
    spread(.metric, mean)
  ,
  mutate(collect_metrics(imb_rs), wflow_id = &quot;default&quot;, model = &quot;bag_tree&quot;) %&gt;% 
    select(.metric, mean, wflow_id, model) %&gt;% 
    spread(.metric, mean)
  ,
  rank_results(chi_models, rank_metric = &quot;mn_log_loss&quot;, select_best = TRUE) %&gt;% 
    filter(wflow_id==&quot;smote_bag_spec&quot;) %&gt;% 
    select(.metric, mean, wflow_id, model) %&gt;% 
    spread(.metric, mean)
  ,
  rank_results(chi_models, rank_metric = &quot;mn_log_loss&quot;, select_best = TRUE) %&gt;% 
    filter(rank==1) %&gt;% 
    select(.metric, mean, wflow_id, model) %&gt;% 
    spread(.metric, mean)
) </code></pre>
</details>
<pre class="r"><code>benchmark  %&gt;%
  print_table(round = 5, bm = T)</code></pre>
<p>Como no post da Julia, a logloss e a precis√£o dos modelos que utilizaram m√©todos de balanceamento dos dados pioraram em rela√ß√£o ao modelo de <em>Bagged Decision Tree</em> sem o uso desses pipelines. Apesar da piora em rela√ß√£o ao modelo de base nota-se que outros m√©todos como <em>Tomek Links</em> e <em>Adasyn</em> se sa√≠ram ligeiramente melhores do que o <em>Smote</em> (al√©m disso vimos que o <em>Smote</em> com sua configura√ß√£o <em>default</em> n√£o necessariamente produriz√° os melhores resultados).</p>
<p>Este tipo de performance √© muito comum e at√© esperado visto que estamos avaliando o modelo atrav√©s de uma √∫nica m√©trica (com os mesmos pontos de corte e com o mesmo algoritmo). Normalmente no mundo real monitoramos diversas m√©tricas e experimentamos mais configura√ß√µes de hiperpar√¢metros de diferentes modelos com diferentes pipelines.</p>
</div>
</div>
<div id="conclus√£o" class="section level1">
<h1>Conclus√£o</h1>
<p>Assim como n√£o existe melhor modelo, n√£o existe melhor t√©cnica de balanceamento de dados. Portanto, na busca de melhores resultados n√≥s podemos tentar otimizar qual abordagem ser√° uyilizada bem como seus hiperpar√¢metros (em conjunto com os hiperpar√¢metros dos modelos em quest√£o).</p>
<p>Esta abordagem em R √© nova para mim (estou mais acostumado a utilizar em Python com o m√©todo <code>sklearn.pipeline.Pipeline</code> em conjunto com a biblioteca <a href="https://pypi.org/project/imblearn/">imblearn</a>) ent√£o qualquer cr√≠tica e sugest√£o de melhoria ser√° muito bem vinda! Basta entrar em contato ou deixar aqui nos coment√°rios!</p>
<p>Bons estudos e espero que gostem! üöÄ</p>
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias</h1>
<ul>
<li><a href="https://www.tidyverse.org/blog/2021/03/workflowsets-0-0-1/" class="uri">https://www.tidyverse.org/blog/2021/03/workflowsets-0-0-1/</a></li>
<li><a href="https://www.kaggle.com/c/sliced-s01e02-xunyc5" class="uri">https://www.kaggle.com/c/sliced-s01e02-xunyc5</a></li>
<li><a href="https://juliasilge.com/blog/sliced-aircraft/" class="uri">https://juliasilge.com/blog/sliced-aircraft/</a></li>
<li><a href="https://topepo.github.io/caret/subsampling-for-class-imbalances.html" class="uri">https://topepo.github.io/caret/subsampling-for-class-imbalances.html</a></li>
<li><a href="https://machinelearningmastery.com/bagging-and-random-forest-for-imbalanced-classification/" class="uri">https://machinelearningmastery.com/bagging-and-random-forest-for-imbalanced-classification/</a></li>
<li><a href="https://machinelearningmastery.com/what-is-imbalanced-classification/" class="uri">https://machinelearningmastery.com/what-is-imbalanced-classification/</a></li>
<li><a href="https://machinelearningmastery.com/framework-for-imbalanced-classification-projects/" class="uri">https://machinelearningmastery.com/framework-for-imbalanced-classification-projects/</a></li>
<li><a href="https://machinelearningmastery.com/data-sampling-methods-for-imbalanced-classification/" class="uri">https://machinelearningmastery.com/data-sampling-methods-for-imbalanced-classification/</a></li>
</ul>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2021-06-28-imbalanced-workflowsets/">Otimizando pipelines que envolvem dados desbalanceados</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">imbalanced</category>
      <category domain="tag">imbalanced-data</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">machine-learning</category>
      <category domain="tag">pratica</category>
      <category domain="tag">r</category>
      <category domain="tag">random-forest</category>
      <category domain="tag">tidymodels</category>
      <category domain="tag">tidyverse</category>
      <category domain="tag">tunning</category>
    </item>
    <item>
      <title>Ci√™ncia de Dados - Uma vis√£o geral</title>
      <link>https://gomesfellipe.github.io/post/2021-05-24-ciencia-de-dados-uma-visao-geral/</link>
      <pubDate>Mon, 24 May 2021 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2021-05-24-ciencia-de-dados-uma-visao-geral/</guid>
      <description>Nesta apresenta√ß√£o tive a oportunidade de falar um pouco sobre a minha vis√£o e cases relacionados a esta √°rea t√£o extensa e incr√≠vel que √© a ci√™ncia de dados!</description>
      <content:encoded>&lt;![CDATA[
        


<p>Com a elevada quantidade de dados sendo produzidos a todo instante e o poder computacional cada vez maior, a ci√™ncia de dados tem ganhado muito espa√ßo no mercado. Isso ocorre pois suas ferramentas nos permitem descobrir solu√ß√µes ocultas a partir de enormes massas de dados desorganizados combinando programa√ß√£o, matem√°tica, estat√≠stica e compreens√£o contextual.</p>
<div id="o-bom-filho-√†-casa-torna" class="section level1">
<h1>O bom filho √† casa torna!</h1>
<p>No dia 23 de abr. de 2021 tive o enorme prazer de apresentar no <a href="https://edu.ieee.org/br-uff/ramo-uff/">Ramo Estudantil IEEE UFF</a> um pouco sobre a minha vis√£o sobre √°reas que comp√µe a ci√™ncia de dados e apresentar alguns cases relacionados!</p>
<p>Sou muito grato √† organiza√ß√£o por todo o cuidado e capricho do evento e muito satisfeito de poder retornar (apesar das circunst√¢ncias do momento de pandemia) a esta universidade que mudou minha vida e poder contribuir um pouquinho para o desenvolvimento de alunos interessados em seguir na √°rea.</p>
<p>Conte√∫dos que foram (brevemente) abordados:</p>
<ul>
<li>O que √© ci√™ncia de dados</li>
<li>Estat√≠stica e Ci√™ncia de dados</li>
<li>Big Data</li>
<li>Machine Learning</li>
<li>Aplica√ß√µes (e cases)</li>
<li>Dicas de Carreira</li>
</ul>
<p>Foi uma apresenta√ß√£o de 1 hora e esta gravada no canal do Ramo Estudantil IEEE - UFF no Youtube no link: <a href="https://www.youtube.com/watch?v=9UWhZ1s3Ybc" class="uri">https://www.youtube.com/watch?v=9UWhZ1s3Ybc</a></p>
<div style="text-align:center;">
<p><iframe width="560" height="315"
    src="https://www.youtube.com/embed/9UWhZ1s3Ybc"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen>
</iframe></p>
</div>
<p>Tentei tratar o assunto de forma mais leve poss√≠vel, recheada de memes e linguagem acess√≠vel, espero que gostem do conte√∫do!</p>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2021-05-24-ciencia-de-dados-uma-visao-geral/">Ci√™ncia de Dados - Uma vis√£o geral</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">big-data</category>
      <category domain="tag">carreira</category>
      <category domain="tag">data-science</category>
      <category domain="tag">estatistica</category>
      <category domain="tag">machine-learning</category>
    </item>
    <item>
      <title>Prevendo a qualidade do sono utilizando Machine Learning</title>
      <link>https://gomesfellipe.github.io/post/2021-02-28-qualidade-do-sono-machine-learning/</link>
      <pubDate>Thu, 04 Mar 2021 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2021-02-28-qualidade-do-sono-machine-learning/</guid>
      <description>Utilizaremos dados reais coletados pelo celular para gerar previs√µes a partir de uma pequena base de dados com target desbalanceada</description>
      <content:encoded>&lt;![CDATA[
        

<div id="TOC">
<ul>
<li><a href="#qualidade-de-sono" id="toc-qualidade-de-sono">Qualidade de sono? ü§®</a></li>
<li><a href="#como-funciona-aplicativo-sleep-cycle" id="toc-como-funciona-aplicativo-sleep-cycle">Como funciona aplicativo Sleep Cycle? <img src="https://www.sleepcycle.com/wp-content/uploads/2020/09/sleep_cycle_app_icon-480x480.png" style="width:3.0%" /></a></li>
<li><a href="#objetivo" id="toc-objetivo">Objetivo üéØ</a></li>
<li><a href="#explorar-dados" id="toc-explorar-dados">Explorar dados üîé</a>
<ul>
<li><a href="#limpeza-e-prepara%C3%A7%C3%A3o-dos-dados" id="toc-limpeza-e-prepara√ß√£o-dos-dados">Limpeza e prepara√ß√£o dos dados</a></li>
<li><a href="#imputar-dados-de-fontes-externas" id="toc-imputar-dados-de-fontes-externas">Imputar dados de fontes externas</a></li>
<li><a href="#insights" id="toc-insights">Insights</a></li>
<li><a href="#reter-dados" id="toc-reter-dados">Reter dados</a></li>
</ul></li>
<li><a href="#modelagem" id="toc-modelagem">Modelagem üöÄ</a>
<ul>
<li><a href="#amostragem" id="toc-amostragem">Amostragem</a></li>
<li><a href="#engenharia-de-recursos" id="toc-engenharia-de-recursos">Engenharia de recursos</a></li>
<li><a href="#modelo-nulo-baseline" id="toc-modelo-nulo-baseline">Modelo Nulo (Baseline)</a></li>
<li><a href="#%C3%A1rvore-de-decis%C3%B5es" id="toc-√°rvore-de-decis√µes">√Årvore de decis√µes</a></li>
<li><a href="#random-forest" id="toc-random-forest">Random Forest</a></li>
<li><a href="#lightgbm" id="toc-lightgbm">LightGBM</a></li>
</ul></li>
<li><a href="#sele%C3%A7%C3%A3o-do-modelo" id="toc-sele√ß√£o-do-modelo">Sele√ß√£o do modelo ü§î</a></li>
<li><a href="#previs%C3%A3o-em-dados-novos" id="toc-previs√£o-em-dados-novos">Previs√£o em dados novos üí´</a></li>
<li><a href="#conclus%C3%A3o" id="toc-conclus√£o">Conclus√£o üçª</a></li>
<li><a href="#refer%C3%AAncias" id="toc-refer√™ncias">Refer√™ncias üß≥</a></li>
</ul>
</div>

<style>
.column {
float: left;
width: 50%;
padding: 10px;
}

.column4 {
float: left;
width: 33%;
padding: 10px;
}

.column8 {
float: left;
width: 66%;
padding: 10px;
}

.row:after {
content: "";
display: table;
clear: both;
}

.center {
display: flex;
justify-content: center;
align-items: center;
height: 200px;
}
</style>
<div id="qualidade-de-sono" class="section level1">
<h1>Qualidade de sono? ü§®</h1>
<p>Sim, exatamente! Neste post analisaremos dados de um <em>tracking</em> que venho fazendo desde 2017 com informa√ß√µes relacionadas √† um sono de qualidade.</p>
<div class="row">
<div class="column8">
<p>Boas noites de sono nos tornam mais felizes, mais saud√°veis, mais inteligentes, mais dispostos e evita problemas de cansa√ßo, falta de concentra√ß√£o, depress√£o e ansiedade.</p>
<p>Resumindo, a nossa qualidade de vida est√° diretamente ligada √† qualidade do nosso sono, pois ao dormir nosso corpo realiza fun√ß√µes extremamente importantes como por exemplo o fortalecimento do sistema imunol√≥gico, secre√ß√£o e libera√ß√£o de horm√¥nios, consolida√ß√£o da mem√≥ria, entre outras<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>.</p>
</div>
<div class="column4">
<div class="float">
<img src="https://media.giphy.com/media/mguPrVJAnEHIY/giphy.gif" alt="Via Giphy" />
<div class="figcaption"><a href="https://media.giphy.com/media/mguPrVJAnEHIY/giphy.gif">Via Giphy</a></div>
</div>
</div>
</div>
<p>Alguns fatores podem auxiliar a determinar se uma noite foi bem dormida como por exemplo: a regularidade do hor√°rio de dormir e de acordar, a frequ√™ncia card√≠aca (bpm), n√∫mero de passos dados no dia, tempo na cama, tempo antes de dormir, ronco, tipo de clima etc..</p>
<div class="row">
<div class="column4">
<div class="float">
<img src="https://media.giphy.com/media/xUPJPlFxssGpmLemru/giphy.gif" style="width:80.0%" alt="Via Gyiphy" />
<div class="figcaption"><a href="https://media.giphy.com/media/xUPJPlFxssGpmLemru/giphy.gif">Via Gyiphy</a></div>
</div>
</div>
<div class="column8">
<p>Felizmente, existe um aplicativo chamado <a href="sleepcycle.com/">Sleep Cycle</a> que √© capaz de <em>trackear</em> todas essas informa√ß√µes durante o uso do app, dentre outras funcionalidades. Desde 2017 tenho acompanhado meu sono atrav√©s dele, principalmente pela funcionalidade de <a href="https://www.sleepcycle.com/how-sleep-cycle-works/">rastreio dos padr√µes de sono para despertar durante sua fase mais leve, sem um despertador convencional</a> e tenho curtido bastante!</p>
</div>
</div>
<p>A proposta principal do aplicativo √© monitorar os sinais do corpo para nos despertar suavemente quando estivermos no est√°gio de sono mais leve poss√≠vel, pois acordar durante o sono leve √© como acordar naturalmente descansado!</p>
</div>
<div id="como-funciona-aplicativo-sleep-cycle" class="section level1">
<h1>Como funciona aplicativo Sleep Cycle? <img src="https://www.sleepcycle.com/wp-content/uploads/2020/09/sleep_cycle_app_icon-480x480.png" style="width:3.0%" /></h1>
<p><small>Tradu√ß√£o livre de <a href="https://www.sleepcycle.com/how-sleep-cycle-works/"><em>How Sleep Cycle works</em></a>:</small></p>
<p>‚ÄúO funcionamento b√°sico desse aplicativo se baseia que nos mexemos predominantemente durante o sono leve. J√° durante o sono pesado, os m√∫sculos tendem a permanecer relaxados, e em sono REM a movimenta√ß√£o muscular abaixo do pesco√ßo fica paralizada.</p>
<p>Assim sendo √© poss√≠vel selecionar um hor√°rio que gostaria de acordar, como de 6:30 at√© 7:00, e o aplicativo rastrear√° os movimentos na cama para acordar apenas quando entrar em sono leve durnte este per√≠odo.</p>
<p>Dessa forma, estar√≠amos aumentando as chances de acordar mais bem-disposto, j√° que seu sono foi interrompido em uma fase mais leve de descanso.‚Äù</p>
<p>Vejamos dois gr√°ficos que exemplificam dois dos poss√≠veis cen√°rios de uma noite de sono:</p>
<div class="row">
<div class="column">
<center>
<strong>Exemplo 1 - sono regular</strong>
<img src="https://www.sleepcycle.com/wp-content/uploads/2019/08/sleepcycle_regular_sleep.png" style="width:80.0%" alt="via SleepCycle.com" />
</br>
<small>Os picos representam os ciclos do sono, incluindo todas as fases do sono.</small>
</center>
</div>
<div class="column">
<center>
<strong>Exemplo 2 - sono irregular</strong>
<img src="https://www.sleepcycle.com/wp-content/uploads/2019/08/sleepcycle_irregular_sleep.png" style="width:80.0%" alt="via SleepCycle.com" />
</br>
<small>Ciclos de sono mais irregulares, onde o usu√°rio provavelmente n√£o dormiu t√£o bem como em nosso primeiro exemplo.</small>
</center>
</div>
</div>
<p>Esta √© a principal informa√ß√£o coletada no aplicativo e que permite um ‚Äúdespertar tranquilo‚Äù!</p>
<!-- Agora que j√° entendemos as benef√≠cios de uma noite bem dormida, de onde v√™m os dados, como o app funciona e quantas horas proporcionam uma boa noite de sono, vamos direto ao objetivo deste post! -->
</div>
<div id="objetivo" class="section level1">
<h1>Objetivo üéØ</h1>
<p>Apesar do aplicativo captar diversos dados sobre a noite de sono, o ‚Äúhumor ao acordar‚Äù √© uma informa√ß√£o fornecida pelo usu√°rio assim que desativa o alarme, quando a seguinte tela √© exibida:</p>
<center>
<img src="/post/2021-02-28-qualidade-do-sono-machine-learning/mood.jpg" style="width:80%;">
</center>
<p>Como houveram diversos dias em que utilizei o aplicativo mas n√£o assinalei o humor (seja por ter desativado o recurso por algum tempo ou simplesmente por ter ignorado üòÖ) vamos trabalhar para responder a seguinte pergunta:</p>
<blockquote>
<p>Qual foi a probabilidade de ter acordado de <strong>mal humor</strong> durante o per√≠odo de <em>tracking</em> do app, nos dias cujo esse dado √© faltante?</p>
</blockquote>
<p>Onde <strong>mal humor</strong> ser√° a classe positiva da <strong>target</strong>, traduzido nos dados da seguinte forma:</p>
<p><span class="math display">\[
mood=
\begin{cases}
Bom, &amp; \text{se}\  mood = Bom \\
Ruim, &amp; \text{c.c}\
\end{cases}
\]</span></p>
<p>Logo, <code>mood</code> ser√° bin√°ria, avaliando se o humor foi <code>Bom</code> ou <code>Ruim</code> ao acordar, onde <code>Ruim</code> a combina√ß√£o do status üòë (Ok) e üò° (Mau) e ser√° a classe mais importante para controlar os erros de previs√£o.</p>
<p>Tomei a liberdade de fazer essa transforma√ß√£o pois desde o in√≠cio do uso do app, marco como <code>Ruim</code> apenas quando realmente n√£o descansei de forma satisfat√≥ria. Isso pode ter ocorrido por diversos fatores, como por exemplo: acordar ap√≥s um pesadelo; acordar com barulho da rua ou de casa; acordar meio doente ou passando mal e por ai vai..</p>
<p>Por enquanto, estas informa√ß√µes ser√£o suficientes. Vejamos na an√°lise explorat√≥ria como se apresenta a vari√°vel target e quais dados dispon√≠veis para atingir tal objetivo.</p>
</div>
<div id="explorar-dados" class="section level1">
<h1>Explorar dados üîé</h1>
<p>Carregar as depend√™ncias:</p>
<pre class="r"><code>library(tidyverse)  # datascience toolkit 
library(lubridate)  # manipule date
library(patchwork)  # grid ggplot
library(tidymodels) # machine learning toolkit
library(reactable)  # print tables 
library(treesnip)   # lightgbm

# Definir tema para ggplot
theme_set(theme_bw()) </code></pre>
<p>Vamos carregar fun√ß√µes que foram desenvolvidas ao longo das an√°lises para facilitar tanto na apresenta√ß√£o dos resultados quanto na portabilidade dos c√≥digos (bastando pequenos ajustes para ‚Äúrecicl√°-los‚Äù ‚ôªÔ∏è):</p>
<details>
<summary>
(<em>Clique aqui para exibir as fun√ß√µes customizadas</em>)
</summary>
<pre class="r"><code># Para o print de tabelas
print_table &lt;- function(x, round=0, evalue_model = F, ...){ 
  
  if(round&gt;0) x &lt;- x %&gt;% mutate_if(is.numeric, ~round(.x, round))
  
  if(evalue_model == T){
    
    reactable::reactable(x, striped = T, bordered = T, 
                         highlight = T, pagination = F,
                         width = 800,
                         defaultColDef = colDef(minWidth = 85),
                         defaultSorted = list(auc_pr = &quot;desc&quot;),
                         columns = list(
                           model = colDef(minWidth = 110),
                           tp = colDef(minWidth = 40),
                           fp = colDef(minWidth = 40),
                           fn = colDef(minWidth = 40),
                           tn = colDef(minWidth = 40)),
                         ...)  
    
  }else{
    reactable::reactable(x, striped = T, bordered = T, width = 800,
                         highlight = T, pagination = F, ...)  
  }
  
  
}

# Graficos de features numericas
plot_num &lt;- function(data, num_feature, 
                     title = NULL, bins = 30, legend = NULL){
  
  if(is.null(title)) title = num_feature
  
  data = data %&gt;% filter(!is.na(mood))
  
  p_shapiro = round(shapiro.test(data$air_pressure_pa)$p.value, 5)
  
  p1 &lt;- 
    data %&gt;% 
    ggplot(aes_string(x = num_feature, fill = &quot;mood&quot;))+
    geom_histogram(aes(y=..density..), bins = bins, alpha = 0.5,
                   show.legend = ifelse(!is.null(legend), T, F))+
    geom_density(alpha = 0.5,
                 show.legend = ifelse(!is.null(legend), T, F))+
    labs(y = &quot;&quot;, x= &quot;&quot;, title = title)+
    scale_fill_viridis_d(end = 0.8, direction = 1)
  
  if(!is.null(legend)){
    p1 = p1 + theme(legend.position = legend)
  }
  
  p2 &lt;- 
    data %&gt;% 
    ggplot(aes_string(x = num_feature))+
    geom_boxplot(aes(y = &quot;&quot;, color = mood), 
                 show.legend = F)+
    labs(y = &quot;&quot;, x= &quot;&quot;, 
         caption = paste0(&quot;Shapiro-Wilk normality test: &quot;,
                          ifelse(p_shapiro == 0, &quot;P&lt;0.05&quot;, p_shapiro) ))+
    scale_color_viridis_d(end = 0.8, direction = 1)
  
  p1 / p2  + plot_layout(heights = c(4/5, 1/5))
}  

# Graficos de features categoricas
plot_cat &lt;- function(data, cat_feature, title = NULL, label = TRUE, legend = NULL){
  
  data = data %&gt;% filter(!is.na(mood))
  
  valor_p = round(chisq.test(data %&gt;% pull(cat_feature), 
                             data$mood, 
                             simulate.p.value = T)$p.value, 5)
  
  to_plot = data %&gt;%
    count(!!as.name(cat_feature), mood) %&gt;% 
    group_by(!!as.name(cat_feature)) 
  
  final_plot = to_plot %&gt;% 
    mutate(prop = n/sum(n),
           lab = paste0(round(prop*100, 2), &quot;%&quot;)) %&gt;% 
    ggplot()+
    geom_bar(aes_string(x = cat_feature, y = &quot;n&quot;, fill = &quot;mood&quot;),
             stat = &quot;identity&quot;, alpha = 0.7, 
             position = position_dodge2(0.9),
             show.legend = ifelse(!is.null(legend), T, F))+
    scale_fill_viridis_d(end = 0.8, direction = 1)+
    labs(title = title, y = &quot;&quot;,
         caption = paste0(&quot;Pearson&#39;s Chi-squared test: &quot;, valor_p))
  
  if(label == TRUE){
    final_plot = final_plot+
      geom_label(aes_string(x = cat_feature, y = &quot;n&quot;, label = &quot;lab&quot;),
                 position = position_dodge2(0.9), show.legend = F)  
  }
  
  if(!is.null(legend)){
    final_plot = final_plot + theme(legend.position = legend)
  }
  
  return(final_plot)
  
}

# Grafico interativo de features temporais
plot_dygraph &lt;- function(x, order.by, feature, title = NULL){
  x %&gt;%  
    xts::xts(order.by = order.by) %&gt;% 
    .[,feature] %&gt;%
    dygraphs::dygraph(main = title) %&gt;% 
    dygraphs::dyRangeSelector()
}

# Calcula o ponto de corte que maximiza a funcao f beta
threshold_max &lt;- function(x){
  
  fbeta &lt;- function(precision, recall){ 
    (beta+1)*(precision*recall)/(beta*(precision+recall))
  }
  
  # https://machinelearningmastery.com/fbeta-measure-for-machine-learning/
  # F05: + precision - recall
  # F1 : + precision + recall
  # F2 : - precision + recall 
  beta = 0.5
  
  x  %&gt;%
    pr_curve(mood, .pred_Ruim) %&gt;% 
    mutate(fbeta = fbeta(precision, recall) ) %&gt;% 
    filter(fbeta == max(fbeta, na.rm = T))
}

# Plot da matriz de confusao e da funcao das funcoes de densidade estimadas
conf_mat_plot &lt;- function(x, null_model = FALSE){
  trs &lt;- threshold_max(x)$.threshold
  
  if(null_model==FALSE){
    x &lt;- x %&gt;% 
      mutate(.pred_class = ifelse(.pred_Ruim &gt;= trs, &quot;Ruim&quot;, &quot;Bom&quot;) %&gt;%
               factor(levels = c(&quot;Ruim&quot;, &quot;Bom&quot;), ordered = TRUE))  
  }
  
  p1 &lt;- 
    x %&gt;%
    select(.pred_class, mood) %&gt;%
    table() %&gt;% 
    conf_mat() %&gt;% 
    autoplot(type = &quot;heatmap&quot;)+
    labs(title = &quot;Matriz de Confusao&quot;,
         subtitle = paste0(&quot;Threshold max F0.5: &quot;, round(trs, 4)))
  
  p2 &lt;- 
    x  %&gt;%
    ggplot() +
    geom_density(aes(x = .pred_Ruim, fill = mood), 
                 alpha = 0.5)+
    labs(title = &quot;Distribui√ß√µes de probabilidade previstas&quot;,
         subtitle = &quot;por classe&quot;)+ 
    scale_x_continuous(limits = 0:1)+
    geom_vline(aes(xintercept = trs, color = &quot;threshold max F0.5&quot;), linetype = 2) +
    scale_color_manual(name = &quot;&quot;, values = c(`threshold max F0.5` =  &quot;red&quot;))+
    scale_fill_viridis_d(end = 0.7, direction = 1)
  
  p1 | p2
} 

# Conjunto de metricas utilizadas para avaliar os modelos
evalue_model &lt;- function(x, model = &quot;&quot;, null_model=FALSE){
  
  trs &lt;- threshold_max(x)$.threshold
  
  if(null_model==FALSE){
    x &lt;- x %&gt;%
      mutate(.pred_class = ifelse(.pred_Ruim &gt;= trs, &quot;Ruim&quot;, &quot;Bom&quot;) %&gt;% 
               factor(levels = c(&quot;Ruim&quot;, &quot;Bom&quot;), ordered = TRUE))
  }
  
  cm &lt;- x %&gt;% 
    select(.pred_class, mood) %&gt;% 
    table() 
  
  tibble(
    model = model,
    tp = cm[1,1],
    fp = cm[1,2],
    fn = cm[2,1],
    tn = cm[2,2],
    auc_roc   = yardstick::roc_auc(x, mood, `.pred_Ruim`)$.estimate,
    auc_pr    = yardstick::pr_auc(x, mood, `.pred_Ruim`)$.estimate,
    logloss   = yardstick::mn_log_loss_vec(x$mood, x$.pred_Ruim),
    f1        = yardstick::f_meas_vec(x$mood, x$.pred_class, beta = 1),
    f05       = yardstick::f_meas_vec(x$mood, x$.pred_class, beta = 0.5),
    f2        = yardstick::f_meas_vec(x$mood, x$.pred_class, beta = 2),
    precision = yardstick::precision_vec(x$mood, x$.pred_class),
    recall    = yardstick::recall_vec(x$mood, x$.pred_class),
    trs_fbeta = trs
  ) 
}  

plot_auc &lt;- function(x){
  
  p1 &lt;-  
    x %&gt;% 
    group_by(model) %&gt;%
    roc_curve(mood, .pred_Ruim) %&gt;%
    ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +
    geom_line(size = 1, alpha = 0.5, show.legend = F) +
    geom_abline(lty = 2, alpha = 0.5, color = &quot;gray50&quot;, size = 1.3)+
    labs(title = &quot;AUC&quot;)+
    scale_color_viridis_d(direction = 1)
  
  p2 &lt;- 
    x %&gt;%
    group_by(model) %&gt;%
    pr_curve(mood, .pred_Ruim) %&gt;%
    ggplot(aes(x = recall, y = precision, color = model)) +
    geom_line(size = 1.15, alpha = 0.5) +
    # geom_abline(slope = -1, intercept = 1, lty = 2, alpha = 0.5, color = &quot;gray50&quot;, size = 1.2)+
    labs(title = &quot;PR AUC&quot;)+
    theme(legend.position = &quot;right&quot;)+
    scale_color_viridis_d(direction = 1)
  
  (p1 | p2)
}</code></pre>
</details>
<p>¬†</p>
<p>Importar os dados obtidos no app <a href="https://www.sleepcycle.com/">SleepCycle</a> e padronizar nomes das colunas:</p>
<pre class="r"><code>sleep &lt;- read_csv2(&quot;sleepdata.csv&quot;) %&gt;% janitor::clean_names(case = &quot;snake&quot;)</code></pre>
<p>A seguir, uma tabela com uma descri√ß√£o do conte√∫do de cada coluna:</p>
<table>
<colgroup>
<col width="9%" />
<col width="11%" />
<col width="79%" />
</colgroup>
<thead>
<tr class="header">
<th>Coluna</th>
<th>Descri√ß√£o curta</th>
<th>Descri√ß√£o detalhada</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>start</strong></td>
<td>In√≠cio</td>
<td>In√≠cio do monitoramento</td>
</tr>
<tr class="even">
<td><strong>end</strong></td>
<td>Fim</td>
<td>Fim do monitoramento</td>
</tr>
<tr class="odd">
<td><strong>sleep_quality</strong></td>
<td>Qualidade do Sono</td>
<td>Qualidade do sono √© baseada em: tempo que passa a dormir, movimentos durante a noite e momentos em que est√° totalmente desperto</td>
</tr>
<tr class="even">
<td><strong>regularity</strong></td>
<td>Regularidade</td>
<td>Informa sobre a regularidade do hor√°rio de dormir e de acordar durante um per√≠odo de tempo. Quanto maior, mais regular tem sido o hor√°rio de acordar e dormir e isso pode resultar em um sono melhor</td>
</tr>
<tr class="odd">
<td><strong>mood</strong> üéØ</td>
<td>Humor</td>
<td>Humor informado no app ao acordar: üòÉ (Bom), üòë (Ok), üò° (Mau), ‚õî (N√£o informado)</td>
</tr>
<tr class="even">
<td><strong>heart_rate_bpm</strong></td>
<td>Frequ√™ncia card√≠aca (bpm)</td>
<td>-</td>
</tr>
<tr class="odd">
<td><strong>steps</strong></td>
<td>Passos</td>
<td>Quantos passos d√° por dia (bom a partir de 10.000 passos por dia)</td>
</tr>
<tr class="even">
<td><strong>alarm_mode</strong></td>
<td>Modo de alarme</td>
<td>Alarme ligado ou apenas monitoramento</td>
</tr>
<tr class="odd">
<td><strong>air_pressure_pa</strong></td>
<td>Press√£o do Ar (Pa)</td>
<td>-</td>
</tr>
<tr class="even">
<td><strong>city</strong></td>
<td>Cidade</td>
<td>-</td>
</tr>
<tr class="odd">
<td><strong>movements_per_hour</strong></td>
<td>Movimentos por hora</td>
<td>-</td>
</tr>
<tr class="even">
<td><strong>time_in_bed_seconds</strong></td>
<td>Tempo na cama (segundos)</td>
<td>-</td>
</tr>
<tr class="odd">
<td><strong>time_asleep_seconds</strong></td>
<td>Tempo adormecido (segundos)</td>
<td>-</td>
</tr>
<tr class="even">
<td><strong>time_before_sleep_seconds</strong></td>
<td>Tempo antes de dormir (segundos)</td>
<td>-</td>
</tr>
<tr class="odd">
<td><strong>window_start</strong></td>
<td>In√≠cio da janela</td>
<td>In√≠cio do modo soneca</td>
</tr>
<tr class="even">
<td><strong>window_stop</strong></td>
<td>Fim da janela</td>
<td>Fim do modo soneca</td>
</tr>
<tr class="odd">
<td><strong>did_snore</strong></td>
<td>Ronco</td>
<td>Detector de ru√≠dos (pode captar outros barulhos que n√£o seja ronco)</td>
</tr>
<tr class="even">
<td><strong>snore_time</strong></td>
<td>Hora do ronco</td>
<td>-</td>
</tr>
<tr class="odd">
<td><strong>weather_temperature_c</strong></td>
<td>Temperatura (¬∞C)</td>
<td>-</td>
</tr>
<tr class="even">
<td><strong>weather_type</strong></td>
<td>Tipo de clima</td>
<td>-</td>
</tr>
<tr class="odd">
<td><strong>notes</strong></td>
<td>Notas</td>
<td>Alguma nota ao acordar</td>
</tr>
</tbody>
</table>
<p>¬†</p>
<div id="limpeza-e-prepara√ß√£o-dos-dados" class="section level2">
<h2>Limpeza e prepara√ß√£o dos dados</h2>
<p>Vamos realizar uma limpeza inicial, preparando os dados para possibilitar as an√°lise em um objeto <code>tibble</code> minimamente arrumado:</p>
<pre class="r"><code>sleep &lt;- sleep %&gt;% 
  # fix target
  mutate(mood = case_when(mood == &quot;Bom&quot; ~ &quot;Bom&quot;,
                          mood == &quot;Mau&quot; ~ &quot;Ruim&quot;,
                          mood == &quot;Ok&quot; ~ &quot;Ruim&quot;,
                          is.na(mood) ~ NA_character_),
         mood = factor(mood, levels = c(&quot;Ruim&quot;, &quot;Bom&quot;), ordered = TRUE)) %&gt;% 
  # fix window
  mutate_at(c(&quot;window_start&quot;, &quot;window_stop&quot;), 
            ~ifelse(is.na(.x), end, .x)) %&gt;% 
  # fix string %
  mutate_at(c(&quot;sleep_quality&quot;, &quot;regularity&quot;),
            ~ .x %&gt;% str_remove(&quot;%&quot;) %&gt;% as.numeric() ) %&gt;% 
  # fix heart_rate_bpm e criar bug indicator 
  mutate(heart_rate_bug = ifelse(heart_rate_bpm == 0, &quot;sim&quot;, &quot;nao&quot;)) %&gt;% 
  mutate(heart_rate_bpm = ifelse(heart_rate_bpm == 0, 
                                 NA_integer_, heart_rate_bpm)) %&gt;% 
  # fix dados de soneca
  mutate(snore_time = as.numeric(snore_time),
         did_snore = ifelse(did_snore == TRUE, &quot;sim&quot;, &quot;nao&quot;)) %&gt;% 
  # fix para numerico
  mutate_at(c(&quot;time_before_sleep_seconds&quot;, 
              &quot;time_asleep_seconds&quot;, 
              &quot;time_in_bed_seconds&quot;),
            ~as.numeric(.x) ) %&gt;% 
  # fix movements_per_hour para double
  mutate(movements_per_hour = as.double(movements_per_hour)) %&gt;% 
  # fix weather_type
  mutate(weather_type = 
           factor(weather_type, 
                  levels = c(&quot;No weather&quot;, &quot;Rain&quot;, &quot;Rainy showers&quot;, &quot;Cloudy&quot;,
                             &quot;Partly cloudy&quot;, &quot;Fair&quot;, &quot;Sunny&quot;),
                  ordered = TRUE))  %&gt;% 
  mutate_at(c(&quot;weather_temperature_c&quot;, &quot;air_pressure_pa&quot;),
            ~ as.numeric(.x) %&gt;% if_else(. == 0, NA_real_, .)) %&gt;% 
  # remover unused columns
  select(-one_of(c(&quot;city&quot;, &quot;notes&quot;))) %&gt;% 
  select(mood, everything()) %&gt;% 
  arrange(end)</code></pre>
<p>Qual a estrutura geral dos dados? Ser√° que existe algum padr√£o nos dados ausentes?</p>
<pre class="r"><code>sleep %&gt;% 
  arrange(end) %&gt;% 
  mutate(Date = as.Date(end))%&gt;%
  # complete(Date = seq.Date(min(Date), max(Date), by=&quot;day&quot;))  %&gt;%  
  visdat::vis_dat() </code></pre>
<center>
<img src="/post/2021-02-28-qualidade-do-sono-machine-learning/unnamed-chunk-7-1.png" style="width:80.0%" />
</center>
<p>Os dados ausentes ocorrem tanto espalhados (<code>heart_rate_bpm</code>) quanto em sequ√™ncia (<code>air_pressure_pa</code>, <code>weather_temperature_c</code>, <code>mood</code>) portando, adotaremos as seguintes estrat√©gias para inputar dados ausentes:</p>
<ol style="list-style-type: decimal">
<li><code>air_pressure_pa</code>: Ser√° obtidos no site <a href="https://www.data.rio/datasets/dados-hor%C3%A1rios-do-monitoramento-da-qualidade-do-ar-monitorar?selectedAttribute=Pres">data.rio/datasets</a> e caso ainda exista dados ausentes, ser√° preenchido com as m√©dias m√≥veis dos √∫ltimos 7 dias;</li>
<li><code>weather_temperature_c</code>: Mesma estrat√©gia do item (1);</li>
<li><code>heart_rate_bpm</code>: C√°lculo das m√©dias m√≥veis dos √∫ltimos 7 dias;</li>
<li><code>mood</code>: Como √© a <em>target</em>, as inst√¢ncias aonde <code>is.na(mood)</code> ser√£o retidas para estima√ß√£o ap√≥s o ajuste do modelo.</li>
</ol>
</div>
<div id="imputar-dados-de-fontes-externas" class="section level2">
<h2>Imputar dados de fontes externas</h2>
<p>O preenchimento das features <code>air_pressure_pa</code>, <code>weather_temperature_c</code> ser√£o realizados a partir do download de dados p√∫blicos do Rio de Janeiro no link: <a href="https://www.data.rio/datasets/dados-hor%C3%A1rios-do-monitoramento-da-qualidade-do-ar-monitorar?selectedAttribute=Pres">data.rio/datasets</a>. Para obter este dado utilizaremos a fun√ß√£o <code>get_rj_data()</code> desenvolvida para este post, que est√° omitida mas para quem tiver interesse basta conferir clicando no item abaixo:</p>
<details>
<summary>
(<em>C√≥digo da fun√ß√£o <code>get_rj_data()</code></em>)
</summary>
<pre class="r"><code>get_rj_data &lt;- function(){ 
  
  if(!file.exists(&quot;rj_data.rds&quot;)){ 
    
    url &lt;- &quot;https://opendata.arcgis.com/datasets/5b1bf5c3e5114564bbf9b7a372b85e17_2.csv?outSR=%7B%22latestWkid%22%3A4326%2C%22wkid%22%3A4326%7D&quot;
    
    download.file(url, &quot;rj_data.csv&quot;)
    
    rj_data &lt;- readr::read_csv(&quot;rj_data.csv&quot;)
    
    saveRDS(rj_data, &quot;rj_data.rds&quot;)
    
  }else{
    rj_data &lt;- readRDS(&quot;rj_data.rds&quot;)
  }
  
  # preparar dados de pressao atmosferica e temperatura no periodo desejado
  rj_data &lt;- rj_data %&gt;% 
    mutate(Data = ymd_hms(Data)) %&gt;% 
    filter(Data &gt;= min(sleep$start) &amp;  Data &lt;= max(sleep$start)) %&gt;% 
    group_by(Data = as.Date(Data)) %&gt;% 
    summarise(air_pressure_pa = mean(Pres/10, rm.na=T),
              weather_temperature_c = mean(Temp, rm.na=T))
  
  return(rj_data)
  
}</code></pre>
</details>
<!-- &nbsp; -->
<p>Com acesso aos dados, hora de combinar as bases e preencher os dados faltantes:</p>
<pre class="r"><code>sleep &lt;- sleep %&gt;% 
  mutate(Data = as.Date(start)) %&gt;%
  # to numeric
  mutate_at(c(&quot;weather_temperature_c&quot;, &quot;air_pressure_pa&quot;),
            ~ as.numeric(.x) %&gt;% if_else(. == 0, NA_real_, .)) %&gt;% 
  # join Rio data
  left_join(get_rj_data() , by = c(&quot;Data&quot;)) %&gt;% 
  # fill with new data
  mutate(air_pressure_pa = ifelse(is.na(air_pressure_pa.x),
                                  air_pressure_pa.y,
                                  air_pressure_pa.x)) %&gt;%
  mutate(weather_temperature_c = ifelse(is.na(weather_temperature_c.x),
                                        weather_temperature_c.y, 
                                        weather_temperature_c.x)) %&gt;%
  # remove aux columns
  select(-air_pressure_pa.x, -air_pressure_pa.y,
         -weather_temperature_c.x, -weather_temperature_c.y,
         -Data)</code></pre>
</div>
<div id="insights" class="section level2">
<h2>Insights</h2>
<p>Nesta se√ß√£o vamos responder algumas perguntas com dados!</p>
<div id="start-e-end" class="section level3">
<h3><code>start</code> e <code>end</code></h3>
<p>Qual a m√©dia mensal de horas dormidas e que horas costumo acordar, em m√©dia, mensalmente ao longo desses anos?</p>
<details>
<summary>
(<em>C√≥digo para gr√°fco abaixo</em>)
</summary>
<pre class="r"><code>dy1 &lt;- sleep %&gt;% 
  complete(start = seq.Date(min(as.Date(start)), max(as.Date(start)), by=&quot;day&quot;)) %&gt;%
  mutate(dif_sleep_hours = as.numeric(end - start)/60) %&gt;% 
  mutate(dif_sleep_hours = zoo::rollmean(dif_sleep_hours, k =  30, fill = NA)) %&gt;%
  plot_dygraph(order.by = .$start, feature =  &#39;dif_sleep_hours&#39;)

dy2 &lt;- sleep %&gt;% 
  complete(start = seq.Date(min(as.Date(start)), max(as.Date(start)), by=&quot;day&quot;)) %&gt;%
  mutate(end_hour = hour(end)) %&gt;% 
  mutate(end_hour = zoo::rollmean(end_hour, k =  30, fill = NA)) %&gt;%
  plot_dygraph(order.by = .$start, feature =  &#39;end_hour&#39;)</code></pre>
</details>
<p>¬†¬†</p>
<!-- <div class="row"> -->
<!-- <div class="column"> -->
<!-- <center> -->
<!-- **Tempo dormindo (em horas)** -->
<!-- </br> -->
<!-- <small>M√©dia m√≥vel 30 dias</small> -->
<!-- ```{r, fig.height=2, fig.width=3,echo = F} -->
<!-- dy1 -->
<!-- ``` -->
<!-- </br> -->
<!-- <small>O tempo que passa dormindo parece variar (em m√©dia) em torno de 6 √† 7 horas</small> -->
<!-- </center> -->
<!-- </div> -->
<!-- <div class="column"> -->
<!-- <center>   -->
<!-- **Hora que acorda** -->
<!-- </br> -->
<!-- <small>M√©dia m√≥vel 30 dias</small> -->
<!-- ```{r, fig.height=2, fig.width=3,echo = F} -->
<!-- dy2 -->
<!-- ``` -->
<!-- </br> -->
<!-- <small>O pico no in√≠cio no gr√°fico corresponde ao pen√∫ltimo semestre da facultado. No final de 2017 comecei a trabalhare passei a acordar mais cedo  </small> -->
<!-- </center> -->
<!-- </div> -->
<!-- </div> -->
<p><img src="/post/2021-02-28-qualidade-do-sono-machine-learning/img1.png" style="width:80.0%" /></p>
<div class="w3-panel w3-sand w3-border">
<p>‚ö†Ô∏è Note que existem alguns espa√ßos vazios, que correspondem aos dias que o app n√£o foi utilizado.</p>
</div>
</div>
<div id="window_start-e-window_stop" class="section level3">
<h3><code>window_start</code> e <code>window_stop</code></h3>
<p>Quanto tempo costumo usar o modo ‚Äúsoneca‚Äù ao longo da semana? E aos finais de semana?</p>
<details>
<summary>
(<em>C√≥digo para gr√°fco abaixo</em>)
</summary>
<pre class="r"><code>p &lt;- sleep %&gt;% 
  mutate(mood = ifelse(is.na(mood), &quot;NA&quot;, as.character(mood)) %&gt;% 
           factor(levels = c(&quot;Ruim&quot;, &quot;NA&quot;, &quot;Bom&quot;))) %&gt;% 
  mutate(nap_minutes = (window_stop - window_start) / 30,
         final_de_semana = lubridate::wday(start) %in% c(1, 7)) %&gt;% 
  count(mood, final_de_semana, nap_minutes) %&gt;% 
  group_by(mood, final_de_semana) %&gt;% 
  mutate(
    fnap_minutes = case_when(
      nap_minutes == 0 ~ &quot;Sem modo soneca&quot;,
      nap_minutes == 20 ~ &quot;20 minutos&quot;,
      nap_minutes == 30 ~ &quot;30 minutos&quot;,
      nap_minutes == 60 ~ &quot;1 hora&quot;),
    fnap_minutes = reorder(fnap_minutes, nap_minutes),
    final_de_semana = ifelse(final_de_semana == T, &quot;Final de semena&quot;, &quot;Dia de semana&quot;),
    label = paste0( n, &quot; (&quot;, round(n/sum(n)*100, 2), &quot;%)&quot;)
  ) %&gt;% 
  ggplot(aes(x = fnap_minutes, y = n, label = label, fill = mood))+
  geom_bar(stat = &quot;identity&quot;, alpha = 0.8)+
  scale_fill_viridis_d(end = 0.7, direction = 1)+
  # ggrepel::geom_label_repel(aes(label = label))+
  labs(x = &quot;&quot;, y = &quot;&quot;)+
  facet_wrap(~final_de_semana)+
  theme(axis.text.x = element_text(angle = 30, hjust=1))</code></pre>
</details>
<p>¬†</p>
<pre class="r"><code>p %&gt;% plotly::ggplotly() %&gt;% plotly::config(displayModeBar = F)</code></pre>
<p><img src="/post/2021-02-28-qualidade-do-sono-machine-learning/img2.png" style="width:80.0%" /></p>
<p>Como era de se esperar, os dias em que <code>mood=="Ruim"</code> ocorrem mais quando o modo soneca n√£o √© ativado pois acaba mesmo sendo menos prop√≠cio a voltar a dormir. Outro detalhe √© que muitas vezes usei o soneca por um tempo muito prolongado! (üò± pelo menos <code>mood=="Bom"</code> na maioria desses casos!)</p>
<p>J√° nos finais de semana, ocorre pouqu√≠ssimo <code>mood== "Ruim"</code> e praticamente n√£o h√° uso do alarme e quando h√°, n√£o utiliza soneca.</p>
</div>
<div id="weather_type-e-alarm_mode" class="section level3">
<h3><code>weather_type</code> e <code>alarm_mode</code></h3>
<p>Ser√° que o humor ao acordar esta relacionado com o tipo de clima ou com o modo utilizado no alarme?</p>
<pre class="r"><code>p1 &lt;- plot_cat(sleep, cat_feature=&quot;weather_type&quot;, 
               title = &quot;Tipo de clima&quot;, label = F)+ 
  theme(axis.text.x = element_text(angle = 30, hjust=1))

p2 &lt;- plot_cat(sleep, cat_feature=&quot;alarm_mode&quot;, 
               title = &quot;Modo de alarme&quot;, label = F, legend = &quot;right&quot;)

p1 | p2</code></pre>
<center>
<img src="/post/2021-02-28-qualidade-do-sono-machine-learning/unnamed-chunk-15-1.png" style="width:80.0%" />
</center>
<p>Nota-se que n√£o existem evid√™ncias estatisticas para afimar que essas features (sozinhas) est√£o associadas √† target, por√©m como ser√£o utilizados modelos baseados em √°rvores que experimentam diversas combina√ß√µes de features, vamos manter na base e deixar o modelo decidir como usar.</p>
</div>
<div id="sleep_quality-e-time_in_bed_seconds" class="section level3">
<h3><code>sleep_quality</code> e <code>time_in_bed_seconds</code></h3>
<p>A qualidade de sono e o tempo da cama est√£o normalmente distribu√≠dos em torno de uma m√©dia?</p>
<pre class="r"><code>p1 &lt;- sleep %&gt;% plot_num(&quot;sleep_quality&quot;)
p2 &lt;- sleep %&gt;% plot_num(&quot;time_in_bed_seconds&quot;, legend = &quot;right&quot;)

p1 | p2</code></pre>
<center>
<img src="/post/2021-02-28-qualidade-do-sono-machine-learning/unnamed-chunk-16-1.png" style="width:80.0%" />
</center>
<p>Existem alguns registros em que o tempo na cama √© menor que 10.000 segundos (~3horas) o que corresponde aos pequenos cochilos que registrei no app. N√£o foram muitos registros mas talvez seja √∫til na modelagem pois existem ocorr√™ncias de humor (<code>mood</code>) <code>Bom</code> e <code>Ruim</code> ali.</p>
<p>Como a correla√ß√£o de spearman entre estas duas feautures √© muito alta (0.8705) √© poss√≠vel notar que baixa qualidade do sono esta altamente correlacionada com o tempo na cama.</p>
<p>Mais uma pergunta sobre estas features: Como a m√©dia mensal da qualidade do sono e do tempo na cama em horas est√£o distribu√≠dos ao longo do tempo?</p>
<details>
<summary>
(<em>C√≥digo para gr√°fco abaixo</em>)
</summary>
<pre class="r"><code>dy1 &lt;- sleep %&gt;% 
  complete(start = seq.Date(min(as.Date(start)),
                            max(as.Date(start)), by=&quot;day&quot;)) %&gt;%
  mutate(sleep_quality = zoo::rollmean(sleep_quality, k =  30, fill = NA)) %&gt;%
  plot_dygraph(order.by = .$start, feature =  &#39;sleep_quality&#39;)

dy2 &lt;- sleep %&gt;% 
  complete(start = seq.Date(min(as.Date(start)), 
                            max(as.Date(start)), by=&quot;day&quot;)) %&gt;%
  mutate(time_in_bed_seconds = 
           zoo::rollmean(time_in_bed_seconds, k =  30, fill = NA)) %&gt;%
  mutate(time_in_bed_seconds = time_in_bed_seconds / 60 / 60) %&gt;% 
  plot_dygraph(order.by = .$start, feature =  &#39;time_in_bed_seconds&#39;)</code></pre>
</details>
<p>¬†¬†</p>
<!-- <div class="row"> -->
<!-- <div class="column"> -->
<!-- <center> -->
<!-- **Qualidade do sono** -->
<!-- </br> -->
<!-- <small>M√©dia m√≥vel 30 dias</small> -->
<!-- ```{r, fig.height=2, fig.width=3, echo = F} -->
<!-- dy1 -->
<!-- ``` -->
<!-- </br> -->
<!-- <small>Parece que a qualidade do sono vem aumentando desde final de 2019, mantendo um patamar semlhante ao final e 2018.</small> -->
<!-- </center> -->
<!-- </div> -->
<!-- <div class="column"> -->
<!-- <center>   -->
<!-- **Tempo na cama em horas** -->
<!-- </br> -->
<!-- <small>M√©dia m√≥vel 30 dias</small> -->
<!-- ```{r, fig.height=2, fig.width=3, echo = F} -->
<!-- dy2 -->
<!-- ``` -->
<!-- </br> -->
<!-- <small>O tempo na cama varia entre 6 √† 7 horas (Apesar de alguns picos em 2020, provavelmente por conta da pandemia do corona virus quando estabeleceu-se o home office)</small> -->
<!-- </center> -->
<!-- </div> -->
<!-- </div> -->
<p><img src="/post/2021-02-28-qualidade-do-sono-machine-learning/img3.png" style="width:80.0%" /></p>
</div>
</div>
<div id="reter-dados" class="section level2">
<h2>Reter dados</h2>
<p>Antes de iniciar o processo de modelagem, ser√° necess√°rio reter dados aonde <code>mood</code> √© <code>NA</code>, pois faremos as previs√µes nestes dados apenas ap√≥s o ajuste e sele√ß√£o do modelo final.</p>
<pre class="r"><code>new_sleep &lt;- sleep %&gt;% filter(is.na(mood))
sleep &lt;- sleep %&gt;% filter(!is.na(mood))</code></pre>
</div>
</div>
<div id="modelagem" class="section level1">
<h1>Modelagem üöÄ</h1>
<p>Hora de criar alguns modelos para estimar a probabilidade das classes da target: <code>mood</code>.</p>
Como estamos diante de um cen√°rio onde os dados est√£o desbalanceados, ser√° necess√°rio tomar algumas decis√µes muito importantes (sim, cientistas de dados precisam tomar decis√µes o tempo inteiro).
<div class="row">
<div class="column8">
<div class="center">
<span>
<div>
<p>Neste caso, as quest√µes s√£o as seguintes:</p>
<ol style="list-style-type: decimal">
<li>Qual a classe mais importante?</li>
<li>Qual a m√©trica ser√° utilizada para selecionar os modelos?</li>
<li>Qual ser√° o <em>threshold</em>?</li>
<li>Qual ser√° estrat√©gia para lidar com o desbalanceamento?</li>
<li>Quais m√©tricas ser√£o monitoradas?</li>
</ol>
</div>
<p></span></p>
</div>
</div>
<div class="column4">
<p></br>
<img src="https://media.giphy.com/media/XeH1MFu4x3etVsllUN/giphy.gif" alt="Via Giphy" /></p>
</div>
</div>
<p>A classe mais importante para nossa previs√£o √© a positiva, ou seja, <code>mood=="Ruim"</code>. Sendo assim desejamos <strong>evitar falsos positivos</strong>.</p>
<p>A m√©trica utilizada para selecionar os modelos ser√° a <a href="https://www.kaggle.com/dansbecker/what-is-log-loss"><strong>logloss</strong></a>. Esta √© uma m√©trica probabilistica que foca na incerteza que o modelo tem nas previs√µes e penaliza as previs√µes que est√£o erradas<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>.</p>
<p>Ap√≥s calibrar a probabilidade, estabeleceremos um ponto de corte que <strong>maximizar a medida F-Beta</strong>, (que √© uma abstra√ß√£o da medida <em>F1</em>, m√©dia harm√¥nica entre <em>Precision</em> e <em>Recall</em>) onde <em>Beta = 0.5</em>. Essa medida tem o efeito de aumentar a import√¢ncia da <em>Precision</em> e diminui a import√¢ncia do <em>Recall</em>. <a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<p>Parra lidar com o desbalanceamento da <em>target</em>, utilizaremos o m√©todo de <em>undersampling</em> chamado ** <em>Tomek Links</em> **<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>. Este m√©todo faz uma amostragem da classe majorit√°ria de forma ‚Äúmais esperta‚Äù que uma simples amostragem aleat√≥ria.</p>
<p>Por fim, a principal m√©trica que ser√° monitorada ser√° a <strong><em>AUC-PR</em></strong><a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> (<em>Area Under Precision Recall Curve</em>). Ela √© uma esp√©cie de <em>AUC</em> que c√°lculada a √°rea sobre a <em>Precision</em> x <em>Recall</em>. Essa m√©trica √© prefer√≠vel neste caso pois foca maisn na classe positiva e a <em>ROC AUC</em> tente a superestimar os valores nesse caso.</p>
<div id="amostragem" class="section level2">
<h2>Amostragem</h2>
<p>Para preparar os dados para modelagem vamos dividir os dados em treino (70%) e teste (30%).</p>
<pre class="r"><code>set.seed(123456789)

# treino e teste
sleep_split &lt;- initial_split(data = sleep, strata = mood, prop = 0.7)
sleep_train &lt;- training(sleep_split)
sleep_test  &lt;- testing(sleep_split)</code></pre>
<p>Al√©m disso, vamos dividir o conjunto de treino em 4 folds para obter resultados de valida√ß√£o cruzada. Este valor corresponde metade da quantidade em que <code>mood=="Ruim"</code> nos dados teste.</p>
<pre class="r"><code>set.seed(123456789)
k_fold &lt;- sleep_test %&gt;% count(mood) %&gt;% filter(mood==&quot;Ruim&quot;) %&gt;% pull(n)

sleep_folds &lt;- sleep_train %&gt;% 
  rsample::vfold_cv(v = round(k_fold/2), repeats = 10, strata = mood)</code></pre>
<p>A decis√£o de utilizar o valor de <code>k</code> como metade do tamanho da classe minorit√°ria foi uma decis√£o pessoal, n√£o sei se √© √≥tima mas foi conveniente neste caso.</p>
<p>Como ficou dividido:</p>
<details>
<summary>
(<em>C√≥digo para tabela abaixo</em>)
</summary>
<pre class="r"><code>tab &lt;- 
  full_join(sleep_train %&gt;% count(mood) %&gt;% mutate(prop = n/sum(n)*100),
            sleep_test %&gt;% count(mood) %&gt;% mutate(prop = n/sum(n)*100),
            by = &quot;mood&quot;) %&gt;% 
  print_table(round = 2,
              columns = list(
                n.x = colDef(name = &quot;N&quot;),
                prop.x = colDef(name = &quot;(%)&quot;, align = &quot;left&quot;),
                n.y = colDef(name = &quot;N&quot;),
                prop.y = colDef(name = &quot;(%)&quot;, align = &quot;left&quot;)
              ), 
              columnGroups = list(
                colGroup(name = &quot;Train&quot;, columns = c(&quot;n.x&quot;, &quot;prop.x&quot;)),
                colGroup(name = &quot;Test&quot;, columns = c(&quot;n.y&quot;, &quot;prop.y&quot;))
              ))</code></pre>
</details>
<p>¬†</p>
<pre class="r"><code>tab</code></pre>
<div class="w3-panel w3-pale-red w3-border">
<p>‚ò†Ô∏è A quantidade reduzida de dados para teste reflete a baixa quantidade de dados no geral!</p>
</div>
</div>
<div id="engenharia-de-recursos" class="section level2">
<h2>Engenharia de recursos</h2>
<p>Hora de criar o objeto que vai conter todos os passos do pr√©-processamento necess√°rio! Esse passo √© muito importante pois algumas estat√≠sticas precisam ser calculadas nos dados de treino isoladamente para n√£o ‚Äúdar pistas‚Äù para modelo sobre as informa√ß√µes contidas nos dados de teste, comprometendo o desempenho do modelo em novos dados.</p>
<p>De forma semelhante (mas n√£o igual) ao <code>sklearn.pipeline.Pipeline</code>, dispon√≠vel para Python, na linguagem R existe o pacote <code>recipes</code> que permite a cria√ß√£o de ‚Äúreceitas‚Äù com a fun√ß√£o <code>recipe()</code> e que pode ser utilizada em um <code>workflow()</code> para treinar o modelo na sequ√™ncia.</p>
<p>Sendo assim, algumas das opera√ß√µes realizadas no pr√©processamento do modelo:</p>
<ul>
<li>Criar feature: <code>ano</code>;</li>
<li>Criar feature: <code>mes</code>;</li>
<li>Criar feature: <code>dia da semana</code>;</li>
<li>Criar feature: <code>dia do mes</code>;</li>
<li>Criar feature: <code>hora que acordou</code>;</li>
<li>Criar feature: <code>final de semana</code>;</li>
<li>Criar feature: <code>tempo dormindo</code>;;</li>
<li>Criar feature: <code>tempo de soneca</code></li>
<li>Criar feature: <code>quarentena</code>;</li>
<li>Inputar m√©dia movel semanal para preencher as features de <code>weather_temperature_c</code> e <code>air_pressure_pa</code> no RJ;</li>
<li>Transformar categ√≥ricas em dummy;</li>
<li>Remover colunas com dados inv√°lidos para modelo (timestamp);</li>
<li>Preencher os dados faltantes de <code>heart_rate_bpm</code> utilizando o algor√≠tmo <code>knn</code> com 2 vizinhos mais pr√≥ximos;</li>
<li>Aplicar o algoritmo <em>Tomek Links</em>, que √© um m√©todo de <em>undersampling</em>.</li>
</ul>
<p>Caso queira saber mais sobre m√©todos de <em>undersampling</em> para tratar dados desbalanceados sugiro a leitura <a href="https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/">deste excelente post</a>! (Os c√≥digos est√£o em Python por√©m a explica√ß√£o da teoria √© o que importa neste caso)</p>
<p>Preparar objeto <code>recipe</code> que cont√©m um conjunto de etapas para pr√©-processamento de dados:</p>
<pre class="r"><code>sleep_recipe &lt;- 
  recipe(mood~., data = sleep_train) %&gt;%
  step_ordinalscore(weather_type) %&gt;% 
  step_mutate(
    ano = factor(year(end)),
    mes = month(end),
    dia_semana = wday(end) %&gt;% ifelse(. == 7, 0, .),
    dia_mes = mday(end),
    end_hour = hour(end),
    final_de_semana = 
      ifelse(lubridate::wday(start) %in% c(1, 7),  &quot;sim&quot;, &quot;nao&quot;) %&gt;% as.factor(),
    dif_sleep_hours = as.numeric(end - start)/60,
    dif_nap = as.numeric(window_stop - window_start) / 60,
    quarentena = ifelse(start &gt; dmy(&quot;20/03/2020&quot;), &quot;sim&quot;, &quot;nao&quot;) %&gt;% as.factor(),
    nap_minutes = (window_stop - window_start) / 30
  ) %&gt;% 
  step_mutate_at(c(&quot;weather_temperature_c&quot;, &quot;air_pressure_pa&quot;),
                 fn = ~ imputeTS::na_ma(.x, k = 7, weighting = &quot;simple&quot;)) %&gt;% 
  step_dummy(all_nominal(), -all_outcomes())  %&gt;% 
  step_mutate_at(starts_with(&quot;ano&quot;), # Fix 2018 nos novos dados
                 fn = ~ ifelse(is.na(.x), 0, .x)) %&gt;% 
  step_rm(start, end, window_start, window_stop)%&gt;%
  step_knnimpute(heart_rate_bpm, neighbors = 2) %&gt;% 
  themis::step_tomek(mood) %&gt;%
  prep()

# bake(sleep_recipe, new_data = NULL)</code></pre>
<p>Finalmente! ü•µ</p>
<p>Com os dados devidamente preparados, vamos ligar as turbinas e partir para modelagem!</p>
<pre class="r"><code>doParallel::registerDoParallel(4)</code></pre>
</div>
<div id="modelo-nulo-baseline" class="section level2">
<h2>Modelo Nulo (Baseline)</h2>
<p>Este n√£o √© o tipo de modelo que serve para resolver problemas reais mas pode servir como um bom baseline (‚Äúpior que isso n√£o fica‚Äù) pois ele vai prever apenas a classe majorit√°ria, e com base nisso, poderemos comparar as m√©tricas de performance do ajuste para saber se nossos modelos est√£o (no m√≠nimo) performando melhor que um modelo que classifica unicamente 1 classe,</p>
<pre class="r"><code>null_model &lt;- null_model(mode = &quot;classification&quot;) %&gt;% 
  set_engine(&quot;parsnip&quot;)</code></pre>
<p>Definir o objeto <code>workflow</code>:</p>
<pre class="r"><code>null_wflow_bas &lt;- workflow() %&gt;% 
  add_recipe(sleep_recipe) %&gt;% 
  add_model(null_model) </code></pre>
<p>Realizar ajuste final nos dados de treino:</p>
<pre class="r"><code>null_final_fit_bas &lt;- null_wflow_bas %&gt;% last_fit(sleep_split) </code></pre>
<p>Coletar previs√µes nos dados de teste:</p>
<pre class="r"><code>null_test_preds_bas &lt;- collect_predictions(null_final_fit_bas)</code></pre>
<p>Avaliar desempenho do modelo nos dados de teste:</p>
<pre class="r"><code>null_test_preds_bas %&gt;% 
  mutate(mood = factor(mood, levels = c(&quot;Ruim&quot;, &quot;Bom&quot;), ordered = TRUE)) %&gt;%
  conf_mat_plot(null_model = T)</code></pre>
<center>
<img src="/post/2021-02-28-qualidade-do-sono-machine-learning/unnamed-chunk-31-1.png" style="width:80.0%" />
</center>
<p>Modelo nulo pronto! Vamos para a modelagem propriamente dita!</p>
</div>
<div id="√°rvore-de-decis√µes" class="section level2">
<h2>√Årvore de decis√µes</h2>
<p>Este algor√≠timo √© um √≥timo ponto de partida pois possui alta explicabilidade, gerando um plot intuitivo e muito f√°cil de interpretar. As <em>features</em> que aparecem no topo s√£o as mais importantes e cada n√≥ seguinte √© gerado a partir de regras que otimizam a divis√£o dos dados daquele ramo.</p>
<p>Existem recursos interessantes ao trabalhar com √°rvores, como determinar uma regra de parada ou ainda deixar a √°rvore crescer e depois realizar a poda. Primeiramente vamos ajusta uma √°rvore de decis√µes <em>default</em> e em seguida realizar algum tipo de tunning para tentar obter resultados melhores.</p>
<!-- `gini`: -->
<!-- Se selecionarmos dois itens de uma populacao aleatoriamente, entao eles devem ser da mesma classe e a probabilidade para isto √© 1 se a popula√ß√£o √© pura. -->
<div id="default" class="section level3">
<h3>Default</h3>
<p>Os par√¢metros <em>default</em> foram definidos baseados na documenta√ß√£o oficial do pacote <code>rpart</code> em <a href="https://cran.r-project.org/web/packages/rpart/rpart.pdf" class="uri">https://cran.r-project.org/web/packages/rpart/rpart.pdf</a> e o <em>de/para</em> para defini√ß√£o dos par√¢metros na p√°gina do pacote <code>parsnip</code> em <a href="https://parsnip.tidymodels.org/reference/decision_tree.html" class="uri">https://parsnip.tidymodels.org/reference/decision_tree.html</a></p>
<pre class="r"><code>tree_model_bas &lt;- decision_tree(
  cost_complexity = 0.01, # cp
  tree_depth = 30,        # maxdepth
  min_n = 20              # minsplit
) %&gt;% 
  set_engine(&quot;rpart&quot;) %&gt;%
  set_mode(&quot;classification&quot;)</code></pre>
<p>Definir o objeto <code>workflow</code>:</p>
<pre class="r"><code>tree_wflow_bas &lt;- workflow() %&gt;% 
  add_recipe(sleep_recipe) %&gt;% 
  add_model(tree_model_bas) </code></pre>
<p>Ajustar modelo via valida√ß√£o cruzada:</p>
<pre class="r"><code>tree_res_bas &lt;- fit_resamples(
  tree_wflow_bas,
  sleep_folds,
  metrics = metric_set(pr_auc, roc_auc, mn_log_loss),
  control = control_resamples(save_pred = TRUE)
)
# Salvar &quot;cache&quot; da otimizacao 
saveRDS(tree_res_bas, &quot;tree_res_bas.rds&quot;)</code></pre>
<p>Finalizar o modelo:</p>
<pre class="r"><code># Finalizar workflow com parametros selecionados (default nesse caso)
tree_final_wflow_bas &lt;- 
  finalize_workflow(
    tree_wflow_bas,
    select_best(tree_res_bas, metric = &#39;mn_log_loss&#39;) 
  )

# Realizar ajuste final nos dados de treino
tree_final_fit_bas &lt;- tree_final_wflow_bas %&gt;% last_fit(sleep_split) 

# Coletar previs√µes nos dados de teste
tree_test_preds_bas &lt;- collect_predictions(tree_final_fit_bas)</code></pre>
<p>Vejamos como ficou o modelo baseline:</p>
<details>
<summary>
(<em>C√≥digo do objeto <code>tre_model_bas</code></em>)
</summary>
<pre class="r"><code>tre_model_bas &lt;- 
  tree_final_fit_bas$.workflow[[1]] %&gt;% 
  pull_workflow_fit()</code></pre>
</details>
<p>¬†</p>
<pre class="r"><code>rattle::fancyRpartPlot(tre_model_bas$fit, sub = NULL, cex = 0.6)</code></pre>
<center>
<img src="/post/2021-02-28-qualidade-do-sono-machine-learning/unnamed-chunk-38-1.png" style="width:80.0%" />
</center>
<p>Note que o modelo <em>default</em> se baseia nas features <code>time_before_sleep_seconds</code> e <code>steps</code>. Talvez, com outra combina√ß√£o de par√¢metros seja poss√≠vel conseguir um modelo uma √°rvore um pouco maior com resultado igual/melhor.</p>
<p>Como s√£o apenas duas features, √© poss√≠vel visualizar os regras de classifica√ß√£o a partir de um gr√°fio de dispers√£o</p>
<pre class="r"><code>sleep_train %&gt;%
  ggplot(aes(time_before_sleep_seconds, steps)) +
  parttree::geom_parttree(data = tre_model_bas$fit, alpha = 0.3) +
  geom_jitter(aes(color = mood), alpha = 0.7) +
  scale_color_viridis_d(end = 0.8, direction = 1)</code></pre>
<center>
<img src="/post/2021-02-28-qualidade-do-sono-machine-learning/unnamed-chunk-39-1.png" style="width:80.0%" />
</center>
<p>Vamos avaliar desempenho do modelo nos dados de teste:</p>
<pre class="r"><code>tree_test_preds_bas %&gt;% 
  mutate(mood = factor(mood, levels = c(&quot;Ruim&quot;, &quot;Bom&quot;), ordered = TRUE)) %&gt;% 
  conf_mat_plot()</code></pre>
<center>
<img src="/post/2021-02-28-qualidade-do-sono-machine-learning/unnamed-chunk-40-1.png" style="width:80.0%" />
</center>
<p>O modelo n√£o esta muito bom‚Ä¶ mas tamb√©m n√£o esta muito ruim para come√ßar! üòÖ</p>
<p>Coram 5/8 acertos para classe de interesse, vamos tentar fazer o tunning deste modelo!</p>
</div>
<div id="tunning" class="section level3">
<h3>Tunning</h3>
<p>Definir o modelo que ser√° utilizado:</p>
<pre class="r"><code>tree_model_tun &lt;- decision_tree(
  min_n = tune(),
  cost_complexity = tune(), 
  tree_depth = tune()
) %&gt;%
  set_engine(&quot;rpart&quot;) %&gt;%
  set_mode(&quot;classification&quot;)
# tree_model_tun %&gt;% translate()</code></pre>
<p>Definir o objeto <code>workflow</code>:</p>
<pre class="r"><code>tree_wflow_tun &lt;- workflow() %&gt;% 
  add_recipe(sleep_recipe) %&gt;% 
  add_model(tree_model_tun) </code></pre>
<p>O grid utilizado foi alterado para tentar previnir que a √°rvore tenha apenas o n√≥ raiz pois o grid default, combinado com o <em>threshold</em>, estava gerando um ‚Äúcotoco‚Äù.</p>
<ul>
<li><code>min_n</code>: [1, 5]</li>
<li><code>cost_complexity</code>: (transformed scale): [-10, -1]</li>
<li><code>tree_depth</code>: [10, 20]</li>
</ul>
<p>Definir um grid aleat√≥rio para otimiza√ß√£o dos hiperpar√¢metros:</p>
<pre class="r"><code>tree_params &lt;- 
  tree_model_tun %&gt;% 
  parameters() %&gt;%
  update(
    min_n = min_n(c(1, 5)), 
    cost_complexity = cost_complexity(),
    tree_depth = tree_depth(c(10, 20)) 
  )

tree_grid &lt;-grid_regular(tree_params, levels = 3)</code></pre>
<p>Ajustar modelo:</p>
<pre class="r"><code>tree_res_tun &lt;- 
  tree_wflow_tun %&gt;% 
  tune_grid(
    resamples = sleep_folds,
    grid = tree_grid,
    metrics = metric_set(roc_auc, mn_log_loss, pr_auc),
    control = control_grid(save_pred = TRUE)
  )

# Salvar cache da otimizacao 
# saveRDS(tree_res_tun, &quot;tree_res_tun.rds&quot;)</code></pre>
<p>Impacto de cada hiperpar√¢metro no resultado das m√©tricas c√°lculadas para cada modelo:</p>
<details>
<summary>
(<em>C√≥digo do gr√°fico</em>)
</summary>
<pre class="r"><code>id_best_model &lt;- 
  show_best(tree_res_tun, metric = &#39;mn_log_loss&#39;) %&gt;%
  slice(1) %&gt;% 
  pull(.config)

plot_tree_tun &lt;- 
  tree_res_tun %&gt;% 
  collect_metrics() %&gt;% 
  mutate(best_model = if_else(.config == id_best_model, 
                              &quot;BestModel&quot;, &quot;Try&quot;)
         # cost_complexity = log(cost_complexity)-10
  ) %&gt;% 
  select(.metric, mean, best_model,
         cost_complexity:min_n) %&gt;%
  pivot_longer(cost_complexity:min_n,
               values_to = &quot;value&quot;,
               names_to = &quot;parameter&quot;
  ) %&gt;% 
  mutate(parameter = case_when(
    parameter == &quot;cost_complexity&quot; ~ &quot;Cost-Complexity Parameter&quot;,
    parameter == &quot;tree_depth&quot; ~ &quot;Tree Depth&quot;,
    parameter == &quot;min_n&quot; ~ &quot;Minimal Node Size&quot;,
    
  ))%&gt;% {
    ggplot(., aes(value, mean, color = best_model)) +
      geom_point(alpha = 0.6, show.legend = FALSE) +
      geom_point(data = subset(., best_model == &#39;BestModel&#39;), 
                 size = 4, shape = 3)+
      scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;))+
      facet_grid(.metric~parameter, scales = &quot;free&quot;) +
      labs(x = NULL, y = NULL)
  }
# Codigo para mesmo grafico sem cor para melhor modelo:
# autoplot(tree_res_tun)</code></pre>
</details>
<p>¬†</p>
<pre class="r"><code>plot_tree_tun %&gt;% 
  plotly::ggplotly()%&gt;% 
  plotly::layout(showlegend = FALSE) %&gt;% 
  plotly::config(displayModeBar = F)</code></pre>
<center>
<img src="/post/2021-02-28-qualidade-do-sono-machine-learning/img4.png" style="width:80.0%" />
</center>
<p>5 Melhores resultados:</p>
<pre class="r"><code>show_best(tree_res_tun, metric = &#39;mn_log_loss&#39;) %&gt;% 
  select(-.estimator, -n, -.config)</code></pre>
<pre><code>## # A tibble: 5 x 6
##   cost_complexity tree_depth min_n .metric      mean std_err
##             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;
## 1             0.1         10     1 mn_log_loss  1.64   0.212
## 2             0.1         15     1 mn_log_loss  1.64   0.212
## 3             0.1         20     1 mn_log_loss  1.64   0.212
## 4             0.1         10     3 mn_log_loss  1.64   0.212
## 5             0.1         15     3 mn_log_loss  1.64   0.212</code></pre>
<p>Finalizar o modelo com o conjunto de par√¢metros encontrados no processo de otimiza√ß√£o:</p>
<pre class="r"><code># finalizar workflow definindo modelo final
tree_final_wflow_tun &lt;- 
  finalize_workflow(
    tree_wflow_tun,
    select_best(tree_res_tun, metric = &#39;mn_log_loss&#39;) )

# Realizar ajuste final nos dados de treino
tree_final_fit_tun &lt;- tree_final_wflow_tun %&gt;% last_fit(sleep_split) 

# Coletar previs√µes nos dados de teste
tree_test_preds_tun &lt;- collect_predictions(tree_final_fit_tun)</code></pre>
<p>Vejamos como ficou foi o ajuste do modelo utilizando a configura√ß√£o obtida no ap√≥s o <em>tunning</em> final:</p>
<details>
<summary>
(<em>C√≥digo do gr√°fico</em>)
</summary>
<pre class="r"><code>tre_model_tun &lt;- pull_workflow_fit(tree_final_fit_tun$.workflow[[1]])</code></pre>
</details>
<p>¬†</p>
<pre class="r"><code>rattle::fancyRpartPlot(tre_model_tun$fit, sub = NULL, cex = 0.6)</code></pre>
<center>
<img src="/post/2021-02-28-qualidade-do-sono-machine-learning/unnamed-chunk-51-1.png" style="width:80.0%" />
</center>
<p>Avaliar desempenho do modelo nos dados de teste:</p>
<pre class="r"><code>tree_test_preds_tun %&gt;% 
  mutate(mood = factor(mood, levels = c(&quot;Ruim&quot;, &quot;Bom&quot;), ordered = TRUE)) %&gt;%
  conf_mat_plot()</code></pre>
<center>
<img src="/post/2021-02-28-qualidade-do-sono-machine-learning/unnamed-chunk-52-1.png" style="width:80.0%" />
</center>
<p>Ao comparar o modelo default com o modelo ap√≥s o <em>tunning</em> √© poss√≠vel notar que o n√∫mero de verdadeiros positivos foi menor por√©m o n√∫mero de fasos positivos tbm foi menor devido ao elevado <code>trs_fbeta</code> encontrado (maximizando F0.5).</p>
<p>No geral, o modelo tunado ficou pior que o modelo default mas como o modelo de √°rvore de deci√µes costuma ser bem inst√°vel, ainda mais em um cen√°rio de dados desbalanceados vamos apenas guardar estes resultados e dar mais um passo, combinando diversas √°rvore de decis√µes!</p>
</div>
</div>
<div id="random-forest" class="section level2">
<h2>Random Forest</h2>
<p>O <em>Random Forest</em> √© um algoritmo que (de forma simplificada) realiza bootstrap em cima de √°rvores de decis√µes (modelos que utilizamos anteriormente) construindo modelos de √°rvores de decis√µes em diferentes amostras com diferentes combina√ß√µes de <em>features</em> e assim uma previs√£o final √© feita ap√≥s uma ‚Äúvota√ß√£o entre os modelos‚Äù.</p>
<div id="default-1" class="section level3">
<h3>Default</h3>
<p>Os par√¢metros <em>default</em> foram definidos baseados na documenta√ß√£o oficial do pacote <code>ranger</code> em <a href="https://cran.r-project.org/web/packages/ranger/ranger.pdf" class="uri">https://cran.r-project.org/web/packages/ranger/ranger.pdf</a> e o <em>de/para</em> para defini√ß√£o dos par√¢metros na p√°gina do pacote <code>parsnip</code> em <a href="https://parsnip.tidymodels.org/reference/rand_forest.html" class="uri">https://parsnip.tidymodels.org/reference/rand_forest.html</a></p>
<pre class="r"><code># raiz quadrada do numero de features 
n_col = ncol(juice(sleep_recipe))

rf_model_bas &lt;- rand_forest(
  mtry = sqrt(n_col) %&gt;% floor(), # mtry
  trees = 500,                    # num.trees
  min_n = 1                       # min.node.size 
) %&gt;% 
  set_engine(&quot;ranger&quot;, num.threads = 4, importance = &quot;permutation&quot;) %&gt;% 
  set_mode(&quot;classification&quot;)</code></pre>
<p>Definir o objeto <code>workflow</code>:</p>
<pre class="r"><code>rf_wflow_bas &lt;- workflow() %&gt;% 
  add_recipe(sleep_recipe) %&gt;% 
  add_model(rf_model_bas) </code></pre>
<p>Ajustar modelo via valida√ß√£o cruzada:</p>
<pre class="r"><code>rf_res_bas &lt;- fit_resamples(
  rf_wflow_bas,
  sleep_folds,
  metrics = metric_set(roc_auc, mn_log_loss, pr_auc),
  control = control_resamples(save_pred = TRUE)
)</code></pre>
<p>Finalizar o modelo com o conjunto de par√¢metros encontrados no processo de otimiza√ß√£o:</p>
<pre class="r"><code># Finalizar workflow com parametros selecionados (default nesse caso)
rf_final_wflow_bas &lt;- 
  finalize_workflow(
    rf_wflow_bas,
    select_best(rf_res_bas, metric = &#39;mn_log_loss&#39;) )

# Realizar ajuste final nos dados de treino
rf_final_fit_bas &lt;- rf_final_wflow_bas %&gt;% last_fit(sleep_split) 

# Coletar previs√µes nos dados de teste
rf_test_preds_bas &lt;- collect_predictions(rf_final_fit_bas)</code></pre>
<p>Avaliar desempenho do modelo nos dados de teste:</p>
<pre class="r"><code>rf_test_preds_bas %&gt;% 
  mutate(mood = factor(mood, levels = c(&quot;Ruim&quot;, &quot;Bom&quot;), ordered = TRUE)) %&gt;%
  conf_mat_plot()</code></pre>
<center>
<img src="/post/2021-02-28-qualidade-do-sono-machine-learning/unnamed-chunk-58-1png" style="width:80.0%" />
</center>
<p>Este modelo n√£o fez nenhuma previs√£o de falso positivo! Por√©m note que o <code>trs_fbeta</code> ficou bastante alto, o que deve ter ocorrido como reflexo do elevado <code>logloss</code> que indicaria que a incerteza que o modelo tem nas previs√µes esta alta.</p>
</div>
<div id="tunning-1" class="section level3">
<h3>Tunning</h3>
<p>Definir o modelo que ser√° utilizado:</p>
<pre class="r"><code>rf_model_tun &lt;- rand_forest(
  mtry = tune(),
  trees = tune(), 
  min_n = tune()
) %&gt;% 
  set_engine(&quot;ranger&quot;, num.threads = 4, importance = &quot;permutation&quot;) %&gt;% 
  set_mode(&quot;classification&quot;)
# tree_model %&gt;% translate()</code></pre>
<p>Definir o objeto <code>workflow</code>:</p>
<pre class="r"><code>rf_wflow_tun &lt;- workflow() %&gt;% 
  add_recipe(sleep_recipe) %&gt;% 
  add_model(rf_model_tun) </code></pre>
<p>O grid utilizado tentar√° valores superiores e inferiores ao n√∫mero de √°rvores <em>default</em> do algoritmo e vamos incluir o valor 1 ao <code>min_n</code> pois √°rvores mais longas neste m√©todo podem ser √∫teis. O <code>mtry</code> ser√° calculado baseado nas informa√ß√µes do dataset de treino.</p>
<ul>
<li><code>trees</code>: [100, 900]</li>
<li><code>min_n</code>: [1, 40]</li>
<li><code>mtry</code>: [1, 20]</li>
</ul>
<p>Definir t√©cnica de otimiza√ß√£o de hiperpar√¢metros</p>
<pre class="r"><code>rf_grid &lt;-grid_max_entropy(
  trees() %&gt;% range_set(c(100, 900)), # Default Range: [1, 2000]
  min_n() %&gt;% range_set(c(1, 40)),    # Default Range: [2, 40]
  finalize(mtry(), sleep_train),
  size = 30)</code></pre>
<p>Ajustar modelo:</p>
<pre class="r"><code>rf_res_tun &lt;- 
  rf_wflow_tun %&gt;% 
  tune_grid(
    resamples = sleep_folds,
    grid = rf_grid,
    metrics = metric_set(roc_auc, mn_log_loss, pr_auc),
    control = control_grid(save_pred = TRUE)
  )

# Salvar cache da otimizacao 
saveRDS(rf_res_tun, &quot;rf_res_tun.rds&quot;)</code></pre>
<p>Impacto de cada hiperpar√¢metro no resultado das m√©tricas c√°lculadas para cada modelo:</p>
<details>
<summary>
(<em>C√≥digo do gr√°fico</em>)
</summary>
<pre class="r"><code>id_best_model &lt;- 
  show_best(rf_res_tun, metric = &#39;mn_log_loss&#39;) %&gt;%
  slice(1) %&gt;% 
  pull(.config)

plot_rf_tun &lt;- 
  rf_res_tun %&gt;% 
  collect_metrics() %&gt;% 
  mutate(best_model = if_else(.config == id_best_model, 
                              &quot;BestModel&quot;, &quot;Try&quot;)) %&gt;% 
  select(.metric, mean, best_model,
         mtry:min_n) %&gt;%
  pivot_longer(mtry:min_n,
               values_to = &quot;value&quot;,
               names_to = &quot;parameter&quot;
  ) %&gt;% 
  mutate(parameter = case_when(
    parameter == &quot;mtry&quot; ~ &quot;Randomly Selected Predictors&quot;,
    parameter == &quot;min_n&quot; ~ &quot;Minimal Node Size&quot;,
    parameter == &quot;trees&quot; ~ &quot;# Trees&quot;
  )) %&gt;% {
    ggplot(., aes(value, mean, color = best_model)) +
      geom_point(alpha = 0.6, show.legend = FALSE) +
      geom_point(data = subset(., best_model == &#39;BestModel&#39;), 
                 size = 4, shape = 3)+
      scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;))+
      facet_grid(.metric~parameter, scales = &quot;free&quot;) +
      labs(x = NULL, y = NULL)
  }
# Codigo para mesmo grafico sem cor para melhor modelo:
# autoplot(rf_res_tun)</code></pre>
</details>
<p>¬†</p>
<pre class="r"><code>plot_rf_tun %&gt;% 
  plotly::ggplotly()%&gt;% 
  plotly::layout(showlegend = FALSE) %&gt;% 
  plotly::config(displayModeBar = F)</code></pre>
<center>
<img src="/post/2021-02-28-qualidade-do-sono-machine-learning/img5.png" style="width:80.0%" />
</center>
<p>Melhores resultados:</p>
<pre class="r"><code>show_best(rf_res_tun, metric = &#39;mn_log_loss&#39;) %&gt;% 
  select(-.estimator, -n, -.config)</code></pre>
<p>Finalizar o modelo com o conjunto de par√¢metros encontrados no processo de otimiza√ß√£o:</p>
<pre class="r"><code># finalizar workflow definindo modelo final
rf_final_wflow_tun &lt;- 
  finalize_workflow(
    rf_wflow_tun,
    select_best(rf_res_tun, metric = &#39;mn_log_loss&#39;) )

# Realizar ajuste final nos dados de treino
rf_final_fit_tun &lt;- rf_final_wflow_tun %&gt;% last_fit(sleep_split) 

# Coletar previs√µes nos dados de teste
rf_test_preds_tun &lt;- collect_predictions(rf_final_fit_tun)</code></pre>
<p>Vejamos como ficou foi o ajuste do modelo utilizando a configura√ß√£o obtida no ap√≥s o <em>tunning</em> final:</p>
<p>Avaliar desempenho do modelo nos dados de teste:</p>
<pre class="r"><code>rf_test_preds_tun %&gt;% 
  mutate(mood = factor(mood, levels = c(&quot;Ruim&quot;, &quot;Bom&quot;), ordered = TRUE)) %&gt;%
  conf_mat_plot()</code></pre>
<center>
<img src="/post/2021-02-28-qualidade-do-sono-machine-learning/unnamed-chunk-68-1.png" style="width:80.0%" />
</center>
<p>Note que apesar do maior n√∫mero de Verdadeiros Positivos, este modelo apresentou um Falso Positivo. Parece estranho pois √© exatamente o que queriamos evitar por√©m √© poss√≠vel notar que o <code>logloss</code> foi bem inferior e o <code>trs_fbeta</code> est√° bem mais razoavel agora.</p>
<p>Importancia de cada <em>feature</em> conforme o modelo:</p>
<pre class="r"><code>vip::vip(pull_workflow_fit(rf_final_fit_tun$.workflow[[1]]))</code></pre>
<center>
<img src="/post/2021-02-28-qualidade-do-sono-machine-learning/unnamed-chunk-70-1.png" style="width:80.0%" />
</center>
<p>Diferente do modelo baseado em 1 unica √°rvore de decis√µes, a <em>feature</em> <code>steps</code> n√£o foi t√£o importante assim. A <code>time_asleep_seconds</code> foi a mais importante mas com a ordem de grandeza muito pr√≥xima de <code>time_before_sleep_seconds</code>.</p>
<p><em>Random Forest</em> √© um excelente modelo e poder√≠amos investir mais tempo tentando otimizando sua performance mas para este post acho que j√° esta suficiente. Vamos para o pr√≥ximo modelo! üòç</p>
</div>
</div>
<div id="lightgbm" class="section level2">
<h2>LightGBM</h2>
<p>Este modelo consiste em um m√©todo de <em>boosting</em>. Tamb√©m √© baseado nos modelos de √°rvore de decis√µes, mas, diferentemente do <em>Random Forest</em>, suas √°rvores s√£o calculadas em sequ√™ncia, ‚Äúaprendendo‚Äù com o erro das √°rvores anteriores.</p>
<p>A mec√¢nica do <em>LightGBM</em> √© um pouco diferente do <em>XGBoost.</em> N√£o entrarei em detalhes sobre a teoria neste post at√© porque a documenta√ß√£o oficial no github em <a href="https://github.com/microsoft/LightGBM" class="uri">https://github.com/microsoft/LightGBM</a> √© bastante rica, e seus recursos s√£o muito bem apresentados neste link: <a href="https://github.com/microsoft/LightGBM/blob/master/docs/Features.rst" class="uri">https://github.com/microsoft/LightGBM/blob/master/docs/Features.rst</a></p>
<p>Links √∫teis para consulta ao trabalhar com este algoritmo:</p>
<ul>
<li>Documenta√ß√£o oficial: <a href="https://lightgbm.readthedocs.io/en/latest/" class="uri">https://lightgbm.readthedocs.io/en/latest/</a></li>
<li>Excelente post: <a href="https://machinelearningmastery.com/light-gradient-boosted-machine-lightgbm-ensemble/" class="uri">https://machinelearningmastery.com/light-gradient-boosted-machine-lightgbm-ensemble/</a></li>
<li>Documenta√ß√£o oficial do pacote <code>treesnip</code>: <a href="https://curso-r.github.io/treesnip/articles/working-with-lightgbm-catboost.html" class="uri">https://curso-r.github.io/treesnip/articles/working-with-lightgbm-catboost.html</a></li>
<li>Reposit√≥rio no github do pacote <code>treesnip</code>: <a href="https://github.com/curso-r/treesnip" class="uri">https://github.com/curso-r/treesnip</a></li>
<li>√ìtimo link para consulta dos par√¢metros: <a href="https://sites.google.com/view/lauraepp/parameters" class="uri">https://sites.google.com/view/lauraepp/parameters</a></li>
</ul>
<div id="default-2" class="section level3">
<h3>Default</h3>
<p>Os par√¢metros <em>default</em> foram definidos baseados na documenta√ß√£o oficial do pacote <code>lightgbm</code> em <a href="https://lightgbm.readthedocs.io/en/latest/" class="uri">https://lightgbm.readthedocs.io/en/latest/</a> e o <em>de/para</em> para defini√ß√£o dos par√¢metros na p√°gina do (incr√≠vel ü§©) pacote <code>treesnip</code> em <a href="https://github.com/curso-r/treesnip/blob/master/R/lightgbm.R" class="uri">https://github.com/curso-r/treesnip/blob/master/R/lightgbm.R</a></p>
<pre class="r"><code>lgbm_model_bas &lt;- parsnip::boost_tree(
  mode = &quot;classification&quot;,
  trees = 100,       # num_iterations
  learn_rate = 0.1,  # fixo
  min_n = 20,        # min_data_in_leaf
  tree_depth = 6,    # max_depth
  sample_size = 1,   # bagging_fraction
  mtry = 1,          # feature_fraction
  loss_reduction = 0 # min_gain_to_split
) %&gt;%  
  set_engine(&quot;lightgbm&quot;,
             nthread = 4,
             importance = &quot;permutation&quot;) %&gt;% 
  set_mode(&quot;classification&quot;)</code></pre>
<p>Definir o objeto <code>workflow</code>:</p>
<pre class="r"><code>lgbm_wflow_bas &lt;- workflow() %&gt;% 
  add_recipe(sleep_recipe) %&gt;% 
  add_model(lgbm_model_bas) </code></pre>
<p>Ajustar modelo via valida√ß√£o cruzada:</p>
<pre class="r"><code>lgbm_res_bas &lt;- fit_resamples(
  lgbm_wflow_bas,
  sleep_folds,
  metrics = metric_set(roc_auc, mn_log_loss, pr_auc),
  control = control_resamples(save_pred = TRUE)
)
saveRDS(lgbm_res_bas, &quot;lgbm_res_bas.rds&quot;)</code></pre>
<p>Finalizar o modelo com o conjunto de par√¢metros encontrados no processo de otimiza√ß√£o:</p>
<pre class="r"><code># Finalizar workflow com parametros selecionados (default nesse caso)
lgbm_final_wflow_bas &lt;- 
  finalize_workflow(
    lgbm_wflow_bas,
    select_best(lgbm_res_bas, metric = &#39;mn_log_loss&#39;) )

# Realizar ajuste final nos dados de treino
lgbm_final_fit_bas &lt;- lgbm_final_wflow_bas %&gt;% last_fit(sleep_split) 

# Coletar previs√µes nos dados de teste
lgbm_test_preds_bas &lt;- collect_predictions(lgbm_final_fit_bas)</code></pre>
<p>Avaliar desempenho do modelo nos dados de teste:</p>
<pre class="r"><code>lgbm_test_preds_bas %&gt;% 
  mutate(mood = factor(mood, levels = c(&quot;Ruim&quot;, &quot;Bom&quot;), ordered = TRUE)) %&gt;%
  conf_mat_plot()</code></pre>
<center>
<img src="/post/2021-02-28-qualidade-do-sono-machine-learning/unnamed-chunk-76-1.png" style="width:80.0%" />
</center>
</div>
<div id="tunning-2" class="section level3">
<h3>Tunning</h3>
<p>Para o tunning vamos utilizar uma estrat√©gia um pouco diferente. Vamos fixar o n√∫mero de √°rvores <code>trees</code> e a taxa de aprendizado <code>learning_rate</code> pois vamos separar mais uma pequena parte dos dados para usar o recurso <code>early_stopping</code>. Esta op√ß√£o basicamente ‚Äútrava‚Äù o crescimento da √°rvore caso o modelo n√£o melhore a performance a partir da n-√©sima itera√ß√£o.</p>
<pre class="r"><code>lgbm_model_tun &lt;- parsnip::boost_tree(
  mode = &quot;classification&quot;,
  trees = 700,             # autotune com early stopping
  learn_rate = 0.01,       # early stopping
  min_n = tune(),          # min_data_in_leaf
  tree_depth = tune(),     # max_depth
  sample_size = 1,         # bagging_fraction, n funciona com goss
  mtry = tune(),           # feature_fraction
  loss_reduction = tune()  # min_gain_to_split
) %&gt;%  
  set_engine(&quot;lightgbm&quot;, nthread = 4, 
             # parametros para early stopping
             early_stop = 30,
             validation = .20,
             eval_metric = &quot;mn_log_loss&quot;,
             importance = &quot;permutation&quot;
             # feature_fraction = tune(&quot;feature_fraction&quot;)
  ) %&gt;% 
  set_mode(&quot;classification&quot;)
# tree_model %&gt;% translate()</code></pre>
<p>Definir o objeto <code>workflow</code>:</p>
<pre class="r"><code>lgbm_wflow_tun &lt;- workflow() %&gt;% 
  add_recipe(sleep_recipe) %&gt;% 
  add_model(lgbm_model_tun) </code></pre>
<p>Definir grid para otimiza√ß√£o de hiperpar√¢metros baseados nas sugest√µes de <a href="https://github.com/Laurae2">github/Laurae2</a> em uma <a href="https://github.com/microsoft/LightGBM/issues/695">issue</a> no reposit√≥rio <a href="https://github.com/microsoft/LightGBM/issues/695">oficial</a> do modelo</p>
<pre class="r"><code>lightgbm_params &lt;- 
  dials::parameters(
    # learn_rate(),           # learning_rate
    # trees()                 # num_iterations
    min_n(),                  # min_data_in_leaf
    tree_depth(c(2, 63)),     # max_depth
    # sample_prop(c(0.4, 1)), # bagging_fraction (vai para sample_size)
    mtry(),                   # feature_fraction
    loss_reduction()          # min_gain_to_split
  ) 

lgbm_grid &lt;- lightgbm_params %&gt;% 
  finalize(sleep_train) %&gt;% 
  grid_max_entropy(size = 30)</code></pre>
<p>Ajustar modelo:</p>
<pre class="r"><code>lgbm_res_tun &lt;- 
  lgbm_wflow_tun %&gt;% 
  tune_grid(
    resamples = sleep_folds,
    grid = lgbm_grid,
    metrics = metric_set(roc_auc, mn_log_loss, pr_auc),
    control = control_grid(save_pred = TRUE)
  )

# Salvar cache da otimizacao 
saveRDS(lgbm_res_tun, &quot;lgbm_res_tun.rds&quot;)</code></pre>
<p>Impacto de cada hiperpar√¢metro no resultado das m√©tricas c√°lculadas para cada modelo:</p>
<details>
<summary>
(<em>C√≥digo do gr√°fico</em>)
</summary>
<pre class="r"><code>id_best_model &lt;- 
  show_best(lgbm_res_tun, metric = &#39;mn_log_loss&#39;)[1, ] %&gt;% 
  pull(.config)

plot_lgbm_tun &lt;- 
  lgbm_res_tun %&gt;% 
  collect_metrics() %&gt;% 
  mutate(best_model = if_else(.config == id_best_model, 
                              &quot;BestModel&quot;, &quot;Try&quot;)) %&gt;% 
  select(.metric, mean, best_model,
         mtry:loss_reduction) %&gt;%
  pivot_longer(mtry:loss_reduction,
               values_to = &quot;value&quot;,
               names_to = &quot;parameter&quot;
  ) %&gt;% {
    ggplot(., aes(value, mean, color = best_model)) +
      geom_point(alpha = 0.6, show.legend = FALSE) +
      geom_point(data = subset(., best_model == &#39;BestModel&#39;), 
                 size = 4, shape = 3)+
      scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;))+
      facet_grid(.metric~parameter, scales = &quot;free&quot;) +
      labs(x = NULL, y = NULL)
  }

# Codigo para mesmo grafico sem cor para melhor modelo:
# autoplot(lgbm_res_tun)</code></pre>
</details>
<p>¬†</p>
<pre class="r"><code>plot_lgbm_tun %&gt;% 
  plotly::ggplotly()%&gt;% 
  plotly::layout(showlegend = FALSE) %&gt;% 
  plotly::config(displayModeBar = F)</code></pre>
<center>
<img src="/post/2021-02-28-qualidade-do-sono-machine-learning/img6.png" style="width:80.0%" />
</center>
<p>Melhores resultados:</p>
<pre class="r"><code>show_best(lgbm_res_tun, metric = &#39;mn_log_loss&#39;) %&gt;% 
  select(-.estimator, -n, -.config)</code></pre>
<pre><code>## # A tibble: 5 x 7
##    mtry min_n tree_depth loss_reduction .metric      mean std_err
##   &lt;int&gt; &lt;int&gt;      &lt;int&gt;          &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;
## 1     2    35         28 0.000000000108 mn_log_loss 0.259 0.00888
## 2     1    16          5 0.00000160     mn_log_loss 0.267 0.0100 
## 3     4    28         31 0.000507       mn_log_loss 0.272 0.0111 
## 4     5    39         62 0.000164       mn_log_loss 0.273 0.00955
## 5     2    14         19 0.0642         mn_log_loss 0.274 0.0138</code></pre>
<p>Finalizar o modelo com o conjunto de par√¢metros encontrados no processo de otimiza√ß√£o:</p>
<pre class="r"><code># finalizar workflow definindo modelo final
lgbm_final_wflow_tun &lt;- 
  finalize_workflow(
    lgbm_wflow_tun,
    select_best(lgbm_res_tun, metric = &#39;mn_log_loss&#39;) )

# Realizar ajuste final nos dados de treino
lgbm_final_fit_tun &lt;- lgbm_final_wflow_tun %&gt;% last_fit(sleep_split) 

# Coletar previs√µes nos dados de teste
lgbm_test_preds_tun &lt;- collect_predictions(lgbm_final_fit_tun)</code></pre>
<p>Vejamos como ficou foi o ajuste do modelo utilizando a configura√ß√£o obtida no ap√≥s o <em>tunning</em> final:</p>
<p>Avaliar desempenho do modelo nos dados de teste:</p>
<pre class="r"><code>lgbm_test_preds_tun %&gt;% 
  mutate(mood = factor(mood, levels = c(&quot;Ruim&quot;, &quot;Bom&quot;), ordered = TRUE)) %&gt;%
  conf_mat_plot()</code></pre>
<center>
<img src="/post/2021-02-28-qualidade-do-sono-machine-learning/unnamed-chunk-87-1.png" style="width:80.0%" />
</center>
<p>Que maravilha! Modelos acertaram mais a classe de interesse do que os anteriores (apesar do <em>default</em> ainda apresentar alta propor√ß√£o de falsos positivos). Note ainda que o LightGBM ap√≥s o <em>tunning</em> apresentou as melhores m√©tricas no geral (melhor AUC-PR, menor <em>logloss</em> e um bom equil√≠brio no <em>trade-off</em> de <em>Precision</em> x <em>Recall</em>).</p>
<p>Vejamos quais as <em>features</em> mais importantes no ajuste do modelo:</p>
<pre class="r"><code>lgbm_imp_tun &lt;- lightgbm::lgb.importance(lgbm_final_fit_tun$.workflow[[1]]$fit$fit$fit, percentage = T)

lgbm_imp_tun%&gt;% 
  mutate(Feature = reorder(Feature, Gain)) %&gt;% 
  ggplot(aes(x = Feature, y = Gain))+
  geom_bar(stat = &quot;identity&quot;)+
  labs(y = &quot;Importance&quot;, x= &quot;&quot;)+
  coord_flip()</code></pre>
<center>
<img src="/post/2021-02-28-qualidade-do-sono-machine-learning/unnamed-chunk-89-1.png" style="width:80.0%" />
</center>
</div>
</div>
</div>
<div id="sele√ß√£o-do-modelo" class="section level1">
<h1>Sele√ß√£o do modelo ü§î</h1>
<!-- Curva Roc e Precision-Recall Curve: -->
<!-- ```{r} -->
<!-- bind_rows( -->
<!--   # null_res_bas %>% unnest(.predictions) %>% mutate(model = "null baseline"),   -->
<!--   tree_res_bas %>% unnest(.predictions) %>% mutate(model = "rpart baseline"),   -->
<!--   tree_res_tun %>% unnest(.predictions) %>% mutate(model = "rpart tunning"), -->
<!--   rf_res_bas %>% unnest(.predictions) %>% mutate(model = "rf baseline"), -->
<!--   rf_res_tun %>% unnest(.predictions) %>% mutate(model = "rf tunning"), -->
<!--   lgbm_res_bas %>% unnest(.predictions) %>% mutate(model = "lgbm baseline"), -->
<!--   lgbm_res_tun %>% unnest(.predictions) %>% mutate(model = "lgbm tunning") -->
<!-- ) %>%   -->
<!--   plot_auc() +  -->
<!--   plot_annotation(title = 'Resultados nos dados de treino', -->
<!--                   theme = theme(plot.title = element_text(hjust = 0.4))) -->
<!-- ``` -->
<p>Comparar os modelos de forma visual com os gr√°ficos da ROC AUC e da PR AUC:</p>
<details>
<summary>
(<em>C√≥digo do gr√°fico</em>)
</summary>
<pre class="r"><code>auc_plots &lt;- 
  bind_rows(
    null_test_preds_bas %&gt;% mutate(model = &quot;null baseline&quot;),
    tree_test_preds_bas %&gt;% mutate(model = &quot;rpart default&quot;),
    tree_test_preds_tun %&gt;% mutate(model = &quot;rpart tunning&quot;),
    rf_test_preds_bas %&gt;% mutate(model = &quot;rf default&quot;),
    rf_test_preds_tun %&gt;% mutate(model = &quot;rf tunning&quot;),
    lgbm_test_preds_bas %&gt;% mutate(model = &quot;lgbm default&quot;),
    lgbm_test_preds_tun %&gt;% mutate(model = &quot;lgbm tunning&quot;)
  ) %&gt;% 
  plot_auc() + 
  plot_annotation(title = &#39;Resultados nos dados de teste&#39;,
                  theme = theme(plot.title = element_text(hjust = 0.4)))</code></pre>
</details>
<p>¬†</p>
<pre class="r"><code>auc_plots</code></pre>
<center>
<img src="/post/2021-02-28-qualidade-do-sono-machine-learning/unnamed-chunk-91-1.png" style="width:80.0%" />
</center>
<p>Apenas olhando o gr√°fico n√£o da para fazer uma an√°lise conclusiva, vejamos as medidas de qualidade (ordenado por <code>auc_pr</code>):</p>
<details>
<summary>
(<em>C√≥digo da tabela</em>)
</summary>
<pre class="r"><code>test_results &lt;- 
  bind_rows(
    evalue_model(null_test_preds_bas, model = &quot;null baseline&quot;, null_model = TRUE),
    evalue_model(tree_test_preds_bas, model = &quot;rpart default&quot;),
    evalue_model(tree_test_preds_tun, model = &quot;rpart tunning&quot;),
    evalue_model(rf_test_preds_bas, model = &quot;rf default&quot;),
    evalue_model(rf_test_preds_tun, model = &quot;rf tunning&quot;),
    evalue_model(lgbm_test_preds_bas, model = &quot;lgbm default&quot;),
    evalue_model(lgbm_test_preds_tun, model = &quot;lgbm tunning&quot;)
  ) %&gt;% print_table(round = 4, evalue_model = T)   </code></pre>
</details>
<p>¬†</p>
<pre class="r"><code>test_results</code></pre>
<table>
<colgroup>
<col width="13%" />
<col width="3%" />
<col width="3%" />
<col width="3%" />
<col width="3%" />
<col width="7%" />
<col width="6%" />
<col width="7%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
<col width="10%" />
<col width="7%" />
<col width="10%" />
</colgroup>
<thead>
<tr class="header">
<th>model</th>
<th>tp</th>
<th>fp</th>
<th>fn</th>
<th>tn</th>
<th>auc_roc</th>
<th>auc_pr</th>
<th>logloss</th>
<th>f1</th>
<th>f05</th>
<th>f2</th>
<th>precision</th>
<th>recall</th>
<th>trs_fbeta</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>lgbm tunning</td>
<td>6</td>
<td>1</td>
<td>2</td>
<td>72</td>
<td>0.8699</td>
<td>0.7804</td>
<td>0.2190</td>
<td>0.8000</td>
<td>0.8333</td>
<td>0.7692</td>
<td>0.8571</td>
<td>0.7500</td>
<td>0.5937</td>
</tr>
<tr class="even">
<td>rf default</td>
<td>4</td>
<td>0</td>
<td>4</td>
<td>73</td>
<td>0.8399</td>
<td>0.6916</td>
<td>0.6613</td>
<td>0.6667</td>
<td>0.8333</td>
<td>0.5556</td>
<td>1.0000</td>
<td>0.5000</td>
<td>0.7140</td>
</tr>
<tr class="odd">
<td>rf tunning</td>
<td>5</td>
<td>1</td>
<td>3</td>
<td>72</td>
<td>0.8493</td>
<td>0.6658</td>
<td>0.2722</td>
<td>0.7143</td>
<td>0.7812</td>
<td>0.6579</td>
<td>0.8333</td>
<td>0.6250</td>
<td>0.3888</td>
</tr>
<tr class="even">
<td>null baseline</td>
<td>0</td>
<td>0</td>
<td>8</td>
<td>73</td>
<td>0.5000</td>
<td>0.5494</td>
<td>0.3236</td>
<td></td>
<td></td>
<td></td>
<td>0.0000</td>
<td>0.1141</td>
<td></td>
</tr>
<tr class="odd">
<td>lgbm default</td>
<td>6</td>
<td>3</td>
<td>2</td>
<td>70</td>
<td>0.8527</td>
<td>0.5038</td>
<td>0.2313</td>
<td>0.7059</td>
<td>0.6818</td>
<td>0.7317</td>
<td>0.6667</td>
<td>0.7500</td>
<td>0.4286</td>
</tr>
<tr class="even">
<td>rpart default</td>
<td>5</td>
<td>9</td>
<td>3</td>
<td>64</td>
<td>0.7312</td>
<td>0.4471</td>
<td>0.3267</td>
<td>0.4545</td>
<td>0.3906</td>
<td>0.5435</td>
<td>0.3571</td>
<td>0.6250</td>
<td>0.5714</td>
</tr>
<tr class="odd">
<td>rpart tunning</td>
<td>4</td>
<td>7</td>
<td>4</td>
<td>66</td>
<td>0.7243</td>
<td>0.4390</td>
<td>0.3399</td>
<td>0.4211</td>
<td>0.3846</td>
<td>0.4651</td>
<td>0.3636</td>
<td>0.5000</td>
<td>0.7857</td>
</tr>
</tbody>
</table>
<p>O modelo LightGBM ap√≥s o processo de tunning foi o que apresentou as melhores medidas no geral. Note que o LightGBM com os par√¢metro default ficou pior do que o modelo nulo üò±! Isso mostra como o processo de tunning pode ser importante. Al√©m disso note que o modelo <code>rf baseline</code> apresentou o segundo maior AUC-PR mas o pior logloss (note que o <code>threshold</code> est√° muito alto e as demais m√©tricas n√£o ficaram muito boas).</p>
<p>Portanto, apenas os modelos <em>LightGBM</em> e <em>Random Forest</em> apresentaram resultados melhores que um modelo nulo (sempre estima a classe majorit√°ria) e como o LightGBM foi o mais satisfat√≥rio, este ser√° o modelo selecionado. üòé</p>
</div>
<div id="previs√£o-em-dados-novos" class="section level1">
<h1>Previs√£o em dados novos üí´</h1>
<p>Obter as previs√µes nos novos dados:</p>
<pre class="r"><code>trs_final &lt;- evalue_model(lgbm_test_preds_tun, model = &quot;lgbm tunning&quot;)$trs_fbeta

final &lt;- 
  predict(lgbm_final_fit_tun$.workflow[[1]], new_sleep, type = &quot;prob&quot;) %&gt;% 
  mutate(.pred_class = ifelse(.pred_Ruim &gt;= trs_final, &quot;Ruim&quot;, &quot;Bom&quot;)) 

# new_sleep %&gt;% filter(final$.pred_class == &quot;Ruim&quot;)</code></pre>
<p>Comparar a quantidade de previs√µes de cada classe com o conjunto de treino/teste:</p>
<details>
<summary>
(<em>C√≥digo para tabela abaixo</em>)
</summary>
<pre class="r"><code>tab &lt;- 
  full_join(sleep_train %&gt;% count(mood) %&gt;% mutate(prop = n/sum(n)*100),
            sleep_test %&gt;% count(mood) %&gt;% mutate(prop = n/sum(n)*100),
            by = &quot;mood&quot;) %&gt;% 
  full_join(final %&gt;% 
              count(mood = .pred_class) %&gt;% mutate(prop = n/sum(n)*100)) %&gt;% 
  print_table(round = 2,
              columns = list(
                n.x = colDef(name = &quot;N&quot;),
                prop.x = colDef(name = &quot;(%)&quot;, align = &quot;left&quot;),
                n.y = colDef(name = &quot;N&quot;),
                prop.y = colDef(name = &quot;(%)&quot;, align = &quot;left&quot;),
                n = colDef(name = &quot;N&quot;),
                prop = colDef(name = &quot;(%)&quot;, align = &quot;left&quot;)
              ), 
              columnGroups = list(
                colGroup(name = &quot;Train&quot;, columns = c(&quot;n.x&quot;, &quot;prop.x&quot;)),
                colGroup(name = &quot;Test&quot;, columns = c(&quot;n.y&quot;, &quot;prop.y&quot;)),
                colGroup(name = &quot;New Data&quot;, columns = c(&quot;n&quot;, &quot;prop&quot;))
              ))</code></pre>
</details>
<p>¬†¬†</p>
<pre class="r"><code>tab</code></pre>
<details>
<summary>
(<em>C√≥digo do c√°lculo das medidas abaixo</em>)
</summary>
<pre class="r"><code># ref: https://en.wikipedia.org/wiki/Sensitivity_and_specificity

# Obter medidas da matriz de confusao
tp = evalue_model(lgbm_test_preds_tun, model = &quot;lgbm tunning&quot;)$tp
tn = evalue_model(lgbm_test_preds_tun, model = &quot;lgbm tunning&quot;)$tn
fn = evalue_model(lgbm_test_preds_tun, model = &quot;lgbm tunning&quot;)$fn
fp = evalue_model(lgbm_test_preds_tun, model = &quot;lgbm tunning&quot;)$fn

# true positive rate
tpr = tp / (tp + fn)
# false negative rate
fnr = 1 - tpr
#false positive rate
fpr = fp / (fp + tn)</code></pre>
</details>
<p>¬†¬†</p>
<p>Como nosso modelo foi otimizado para ser menos ‚Äúalarmista‚Äù (com uma Taxa de Falso Positivo: 2.7%) √© poss√≠vel que o modelo tenha deixado passar alguns dias em que <code>mood=="Ruim"</code> (Taxa de Falso Negativo: 25%). N√£o vejo isto como um grande problema pois dado a pequena quantidade de dados dispon√≠veis at√© que o resultado para a classe de interesse estava bem razo√°vel (Taxa de Verdadeiro Positivo: 75%).</p>
<p>Para n√£o alongar aida mais o post com an√°lise explorat√≥ria das previs√µes, vamos comparar como foram as previs√µes nestes novos dados em rela√ß√£o aos dados utilizados para treinar o modelo e ver se, pelo menos visualmente, o modelo esteja conseguindo prever de semelhante ao padr√£o de dados conhecidos.</p>
<p>A t√©cnica <a href="https://cran.r-project.org/web/packages/umap/vignettes/umap.html">UMAP</a> ser√° utilizada com a finalidade de reduzir a dimensionalidade para visualiza√ß√£o:</p>
<details>
<summary>
(<em>C√≥digo para gr√°fico abaixo</em>)
</summary>
<pre class="r"><code># Treinar UMAP: 
sleep_umap &lt;-  juice(sleep_recipe) %&gt;% select(-mood) %&gt;% umap::umap()

# Aplicar em novos dados:
new_data &lt;- bake(sleep_recipe, new_sleep) %&gt;% select(-mood)
new_data_umap &lt;- predict(sleep_umap, new_data)

# Preparar plot comparando treino com novos dados:
umap_plot &lt;-
  bind_rows(
    sleep_umap$layout %&gt;% 
      as_tibble() %&gt;% 
      bind_cols(juice(sleep_recipe) %&gt;% select(mood))  %&gt;% 
      bind_cols(dataset =  &quot;Train&quot;)
    ,
    new_data_umap %&gt;% 
      as_tibble() %&gt;% 
      mutate(mood = factor(final$.pred_class,
                           levels = c(&quot;Ruim&quot;, &quot;Bom&quot;)))  %&gt;% 
      bind_cols(dataset =  &quot;New Data&quot;)
  ) %&gt;%
  mutate(dataset = factor(dataset, levels = c(&quot;New Data&quot;, &quot;Train&quot;))) %&gt;% {
    ggplot(., aes(x = V1, y = V2, color = mood, shape = mood))+
      geom_point(show.legend = F)+
      geom_point(aes(x = V1, y = V2, color = mood), 
                 data = subset(., mood == &#39;Ruim&#39;), 
                 size = 2, shape = 3)+
      labs(x = &quot;&quot;, y = &quot;&quot;, 
           title = &quot;UMAP (Uniform Manifold Approximation and Projection)&quot;)+
      scale_color_viridis_d(end = 0.8, direction = 1)+
      # scale_size_manual(values=c(2,5))+
      theme(legend.position = &quot;bottom&quot;)+
      facet_wrap(~dataset)
  }</code></pre>
</details>
<p>¬†¬†</p>
<pre class="r"><code>umap_plot %&gt;% 
  plotly::ggplotly()%&gt;% 
  plotly::layout(showlegend = FALSE) %&gt;% 
  plotly::config(displayModeBar = F)</code></pre>
<p>Parece que o modelo fez previs√µes nos novos dados em um padr√£o espec√≠fico dos dados (√† direita) enquanto que nos dados de treino podemos observar alguma informa√ß√£o da classe <code>Ruim</code> na massa de dados √† esquerda. Isso pode estar acontecendo devido ao foco que demos para minimizar falsos positivos. √â uma boa indica√ß√£o para analisar melhor o padr√£o que o modelo esta aprendendo em rela√ß√£o aos falsos negativos.</p>
<center>
<img src="/post/2021-02-28-qualidade-do-sono-machine-learning/img7.png" style="width:80.0%" />
</center>
</div>
<div id="conclus√£o" class="section level1">
<h1>Conclus√£o üçª</h1>
<p>Apesar da pequena quantidade dados dados dispon√≠veis, conseguimos ajustar um modelo razo√°vel para prever a qualidade de sono em dias que n√£o foram registrados!</p>
<div class="row">
<div class="column8">
<p>Utilizamos diversas t√©cnicas de <em>Machine Leaning</em> combinadas em dados reais (que n√£o s√£o nada comportados) e, obviamente, para colocar um modelo em produ√ß√£o na vida real seria necess√°rio aplicar mais uma s√©rie de an√°lises, al√©m de entender como o modelo est√° funcionando, aplicando t√©cnicas de <a href="https://en.wikipedia.org/wiki/Explainable_artificial_intelligence">XAI</a> (Explainable AI) mas isso pode ser assunto para um futuro <em>post</em>, hora de dormir! üò¥</p>
<p>Espero que este pequeno ‚Äú<em>case</em>‚Äù seja √∫til para voc√™! Para mim foi √≥timo combinar a pr√°tica do uso do pacote <code>tidymodels</code> para resolver um problema com dados reais com um estudo que me trouxe mais auto-conhecmento e um monte de <em>insights</em> pessoais.</p>
</div>
<div class="column4">
<div class="float">
<img src="https://media.giphy.com/media/U7Lvtcuqh4WZy/giphy.gif" alt="Via Giphy" />
<div class="figcaption"><a href="https://media.giphy.com/media/U7Lvtcuqh4WZy/giphy.gif">Via Giphy</a></div>
</div>
</div>
</div>
<hr />
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias üß≥</h1>
<ul>
<li><a href="https://juliasilge.com/blog/wind-turbine/" class="uri">https://juliasilge.com/blog/wind-turbine/</a></li>
<li><a href="https://juliasilge.com/blog/hotels-recipes/" class="uri">https://juliasilge.com/blog/hotels-recipes/</a></li>
<li><a href="https://juliasilge.com/blog/xgboost-tune-volleyball/" class="uri">https://juliasilge.com/blog/xgboost-tune-volleyball/</a></li>
<li><a href="http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/" class="uri">http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/</a></li>
<li><a href="https://machinelearningmastery.com/imbalanced-classification-with-python/" class="uri">https://machinelearningmastery.com/imbalanced-classification-with-python/</a></li>
<li><a href="https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/" class="uri">https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/</a></li>
<li><a href="https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/" class="uri">https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/</a></li>
<li><a href="https://machinelearningmastery.com/fbeta-measure-for-machine-learning/" class="uri">https://machinelearningmastery.com/fbeta-measure-for-machine-learning/</a></li>
<li><a href="https://sites.google.com/view/lauraepp/parameters" class="uri">https://sites.google.com/view/lauraepp/parameters</a></li>
<li><a href="https://github.com/microsoft/LightGBM/issues/695" class="uri">https://github.com/microsoft/LightGBM/issues/695</a></li>
</ul>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p><a href="https://www.usp.br/espacoaberto/?materia=a-importancia-de-dormir-bem" class="uri">https://www.usp.br/espacoaberto/?materia=a-importancia-de-dormir-bem</a><a href="#fnref1" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn2"><p><a href="https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/" class="uri">https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/</a><a href="#fnref2" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn3"><p><a href="https://machinelearningmastery.com/fbeta-measure-for-machine-learning/" class="uri">https://machinelearningmastery.com/fbeta-measure-for-machine-learning/</a><a href="#fnref3" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn4"><p><a href="https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/" class="uri">https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/</a><a href="#fnref4" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn5"><p><a href="https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/" class="uri">https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/</a><a href="#fnref5" class="footnote-back">‚Ü©Ô∏é</a></p></li>
</ol>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2021-02-28-qualidade-do-sono-machine-learning/">Prevendo a qualidade do sono utilizando Machine Learning</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">imbalanced</category>
      <category domain="tag">imbalanced-data</category>
      <category domain="tag">lightgbm</category>
      <category domain="tag">r</category>
      <category domain="tag">random-forest</category>
      <category domain="tag">threshold-movel</category>
      <category domain="tag">tidymodels</category>
      <category domain="tag">tidyverse</category>
      <category domain="tag">tunning</category>
    </item>
    <item>
      <title>Desenvolva um bot e receba resultados de Machine Learning no seu Smartphone para ajudar nos investimentos</title>
      <link>https://gomesfellipe.github.io/post/2020-03-25-investment-alert/investment-alert/</link>
      <pubDate>Wed, 25 Mar 2020 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2020-03-25-investment-alert/investment-alert/</guid>
      <description>Entenda a l√≥gica de como montar uma carteira, coletar dados de finan√ßa em tempo real, treinar um modelo de Machine Learning com Prophet (Facebook Open Source) e receber an√°lises automatizadas no Smartphone</description>
      <content:encoded>&lt;![CDATA[
        


<style>
.column {
  float: left;
  width: 50%;
  padding: 10px;
}

.column4 {
  float: left;
  width: 33%;
  padding: 10px;
}

.column8 {
  float: left;
  width: 66%;
  padding: 10px;
}

.row:after {
  content: "";
  display: table; 
  clear: both;
}
</style>
<div id="por-que-investir" class="section level1">
<h1>Por que investir?</h1>
<p>Como esta sua situa√ß√£o financeira? Caso tenha alguma reserva pode ser interessante pensar em investimentos pois a poupan√ßa j√° n√£o √© mais garantia de lucro no longo prazo, n√£o acredita?</p>
<p>Estamos no final do primeiro trimestre de 2020 e desde 2019 j√° liam-se not√≠cias como esta abaixo que levam √† reflex√£o sobre reeduca√ß√£o financeira pois alertam sobre a necessidade da busca por novas oportunidades de investimento.</p>
<center>
<img src="/post/2020-03-25-investment-alert/noticia_poupanca.png" style="width:50.0%" /> <br><small>Fonte: <a href="https://noticias.r7.com/economia/economize/poupanca-em-baixa-exige-busca-por-novos-investimentos-em-2020-25122019" class="uri">https://noticias.r7.com/economia/economize/poupanca-em-baixa-exige-busca-por-novos-investimentos-em-2020-25122019</a></small>
</center>
<p></br></p>
<p>Com a finalidade de fomentar um pouco a discuss√£o sobre investimentos, trouxe nesse post algumas sugest√µes e id√©ias de como elaborar uma carteira e otimizar as escolhas para equilibrar risco em novos investimentos combinando elementos de estat√≠stica, machine learning e programa√ß√£o em R.</p>
<p>Ao final do post criaremos um <a href="https://core.telegram.org/bots">rob√¥ no telegram</a> que coletar√° os dados das cota√ß√µes adquiridas, aplicar√° o modelo <a href="https://facebook.github.io/prophet/docs/quick_start.html">Prophet do Facebook</a> para forecast e analisar√° a desmontagem da carteira segundo os crit√©rios estabelecidos e enviar√° mensagens para n√≥s com uma tabela financeira automatizada via Telegram como mostra na anima√ß√£o:</p>
<p></br></p>
<div class="row">
<div class="column4">
<center>
<img src="/post/2020-03-25-investment-alert/watch.png" />
</center>
</div>
<div class="column8">
<p><img src="/post/2020-03-25-investment-alert/report_stocks.gif" style="width:90.0%" /></p>
</div>
</div>
<p></br></p>
<div class="w3-panel w3-pale-red w3-border">
<p><i class="fa fa-exclamation-triangle" aria-hidden="true"></i> <strong>AVISO</strong>: Este post <strong>n√£o</strong> tem como finalidade ser um guia de investimentos (J√° existem muitas consultorias especializadas nisso por ai). Todos as decis√µes tomadas como diversifica√ß√£o da carteira, sele√ß√£o de a√ß√µes e crit√©rios para desmontagem da carteira s√£o <strong>exemplos</strong> e servem para ilustrar algumas <strong>possibilidades</strong> que um cientista de dados t√™m na hora de desenvolver ferramentas para auxiliar √† tomada de decis√£o.</p>
</div>
<p></br></p>
<div id="diferen√ßa-de-poupar-e-investir" class="section level2">
<h2>Diferen√ßa de poupar e investir</h2>
<p>De acordo com um <a href="https://www.infomoney.com.br/minhas-financas/brasileiros-nao-sabem-a-diferenca-entre-poupar-e-investir-afirma-especialista-2/">especialista entrevistado pela InfoMoney</a>:</p>
<blockquote>
‚ÄúPoupar √© guardar dinheiro para usar no futuro, comprar alguma coisa com ele. Investimento √© juntar dinheiro, n√£o mexer nele, para que este gere rendimentos e a√≠ sim, usar os lucros mais para frente. √â o recomendado para quem quer viver de renda no futuro, por exemplo‚Äù (‚Ä¶)
<div align="right">
<font size="1">InfoMoney - Ago 2015</font>
</div>
</blockquote>
<p>Ou seja, poupar √© acumular agora para utilizar depois, e normalmente envolve mudan√ßa de h√°bitos, pois requer uma redu√ß√£o nos gastos pessoais e familiares, j√° investir √© usar esse dinheiro poupado em aplica√ß√µes que rendam.</p>
<p>Como todo mundo sabe, n√£o existe investimento sem risco e este risco deve ser controlado e utilizado a nosso favor de forma que gere alguma seguran√ßa financeira.</p>
</div>
</div>
<div id="montagem-da-carteira" class="section level1">
<h1>Montagem da carteira</h1>
<div class="row">
<div class="column8">
<p>N√£o colocar todos os ovos na mesma cesta significa que voc√™ deve diversificar o seu investimento. Provavelmente voc√™ j√° ouviu essa frase alguma vez na vida e ela certamente faz sentido!</p>
<p>Diversificar a carteira ir√° proteger seus investimentos diminuindo o risco pois, imagine s√≥, voc√™ investe todo o seu dinheiro em uma empresa e ela passa por alguma crise assim seu dinheiro estar√° todo comprometido!</p>
<p>Existem diversos motivos para se diversificar a carteira mas acho que essa met√°fora dos ovos j√° resume bem pois acredito que ningu√©m queira perder tudo em uma queda.</p>
</div>
<div class="column4">
<p><img src="/post/2020-03-25-investment-alert/ovos_mesm_cesta.png" style="width:90.0%" /></p>
</div>
</div>
<div id="como-dividir-a-carteira" class="section level2">
<h2>Como dividir a carteira?</h2>
<p>Dependendo do risco que voc√™ deseja se expor existem muitas formas de preparar a carteira mas a id√©ias principal consiste em atingir um equil√≠brio entre dois tipos de investimento:</p>
<ul>
<li>Renda fixa: Menor exposi√ß√£o, menor risco (ex.: CDI, Selic e TR)</li>
<li>Renda vari√°vel: Maior exposi√ß√£o, maior risco (ex: A√ß√µes, Commodities, Im√≥veis)</li>
</ul>
<p>Para ajudar a dividir a carteira de investimentos neste post utilizaremos a chamada <a href="https://www.btgpactualdigital.com/blog/coluna-gustavo-cerbasi/defina-sua-estrategia-entre-renda-fixa-ou-variavel">Regra (ou Lei) dos 80</a>. A estrat√©gia √© a seguinte: subtraia da sua idade o n√∫mero 80. O resultado dessa conta vai indicar o percentual a ser investido em <a href="https://pt.wikipedia.org/wiki/Renda_vari%C3%A1vel">renda vari√°vel</a>. Por exemplo, no meu caso: tenho 26 anos, portanto <span class="math inline">\(80-26 = 54\%\)</span> dever√° ser investido em renda vari√°vel. Aos 53 anos esse percentual vai cair√° para a metade, <span class="math inline">\(27\%\)</span>.</p>
<p>A id√©ias principal por tr√°s desta regra que √© que a cada ano que passa, 1% do montante da renda vari√°vel deva ser direcionado para a <a href="https://pt.wikipedia.org/wiki/Renda_fixa">renda fixa</a>.</p>
<p>Para testar diferentes valores seguindo esta regra desenvolvi uma fun√ß√£o que se chama <a href="https://gist.github.com/gomesfellipe/a74710a63b3c8637166b538ad2f460f5"><code>montagem80()</code> (que j√° est√° dispon√≠vel no github)</a>. Vejamos alguns resultados para diferentes cen√°rios e vamos selecionar um para seguir com a montagem da carteira:</p>
<pre class="r"><code># https://gist.github.com/gomesfellipe/a74710a63b3c8637166b538ad2f460f5
devtools::source_gist(&quot;a74710a63b3c8637166b538ad2f460f5&quot;, quiet = T)</code></pre>
<div class="row">
<div class="column">
<ol style="list-style-type: decimal">
<li>Entrada de R$20.000,00 <big>üëà</big></li>
</ol>
<pre class="r"><code>montagem80(entrada = 20000, idade = 26)</code></pre>
<pre><code>## ‚Ä¢ Entrada: R$R$20.000,00
##   ‚îî‚îÄ Renda fixa:     R$9.200,00
##   ‚îî‚îÄ Renda variavel: R$10.800,00
##        (Acao + Cryptomoeda): 
##        R$9.720,00 + R$1.080,00</code></pre>
</div>
<div class="column">
<ol start="2" style="list-style-type: decimal">
<li>R$ 5.000,00 em acoes</li>
</ol>
<pre class="r"><code>montagem80(variavel = 10800, idade = 53)</code></pre>
<pre><code>## ‚Ä¢ Entrada: R$R$40.000,00
##   ‚îî‚îÄ Renda fixa:     R$29.200,00
##   ‚îî‚îÄ Renda variavel: R$10.800,00
##        (Acao + Cryptomoeda): 
##        R$9.720,00 + R$1.080,00</code></pre>
</div>
</div>
<p>Utilizaremos a primeira (1.) configura√ß√£o que esta assinalada em vermelho como exemplo, onde:</p>
<ul>
<li>Entrada: R$20.000,00</li>
<li>Renda fixa: R$9.200,00</li>
<li>Renda Vari√°vel (Acoes): R$9.720,00</li>
<li>Renda Vari√°vel (Crypt): R$1.080,00</li>
</ul>
<p>Note que a entrada deve ser o dobro na segunda (2.) configura√ß√£o caso deseje investir R$10.800,00 (que √© o valor sugerido aos 26 anos para uma entrada de R$20.000,00)</p>
<p>Ap√≥s definir a quantidade a ser investida √© hora de planejar a pr√≥xima etapa: a diversifica√ß√£o.</p>
</div>
<div id="diversifica√ß√£o-da-carteira" class="section level2">
<h2>Diversifica√ß√£o da carteira</h2>
<div class="row">
<div class="column8">
<p>A diversifica√ß√£o √© uma t√©cnica para gest√£o do risco que visa distribuir o capital investido em uma variedade de investimentos dentro de da nossa carteira.</p>
<p>Assim, o risco do portf√≥lio √© consideravelmente reduzido pois reduzimos a volatividade e criamos um equil√≠brio onde um desempenho positivo de um ativo neutraliza as baixas ocorridas em outras aplica√ß√µes. Al√©m disso a diversifica√ß√£o pode ser tanto coma renda vari√°vel quanto com a renda fixa.</p>
<p>Mas lembre-se, n√£o existe uma receita mais eficiente!</p>
</div>
<div class="column4">
</br>
<iframe src="https://giphy.com/embed/qJkRbWM1MfVjq" width="100%" height="150" frameBorder="0" class="giphy-embed" allowFullScreen></iframe>
<p>
<a href="https://giphy.com/gifs/eggs-qJkRbWM1MfVjq">via GIPHY</a>
</p>
</div>
</div>
<div id="renda-fixa" class="section level3">
<h3>Renda fixa</h3>
<p>Normalmente, tamb√©m diversificamos nossa renda fixa por√©m como gostaria de focar nas an√°lises de renda vari√°vel utilizaremos o simulador dispon√≠vel no site <a href="https://verios.com.br/" class="uri">https://verios.com.br/</a> neste <a href="https://simulador-tesouro-direto.verios.com.br/">link</a> para selecionar apenas um t√≠tulo:</p>
<center>
<img src="/post/2020-03-25-investment-alert/tesouro_direto.png" style="width:80.0%" />
<small></br>Fonte: <a href="https://simulador-tesouro-direto.verios.com.br/" class="uri">https://simulador-tesouro-direto.verios.com.br/</a></small>
</center>
<p></br></p>
<p>Como exemplo, escolhi o Tesouro Prefixado 2015 (LTN), note que com essa escolha o lucro planejado seria de quase 3 mil reais nos pr√≥ximos 5 anos.</p>
<div class="w3-panel w3-pale-red w3-border">
<p><i class="fa fa-exclamation-triangle" aria-hidden="true"></i> <strong>AVISO</strong>: Lembrando que esta sele√ß√£o √© apenas um exemplo e existem diversas informa√ß√µes a serem levadas em conta ao se fazer esta escolha al√©m de se poder diversificar tamb√©m. Convido o leitor a procurar saber mais sobre os tipos, pr√≥s e contras do Tesouro Direto.</p>
</div>
</div>
<div id="renda-vari√°vel" class="section level3">
<h3>Renda vari√°vel</h3>
<p>Para elabora√ß√£o da parte da renda vari√°vel da carteira selecionei duas a√ß√µes que s√£o inversamente correlacionadas (de forma totalmente arbitr√°ria) baseado no excelente post "<a href="https://www.tradingcomdados.com/post/2017/07/09/estudo-de-correla%C3%A7%C3%A3o-entre-a%C3%A7%C3%B5es-da-bolsa-de-valores-de-s%C3%A3o-paulo">Estudo de correla√ß√£o entre a√ß√µes da Bolsa de Valores de S√£o Paulo</a>" escrito por Victor Gomes onde o autor faz um estudo de correla√ß√µes das s√©ries hist√≥ricas de a√ß√µes de diferentes setores.</p>
<p>Al√©m disso, o Bitcoin ser√° selecionado para completar a carteira de renda vari√°vel como um ativo de alto risco com bastante volatividade. Mas voc√™ deve estar se perguntando, por que assumir este risco?</p>
<p>Existem muitas hist√≥rias de pessoas que ficaram milion√°rias com o Bitcoin pela sua valoriza√ß√£o inesperada ao longo do tempo, como por exemplo o <a href="https://www.infomoney.com.br/mercados/adolescente-fica-milionario-aos-18-anos-usando-bitcoins-apos-fazer-aposta-com-os-pais/">adolescente que ficou milion√°rio aos 18 anos usando bitcoins ap√≥s fazer aposta com os pais</a>.</p>
<p>Ent√£o <strong>eu</strong> acho que 10% dessa nossa carteira (n√£o esque√ßa que podemos ter mais de uma carteira) √© um risco que pode valer a pena correr e por isso vou inclu√≠-lo.</p>
<p>Portanto, para este exemplo consideramos:</p>
<ul>
<li>TUPY3.SA: <a href="https://www.google.com/search?q=tupy3&amp;oq=tupy3&amp;aqs=chrome..69i57.7273j0j4&amp;sourceid=chrome&amp;ie=UTF-8">Tupy</a></li>
<li>ELET3.SA: <a href="https://www.google.com/search?ei=-dxuXtuyNIbR5OUPxY2m8AE&amp;q=ELET3.SA&amp;oq=ELET3.SA&amp;gs_l=psy-ab.3..0.40045.41040..41701...0.2..0.123.237.0j2......0....2j1..gws-wiz.......0i71.a2fjObIM6cM&amp;ved=0ahUKEwibk86a8p3oAhWGKLkGHcWGCR4Q4dUDCAs&amp;uact=5">Centrais Eletricas Brasileiras SA</a></li>
<li>BTC-USD: <a href="https://www.google.com/search?ei=JN1uXsDJLJm55OUPj8-c8As&amp;q=bitcoin&amp;oq=bitcoin&amp;gs_l=psy-ab.3.0.0i131i70i258j0i131i67j0i131j0i67j0j0i67j0i131j0i67j0j0i131.25383.26281..27215...0.0..0.175.1147.1j8......0....1..gws-wiz.xu0EQx1CnXI">Bitcoin</a></li>
</ul>
<pre class="r"><code>portifolio = c(&quot;TUPY3.SA&quot;,&quot;ELET3.SA&quot;, &quot;BTC-USD&quot;)</code></pre>
<div class="w3-panel w3-pale-red w3-border">
<p><i class="fa fa-exclamation-triangle" aria-hidden="true"></i> <strong>AVISO</strong>: Volto a frisar que as escolhas das a√ß√µes foram feitas de forma arbitr√°ria. Estamos em tempos de incerteza atualmente por conta do corona v√≠rus (espero que todos fiquem bem) o que leva algumas escolhas √† serem ainda mais complexas e imprevis√≠veis.</p>
</div>
<div id="obter-dados" class="section level4">
<h4>Obter dados</h4>
<p>A aquisi√ß√£o das s√©rie hist√≥ricas das cota√ß√µes destes ativos desde 01/01/2016 foram obtidas utilizando o pacote <a href="https://github.com/business-science/tidyquant"><code>tidyquant</code></a> que nos retorna os dados das cota√ß√µes das a√ß√µes informadas em formato ‚Äúarrumado‚Äù (ou seja, familiar com fun√ß√µes do <a href="https://www.tidyverse.org/">tidyverse</a>), veja:</p>
<pre class="r"><code>library(tidyquant) # aquisicao de dados financeiros
stocks &lt;-map_df(portifolio, ~tq_get(.x, get = &quot;stock.prices&quot;, from = &quot; 2016-01-01&quot;))
saveRDS(stocks, &quot;stocks.rds&quot;)</code></pre>
<p>Veja as linhas iniciais do dataset obtido:</p>
<pre class="r"><code>stocks %&gt;% head() %&gt;% kable2()</code></pre>
<center>
<img src="/post/2020-03-25-investment-alert/img1.png" style="width:80.0%" />
</center>
<p>Existem uma s√©rie de vantagens de se trabalhar com os dados neste formato em R, veremos o porque nas pr√≥ximas se√ß√µes.</p>
</div>
<div id="arrumar-dataset" class="section level4">
<h4>Arrumar dataset</h4>
<p>O formato <a href="https://github.com/tidyverts/tsibble"><code>tsibble</code></a> √© um formato moderno para se trabalhar com s√©ries temporais trazendo a filosofia do <code>tidyverse</code> para os dados de s√©ries temporais facilitando o <a href="https://blog.earo.me/2018/12/20/reintro-tsibble/">fluxo de trabalho</a>.</p>
<p>Diversos outros pacotes podem ser combinados utilizando os dados no formato do pacote <code>tsibble</code> como os pacotes <a href="https://robjhyndman.com/hyndsight/fable/"><code>fable</code></a> e o <a href="https://github.com/mitchelloharawild/fable.prophet"><code>prophet</code></a> (sugiro a leitura para quem nao conhece) para aplica√ß√£o de modelagem de s√©ries temporais.</p>
<p>Veja como √© o fluxo ao trabalhar com objetos do tipo <code>tsibble</code></p>
<center>
<img src="/post/2020-03-25-investment-alert/ds-pipeline.png" style="width:60.0%" />
</br>
<small>Fonte: <a href="https://blog.earo.me/2018/12/20/reintro-tsibble/" class="uri">https://blog.earo.me/2018/12/20/reintro-tsibble/</a></small>
</center>
<p></br></p>
<pre class="r"><code>library(tsibble) # series temporais arrumadas
tbl_stocks &lt;- stocks %&gt;% as_tsibble(key = symbol, index = date) </code></pre>
<p>Ap√≥s importar e arrumar o dataset, seguimos para os pr√≥ximos passos.</p>
</div>
<div id="transformar" class="section level4">
<h4>Transformar</h4>
<p>Ap√≥s converter para <code>tsibble</code>, vamos preencher alguns gaps da bolsa como por exemplo os finais de semana (quando a bolsa de valores fica fechada) com o mesmo valor do dia anterior (Para este exemplo vamos fazer esse preenchimento dos gaps do final de semana mas n√£o √© sempre √© necess√°rio):</p>
<pre class="r"><code>tbl_stocks &lt;- 
  tbl_stocks %&gt;% 
  fill_gaps() %&gt;% 
  tidyr::fill(c(open, high, low, close, volume, adjusted),.direction = &quot;down&quot;)</code></pre>
<p>Com os dados arrumados vamos a algumas visualiza√ß√µes.</p>
</div>
<div id="visualizar" class="section level4">
<h4>Visualizar</h4>
<p>Vejamos qual foi o comportamento das s√©ries hist√≥ricas que coletamos desde o in√≠cio de 2016:</p>
<pre class="r"><code>library(forecast) # series temporais
library(fpp3)     # series temporais
d2 &lt;- 
  tbl_stocks %&gt;% 
  group_by(symbol) %&gt;% 
  summarise(y = mean(close))

autoplot(tbl_stocks)+
  geom_line(aes(group = symbol), color = &quot;black&quot;, show.legend = F) + 
  facet_wrap(~symbol, scales = &quot;free_y&quot;, ncol = 1)+
  geom_ma(n=6*30, color = &quot;red&quot;) + 
  theme(legend.position = &quot;none&quot;)+
  labs(subtitle = &quot;m√©dia m√≥vel n = 6 meses&quot;,
       caption = &quot;gomesfellipe.github.io&quot;)</code></pre>
<center>
<img src="/post/2020-03-25-investment-alert/unnamed-chunk-10-1.png" style="width:80.0%" />
</center>
<p>Note que a s√©rie do Bitcoin √© a mais imprevis√≠vel, houve um pico em 2018 mas ap√≥s isso n√£o houve nenhum grande pico como aquele. Em breve ocorrer√° o <a href="https://www.infomoney.com.br/onde-investir/halving-conheca-o-processo-que-pode-levar-o-bitcoin-a-uma-nova-explosao-de-preco/">Halving</a> (a contagem regressiva pode acompanhada <a href="https://www.bitcoinblockhalf.com/">neste link</a>) que √© um processo de choque de oferta e ocorre aproximadamente a cada 4 anos e pode ser uma boa oportunidade de retorno.</p>
<p>As duas s√©ries da bolsa de valores n√£o parecem ter uma correla√ß√£o muito forte, os picos ocorrem de forma alternada e isto pode ser uma caracter√≠stica boa para a carteira pois caso uma delas entre em crise a outra poder√° estar em uma fase boa.</p>
<div class="w3-panel w3-pale-blue w3-border">
<p><i class="fa fa-ambulance" aria-hidden="true"></i> Nota: Esta queda abrupta que ocorreu na bolsa no in√≠cio de 2020 √© o reflexo da <a href="https://pt.wikipedia.org/wiki/Pandemia_de_COVID-19_no_Brasil">Pandemia CODVID-19</a> que j√° come√ßou a apresentar alguns casos no Brasil e isso certamente tem gerando muita incerteza na bolsa de valores. Nem eu nem ningu√©m sabe o que pode acontecer, estou na torcida para que todos fiquem bem e pelo sucesso na conten√ß√£o desse v√≠rus! <i class="fas fa-praying-hands"></i></p>
</div>
</div>
<div id="correla√ß√µes" class="section level4">
<h4>Correla√ß√µes</h4>
<p>Para estudar melhor as conjecturas formadas ao observar o comportamento das s√©ries hist√≥rias (de que os picos e vales se alternam) vamos conferir olhada nos gr√°ficos de dispers√£o e coeficientes de <a href="https://pt.wikipedia.org/wiki/Coeficiente_de_correla%C3%A7%C3%A3o_de_postos_de_Spearman">correla√ß√£o de Spearman</a>:</p>
<pre class="r"><code>points_loess &lt;- function(data, mapping){
  ggplot(data = data, mapping = mapping) + 
    geom_point(alpha = 0.3,size=0.5) + 
    geom_smooth(method = &quot;loess&quot;)
}

tbl_stocks %&gt;%
  as_tibble() %&gt;% 
  select(symbol, date, close) %&gt;% 
  spread(key = symbol, value = close) %&gt;%
  GGally::ggpairs(columns = 2:4,
                  upper = list(continuous = GGally::wrap(&quot;cor&quot;, method = &quot;spearman&quot;)),
                  lower = list(continuous =  points_loess))</code></pre>
<center>
<img src="/post/2020-03-25-investment-alert/unnamed-chunk-11-1.png" style="width:80.0%" />
</center>
<p>Note que a rela√ß√£o entre TUPY3 e ELET3 n√£o √© linear e al√©m disso essa correla√ß√£o √© fraca, o que pode ser ben√©fico para o portf√≥lio pois uma poss√≠vel queda em uma n√£o parece n√£o ter tanto impacto na outra.</p>
<p>Al√©m disso note que a correla√ß√£o do BTC-USD com TUPY3 e ELET3 √© moderada e mesmo apresentando este valores num√©ricos, o Bitcoin √© um ativo negociado em escala global e √© de um setor totalmente diferente das outras a√ß√µes.</p>
<div class="w3-panel w3-pale-red w3-border">
<p><i class="fa fa-exclamation-triangle" aria-hidden="true"></i> <strong>AVISO</strong>: Essa an√°lise meramente num√©rica n√£o √© o suficiente para detectar uma rela√ß√£o de causa, pois correla√ß√£o n√£o implica causalidade. Isto que dizer que poda haver uma causa em comum para ambas ou que seja uma <a href="https://pt.wikipedia.org/wiki/Regress%C3%A3o_esp%C3%BAria">correla√ß√£o esp√∫ria</a>. Quando se trata da bolsa de valores √© necess√°rio tamb√©m conhecer um pouco sobre a situa√ß√£o e hist√≥ria da empresa na qual se investe.</p>
</div>
</div>
<div id="forecast-com-prophet-do-facebook" class="section level4">
<h4>Forecast com Prophet do Facebook</h4>
<p>Poder√≠amos utilizar uma s√©rie de modelos estat√≠sticos, econom√©tricos e de machine learning utilizando tanto as fun√ß√µes nativas do R base ou pacote forecast quanto as fun√ß√µes desenvolvidas para trabalhar de maneira ‚Äúarrumada‚Äù com tsibble mas resolvi fazer uma abordagem diferente neste post escolhendo o modelo disponibilizado pelo Facebook.</p>
<div class="row">
<div class="column">
<p></br>
O <a href="https://facebook.github.io/prophet/">Prophet</a> √© um software de c√≥digo aberto disponibilizado pela equipe de <a href="https://research.fb.com/category/data-science/">Data Science do Facebook</a> que fornece um procedimento para realiza√ß√£o de previs√µes de dados de s√©ries temporais.</p>
</div>
<div class="column">
<p><img src="/post/2020-03-25-investment-alert/prophet_logo.png" style="width:80.0%" />
</br><small>Fonte: <a href="https://facebook.github.io/prophet/" class="uri">https://facebook.github.io/prophet/</a></small></p>
</div>
</div>
<p>Segundo a <a href="https://facebook.github.io/prophet/">documenta√ß√£o oficial</a>, (em tradu√ß√£o livre):</p>
<blockquote>
<p>O Prophet tem como ‚Äúbase em um modelo aditivo no qual tend√™ncias n√£o lineares se ajustam √† sazonalidade anual, semanal e di√°ria, al√©m de efeitos de f√©rias. Funciona melhor com s√©ries temporais que t√™m fortes efeitos sazonais e v√°rias temporadas de dados hist√≥ricos. O Profeta √© robusto para a falta de dados e mudan√ßas na tend√™ncia, e geralmente lida bem com outliers‚Äù.</p>
</blockquote>
<p>Este modelo me pareceu uma boa op√ß√£o para exemplificar a etapa da modelagem de s√©ries temporais deste post. Vamos ver o que o modelo do Facebook tem a nos dizer sobre o futuro das nossas a√ß√µes.</p>
<div id="divis√£o-entre-treino-e-teste-em-s√©ries-temporais" class="section level5">
<h5>Divis√£o entre treino e teste em s√©ries temporais</h5>
<p>Assim como em uma tarefa de machine learning que n√£o envolvem dados temporais, no caso de s√©ries temporais, quando desejamos avaliar o ajuste do nosso modelo tamb√©m dividimos o dataset em treino e teste por√©m utilizamos a data como √≠ndice.</p>
<p>Veja como ser√£o divididas nossas s√©ries hist√≥ricas:</p>
<ul>
<li>treino: inicio da s√©rie at√© 18/11/20;</li>
<li>teste: de 18/11/2020 at√© o final da s√©rie hist√≥rica (2 meses atr√°s)</li>
</ul>
<pre class="r"><code>h = 30 * 2
data_split &lt;- Sys.Date() - h

tbl_stocks_train &lt;-  
  tbl_stocks %&gt;% 
  filter(date &lt;= data_split) %&gt;% 
  select(symbol, date, close)

tbl_stocks_test &lt;- 
  tbl_stocks %&gt;%
  filter(date &gt; data_split) %&gt;% 
  select(symbol, date, close)</code></pre>
<p>Ap√≥s dividir os dados em treino e teste j√° estamos habilitados √† utilizar o modelo. Note que, por default, o modelo espera duas colunas nomeadas como <code>ds</code>: data da s√©rie e <code>y</code>: vari√°vel target da s√©rie.</p>
<p>Al√©m disso faremos uma previs√£o 6 meses a frente dos dados de teste para entender qual a tend√™ncia o modelo estaria adotando.</p>
<pre class="r"><code>library(prophet)

prophet_results &lt;- 
  tbl_stocks_train %&gt;% 
  rename(ds = date, y = close) %&gt;% 
  nest(data = c(ds, y)) %&gt;% 
  mutate(pmodel = map(data, ~ prophet(.x, daily.seasonality=TRUE))
  )%&gt;% 
  mutate(pprediction = map(pmodel, ~.x %&gt;%  
                             make_future_dataframe(periods = h + 30*6) %&gt;%
                             predict(.x,.))) </code></pre>
<p>Veja os resultados do ajuste do modelo:</p>
<pre class="r"><code>library(patchwork)

pmap(list(
  x = prophet_results$pprediction,
  y = split(tbl_stocks_train, tbl_stocks_train$symbol),
  z = split(tbl_stocks_test, tbl_stocks_test$symbol),
  w = prophet_results$symbol
),function(x, y, z, w){
  x %&gt;% 
    as_tibble() %&gt;%
    mutate(ds=as_date(ds)) %&gt;%
    select(ds, trend, yhat, yhat_lower, yhat_upper) %&gt;% 
    ggplot() + 
    geom_line(aes(x=ds, y=yhat, color=&quot;blue&quot;), show.legend = F) +
    geom_line(data=y, aes(x=date, y=close, color=&quot;black&quot;), show.legend = F) +
    geom_line(data=z, aes(x=date, y=close, color=&quot;red&quot;), show.legend = T) +
    geom_ribbon(aes(x=ds, ymin=yhat_lower, ymax=yhat_upper), alpha=0.2)  +
    scale_x_date(#limits = c(as.Date(&quot;2018-06-01&quot;), Sys.Date() + 30*12), 
      date_breaks = &quot;6 month&quot;, date_labels = &quot;%m/%Y&quot;) +
    theme_bw()+
    labs(y = w, x = &quot;&quot;, caption = &quot;gomesfellipe.github.io&quot;)+
    scale_colour_manual(values=c(&quot;black&quot;,&quot;blue&quot;,&quot;red&quot;), name=&quot;&quot;, labels=c(&quot;treino&quot;,&quot;modelo&quot;,&quot;teste&quot;))+
    theme(legend.position = c(1-0.8,1-0.2), 
          legend.background = element_rect(fill=alpha(&#39;lightgrey&#39;, 0.2)),
          legend.direction = &quot;horizontal&quot;)
}
) %&gt;% {.[[1]] / .[[2]] / .[[3]]}

ggsave(&quot;prophet.png&quot;) # salvar para o bot retornar este resultado!</code></pre>
<center>
<img src="/post/2020-03-25-investment-alert/unnamed-chunk-15-1.png" style="width:80.0%" />
</center>
<p>Parece interessante..</p>
<p>Veja que a linha azul (modelo ajustado) se ajusta bem √† linha preta (dados de treino) acompanhando a s√©rie hist√≥rica e captando algumas tend√™ncias n√£o lineares.</p>
<p>Por√©m note que a linha azul se perde completamente da linha vermelha (dados de teste) no in√≠cio de 2020 e acho isso muito razo√°vel pois dificilmente algum modelo iria prever os efeitos de uma Pandemia utilizando apenas a s√©rie hist√≥rica do ativo.</p>
<p>Note ainda que a linha azul se estende at√© o final de 2020 (previs√µes para os pr√≥ximos 6 meses) o que sugere que a s√©rie possu√≠a esta tendencia positiva ao longo dos anos, segundo o modelo Prophet .</p>
<div class="w3-panel w3-pale-yellow w3-border">
<p><i class="fa fa-exclamation-triangle" aria-hidden="true"></i> <strong>Aviso</strong>: Existem v√°rias maneiras de estudar e melhorar a qualidade do ajuste deste modelo mas como o objetivo deste post n√£o √© este deixo como aviso para o leitor.</p>
</div>
<p>Com a interpreta√ß√£o dos resultados conclu√≠da.. vamos √†s compras!</p>
</div>
</div>
</div>
</div>
</div>
<div id="comprando-a√ß√µes" class="section level1">
<h1>Comprando a√ß√µes</h1>
<p>Ap√≥s toda essa exemplifica√ß√£o de como podem ser feitas as an√°lises para a elabora√ß√£o da carteira chegou a hora das compras.</p>
<p>Suponha que tiv√©ssemos realizado nossas compra no fechamento do dia <strong>2018-12-01</strong>, quando os valores de <font color="blue"><strong>fechamento</strong></font> das cota√ß√µes eram as seguintes:</p>
<pre class="r"><code>tbl_stocks %&gt;% 
  filter(date == &quot;2018-12-01&quot;) %&gt;% 
  mutate(close = cell_spec(moeda_real(close), &quot;html&quot;, color = &quot;blue&quot;, bold = T)) %&gt;% 
  mutate_at(c(3:5, 8), ~moeda_real(.x)) %&gt;% 
  kable2()</code></pre>
<center>
<img src="/post/2020-03-25-investment-alert/img2.png" style="width:80.0%" />
</center>
<p>Neste dia a cota√ß√£o para ELET3 era R$24,42 e TUPY3 era R$19,73 e suponha que tenhamos comprado 200 lotes de ELET3 e 150 fracion√°rios de TUPY, totalizando R$7.843,64 (pr√≥ximo ao que tinhaamos planejado no inicio do estudo)</p>
<p>Vamos guardar estes valores:</p>
<pre class="r"><code>cot_inicio = filter(tbl_stocks, date == &quot;2018-12-01&quot;, symbol != &quot;BTC-USD&quot;) %&gt;% pull(close)
qtd_inicio = c(elet = 200, tupy = 150)</code></pre>
<p>Note que o valor do Bitcoin est√° em d√≥lares, para obter o valor em reais (R$) daquele dia vamos utilizar a <a href="https://www.mercadobitcoin.net/api/BTC/day-summary/2020/01/09/">API do Mercado Bitcoin</a>. Como n√£o existe nenhum pacote que forne√ßa estes dados diretamente no R, a requisi√ß√£o ser√° feita normalmente via API com o pacote <code>jsonlite</code>:</p>
<pre class="r"><code>library(jsonlite) # requisicao de api
url &lt;- glue::glue(&quot;https://www.mercadobitcoin.net/api/BTC/day-summary/2019/12/01/&quot;)
safe_fromJSON &lt;- purrr:::safely(fromJSON, as.numeric(NA)) 
consulta &lt;- safe_fromJSON(url)$result %&gt;% map_dfc(~.x)

consulta %&gt;%
  select(-date) %&gt;% 
  mutate_all(~moeda_real(.x)) %&gt;%
  mutate(closing = cell_spec(closing, &quot;html&quot;, color = &quot;blue&quot;)) %&gt;% 
  kable2()</code></pre>
<center>
<img src="/post/2020-03-25-investment-alert/img3.png" style="width:80.0%" />
</center>
<p>O pre√ßo de fechamento foi de R$31.747,38 e suponhamos que tenha sido este o valor pago no dia. (Parece que neste dia o d√≥lar estava em torno de R$4,03)</p>
<p>Para completar esta carteira fict√≠cia vamos adquirir 0,0032 do valor de um Bitcoin</p>
<pre class="r"><code>cot_inicio[3] &lt;- consulta$closing
qtd_inicio[3] &lt;- 0.032</code></pre>
<p>Portanto, ao valor de R$31.747,38 compramos 0.032 Bitcoin totalizando R$1.015,92 completando nossa carteira.</p>
<div id="tabela-financeira" class="section level2">
<h2>Tabela financeira</h2>
<p>Semelhante a uma planilha financeira, criaremos uma tabela financeira automatizada que receber√° como input os valores da montagem e calcular√° automaticamente os valores do desmontagem no tempo atual utilizando dados de APIs abertas.</p>
<blockquote>
‚ÄúAquilo que n√£o se pode medir, n√£o se pode melhorar‚Äù.
<div align="right">
<font size="1">F√≠sico irland√™s William Thomson</font>
</div>
</blockquote>
<p>Primeiro √© necess√°rio obter os valores mais recentes das cota√ß√µes das acoes que compramos na bolsa e para isto ser√° necess√°rio utilizar outro pacote pois o <code>tidyquant</code> s√≥ fornece os dados em frequ√™ncia di√°ria.</p>
<p>Utilizaremos portanto, o pacote <a href="https://www.business-science.io/code-tools/2017/09/03/alphavantager-0-1-0.html">alphavantager</a> que fornece dados de finan√ßas da API gratuita <a href="https://www.alphavantage.co/">Alpha Vantage</a> no formato arrumados e tamb√©m foi desenvolvida pela <a href="https://github.com/business-science">Business Science</a> (mesmo criados do pacote <code>tidyquant</code>).</p>
<div class="w3-panel w3-pale-yellow w3-border">
<p><i class="fa fa-exclamation-triangle" aria-hidden="true"></i> <strong>Aviso</strong>: Na <a href="https://www.alphavantage.co/support/#api-key">documenta√ß√£o da api do Alphavantage</a> recomenda-se a solicita√ß√µes de API com modera√ß√£o, suportando at√© 5 solicita√ß√µes de API por minuto e 500 solicita√ß√µes por dia para obter o melhor desempenho no servidor. Caso deseje segmentar um volume maior de chamadas da API, confira a <a href="https://www.alphavantage.co/premium/">associa√ß√£o premium.</a>.</p>
</div>
<p>J√° para a coleta da cota√ß√£o em tempo real do Bitcoin em reais (R$), utilizaremos novamente a <a href="https://www.mercadobitcoin.com.br/api-doc/?">api do mercado bitcoin</a>.</p>
<pre class="r"><code># Importar dados da bolsa de valores ==================================

library(alphavantager) # api streaming bovespa

AV_API_KEY = Sys.getenv(&quot;AV_API_KEY&quot;)
av_api_key(AV_API_KEY)

consulta_acoes &lt;- map_df(portifolio[1:2], ~{
  alphavantager::av_get(symbol = .x,
         av_fun = &quot;TIME_SERIES_INTRADAY&quot;,
         interval = &quot;1min&quot;,  # &quot;1min&quot;, &quot;5min&quot;, &quot;15min&quot;, &quot;30min&quot; ou &quot;60min&quot;
         outputsize = &quot;compact&quot;) %&gt;%  # &quot;full&quot;
    bind_cols(stock = rep(.x, nrow(.)))
})

# Impotar dados do bitcoin ============================================

coin &lt;- &quot;BTC&quot;
method &lt;- &quot;ticker&quot;
url &lt;- glue::glue(&quot;https://www.mercadobitcoin.net/api/{coin}/{method}/&quot;)

safe_fromJSON &lt;- safely(fromJSON, as.numeric(NA)) 
consulta_bitcoin &lt;- 
  safe_fromJSON(url)$result$ticker %&gt;% 
  as_tibble() %&gt;% 
  transmute(timestamp = lubridate::ymd_hms(as.POSIXct(date, origin=&quot;1970-01-01&quot;)),
            open, high, low, close = sell, volume = NA, stock = &quot;BTC.BR&quot;) %&gt;% 
  mutate_at(c(&#39;open&#39;, &#39;high&#39;, &#39;low&#39;, &#39;close&#39;), ~as.numeric(.x))

# Combinar resultados das consultas ==================================

consulta_atual &lt;- 
  bind_rows(
    consulta_acoes %&gt;% 
      group_by(stock) %&gt;% 
      filter(timestamp == last(timestamp)),
    consulta_bitcoin
  ) 

# Salvar consulta ====================================================
saveRDS(consulta_atual, &quot;consulta_atual.rds&quot;)</code></pre>
<p>Resultados da consulta atual (ap√≥s combinar a requisi√ß√£o da bolsa de valores e do Mercado Bitcoin):</p>
<pre class="r"><code>consulta_atual  %&gt;%
  mutate_at(2:4, ~moeda_real(.x)) %&gt;% 
  mutate_if(is.numeric, ~ifelse(is.na(.x), &quot;-&quot;, format(.x, digits = 2))) %&gt;% 
  mutate(close = cell_spec(moeda_real(close), &quot;html&quot;, color = &quot;blue&quot;)) %&gt;% 
  kable2()</code></pre>
<center>
<img src="/post/2020-03-25-investment-alert/img4.png" style="width:80.0%" />
</center>
<p>Com os valores da Montagem organizados e os valores da Desmontagem coletados em tempo real j√° podemos construir nossa tabela financeira automatizada.</p>
<p>A tabela cont√©m:</p>
<ul>
<li>Valores de cota√ß√£o e quantidade adquiridas de cada uma no momento da montagem da carteira;</li>
<li>Valores de cada cota√ß√£o no momento atual com suas respectivas quantidades dispon√≠veis;</li>
<li>Resultados de o ganho (ou perda) seguido do resultado bruto caso realize a venda agora;</li>
<li>√öltima coluna indica se vale a pena vender ou n√£o aquela cota√ß√£o considerando que o valor de venda √© superior ao valor de compra.</li>
</ul>
<div class="w3-panel w3-pale-yellow w3-border">
<p><i class="fa fa-exclamation-triangle" aria-hidden="true"></i> <strong>Aviso</strong>: Essa opera√ß√£o de vender o ativo caso o pre√ßo da venda seja maior que o da compra √© apenas um exemplo. Poder√≠amos utilizar diversas estat√≠sticas para determinar o momento da opera√ß√£o por√©m adotei esta apenas para ilustrar o funcionamento da tabela financeira, o limite de op√ß√µes √© a sua criatividade!</p>
</div>
<p>Veja a tabela final com os resultados atualizados em tempo real:</p>
<pre class="r"><code>porcentagem &lt;- function(x){paste0(round(x,2), &quot;%&quot;)} # Funcao auxiliar

# Tabela resultado
financas &lt;- 
  tibble(
    ativo = portifolio,
    cot_inicio = cot_inicio,
    qtd_inicio = qtd_inicio,
    vol_inicio = cot_inicio * qtd_inicio,
    cot_atual = consulta_atual$close,
    qtd_atual = qtd_inicio,
    vol_atual = cot_atual * qtd_atual,
    ganho_perda = vol_atual - vol_inicio,
    resultado_bruto = ganho_perda / vol_inicio * 100
  ) 


tabela &lt;- 
  financas %&gt;% 
  mutate(
    cot_inicio = moeda_real(cot_inicio),
    cot_atual = moeda_real(cot_atual),
    vol_inicio = moeda_real(vol_inicio),
    vol_atual = moeda_real(vol_atual),
    qtd_inicio = round(qtd_inicio,4),
    qtd_atual = round(qtd_atual,4),
    ` ` = ifelse(ganho_perda &gt; 0,&quot;\u2713&quot;, &quot;\u2718&quot;) ,
    cot_atual = cell_spec(cot_atual, &quot;html&quot;, color = &quot;blue&quot;),
    ganho_perda = cell_spec(moeda_real(ganho_perda), &quot;html&quot;,
                            color = ifelse(ganho_perda &gt; 0, 
                                           &quot;green&quot;, &quot;red&quot;)),
    resultado_bruto = cell_spec(porcentagem(resultado_bruto), &quot;html&quot;,
                                color = ifelse(resultado_bruto &gt; 0, 
                                               &quot;green&quot;, &quot;red&quot;))) %&gt;% 
  `colnames&lt;-`(stringr::str_replace_all(colnames(.), &quot;(_|[[:space:]])&quot;, &quot;\n&quot;)) %&gt;% 
  # Exibicao
  kable(format = &quot;html&quot;, escape = F) %&gt;%
  kable_styling(c(&quot;striped&quot;, &quot;bordered&quot;, &quot;hover&quot;, &quot;responsive&quot;), full_width = T, font_size = 12) %&gt;%
  add_header_above(c(&quot; &quot;, &quot;Montagem&quot; = 3,
                     &quot;Desmontagem / Atual&quot; = 3, &quot;Resultado&quot; = 3))

tabela</code></pre>
<center>
<img src="/post/2020-03-25-investment-alert/img5.png" style="width:80.0%" />
</center>
<p>Agora que j√° possu√≠mos nossa requisi√ß√£o completa e arrumada em tempo real precisamos ter acesso a esta informa√ß√£o de forma din√¢mica tamb√©m e para isso utilizaremos o bot do Telegram.</p>
</div>
</div>
<div id="bot-telegram" class="section level1">
<h1>Bot Telegram</h1>
<p>Depois de muitas decis√µes tomadas enfim chegamos ao bot! Espero que tenha notado que montar uma carteira n√£o √© uma tarefa f√°cil pois envolve exposi√ß√£o ao risco e tamb√©m exige certo acompanhamento do mercado.</p>
<div class="row">
<div class="column8">
<p></br>
Para criar um bot no Telegram basta seguir os passos do <a href="https://github.com/lbraglia/telegram">readme</a> do pacote <a href="https://github.com/lbraglia/telegram"><code>telegram</code></a> disponibilizado no Github ou seguir os passos desse excelente <a href="https://www.curso-r.com/blog/2017-08-19-r-telegram-bitcoin/">post do curso-r</a> que me inspirou a uns anos atras e hoje me auxiliou novamente para criar este bot. Ao concluir a etapa de configura√ß√£o teremos um novo contado no Telegram, o nosso bot!</p>
<p>Com a configura√ß√£o no aplicativo do Telegram conclu√≠da, o primeiro passo para configurar as a√ß√µes do bot no R √© iniciar um objeto TGBot declarando o id do seu bot. No meu caso o bot se chama <em>Stocks</em> e o id √© <em>fgstockbot</em>.</p>
</div>
<div class="column4">
</br>
<center>
<img src="/post/2020-03-25-investment-alert/bot_telegram.png" style="width:99.0%" />
</center>
</div>
</div>
<p>Com o R conectado ao bot do Telegram j√° somos capazes de criar um conjunto de regras de forma que o bot nos retorne as informa√ß√µes que desejamos.</p>
<p>Desenvolveremos a fun√ß√£o <code>report_stocks()</code> que programa o bot para realizar o seguinte algoritmo:</p>
<ol start="0" style="list-style-type: decimal">
<li>Carregar depend√™ncias e conectar chaves de acesso</li>
<li>Conferir se a bolsa de valores esta aberta</li>
<li>Requisi√ß√£o das cota√ß√µes da bolsa de valores em <em>real-time</em></li>
<li>Requisi√ß√£o da cota√ß√£o do Bitcoin em <em>real-time</em></li>
<li>Combinar resultados</li>
<li>Inserir resultados da coleta na tabela financeira</li>
<li>Se o valor de algum volume atual seja maior que o volume inicial:
<ul>
<li>Calcular valores de desmontagem</li>
<li>Preparar layout da tabela financeira</li>
<li>Salvar resultados</li>
<li>Enviar via Telegram</li>
</ul></li>
<li>Aguardar 20 minutos para a pr√≥xima requisi√ß√£o</li>
<li>Repetir todo o processo</li>
</ol>
<p>Parece complicado mas √© tranquilo pois todos os c√≥digos de cada uma destas tarefas j√° foram desenvolvidos nas se√ß√µes anteriores e ser√£o apenas combinados. A fun√ß√£o <a href="https://gist.github.com/gomesfellipe/357af0735d2aedca60146a7655e33929"><code>report_stocks()</code> j√° esta dispon√≠vel no github</a>, veja a baixo:</p>
<p>(A frequ√™ncia adotata como default pela fun√ß√£o tenta fazer a requisi√ß√£o com a maior frequ√™ncia poss√≠vel na api do Alphavantage)</p>
<script src="https://gist.github.com/gomesfellipe/357af0735d2aedca60146a7655e33929.js"></script>
<p>Ap√≥s todas as devidas configura√ß√µes, basta carregar a fun√ß√£o e executar para obter o seguinte resultado:</p>
<pre class="r"><code># https://gist.github.com/gomesfellipe/357af0735d2aedca60146a7655e33929
devtools::source_gist(&quot;357af0735d2aedca60146a7655e33929&quot;,quiet = T)

# Executar
bot_report_stocks(portifolio = portifolio, 
                  cot_inicio = cot_inicio,
                  qtd_inicio = qtd_inicio)</code></pre>
<center>
<img src="/post/2020-03-25-investment-alert/report_stocks.gif" style="width:70.0%" />
</center>
<p></br>
E assim obtemos um feedback atrav√©s do nosso Smartphone ou Smartwatch em tempo real sobre o desempenho da nossa carteira!</p>
</div>
<div id="conclus√£o-e-pr√≥ximos-passos" class="section level1">
<h1>Conclus√£o e pr√≥ximos passos</h1>
<p>Neste post criamos um bot que coleta os dados e faz an√°lises disponibilizando-as em tempo real no Telegram. Por√©m vimos tamb√©m o qu√£o dif√≠cil pode ser a montagem de uma carteira e as an√°lises envolvendo s√©ries hist√≥ricas de ativos.</p>
<p>Diante dos resultados obtidos aqui existe uma grande gama de op√ß√µes de inova√ß√µes para trabalhos futuros como:</p>
<ul>
<li>Programar o bot para responder a uma <a href="https://support.apple.com/pt-br/guide/watch/apd92a90f882/watchos">menssagem r√°pida no smartwatch</a> com o valor de uma previs√£o;</li>
<li>Desenvolver um Shiny parametrizando o bot para consultar an√°lises em tempo real;</li>
<li>Hospedar a rotina em um servidor para operacionalizar o bot (caso tenha d√∫vidas de como se iniciar um RStudio Server u um Shiny Server <a href="https://gomesfellipe.github.io/post/2018-10-27-server-cloud/server-cloud/">consulte este post do blog</a>);</li>
<li>Criar uma API com <a href="https://www.rplumber.io">plumber</a> para fornecer os resultados;</li>
<li>Treinar modelo utilizando mais dados, mais vari√°veis explicativas e tuning dos par√¢metros;</li>
<li>Criar pacote com um rob√¥ mais geral para responder diferentes consultas.</li>
</ul>
<p>Espero que tenha gostado qualquer d√∫vida deixe nos coment√°rios!</p>
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias</h1>
<ul>
<li><a href="https://www.infomoney.com.br/minhas-financas/brasileiros-nao-sabem-a-diferenca-entre-poupar-e-investir-afirma-especialista-2/" class="uri">https://www.infomoney.com.br/minhas-financas/brasileiros-nao-sabem-a-diferenca-entre-poupar-e-investir-afirma-especialista-2/</a></li>
<li><a href="https://www.btgpactualdigital.com/blog/investimentos/diversificacao-de-investimentos" class="uri">https://www.btgpactualdigital.com/blog/investimentos/diversificacao-de-investimentos</a></li>
<li><a href="https://www.btgpactualdigital.com/blog/coluna-gustavo-cerbasi/defina-sua-estrategia-entre-renda-fixa-ou-variavel" class="uri">https://www.btgpactualdigital.com/blog/coluna-gustavo-cerbasi/defina-sua-estrategia-entre-renda-fixa-ou-variavel</a></li>
<li><a href="https://blog.earo.me/2018/12/20/reintro-tsibble/" class="uri">https://blog.earo.me/2018/12/20/reintro-tsibble/</a></li>
<li><a href="https://www.tradingcomdados.com/post/2017/07/09/estudo-de-correla%C3%A7%C3%A3o-entre-a%C3%A7%C3%B5es-da-bolsa-de-valores-de-s%C3%A3o-paulo" class="uri">https://www.tradingcomdados.com/post/2017/07/09/estudo-de-correla%C3%A7%C3%A3o-entre-a%C3%A7%C3%B5es-da-bolsa-de-valores-de-s%C3%A3o-paulo</a></li>
<li><a href="https://www.business-science.io/code-tools/2017/10/28/demo_week_h2o.html" class="uri">https://www.business-science.io/code-tools/2017/10/28/demo_week_h2o.html</a></li>
<li><a href="https://www.infomoney.com.br/mercados/adolescente-fica-milionario-aos-18-anos-usando-bitcoins-apos-fazer-aposta-com-os-pais/" class="uri">https://www.infomoney.com.br/mercados/adolescente-fica-milionario-aos-18-anos-usando-bitcoins-apos-fazer-aposta-com-os-pais/</a></li>
<li><a href="https://www.infomoney.com.br/onde-investir/halving-conheca-o-processo-que-pode-levar-o-bitcoin-a-uma-nova-explosao-de-preco/" class="uri">https://www.infomoney.com.br/onde-investir/halving-conheca-o-processo-que-pode-levar-o-bitcoin-a-uma-nova-explosao-de-preco/</a></li>
<li><a href="https://cran.r-project.org/web/packages/telegram/README.html" class="uri">https://cran.r-project.org/web/packages/telegram/README.html</a></li>
<li><a href="https://otexts.com/fpp2/" class="uri">https://otexts.com/fpp2/</a>
<a href="https://www.curso-r.com/blog/2017-08-19-r-telegram-bitcoin/" class="uri">https://www.curso-r.com/blog/2017-08-19-r-telegram-bitcoin/</a></li>
<li><a href="https://www.curso-r.com/blog/2017-08-19-r-telegram-bitcoin/" class="uri">https://www.curso-r.com/blog/2017-08-19-r-telegram-bitcoin/</a></li>
</ul>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2020-03-25-investment-alert/investment-alert/">Desenvolva um bot e receba resultados de Machine Learning no seu Smartphone para ajudar nos investimentos</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">bitcoin</category>
      <category domain="tag">bolsa-de-valores</category>
      <category domain="tag">correlacao</category>
      <category domain="tag">dplyr</category>
      <category domain="tag">estatistica</category>
      <category domain="tag">machine-learning</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">prophet</category>
      <category domain="tag">r</category>
      <category domain="tag">reports</category>
    </item>
    <item>
      <title>Como automatizar relat√≥rios longos e repetitivos com RMarkdown</title>
      <link>https://gomesfellipe.github.io/post/2019-09-13-relatorios-automaticos-com-rmarkdown/relatorios-automaticos-com-rmarkdown/</link>
      <pubDate>Fri, 13 Sep 2019 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2019-09-13-relatorios-automaticos-com-rmarkdown/relatorios-automaticos-com-rmarkdown/</guid>
      <description>Veja como fazer um relat√≥rio estat√≠stico &#34;extenso e repetitivo&#34; sem utilizar copiar e colar nenhuma vez</description>
      <content:encoded>&lt;![CDATA[
        


<div id="problema-de-neg√≥cio" class="section level1">
<h1>Problema de neg√≥cio</h1>
<p>Uma tarefa comum no dia a dia de um estat√≠stico (ou cientista de dados) √© a elabora√ß√£o de relat√≥rios para passsar ao restante da equipe e/ou tomadores de decis√£o os resultados encontrados e muitas vezes essa tarefa pode parecer desgastante quando os relat√≥rios s√£o muitos extensos e repetitivos.</p>
<p>Com a linguagem R, escrever relat√≥rios estat√≠sticos utilizando <a href="https://rmarkdown.rstudio.com/">RMarkdown</a> acaba sendo a escolha padr√£o por ser t√£o simples transformar as an√°lises em documentos, apresenta√ß√µes e dashboards de alta qualidade com poucas linhas de c√≥digo.</p>
<p>Assim, combinando conceitos de programa√ß√£o, como o <a href="https://pt.wikipedia.org/wiki/Loop_(programa%C3%A7%C3%A3o)">Loop</a> no R e a linguagem <a href="https://pt.wikipedia.org/wiki/Markdown">Markdown</a> para produ√ß√£o de relat√≥rios, temos uma poderosa ferramenta para <a href="https://pt.wikipedia.org/wiki/Automa%C3%A7%C3%A3o">Automa√ß√£o</a> de relat√≥rios.</p>
<div id="entendendo-o-problema" class="section level2">
<h2>Entendendo o problema</h2>
<p>Suponha que o seguinte gr√°fico seja apresentado √† voc√™:</p>
<center>
<img src="/post/2019-09-13-relatorios-automaticos-com-rmarkdown/figure-html/unnamed-chunk-1-1.gif" style="width:80.0%" />
</center>
<p>Este gr√°fico animado apresenta a evolu√ß√£o da esperan√ßa de vida ao nascer (em anos) em rela√ß√£o ao PIB per capita (em US$, ajustado pela infla√ß√£o) de 141 pa√≠ses dos 5 continentes durante o per√≠odo de 1952 at√© 2007, a cada 5 anos.</p>
<p>Entraremos em mais detalhes sobre as informa√ß√µes dete gr√°fico a seguir.</p>
</div>
</div>
<div id="fonte-dos-dados" class="section level1">
<h1>Fonte dos dados</h1>
<p>Os dados utilizados neste problema foram importados atrav√©s do pacote <a href="https://cran.r-project.org/web/packages/gapminder/index.html">gapminder</a> que √© um projeto que utiliza dados do site <a href="https://www.gapminder.org/">Gapminder.org</a>.</p>
<p>Segundo sua <a href="https://www.gapminder.org/about-gapminder/">descri√ß√£o no site</a>:</p>
<blockquote>
<p>‚ÄúGapminder √© uma funda√ß√£o independente sueca sem afilia√ß√µes pol√≠ticas, religiosas ou econ√¥micas. (‚Ä¶)‚Äù</p>
</blockquote>
<p>No site √© poss√≠vel obter dados gratuitos para se obter estat√≠sticas confi√°veis e al√©m dos disso a Funda√ß√£o Gapminder apresenta alguns outros projetos como o <a href="https://www.gapminder.org/dollar-street/matrix">Dollar Street</a> que apresenta 30.000 fotos de 264 fam√≠lias em 50 pa√≠ses classificados por renda.</p>
<p>Na p√°gina do projeto √© poss√≠vel ver e comparar os mais variados aspectos da popula√ß√£o ao redor do mundo que v√£o desde casas, itens mais amados, carros at√© banheiros, comida de pets e bebidas alco√≥licas.</p>
<p>O pacote fornece dados da Funda√ß√£o Gapminder como: valores de expectativa de vida, PIB per capta e popula√ß√£o, a cada cinco anos, de 1952 a 2007 (total de 12 anos). Veja as primeiras 5 linhas da base de dados contidos no pacote:</p>
<pre class="r"><code># Base de dados utilizada
head(gapminder)</code></pre>
<pre><code>## # A tibble: 6 x 6
##   country     continent  year lifeExp      pop gdpPercap
##   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;
## 1 Afghanistan Asia       1952    28.8  8425333      779.
## 2 Afghanistan Asia       1957    30.3  9240934      821.
## 3 Afghanistan Asia       1962    32.0 10267083      853.
## 4 Afghanistan Asia       1967    34.0 11537966      836.
## 5 Afghanistan Asia       1972    36.1 13079460      740.
## 6 Afghanistan Asia       1977    38.4 14880372      786.</code></pre>
<p>Essa base de dados possui 1705 linhas de 6 vari√°veis, onde:</p>
<ul>
<li><code>country</code>: factor com 142 levels</li>
<li><code>continent</code>: factor com 5 levels</li>
<li><code>year</code>: sequencia de 1952 at√© 2007 a cada 5 anos</li>
<li><code>lifeExp</code>: esperan√ßa de vida ao nascer, em anos</li>
<li><code>pop</code>: popula√ß√£o</li>
<li><code>gdpPercap</code>: PIB per capita (em US$, ajustado pela infla√ß√£o)</li>
</ul>
</div>
<div id="comportamento-geral-dos-dados" class="section level1">
<h1>Comportamento geral dos dados</h1>
<p>Antes de come√ßar a fazer os relat√≥rios para cada ano, vamos reproduzir a anima√ß√£o apresentada para n√≥s com o comportamento temporal utilizando o pacote <a href="https://github.com/thomasp85/gganimate">gganimate</a>:</p>
<pre class="r"><code># Carregar pacotes
library(ggplot2)
library(dplyr)
library(gapminder)
library(scales)
library(gganimate)

# Definir tema:
theme_set(theme_bw())

# Funcao para customizar legendas:
custom_legend &lt;- function(x){comma(x, big.mark = &quot;.&quot;,decimal.mark = &quot;,&quot;)}

# Comportamento geral:
gapminder %&gt;% 
  filter(country!=&quot;Kuwait&quot;) %&gt;% # remover 1 pais outlier
  ggplot(aes(x = gdpPercap, y = lifeExp, size = pop, 
             label = country, color = continent, alpha= log(gdpPercap))) %+%
  geom_point(show.legend = F) %+%
  geom_text(show.legend = F, size = 3, nudge_y = -0.7) %+%
  scale_size_continuous(labels = custom_legend) %+%
  scale_x_continuous(labels = custom_legend) %+%
  geom_smooth(se=F, color = &quot;black&quot;, show.legend = F, method = &quot;lm&quot;) %+% 
  transition_time(year) %+%
  scale_color_brewer(palette = &quot;Dark2&quot;) %+%
  labs(title = &quot;Year: {frame_time}&quot;)</code></pre>
<center>
<img src="/post/2019-09-13-relatorios-automaticos-com-rmarkdown/unnamed-chunk-1-1.gif" style="width:80.0%" />
</center>
<p>Analisando esta anima√ß√£o √© poss√≠vel notar:</p>
<ul>
<li>Jap√£o √© o pa√≠s que possui a maior expectativa de vida ao longo de todos os anos;</li>
<li>Os pa√≠ses do cont√≠nente africano s√£o os que apresentam expectativa de vida mais baixa e pior <code>gdpPercap</code>.</li>
<li>A Ar√°bia Saudita teve sua <code>gdpPercap</code> aumentada at√© 1978 por√©m a partir da√≠ diminiu bastante.</li>
<li>O pa√≠s com maior <code>gdpPercap</code> e expectativa de vida na Am√©rica √© o Estados Unidos;</li>
<li>A Noroega foi o pa√≠s que mais se descatou com os valores mais elevados e est√°veis ao longo destes 55 anos.</li>
</ul>
<p>Obs[1]: <a href="https://www.google.com/search?q=Kuwait&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=0ahUKEwj53ZXJ4bPjAhVAD7kGHQvwCWgQ_AUIESgC&amp;biw=1574&amp;bih=943">Kuwait</a> foi removida para este gr√°fico animado pois √© um pa√≠s outlier. Segundo o <a href="https://pt.wikipedia.org/wiki/Kuwait">Wikip√©dia</a>:</p>
<blockquote>
<p>‚ÄúO Kuwait tem um PIB (PPC) de US$ 167,9 bilh√µes[96] e uma renda per capita de US$ 81 800,[96] o que o torna o quinto pa√≠s mais rico do mundo.[52] O √≠ndice de desenvolvimento humano (IDH) do Kuwait √© de 0,816, um dos mais elevados do Oriente M√©dio e do mundo √°rabe. Com uma taxa de crescimento do PIB de 5,7%, o Kuwait tem uma das economias que mais crescem na regi√£o.[96]‚Äù</p>
</blockquote>
<p>Para quem tiver curiosidade, os dados de <code>Kuwait</code> podem ser obtidos da seguinte forma:</p>
<pre class="r"><code>gapminder %&gt;% filter(country == &quot;Kuwait&quot;)</code></pre>
<pre><code>## # A tibble: 12 x 6
##    country continent  year lifeExp     pop gdpPercap
##    &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;     &lt;dbl&gt;
##  1 Kuwait  Asia       1952    55.6  160000   108382.
##  2 Kuwait  Asia       1957    58.0  212846   113523.
##  3 Kuwait  Asia       1962    60.5  358266    95458.
##  4 Kuwait  Asia       1967    64.6  575003    80895.
##  5 Kuwait  Asia       1972    67.7  841934   109348.
##  6 Kuwait  Asia       1977    69.3 1140357    59265.
##  7 Kuwait  Asia       1982    71.3 1497494    31354.
##  8 Kuwait  Asia       1987    74.2 1891487    28118.
##  9 Kuwait  Asia       1992    75.2 1418095    34933.
## 10 Kuwait  Asia       1997    76.2 1765345    40301.
## 11 Kuwait  Asia       2002    76.9 2111561    35110.
## 12 Kuwait  Asia       2007    77.6 2505559    47307.</code></pre>
</div>
<div id="resolvendo-o-problema-de-neg√≥cio" class="section level1">
<h1>Resolvendo o problema de neg√≥cio</h1>
<p>Para resolver o problema de se fazer uma an√°lise sobre a expectativa de vida, PIB per capta e popula√ß√£o, para cada continente, para cada ano dispon√≠vel, (ou seja, analisar de 1952 a 2007 a cada cinco anos) faremos um total de 12 relat√≥rios.</p>
<p>Isso √© muito para se arriscar usar <code>ctrl+c</code> e <code>ctrl+v</code> 12 vezes e depois caso precise de alguma mudan√ßa, alterar o relat√≥rio 12 vezes.</p>
<p>Portanto utilizaremos uma estrat√©gia parecida com a que apresentei no √∫ltimo post sobre como <a href="https://gomesfellipe.github.io/post/2019-04-05-split-apply-combine/split-apply-combine/">Hackear o R com a estrat√©cia Split-Appy-Combine</a>.</p>
<p>Primeiramente vamos separar nosso dataset por ano utilizando a fun√ß√£o <code>tidyr::nest()</code>:</p>
<pre class="r"><code>library(tidyr) # funcao nest

# separar por ano:
nested_gapminder &lt;- gapminder %&gt;% nest(-year)</code></pre>
<p>Selecionei um dos anos como exemplo e utilizei os objetos <code>nested_gapminder$year[1]</code> e <code>nested_gapminder$data[[1]]</code> para desenvolver uma fun√ß√£o que realizasse todas as an√°lises que eu precisasse.</p>
<p>Essa fun√ß√£o foi salva em um script separado chamado <code>analise.R</code> e pode ser encontrada <a href="">neste link</a>. Para caregar a fun√ß√£o localmente basta utilizar a fun√ß√£o <code>source()</code>, veja;</p>
<pre class="r"><code>source(&quot;analise_gapminder.R&quot;)</code></pre>
<p>Veja nas se√ß√µes a seguir os outputs da fun√ß√£o antes de encapsul√°-la em um arquivo RMarkdown (.Rmd) para fazer o looping:</p>
<div id="resultados-para-o-ano-2007" class="section level2">
<h2>Resultados para o ano 2007</h2>
<p>A seguir vamos criar o objeto <code>x</code> que ser√° o data set referente ao ano <code>title</code>. Em seguida vamos aplicar a fun√ß√£o carregada anteriormente para obter os resultados das an√°lises e salvar no objeto <code>resutls</code></p>
<pre class="r"><code>library(magrittr) # pipe %$%

# Obter resultados
x       &lt;- nested_gapminder %&gt;% filter(year == 2007) %&gt;% unnest()
title   &lt;- nested_gapminder %&gt;% filter(year == 2007) %$% year
results &lt;- analise_gapminder(x, title)</code></pre>
<p>Vejamos como o Brasil esta em rela√ß√£o aos outros pa√≠ses com um gr√°fico que resume os resultados do modelo ajustado:</p>
<pre class="r"><code>results$grafico_geral_regressao</code></pre>
<center>
<img src="/post/2019-09-13-relatorios-automaticos-com-rmarkdown/unnamed-chunk-9-1.png" style="width:80.0%" />
</center>
<p>Comportamento dos dados por Continente</p>
<pre class="r"><code>results$grafico_por_continente</code></pre>
<center>
<img src="/post/2019-09-13-relatorios-automaticos-com-rmarkdown/unnamed-chunk-10-1.png" style="width:80.0%" />
</center>
<p>Ap√≥s ajustar o modelo de regress√£o, vamos obter algumas estat√≠sticas descritivas com mais gr√°ficos informativos!</p>
<p>O gr√°fico abaixo apresenta uma <a href="http://www.leg.ufpr.br/lib/exe/fetch.php/projetos:saudavel:loess.pdf">Regress√£o Local (LOESS)</a> com destaque nos pa√≠ses que tiveram <code>gdpPercap</code> e <code>lifeExp</code> acima da m√©dia</p>
<pre class="r"><code>results$grafico_zoom_acima_media</code></pre>
<center>
<img src="/post/2019-09-13-relatorios-automaticos-com-rmarkdown/unnamed-chunk-11-1.png" style="width:80.0%" />
</center>
<p>E agora podemos conferir um gr√°fico que apresenta uma <a href="http://www.leg.ufpr.br/lib/exe/fetch.php/projetos:saudavel:loess.pdf">Regress√£o Local (LOESS)</a> com destaque nos pa√≠ses que tiveram <code>gdpPercap</code> e <code>lifeExp</code> acima da m√©dia</p>
<pre class="r"><code>results$grafico_zoom_abaixo_media</code></pre>
<center>
<img src="/post/2019-09-13-relatorios-automaticos-com-rmarkdown/unnamed-chunk-12-1.png" style="width:80.0%" />
</center>
<p>Maravilha! Muitas informa√ß√µes interessantes mas n√£o resolvemos o problema por inteiro. Resta aplicar as mesmas an√°lises para os demais anos do nosso dataset.</p>
</div>
</div>
<div id="automatizar-as-analises-para-os-pr√≥ximos-anos" class="section level1">
<h1>Automatizar as analises para os pr√≥ximos anos</h1>
<p>A linha a seguir √© a que realiza toda a m√°gica!</p>
<p>A fun√ß√£o <code>knit_child()</code> compila o c√≥digo R e retorna uma sa√≠da pura (Latex, html ou word sem c√≥digo R), ent√£o se fizermos um looping da seguinte maneira teremos replicado nossas an√°lises para todos os demais anos:</p>
<pre><code>rmarkdown::render(&quot;gapminder_automatico_master.Rmd&quot;)</code></pre>
<p>Veja o conte√∫do do script <code>gapminder_automatico_master.Rmd</code>:</p>
<script src="https://gist.github.com/gomesfellipe/86af044b4e8a874756a2f4c379cfc01b.js"></script>
<p>Note que este script chama outro arquivo <code>.Rmd</code> chamado <code>gapminder_automatico_child.Rmd</code>, que tem o seguinte conte√∫do:</p>
<script src="https://gist.github.com/gomesfellipe/2a9d666e041907ca88dd2188cbc72924.js"></script>
<p>Veja os resultados do looping:</p>
<iframe src="/post/2019-09-13-relatorios-automaticos-com-rmarkdown/gapminder_automatico_master.pdf" width="600" height="827" style="border: none;">
</iframe>
</div>
<div id="conclus√£o" class="section level1">
<h1>Conclus√£o</h1>
<p>A Abordagem para criar chunks <em>filhos</em> de RMarkdown com a fun√ß√£o <code>knit_child()</code> abre muitas portas para an√°lises de dados! Neste post fizemos um exemplo simples de automa√ß√£o de relat√≥rios por√©m esses resultados podem ser cada vez mais customiz√°veis e utilizados em RPA - <a href="https://en.wikipedia.org/wiki/Robotic_process_automation">Robotic Process Automation</a> - de forma que seja poss√≠vel automatizar processos que antes s√≥ poderiam ser executados por humanos!</p>
</div>
<div id="referencias" class="section level1">
<h1>Referencias</h1>
<ul>
<li><a href="https://cran.r-project.org/web/packages/gganimate/vignettes/gganimate.html" class="uri">https://cran.r-project.org/web/packages/gganimate/vignettes/gganimate.html</a></li>
<li><a href="https://cran.r-project.org/web/packages/ggforce/vignettes/Visual_Guide.html" class="uri">https://cran.r-project.org/web/packages/ggforce/vignettes/Visual_Guide.html</a></li>
<li><a href="https://cran.r-project.org/web/packages/ggExtra/vignettes/ggExtra.html" class="uri">https://cran.r-project.org/web/packages/ggExtra/vignettes/ggExtra.html</a></li>
<li><a href="https://cran.r-project.org/web/packages/gapminder/gapminder.pdf" class="uri">https://cran.r-project.org/web/packages/gapminder/gapminder.pdf</a></li>
<li><a href="https://www.gapminder.org/data/" class="uri">https://www.gapminder.org/data/</a></li>
<li><a href="https://stackoverflow.com/questions/43873345/knit-child-in-a-loop-variable-as-title" class="uri">https://stackoverflow.com/questions/43873345/knit-child-in-a-loop-variable-as-title</a></li>
</ul>
</div>
<div id="apendice" class="section level1">
<h1>Apendice</h1>
<div id="fun√ß√£o-analise.r" class="section level2">
<h2>Fun√ß√£o <code>analise.R</code></h2>
<p>Veja o conte√∫do da fun√ß√£o <code>analise.R</code> preparada para esta analise:</p>
<pre class="r"><code># Funcao para analise por ano:
analise_gapminder &lt;- function(x, title){
  
  # Carregar dependencias:
  require(broom)
  require(ggforce)
  require(ggpmisc)
  require(ggExtra)
  
  # Funcao para customizar legendas:
  custom_legend &lt;- function(x){comma(x, big.mark = &quot;.&quot;,decimal.mark = &quot;,&quot;)}
  
  # Obter dados do Brasil:
  brazil &lt;- x %&gt;% filter(country == &quot;Brazil&quot;)
  
  # Resultados do ajuste de regressao ---------------------------------------
  mytable &lt;- 
    lm(lifeExp ~ gdpPercap, data = x) %&gt;% 
    tidy() %&gt;% 
    mutate_if(is.numeric, ~round(.x, 4)) %&gt;% 
    `colnames&lt;-`(c(&quot;Termo&quot;, &quot;Estimativa&quot;, &quot;Desv.Pad.&quot;, &quot;Estatistica&quot;, &quot;Valor p&quot;))
  
  # r2:
  r2 &lt;- round(summary(lm(lifeExp ~ gdpPercap, data = x))$r.squared,4)*100
  
  # residuos do modelo:
  res &lt;- lm(lifeExp ~ gdpPercap, data = x)$residuals
  
  # resutado para teste de kolmogorov-smirnov
  ks_test &lt;- ks.test(res, &quot;pnorm&quot;, mean(res), sd(res))$p.value %&gt;% round(5)
  
  # Grafico geral com regressao e boxplots ----------------------------------
  grafico_geral_regressao &lt;- 
    x %&gt;% 
    ggplot(aes(x = gdpPercap, y = lifeExp, size = pop, label = country, color = country)) %+%
    geom_point(show.legend = F) %+%
    geom_text(show.legend = F, size = 3, nudge_y = -0.5) %+%
    scale_size_continuous(labels = custom_legend) %+%
    scale_x_log10(labels = custom_legend) %+%
    scale_color_manual(values = country_colors) %+%
    geom_smooth(se=F, color = &quot;black&quot;, show.legend = F, method = &quot;lm&quot;) %+%
    annotate(&quot;segment&quot;, color=&quot;blue&quot;, arrow=arrow(length=unit(0.05,&quot;npc&quot;)),
             x=brazil$gdpPercap, xend=brazil$gdpPercap,
             y=brazil$lifeExp-6, yend=brazil$lifeExp-1) %+%
    annotate(&quot;text&quot;, color=&quot;blue&quot;, label = &quot;Brasil&quot;,
             x=brazil$gdpPercap, y=brazil$lifeExp-7) %+%
    labs(title = paste0(title, &quot;: lifeExp ~ gdpPercap&quot;),
         subtitle = &quot;Regress√£o linear e destaque no Brasil&quot;,
         caption = paste0(&quot;R¬≤ do modelo: &quot;, r2, &quot;\n&quot;,&quot;p valor para ks.test: &quot;, ks_test),
         x = &quot;gdpPercap (Transforma√ß√£o log10)&quot;) %+%
    annotate(geom = &quot;table&quot;, x = Inf, y = -Inf,
             label = list(mytable), 
             vjust = 0, hjust = 1) %&gt;%  
    ggMarginal(type = &quot;boxplot&quot;, fill=&quot;transparent&quot;,size = 10)
  
  # Comportamento separado por continente -----------------------------------
  grafico_por_continente &lt;- 
    x %&gt;% 
    filter(continent != &quot;Oceania&quot;) %&gt;% 
    ggplot(aes(x = gdpPercap, y = lifeExp, size = pop, label = country, color = country)) %+%
    geom_point(show.legend = F) %+%
    geom_text(show.legend = F, size = 3, nudge_y = -0.5) %+%
    facet_wrap(~continent, scales = &quot;free&quot;) %+%
    scale_x_continuous(labels = custom_legend) %+%
    scale_color_manual(values = country_colors) %+% 
    geom_smooth(method = &quot;lm&quot;, color = &quot;black&quot;, se=F, show.legend = F) %+%
    labs(title = paste0(title, &quot;: lifeExp ~ gdpPercap, por continente&quot;))
  
  # Acima da media ----------------------------------------------------------
  grafico_zoom_acima_media &lt;- 
    x %&gt;% 
    ggplot(aes(x = gdpPercap, y = lifeExp, size = pop, label = country, color = country)) %+%
    geom_point(show.legend = F) %+% 
    geom_text(show.legend = F, size = 3, nudge_y = -0.5) %+%
    scale_size_continuous(labels = custom_legend) %+%
    scale_x_continuous(labels = custom_legend) %+%
    scale_color_manual(values = country_colors) %+%
    facet_zoom(y = lifeExp   &gt; median(x$lifeExp),
               x = gdpPercap &gt; median(x$gdpPercap), split = T) %+%
    geom_smooth(se=F, color = &quot;red&quot;, show.legend = F, method = &quot;loess&quot;)  %+%
    labs(title = paste0(title, &quot;: lifeExp ~ gdpPercap com zoom nos pa√≠ses acima da mediana&quot;))
  
  # Abaixo da media ---------------------------------------------------------
  grafico_zoom_abaixo_media &lt;- 
    x %&gt;% 
    ggplot(aes(x = gdpPercap, y = lifeExp, size = pop, label = country, color = country)) %+%
    geom_point(show.legend = F) %+%
    geom_text(show.legend = F, size = 3, nudge_y = -0.5) %+%
    scale_size_continuous(labels = custom_legend) %+%
    scale_x_continuous(labels = custom_legend) %+%
    scale_color_manual(values = country_colors) %+%
    facet_zoom(y = lifeExp   &lt; median(x$lifeExp),
               x = gdpPercap &lt; median(x$gdpPercap), split = T) %+%
    geom_smooth(se=F, color = &quot;red&quot;, show.legend = F, method = &quot;loess&quot;)   %+%
    labs(title = paste0(title, &quot;: lifeExp ~ gdpPercap com zoom nos pa√≠ses abaixo da mediana&quot;))
  
  # Output ------------------------------------------------------------------
  list(
    brazil  = brazil,
    mytable = mytable,
    r2      = r2,
    grafico_geral_regressao   = grafico_geral_regressao,
    grafico_por_continente    = grafico_por_continente,
    grafico_zoom_acima_media  = grafico_zoom_acima_media,
    grafico_zoom_abaixo_media = grafico_zoom_abaixo_media,
    ks_test = ks_test
  )
}</code></pre>
</div>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2019-09-13-relatorios-automaticos-com-rmarkdown/relatorios-automaticos-com-rmarkdown/">Como automatizar relat√≥rios longos e repetitivos com RMarkdown</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">data-mining</category>
      <category domain="tag">estatistica</category>
      <category domain="tag">otimizacao</category>
      <category domain="tag">pratica</category>
      <category domain="tag">r</category>
      <category domain="tag">regressao</category>
      <category domain="tag">relatorios</category>
      <category domain="tag">reports</category>
      <category domain="tag">rmarkdown</category>
      <category domain="tag">rstudio</category>
    </item>
    <item>
      <title>An√°lise de sobreviv√™ncia com dados do jogo PUBG dispon√≠veis no Kaggle</title>
      <link>https://gomesfellipe.github.io/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle/</link>
      <pubDate>Wed, 28 Nov 2018 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle/</guid>
      <description>O que interefere na probabilidade de um indiv√≠duo sobreviver? Quais fatores apresentam efeito no risco de morte em um intervalo de tempo? Neste post buscaremos evid√™ncias estat√≠sticas para responder estas perguntas em dados abertos do PUBG hospedados no Kaggle</description>
      <content:encoded>&lt;![CDATA[
        


<div id="an√°lise-de-sobreviv√™ncia-e-pubg" class="section level1">
<h1>An√°lise de sobreviv√™ncia e PUBG</h1>
<p>An√°lise de sobreviv√™ncia √© um termo que se refere a situa√ß√µes m√©dicas e √© caracterizada pela sua vari√°vel resposta, que pode ser apresentada de tr√™s formas: probabilidade de sobreviv√™ncia, taxa de incid√™cia e taxa de incid√™ncia acumulada.</p>
<p>Na engenharia este termo tamb√©m √© conhecido como confiabilidade, no entanto, condi√ß√µes parecidas podem ocorrer em (inusitadas) outras √°reas.</p>
<p>PUBG √© um jogo online multiplayer de batalha em que 100 jogadores s√£o lan√ßados em uma ilha e tem como objetivo principal <strong>sobreviver</strong>, a √°rea de jogo diminui progressivamente, confinando os sobreviventes a um espa√ßo cada vez menor e for√ßando encontros e o vencedor √© o √∫ltimo jogador (ou time) a permanecer vivo.</p>
<p>Um √∫nico jogo dura aproximadamente de 30-35 minutos e neste tempo o jogador coleta itens (arma, cura, boost), abate outros jogadores, comete e leva dano de seus advers√°rios, pode dirigir ve√≠culos dentre outras a√ß√µes enquanto tentam sobrevier ao mesmo tempo.</p>
<p>Quest√µes que surgiram em mente ap√≥s um per√≠odo de estudos de an√°lise de sobreviv√™ncia e confiabilidade e ouvindo pessoas falarem sobre esta modalidade de jogo:</p>
<ul>
<li>O que interefere na probabilidade de um indiv√≠duo sobreviver?</li>
<li>O que tem efeito no risco de um jogador ser abatido em um intervalo de tempo?</li>
</ul>
<p>Faremos uma abordagem estat√≠stica aqui, ap√≥s uma breve an√°lise explorat√≥ria os dados ser√£o avaliados utilizando o modelo de Kaplan-Meier, que √© um estimador de forma n√£o param√©trica para a fun√ß√£o de sobreviv√™ncia e o modelo semiparam√©trico de regress√£o de riscos proporcionais de Cox.</p>
</div>
<div id="a-base-de-dados" class="section level1">
<h1>A Base de dados</h1>
<p>A base de dados utilizada foi obtida atrav√©s do Kaggle em ‚ÄúPUBG Match Deaths and Statistics‚Äù: <a href="https://www.kaggle.com/skihikingkevin/pubg-match-deaths" class="uri">https://www.kaggle.com/skihikingkevin/pubg-match-deaths</a> que conta com mais de 65 milh√µes de registros de mortes no jogo PlayerUnknown Battleground‚Äôs matches - PUBG.</p>
<p><a href="https://www.kaggle.com/gomes555/analise-de-sobrevivencia-km-e-cox/">Existe uma vers√£o deste post no kaggle</a> e al√©m desta base, existe uma competi√ß√£o em andamento que vai at√© o dia 30 de Janeiro no link:<a href="https://www.kaggle.com/c/pubg-finish-placement-prediction" class="uri">https://www.kaggle.com/c/pubg-finish-placement-prediction</a> que desafia os jogadores a prever o posicionamento do vencedor em percentil, onde 1 corresponde ao 1¬∫ lugar e 0 corresponde ao √∫ltimo lugar do jogo. Fiz uma participa√ß√£o com um <a href="https://www.kaggle.com/gomes555/xgboost-caret-for-fun">script testando os resultados do algor√≠tmo xgboost com caret</a> e tamb√©m testei uns <a href="https://www.kaggle.com/gomes555/tidyverse-machine-learning-for-fun">ajustes com random forest utilizando o tidyverse</a>. Esses scripts s√£o abertos e est√£o prontos para uso, <a href="https://www.kaggle.com/gomes555">n√£o me renderam a melhor posi√ß√£o</a> mas a intens√£o aqui √©, principalmente, aprender e testar os m√©todos pois S√£o muitas possibilidade para aprender e praticar. Voltando a base de dados:</p>
<p>Segundo a <a href="https://www.kaggle.com/skihikingkevin/pubg-match-deaths#aggregate.zip">descri√ß√£o da base no kaggle</a>:</p>
<p><code>agg_match_stats_x.csv</code> fornece informa√ß√µes de correspond√™ncia mais agregadas sobre os dados de mortes, como tamanho da fila, fpp/tpp, morte do jogador, etc.</p>
<p>As colunas s√£o as seguintes:</p>
<div class="col2">
<ul>
<li><code>match_id</code> : O id √∫nico de correspond√™ncia gerado por pubg.op.gg. √â poss√≠vel fazer uma jun√ß√£o disso com os dados das mortes para ver todas as informa√ß√µes</li>
<li><code>party_size</code> : o n√∫mero m√°ximo de jogadores por equipe. por exemplo, 2 implica que era um sistema de fila dupla</li>
<li><code>player_dist_ride</code> : unidades de distancia total (metros?) que o jogador percorreu em um ve√≠culo</li>
<li><code>player_dist_walk</code> : unidades de distancia total (metros?) percorrida pelo jogador a p√©</li>
<li><code>match_mode</code> : se o jogo foi jogado em primeira pessoa (fpp) ou em terceira pessoa (tpp)</li>
<li><code>team_placement</code> : a classifica√ß√£o final da equipe dentro da partida</li>
<li><code>player_dmg</code> : Total de pontos de vida que o jogador distribuiu</li>
<li><code>player_assists</code> : N√∫mero de assist√™ncias que o jogador marcou</li>
<li><code>game_size</code> : o n√∫mero total de equipes que estavam no jogo</li>
<li><code>player_dbno</code> : N√∫mero de knockdowns que o jogador marcou</li>
<li><code>player_kills</code> : N√∫mero de mortes que o jogador marcou</li>
<li><code>team_id</code> : o ID da equipe √† qual o jogador pertencia</li>
<li><code>date</code> : a data e a hora em que a partida ocorreu</li>
<li><code>player_name</code> : nome do jogador</li>
</ul>
<hr />
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/img.png" /></p>
</div>
<p>A rotinas abaixo carregam os pacotes, fun√ß√µes customizadas e salva em extens√£o <code>.rds</code>uma amostra da base de dados utilizadas ao longo do post:</p>
<pre class="r"><code># Carregar pacotes --------------------------------------------------------
packages &lt;- c(&quot;data.table&quot;, &quot;dplyr&quot;, &quot;purrr&quot;, &quot;survival&quot;  , &quot;survminer&quot;,
              &quot;ggfortify&quot;,&quot;GGally&quot;, &quot;ggplot2&quot;,&quot;moments&quot;, &quot;gridExtra&quot;,&quot;ggExtra&quot;,
              &quot;cowplot&quot;,&quot;lubridate&quot;, &quot;scales&quot;, &quot;knitr&quot;, &quot;kableExtra&quot;, &quot;grid&quot;,
              &quot;broom&quot;, &quot;formattable&quot;, &quot;grid&quot;)
purrr::walk(packages,library, character.only = TRUE, warn.conflicts = FALSE)
rm(packages)

# Funcoes customizadas do github ------------------------------------------
source(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/inicio_e_fim_da_base.R&quot;)
source(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/grafico_descritivo.R&quot;)
source(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/sumario_custom_num.R&quot;)

# Opcoes do documento -----------------------------------------------------
# options(scipen = 99999)

# Tema dos graficos -------------------------------------------------------
theme_set(theme_bw()+
            theme(axis.text.x = element_text(size=17),
                  axis.text.y = element_text(size=17),
                  axis.title.y = element_text(size=20), legend.position = &quot;bottom&quot;))

# Tema das tabelas kable --------------------------------------------------
kable2 &lt;- function(x,linhas=NULL,colunas=NULL, ...){
  k &lt;- 
    kable(x,digits = 4,...) %&gt;%
    kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) %&gt;%
    kable_styling(c(&quot;striped&quot;, &quot;bordered&quot;)) 
  
  if (!is.null(linhas)) {
    # destque na linha:
    k &lt;-  k %&gt;% row_spec(linhas, bold = T, color = &quot;white&quot;, background = &quot;#FFE8BD&quot;)
  }
  
  if (!is.null(colunas)) {
    # destque na colunas:
    k &lt;-  k %&gt;% column_spec(colunas,bold=T, color=&quot;white&quot;, background = &quot;#FFE8BD&quot;)
  }
  k %&gt;%
    scroll_box(width = &quot;850px&quot;)
}</code></pre>
<p>Em uma an√°lise de sobreviv√™ncia √© comum a presen√ßa de observa√ß√µes censuradas, (isto √©, quando ocorre a perda de informa√ß√£o decorrente de n√£o se ter observado a data de ocorr√™ncia do desfecho). No caso dessa base de dados n√£o existe uma vari√°vel que define a censura, pois apenas a morte do jogador √© registrada e √© poss√≠vel que se os jogadores se desconectarem do jogo mesmo que n√£o sejam mortos seja contado como morte de qualquer jeito. Os detalhes por tr√°s da aquisi√ß√£o de dados n√£o trazem essa informa√ß√£o portanto pode n√£o ser poss√≠vel distinguir a censura do desfecho e isso √© um detalhe relevante que deve ser levado em conta.</p>
<pre class="r"><code># Carregar base -----------------------------------------------------------
set.seed(2)   # reprodutivel
pubg_tpp1 &lt;-  # Informacoes dos criterios de selecao no corpo do texto
  map_df(paste0(&quot;agg_match_stats_&quot;,0:4,&quot;.csv&quot;), 
         ~ fread(.x, showProgress = T,
                 data.table = T)[match_mode == &quot;tpp&quot; &amp; party_size == 1 &amp; year(date) == 2018 &amp; player_dist_walk&gt;10 &amp; player_dmg != 0 ][, !c(&quot;match_mode&quot;,&quot;party_size&quot;,&quot;game_size&quot;,&quot;date&quot;, &quot;team_id&quot;,&quot;player_dbno&quot;, &quot;team_placement&quot;), with=FALSE][,player_survive_time := player_survive_time/60] %&gt;% 
           group_by(match_id) %&gt;%
           do(sample_n(.,1)) %&gt;% 
           ungroup() 
  )

# Salvar base coletada ----------------------------------------------------
saveRDS(pubg_tpp1,&quot;pubg_tpp1.rds&quot;)</code></pre>
<!-- <iframe src="https://giphy.com/embed/3oKIPmaM8aFolCcuI0" width="100%" height="270" frameBorder="0" class="giphy-embed" allowFullScreen></iframe> -->
<div class="col2">
<p>Descri√ß√£o da rotina acima e os crit√©rios para a sele√ß√£o da amostra:</p>
<ol style="list-style-type: decimal">
<li>percorre as 5 bases dispon√≠veis: <code>paste0("agg_match_stats_",0:4,".csv")</code></li>
<li>seleciona partidas em terceira pessoa: <code>match_mode == "tpp"</code></li>
<li>com tamanho da equipe = 1 (individual): <code>party_size == 1</code></li>
<li>do ano de 2018: <code>year(date) == 2018</code></li>
<li>andaram mais que 10 unidades de distancia (metros?): <code>player_dist_walk&gt;10</code></li>
<li>fizeram algum dano (evitar jogadores ausentes): <code>player_dmg != 0</code><br />
</li>
<li>remove colunas n√£o utilizadas na analise</li>
<li>converte do tempo para minutos: <code>player_survive_time := player_survive_time/60</code></li>
<li>agrupa por partida: <code>group_by(match_id)</code></li>
<li>seleciona um jogador de cada partida: <code>do(sample_n(.,1))</code></li>
</ol>
<iframe src="https://giphy.com/embed/g4OqNwXDrnfOcbaaUM" width="240" height="300" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
</div>
<p>Note que apenas um jogador de cada partida √© selecionado na inten√ß√£o de obter independ√™ncia entre observa√ß√µes, isso reduziu drasticamente seu tamanho. Agora que a base j√° foi importada e filtrada, faremos a leitura de 200 linhas aleat√≥rias com a finalidade de diminuir o tempo computacional das opera√ß√µes realizadas em seguida.</p>
<pre class="r"><code>set.seed(1)
pubg_tpp1 &lt;- readRDS(&quot;pubg_tpp1.rds&quot;) %&gt;% sample_n(200)%&gt;% 
  select(-one_of(c(&quot;match_id&quot;, &quot;player_name&quot;)))</code></pre>
<p>Veja a seguir de forma visual como as vari√°veis num√©ricas se correlacionam:</p>
<pre class="r"><code>pubg_tpp1 %&gt;% 
  rev %&gt;% 
  grafico_descritivo()</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-4-1.png" style="width:80.0%" /></p>
</center>
<div id="vari√°vel-resposta" class="section level3">
<h3>Vari√°vel resposta</h3>
<p>Vejamos o que acontece ao analisar o tempo de sobreviv√™ncia de cada jogador</p>
<iframe src="https://giphy.com/embed/3oKIP5KxPss1gjwpG0" width="100%" height="270" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
<p>A seguir, a distribui√ß√£o da vari√°vel resposta <code>player_survive_time</code> :</p>
<pre class="r"><code>plot_grid(pubg_tpp1 %&gt;% 
            ggplot(aes(x=player_survive_time))+
            geom_histogram(aes(y = ..density..), bins = 30, fill=&quot;white&quot;, color=&quot;black&quot;)+
            geom_density(alpha=.2, fill=&quot;white&quot;)+
            scale_x_continuous(labels = scales::comma, limits = c(0,40), breaks = seq(0,40,5))+
            labs(x=&quot;&quot;,y=&quot;&quot;, title = &quot;Tempo de sobreviv√™ncia dos jogadores selecionados&quot;)
          ,
          pubg_tpp1 %&gt;% 
            ggplot(aes(x=&quot; &quot;, y=player_survive_time))+
            geom_boxplot()+
            labs(x=&quot;&quot;)+
            coord_flip()
          ,
          ncol = 1, nrow = 2, align = &quot;v&quot;, rel_heights = c(3,1))</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-6-1.png" style="width:80.0%" /></p>
</center>
<p>Note que possue uma <a href="https://binged.it/2BAYX3s">assimetria positiva</a></p>
</div>
<div id="data-wrangling" class="section level3">
<h3>Data Wrangling</h3>
<p>Primeiramente, vejamos as vari√°veis se relacionam entre si e com a vari√°vel resposta com os coeficientes de correla√ß√£o de Pearson:</p>
<pre class="r"><code># Correlations
pubg_tpp1 %&gt;% 
  select_if(is.numeric) %&gt;% 
  cor() %&gt;% 
  corrplot::corrplot(method = &quot;number&quot;,type = &quot;upper&quot;,diag = F, order = &quot;hclust&quot;,number.cex = 0.7, title = &quot;Correlation correlated numerics&quot;, mar=c(0,0,1,0))</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-7-1.png" style="width:80.0%" /></p>
</center>
<p>√â poss√≠vel notar que apenas a vari√°vel <code>player_assists</code> n√£o correlaciona-se com a vari√°vel resposta nem com as demais vari√°veis e <code>player_dmg</code> e <code>player_kills</code> s√£o fortemente correlacionadas, isso indica que pode ser interessante remover uma delas ou juntar toda essa informa√ß√£o em uma √∫nica vari√°vel, veremos‚Ä¶</p>
<p>Al√©m disso nota-se que a dist√¢ncia percorrida a p√© √© fortemente correlacionada com a vari√°vel resposta enquanto que a dist√¢ncia de quem andou de carro n√£o √© t√£o correlacionada. Uma transforma√ß√£o na vari√°vel <code>player_dist_ride</code> para uma dummy <code>drive</code> indicando se o indiv√≠duo dirigiu ou n√£o pode representar melhor esta informa√ß√£o.</p>
<p>Vejamos algumas caracter√≠sticas peculiares:</p>
<pre class="r"><code>pubg_tpp1 %&gt;% 
  select(player_kills, player_dist_ride, player_assists) %&gt;% 
  map_dfr(~quantile(.x,  probs = seq(0,1,0.25)) %&gt;% round(2)) %&gt;% 
  t  %&gt;% tidy() %&gt;% 
  `colnames&lt;-`(c(&quot;vari√°vel&quot;,percent(seq(0,1,0.25)))) %&gt;% 
  kable2()</code></pre>
<p>Praticamente metade da amostra n√£o registrou abates nem possui marca√ß√£o de <code>player_dist_ride</code>. Como a vari√°vel <code>player_dmg</code> apresentou correla√ß√£o com a vari√°vel resposta <code>player_survive_time</code>, vamos fazer algumas transforma√ß√µes:</p>
<ol style="list-style-type: decimal">
<li>Criar uma vari√°vel dummy <code>drive</code> se jogador usou carro</li>
<li>Somar a <code>player_dist_ride</code> e <code>player_dist_walk</code> em uma √∫nica vari√°vel: <code>player_dist</code></li>
<li>Juntar <code>player_kills</code>, <code>player_dmg</code> e <code>player_assists</code> em uma √∫nica vari√°vel: <code>player_performance</code></li>
</ol>
<div id="player-performance" class="section level4">
<h4>Player performance</h4>
<p>Como criar a vari√°vel <code>player_performance</code>?</p>
<iframe src="https://giphy.com/embed/xT9IgnOQS8e8uKkflK" width="100%" height="270" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
<p>Tentei inventar uma metodologia e com certeza devem existir maneiras mais eficientes de se fazer isso, por√©m, deixa eu explicar o que eu pensei, considere a formula:</p>
<p><span class="math display">\[
Playerperformance = log(WPlayerDmg + WPlayerAssists + WPlayerKills)
\]</span></p>
<p>onde:</p>
<p><span class="math display">\[
WPlayerKills = log(PlayerKills+0.5)\\
WPlayerDmg = log(PlayerDmg)\\
WPlayerAssists = PlayerAssists
\]</span></p>
<p>Note que:</p>
<ul>
<li><span class="math inline">\(WPlayerAssists\)</span>: N√£o √© feita qualquer transforma√ß√£o;</li>
<li><span class="math inline">\(WPlayerDmg\)</span>: A distribui√ß√£o fica ‚Äúquase sim√©trica‚Äù ap√≥s a transforma√ß√£o log;</li>
<li><span class="math inline">\(WPlayerKills\)</span>: adiciona-se 0.5 para poder tirar o log pois podem existir zeros nessa vari√°vel e al√©m disso, quem n√£o marcou abate ser√° penalizado com <span class="math inline">\(-1\)</span> na soma final do score: <code>player_performance</code>.</li>
</ul>
<p>Veja a seguir de forma visual a distribui√ß√£o das vari√°veis que far√£o parte da vari√°vel <code>player_performance</code> na parte de cima e na parte inferior o que acontece ap√≥s sua soma, gerando a nova vari√°vel <code>player_performance</code> :</p>
<pre class="r"><code>performance &lt;- tibble(w_player_kills = log(pubg_tpp1$player_kills+0.5),
                      w_player_dmg = log(pubg_tpp1$player_dmg),
                      w_player_assists = pubg_tpp1$player_assists) %&gt;% 
  mutate(player_performance = log(w_player_dmg + w_player_assists + w_player_kills))

grid.arrange(
  performance %&gt;% 
    select(-player_performance) %&gt;% 
    tidyr::gather() %&gt;% 
    ggplot(aes(x=value))+
    geom_histogram(aes(y = ..density..), bins = 30, fill=&quot;white&quot;, color=&quot;black&quot;)+
    geom_density(alpha=.2, fill=&quot;white&quot;)+
    scale_x_continuous(labels = scales::comma, limits = c(-1.5,8), breaks = seq(-1,8,1))+
    labs(x=&quot;&quot;, y=&quot;&quot;)+
    theme(axis.title.x=element_blank(),
          axis.text.x=element_blank(),
          axis.ticks.x=element_blank())+
    facet_wrap(~key, scales = &quot;free&quot;)
  ,
  performance %&gt;% 
    select(player_performance) %&gt;% 
    tidyr::gather() %&gt;% 
    ggplot(aes(x=value))+
    geom_histogram(aes(y = ..density..), fill=&quot;white&quot;, color=&quot;black&quot;,bins = 15)+
    geom_density(alpha=.2, fill=&quot;white&quot;)+
    scale_x_continuous(limits = c(-1.,2.5), breaks = seq(-1,3,0.5))+
    labs(x=&quot;&quot;, y=&quot;&quot;, title = &quot;performance&quot;),
  ncol=1
)</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-9-1.png" style="width:80.0%" /></p>
</center>
</div>
<div id="transforma√ß√µes-na-base" class="section level4">
<h4>Transforma√ß√µes na base</h4>
<p>A seguir faremos as mudan√ßas diretamente no dataset que estamos trabalhando:</p>
<pre class="r"><code>pubg_tpp1 &lt;- 
  pubg_tpp1 %&gt;% 
  mutate(player_dist = log(player_dist_ride + player_dist_walk)) %&gt;%  
  mutate(player_assists_d = if_else(player_assists ==0, 0, 1)) %&gt;% 
  mutate(player_performance = performance$player_performance )%&gt;% 
  mutate(drive = ifelse(player_dist_ride==0, &quot;no&quot;, &quot;yes&quot;) %&gt;% as.factor()) %&gt;% 
  mutate(player_kills_d = ifelse(player_kills==0, &quot;no&quot;, &quot;yes&quot;) %&gt;% as.factor()) </code></pre>
<p>A manipula√ß√£o acima cria as seguintes vari√°veis:</p>
<ol style="list-style-type: decimal">
<li><code>player_dist</code> como o log da soma de <code>player_dist_ride</code> e <code>player_dist_walk</code></li>
<li><code>player_assists_d</code> como uma dummy: 1 se o jogador deu assist√™ncia; 0 c.c.</li>
<li><code>player_performaec</code> como a combina√ß√£o de <code>player_dmg</code>, <code>player_assists</code> e <code>player_kills</code></li>
<li><code>drive</code> como uma dummy: 1 se o jogador dirigiu; 0 c.c.</li>
<li><code>player_kills_d</code> como uma dummy: 1 se jogador matou algu√©m; 0 c.c.</li>
</ol>
<p>Vejamos como ocorre a distribui√ß√£o das vari√°veis num√©ricas ap√≥s as transforma√ß√µes:</p>
<pre class="r"><code>g1 &lt;- 
  pubg_tpp1 %&gt;% 
  # select_if(~ !length(table(.x))==2 &amp; is.numeric(.x)) %&gt;% colnames() %&gt;% 
  select(player_survive_time,player_performance,player_dist) %&gt;% colnames() %&gt;% 
  map2(c(&quot;Densidade&quot;, &quot;&quot;, &quot;&quot;),
       ~ plot_grid(
         pubg_tpp1 %&gt;% 
           ggplot(aes_string(x=.x)) + 
           geom_histogram(aes(y=..density..),colour=&quot;black&quot;, fill=&quot;white&quot;, bins = 15) +
           geom_density(alpha=.2, fill=&quot;lightgrey&quot;) +
           scale_x_continuous()+
           ggtitle(.x)+
           labs(x=&quot;&quot;, y=.y)+
           theme(axis.title.x=element_blank(),
                 axis.text.x=element_blank(),
                 axis.ticks.x=element_blank())
         ,
         pubg_tpp1 %&gt;% 
           ggplot(aes_string(, y=.x))+
           geom_boxplot(aes(x=&quot; &quot;))+
           labs(x=&quot;&quot;, y=&quot;&quot;)+
           coord_flip()+
           theme(axis.title.x=element_blank(),
                 axis.text.x=element_blank(),
                 axis.ticks.x=element_blank()),
         
         ncol = 1, nrow = 2, align = &quot;v&quot;, rel_heights = c(3,1)
       )
  )

dat &lt;- 
  pubg_tpp1 %&gt;% 
  select_if(~.x %&gt;% table %&gt;% length == 2) %&gt;% 
  mutate_at(2,~if_else(.x==0, &quot;no&quot;, &quot;yes&quot;)) %&gt;% 
  .[,-1]

g2 &lt;- map2(colnames(dat),
           c( &quot;Porcentagem&quot;, &quot;&quot;,&quot;&quot;),
           ~ dat[,.x] %&gt;% 
             tidyr::gather() %&gt;% 
             group_by(key, value) %&gt;% 
             summarise(n = n()) %&gt;% 
             mutate(prop = n/sum(n)) %&gt;% 
             ggplot(aes(x = key, y = prop,fill = value)) + 
             geom_bar(position = &quot;fill&quot;,stat = &quot;identity&quot;, alpha=0.7) +
             scale_y_continuous(labels = percent_format())+
             labs(x=&quot;&quot;, y = .y)+
             scale_fill_manual(values = c(&quot;grey&quot;, &quot;#FCC14B&quot;), name = &quot;Legenda:&quot;)
)

grid.arrange(g1[[1]], g1[[2]], g1[[3]],g2[[1]], g2[[2]], g2[[3]], ncol=3, heights=c(3/5, 2/5))</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-11-1.png" style="width:80.0%" /></p>
</center>
<!-- A distribui√ß√£o dos dados ordenada pela vari√°vel resposta `player_survive_time` : -->
<!-- ```{r} -->
<!-- # Sorted -->
<!-- pubg_tpp1 %>%  -->
<!--   select(player_survive_time, everything()) %>%  -->
<!--   mutate_if(~length(unique(.x))==2, as.factor) %>%  -->
<!--   tabplot::tableplot(sortCol = player_survive_time,decreasing = T) -->
<!-- ``` -->
<p>Apos a transforma√ß√£o a distribui√ß√£o e demais informa√ß√µes dos dados, vejamos novamente a distribui√ß√£o das vari√°veis da amostra com os gr√°ficos de dispers√£o, densidade e correla√ß√µes levando em conta se dirigiu ou n√£o:</p>
<pre class="r"><code>grafico_descritivo(x = pubg_tpp1,
                   colNames = c(&#39;player_survive_time&#39;, &quot;player_performance&quot;, &#39;player_dist&#39;,
                                &#39;player_assists_d&#39;,&quot;player_kills_d&quot;, &#39;drive&#39;),
                   color=&#39;drive&#39;,
                   colors = c(&quot;grey&quot;, &quot;#FCC14B&quot;))</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-13-1.png" style="width:80.0%" /></p>
</center>
<p>O fato do jogador ter dirigido ou n√£o exibiu padr√µes interessantes, pode ser que seja significante no ajuste do modelo final.</p>
</div>
</div>
</div>
<div id="an√°lise-de-sobrevivencia" class="section level1">
<h1>An√°lise de sobrevivencia</h1>
<p>O passo inicial de qualquer an√°lise estat√≠stica consiste em uma descri√ß√£o dos dados e o principal componente da an√°lise descritiva envolvendo dados de tempo de vida √© a fun√ß√£o de sobreviv√™ncia: <span class="math inline">\(S(t) = P(T&gt;t)\)</span>, que determina a probabilidade de um indiv√≠duo sobreviver por mais do que um determinado tempo <span class="math inline">\(t\)</span>, ou por no m√≠nimo um tempo igual a <span class="math inline">\(t\)</span>.</p>
<p>A descri√ß√£o dos dados j√° foi realizada, agora faremos a descri√ß√£o envolvendo a fun√ß√£o de sobreviv√™ncia.</p>
<iframe src="https://giphy.com/embed/xT0xeMrCEGPiU5uw0w" width="100%" height="266" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
<div id="kaplan-meier" class="section level2">
<h2>Kaplan-Meier</h2>
<p>Para isso existem algumas alternativas como o estimador de Kaplan-Meier, que utiliza os conceitos de independ√™ncia e de probabilidade condicional para deduzir a probabilidade de sobreviver at√© o tempo <span class="math inline">\(t\)</span>.</p>
<p>Veja a seguir s√£o ajustados os modelos univariados de Kaplan-Meier para cada uma das coivar√°veis da amostra:</p>
<pre class="r"><code>surv &lt;- Surv(pubg_tpp1$player_survive_time)
resultado_km &lt;-
  list(geral            = survfit(surv ~ 1 ,data = pubg_tpp1),
       player_assists_d = survfit(surv ~ player_assists_d ,data = pubg_tpp1),
       drive            = survfit(surv ~ drive,data = pubg_tpp1 ),
       player_kills_d   = survfit(surv ~ player_kills_d,data = pubg_tpp1))</code></pre>
<p>Veja os resultados da fun√ß√£o de sobreviv√™ncia sem levar em considera√ß√£o nenhuma das coivar√°veis:</p>
<pre class="r"><code>surv_summary(resultado_km[[1]], pubg_tpp1) %&gt;% .[1:5,-ncol(.)] %&gt;% cbind(variable = &quot;Geral&quot;) %&gt;% 
  select(variable, everything())%&gt;%
  kable2()</code></pre>
<p>A fun√ß√£o <code>surv_summary()</code> retorna um quadro de dados com as seguintes colunas:</p>
<ul>
<li>time: o tempo em que a curva tem um passo.</li>
<li>n.risk: o n√∫mero de sujeitos em risco em t.</li>
<li>n.evento: o n√∫mero de eventos que ocorrem no tempo t.</li>
<li>n.censor: n√∫mero de eventos censurados.</li>
<li>surv: estimativa da probabilidade de sobreviv√™ncia.</li>
<li>std.err: erro padr√£o de sobreviv√™ncia.</li>
<li>superior: extremidade superior do intervalo de confian√ßa</li>
<li>inferior: extremidade inferior do intervalo de confian√ßa</li>
<li>estratos: indica a estratifica√ß√£o da estimativa de curvas. Os n√≠veis de estratos (um fator) s√£o os r√≥tulos das curvas (se houver).</li>
</ul>
<div id="log-rank" class="section level3">
<h3>Log-rank</h3>
<p>Al√©m da an√°lise visual das estimativas √© importante comparar as curvas de sobreviv√™ncia com testes de hip√≥teses para obter-se signific√¢ncia estat√≠stica para nossas afirma√ß√µes.</p>
<p>O teste log rank √© um teste n√£o param√©trico, que n√£o faz suposi√ß√µes sobre as distribui√ß√µes de sobreviv√™ncia. Essencialmente, o teste log rank compara o n√∫mero observado de eventos em cada grupo com o que seria esperado se a hip√≥tese nula fosse verdadeira. Considere ent√£o <span class="math inline">\(H_0: S_1(t)=S_2(t)\)</span> para todo <span class="math inline">\(t\)</span> no per√≠odo de acompanhamento (ou seja, se as curvas de sobreviv√™ncia fossem id√™nticas). A estat√≠stica utilizada no teste √© um <span class="math inline">\(T\)</span> com distribui√ß√£o aproximadamente <span class="math inline">\(\chi^2\)</span> com 1 grau de liberdade.</p>
<p>O objeto criado abaixo guarda o valor p para o teste de log-rank de cada em cada um dos modelos:</p>
<pre class="r"><code>resultado_log_rank &lt;- 
  c(geral = &quot;&quot;,
    player_assists_d=round(1-pchisq(survdiff(surv~player_assists_d,data = pubg_tpp1)$chisq,1),5),
    drive=round(1-pchisq(survdiff(surv~drive,data=pubg_tpp1)$chisq,1),5),
    player_kills_d=round(1-pchisq(survdiff(surv~player_kills_d,data=pubg_tpp1)$chisq,1),5)
  )</code></pre>
<p>Os gr√°ficos gerados a partir dos modelos ajustados acima bem como o resultado dos testes de log-rank s√£o exibidos na imagem a seguir:</p>
<pre class="r"><code>survplot &lt;- map2(resultado_km,
                 case_when(resultado_log_rank == &#39;0&#39; ~ &quot;log-rank: \n p &lt; 0,00001&quot;,
                           resultado_log_rank == &quot;&quot; ~ &quot;log-rank n√£o se aplica&quot;,
                           resultado_log_rank != &#39;0&#39; | resultado_log_rank != &#39;&#39; ~ 
                             paste0(&quot;log-rank: \n p =&quot;,as.numeric(resultado_log_rank))),
                 ~ autoplot(.x)+
                   ggtitle(stringr::str_remove_all(names(.x$strata)[1],&quot;(=no|=yes)&quot;))+
                   annotate(&quot;label&quot;,y = 0.20, x = 5,
                            label = .y,
                            size = 4, colour = &quot;red&quot;,hjust=0.1)+ 
                   scale_fill_manual(values = c(&quot;grey&quot;, &quot;#FCC14B&quot;))+
                   scale_color_manual(values = c(&quot;grey&quot;, &quot;#FCC14B&quot;))+
                   theme(legend.position = c(0.85,0.7))+
                   scale_x_continuous(limits = c(0,30), breaks = seq(0,30,5))
                 
)
grid.arrange(survplot[[1]], survplot[[2]] ,survplot[[3]], survplot[[4]], ncol=2)</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-17-1.png" style="width:80.0%" /></p>
</center>
<p>O eixo horizontal (eixo x) representa o tempo em minutos, e o eixo vertical (eixo y) mostra a probabilidade de sobreviv√™ncia ou a propor√ß√£o de jogadores que sobrevivem. As linhas representam curvas de sobreviv√™ncia dos dois grupos.</p>
<p>Uma queda vertical nas curvas indica um evento. No tempo zero, a probabilidade de sobreviv√™ncia √© de 1,0 (ou 100% dos jogadores vivos).</p>
<p>Interpreta√ß√£o: Pelo gr√°fico, aparentemente n√£o existe diferen√ßa no tempo de sobreviv√™ncia com estratifica√ß√£o dos dados de acordo com quem deu assist√™ncia ou n√£o, j√° para o teste que compara igualdade de fun√ß√µes de sobreviv√™ncia das demais vari√°veis, existem evidencias estat√≠sticas para rejeitar a hip√≥tese de que n√£o h√° diferen√ßa na sobrevida entre os dois grupos</p>
</div>
</div>
<div id="fun√ß√£o-de-risco-hazard-ou-taxa-de-falha" class="section level2">
<h2>Fun√ß√£o de risco (hazard) ou taxa de falha</h2>
<p>Fun√ß√£o de risco (hazard) ou taxa de falha √© o risco ‚Äúinstant√¢neo‚Äù denotada por <span class="math inline">\(\lambda(t)\)</span> √© uma taxa, n√£o uma probabilidade e pode assumir qualquer valor real maior que zero.</p>
<p>No exemplo representa a taxa de incid√™ncia ou risco acumulado para um indiv√≠duo morrer at√© o momento <span class="math inline">\(t\)</span>, dado que sobreviveu at√© este momento. √â muito informativa quando comparada com a fun√ß√£o de sobreviv√™ncia pois diferentes <span class="math inline">\(S(t)\)</span> podem ter formas semelhantes, enquanto que respectivas <span class="math inline">\(\lambda(t)\)</span> podem diferir drasticamente.</p>
<pre class="r"><code>survplot &lt;-
  map(resultado_km  ,
      ~ ggsurvplot(.x, conf.int = TRUE, 
                   palette = c(&quot;grey&quot;, &quot;#FCC14B&quot;),
                   risk.table = F,break.time.by = 5,
                   fun = &quot;cumhaz&quot;,title = stringr::str_remove_all(names(.x$strata)[1],&quot;(=no|=yes)&quot;))
  )
arrange_ggsurvplots(survplot, print = TRUE,
                    ncol = 2, nrow = 2)</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-18-1.png" style="width:80.0%" /></p>
</center>
<p>O risco cumulativo <span class="math inline">\(H( t)\)</span> pode ser interpretado como a for√ßa cumulativa da mortalidade.
Em outras palavras, corresponde ao n√∫mero de eventos que seriam esperados para cada indiv√≠duo
pelo tempo t se o evento fosse um processo repetitivo.</p>
</div>
<div id="modelo-de-cox" class="section level2">
<h2>Modelo de cox</h2>
<p>√â caracterizado pela presen√ßa dos coeficientes <span class="math inline">\(\beta\)</span>s que medem os efeitos (semelhantes √† an√°lise de regress√£o log√≠stica m√∫ltipla e linear m√∫ltipla) das vari√°veis explicativas sobre a fun√ß√£o de risco. Em um modelo de regress√£o de riscos proporcionais de Cox, a medida do efeito √© a <em>taxa de risco</em>, que √© o risco de falha, dado que o participante sobreviveu at√© um tempo espec√≠fico.</p>
<p>Algumas das suposi√ß√µes para o correto uso do modelo de regress√£o de riscos proporcionais de Co incluem:</p>
<ul>
<li>independ√™ncia dos tempos de sobreviv√™ncia entre indiv√≠duos distintos na amostra,</li>
<li>rela√ß√£o multiplicativa entre os preditores e o risco,</li>
<li>uma taxa de risco constante ao longo do tempo.</li>
</ul>
<p>O modelo de riscos proporcionais de Cox √© chamado de modelo semi-param√©trico , porque n√£o h√° suposi√ß√µes sobre o formato da fun√ß√£o de risco de linha de base. No entanto, existem outras suposi√ß√µes, como observado acima.</p>
<p>√â poss√≠vel utilizar as estat√≠sticas de Wald, da raz√£o de verossimilhan√ßa e escore para fazer infer√™ncias sobre os par√¢metros do modelo</p>
<p>Veja a seguir a signific√¢ncia dos coeficiente estimado em modelos univariados para cada vari√°vel candidata ao modelo:</p>
<pre class="r"><code># Modelos univariados
covariates    &lt;- c(&quot;player_kills&quot;,&quot;player_dist_ride&quot;,&quot;player_performance&quot;,
                   &quot;player_dist_walk&quot;,&quot;player_dmg&quot;, &quot;player_dist&quot;, 
                   &quot;player_assists_d&quot;,&quot;drive&quot;, &quot;player_kills_d&quot;)
univ_formulas &lt;- map(covariates,~ as.formula(paste(&#39;Surv(player_survive_time) ~&#39;, .x)))
univ_models   &lt;- map( univ_formulas, ~coxph(.x, data = pubg_tpp1))

# estrair resultados 
map2_df(univ_models,
        covariates,
        function(x,y){ 
          x                = summary(x)
          p.value          = signif(x$wald[&quot;pvalue&quot;], digits=2)
          wald.test        = signif(x$wald[&quot;test&quot;], digits=2)
          beta             = signif(x$coef[1], digits=2);#coeficient beta
          HR               = signif(x$coef[2], digits=2);#exp(beta)
          HR.confint.lower = signif(x$conf.int[,&quot;lower .95&quot;], 2)
          HR.confint.upper = signif(x$conf.int[,&quot;upper .95&quot;],2)
          HR               = paste0(HR, &quot; (&quot;, HR.confint.lower, &quot;-&quot;, HR.confint.upper, &quot;)&quot;)
          res              = tibble(y,beta, HR, wald.test, p.value)
          colnames(res)    = c(&quot;covariates&quot;,&quot;beta&quot;, &quot;HR (95% CI for HR)&quot;, &quot;wald.test&quot;, &quot;p.value&quot;)
          res
        }) %&gt;% 
  kable2(linhas = 7)</code></pre>
<p>Modelo de Cox usando uma vari√°vel categ√≥rica retorna uma raz√£o de risco, que, acima de 1 indica uma covari√°vel que est√° positivamente associada √† probabilidade do evento e, portanto, negativamente associada ao tempo de sobrevida. O oposto vale para HR menor que um e HR = 1 indica que a covari√°vel n√£o tem efeito.</p>
<pre class="r"><code>final_model  &lt;- 
  coxph(Surv(player_survive_time) ~ player_performance+player_dist+drive,
        data = pubg_tpp1,x=T,method=&quot;breslow&quot;)

summary(final_model)</code></pre>
<pre><code>## Call:
## coxph(formula = Surv(player_survive_time) ~ player_performance + 
##     player_dist + drive, data = pubg_tpp1, x = T, method = &quot;breslow&quot;)
## 
##   n= 200, number of events= 200 
## 
##                       coef exp(coef) se(coef)       z Pr(&gt;|z|)    
## player_performance -0.7469    0.4738   0.1787  -4.179 2.92e-05 ***
## player_dist        -1.7599    0.1721   0.1150 -15.307  &lt; 2e-16 ***
## driveyes            0.8832    2.4186   0.2091   4.225 2.39e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
##                    exp(coef) exp(-coef) lower .95 upper .95
## player_performance    0.4738     2.1105    0.3338    0.6726
## player_dist           0.1721     5.8117    0.1374    0.2156
## driveyes              2.4186     0.4135    1.6055    3.6434
## 
## Concordance= 0.883  (se = 0.008 )
## Likelihood ratio test= 362.7  on 3 df,   p=&lt;2e-16
## Wald test            = 274  on 3 df,   p=&lt;2e-16
## Score (logrank) test = 407.8  on 3 df,   p=&lt;2e-16</code></pre>
<p>No modelo ajustado note-se que existe uma associa√ß√£o negativa entre <code>player_performance</code> e mortalidade e entre <code>player_dist</code> e mortalidade (ou seja, o risco de morte diminui para jogadores que percorrem maiores dist√¢ncias e possuem melhor performance).</p>
<p>As estimativas dos par√¢metros representam o aumento no log esperado do risco relativo para cada aumento de uma unidade no preditor, mantendo os outros preditores constantes.</p>
<p>Para interpretabilidade, calcularemos as taxas de risco exponenciando das estimativas dos par√¢metros. Para a <code>player_performance</code>, <span class="math inline">\(exp(-0.7469196)= 0.4738239\)</span>. Isso implica que diminui para <span class="math inline">\(47.38\)</span> do valor original do risco esperado em rela√ß√£o a um aumento de uma unidade na performance, mantendo as demais vari√°veis constantes. A interpreta√ß√£o de <code>player_dist</code> em escala logar√≠timica √© feita de maneira semelhante.`</p>
<p>J√° para os jogadores onde <code>drive</code> = 1 (que dirigiram durante a partida) existe uma rela√ß√£o positiva, como <span class="math inline">\(exp(0.8831835)= 2.4185871\)</span>. O risco esperado corresponde √† <span class="math inline">\(2.4185871\)</span> do valor original nos que dirigiram em compara√ß√£o aos que n√£o dirigiram, mantendo as demais vari√°veis constantes.</p>
<pre class="r"><code>map2_df(1:3,final_model$coefficients %&gt;% names(),~
          tibble(
            variable = .y,
            beta             = signif(summary(final_model)$coef[.x,1], digits=2), #coeficient beta
            HR               = signif(summary(final_model)$coef[.x,2], digits=2), #exp(beta)
            HR.confint.lower = signif(summary(final_model)$conf.int[.x,&quot;lower .95&quot;], 2),
            HR.confint.upper = signif(summary(final_model)$conf.int[.x,&quot;upper .95&quot;],2)) %&gt;% 
          mutate(HR= paste0(HR, &quot; (&quot;, HR.confint.lower, &quot;-&quot;, HR.confint.upper, &quot;)&quot;)
          )
) %&gt;% kable2()</code></pre>
<p>Em suma:</p>
<ul>
<li>HR = 1: sem efeito</li>
<li>HR &lt;1: Redu√ß√£o do risco</li>
<li>HR&gt; 1: aumento do risco</li>
</ul>
<iframe src="https://giphy.com/embed/2Us3iTghyffcfeI35h" width="100%" height="200" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
<div id="res√≠duos-de-martingal-e-deviance" class="section level3">
<h3>Res√≠duos de Martingal e Deviance</h3>
<p>Como foi visto, o modelo de regress√£o de riscos proporcionais de Cox faz diversas suposi√ß√µes que precisam ser conferidas ap√≥s o ajuste do modelo para chegar a qualidade de seus resultados pois um modelo mais ajustado pode trazer resultados enganosos e que n√£o fa√ßam sentido algum</p>
<iframe src="https://giphy.com/embed/l0CLSXnSgbYma8EOA" width="100%" height="269" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
<p>Gr√°ficos dos res√≠duos Martingal ou deviance contra os tempos fornecem
uma forma de verificar a adequa√ß√£o do modelo ajustado, bem como
ajudar na detec√ß√£o de observa√ß√µes at√≠picas.</p>
<p><strong>Deviance</strong></p>
<p>Esses res√≠duos, que s√£o uma tentativa de tornar os res√≠duos
Martingal mais sim√©tricos em torno do zero, facilitam, em geral,
a detec√ß√£o de pontos at√≠picos (outliers).
Se o modelo for apropriado, esses res√≠duos devem apresentar um
comportamento aleat√≥rio em torno de zero.</p>
<p><strong>Martingal</strong></p>
<p>Esses res√≠duos s√£o vistos como uma estimativa do numero de falhas em excesso
observada nos dados mas n√£o predito pelo modelo. Os mesmos s√£o usados, em geral,
para examinar a melhor forma funcional (linear, quadr√°tica, etc.)
para uma dada covariavel em um modelo de regress√£o assumido para os dados do estudo.</p>
<pre class="r"><code>res &lt;- 
  tibble(residuo_deviance = resid(final_model,type=&quot;deviance&quot;) ,
         residuo_martingal = resid(final_model,type=&quot;martingal&quot;),
         linear_predictors = final_model$linear.predictors)

# Graficos:
grid.arrange(
  ggplot(res, aes(x=linear_predictors, y=residuo_martingal))+ geom_point()+geom_hline(yintercept=0, color=&#39;coral&#39;)+ylab(&quot;Res√≠duos Martingual&quot;),
  ggplot(res, aes(x=linear_predictors, y=residuo_deviance))+ geom_point()+geom_hline(yintercept=0, color=&#39;coral&#39;)+ylab(&quot;Deviance&quot;),
  ncol=2
)</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-22-1.png" style="width:80.0%" /></p>
</center>
<p>Interpreta√ß√£o:</p>
<ul>
<li><strong>Martingal</strong>: Parecido com deviance mais acentuado;</li>
<li><strong>Deviance</strong>: Modelo n√£o eh tao ruim assim, se fosse um modelo linear talvez dever√≠amos tomar cuidado.</li>
</ul>
<div id="residuos-de-schoenfeld" class="section level4">
<h4>Residuos de Schoenfeld</h4>
<p>Em princ√≠pio, os res√≠duos de Schoenfeld s√£o independentes do tempo.
Um gr√°fico que mostra um padr√£o n√£o aleat√≥rio contra o tempo √©
evid√™ncia de viola√ß√£o da suposi√ß√£o de hip√≥tese.</p>
<p>Para testar a suposi√ß√£o de riscos proporcionais:</p>
<pre class="r"><code>final_model %&gt;% cox.zph %&gt;% ggcoxzph</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-23-1.png" style="width:80.0%" /></p>
</center>
<p>A partir da inspe√ß√£o gr√°fica, n√£o h√° padr√£o com o tempo.
A suposi√ß√£o de riscos proporcionais parece ser suportada
pelas covari√°veis</p>
</div>
</div>
</div>
<div id="considera√ß√µes-finais" class="section level2">
<h2>Considera√ß√µes finais</h2>
<iframe src="https://giphy.com/embed/ZacieLN2WI2AedWrz9" width="100%" height="216" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
<p>Como era de se esperar, o risco de ser abatido diminui para jogadores que possuem melhor performance e tamb√©m para os jogadores que percorrem maiores dist√¢ncias (o que mostra que ficar parado no jogo em uma zona pode n√£o ser a melhor ideia, j√° √© quanto mais se movimenta maior a quantidade de itens que podem ser coletados).</p>
<p>Interessante notar que a curva de <strong>sobreviv√™ncia</strong> para os jogadores que dirigiram apresenta resultado oposto ao <strong>risco</strong> esperado nos que dirigiram, isso ocorre pois esses dois modelos calculam medidas diferentes.</p>
</div>
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias</h1>
<ul>
<li>Carvalho,M.A., Andreozzi,V.L., Codec¬∏o,C.T., Campos,D.P., Barbosa,M.T.S., Shimakura,S.E., An√°lise de sobreviv√™ncia: Teoria e aplica√ß√µes em sa√∫de, Segunda Edi√ß√£o, Editora FIOCRUZ, Rio de Janeiro, 2011.</li>
<li>Colosimo,E.A., Giolo,S.R., An√°lise de sobreviv√™ncia aplicada, ABE-Projeto Fisher, S√£o Paulo, 2010</li>
<li>Lewis,E.E., Introduction to reliability engineering, John Wiley, New York, 1987</li>
<li><a href="http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Survival/BS704_Survival6.html" class="uri">http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Survival/BS704_Survival6.html</a></li>
<li><a href="http://www.sthda.com/english/wiki/cox-model-assumptions" class="uri">http://www.sthda.com/english/wiki/cox-model-assumptions</a></li>
</ul>
<p>Cuiriosidades / Leituras futuras:</p>
<ul>
<li>Evaluating Random Forests for Survival Analysis Using Prediction Error Curves: <a href="https://www.jstatsoft.org/article/view/v050i11" class="uri">https://www.jstatsoft.org/article/view/v050i11</a></li>
<li>randomForestSRC: <a href="https://cran.r-project.org/web/packages/randomForestSRC/index.html" class="uri">https://cran.r-project.org/web/packages/randomForestSRC/index.html</a></li>
<li>WTTE-RNN - Less hacky churn prediction: <a href="https://ragulpr.github.io/2016/12/22/WTTE-RNN-Hackless-churn-modeling/" class="uri">https://ragulpr.github.io/2016/12/22/WTTE-RNN-Hackless-churn-modeling/</a></li>
<li>Weibull Time To Event Recurrent Neural Network: <a href="https://github.com/ragulpr/wtte-rnn/" class="uri">https://github.com/ragulpr/wtte-rnn/</a></li>
<li>Neural Networks as Statistical Methods in Survival Analysis: <a href="https://www.stats.ox.ac.uk/pub/bdr/NNSM.pdf" class="uri">https://www.stats.ox.ac.uk/pub/bdr/NNSM.pdf</a></li>
<li>Continuous and Discrete Time Survival Analysis: Neural Network
Approaches: <a href="http://pcwww.liv.ac.uk/~afgt/eleuteri_lyon07.pdf" class="uri">http://pcwww.liv.ac.uk/~afgt/eleuteri_lyon07.pdf</a></li>
<li>Cox Proportional Hazards Model - h2O Documentation: <a href="http://s3.amazonaws.com/h2o-release/h2o/master/1579/docs-website/datascience/coxph.html" class="uri">http://s3.amazonaws.com/h2o-release/h2o/master/1579/docs-website/datascience/coxph.html</a></li>
<li>Introduction to H2OCoxPH: <a href="https://www.slideshare.net/0xdata/introduction-to-h2ocoxph" class="uri">https://www.slideshare.net/0xdata/introduction-to-h2ocoxph</a></li>
</ul>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle/">An√°lise de sobreviv√™ncia com dados do jogo PUBG dispon√≠veis no Kaggle</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">analise-de-sobrevivencia</category>
      <category domain="tag">data-mining</category>
      <category domain="tag">estatistica</category>
      <category domain="tag">gamificacao</category>
      <category domain="tag">gamification</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">modelagem-estatistica</category>
      <category domain="tag">r</category>
      <category domain="tag">survivor</category>
    </item>
    <item>
      <title>Um estudo sobre modelos de aprendizagem baseados em √°rvores com desafio do Kaggle</title>
      <link>https://gomesfellipe.github.io/post/2018-08-31-modelos-em-arvore/modelos-em-arvore/</link>
      <pubDate>Fri, 31 Aug 2018 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2018-08-31-modelos-em-arvore/modelos-em-arvore/</guid>
      <description>Um estudo aplicado de modelos de aprendizagem baseados em √°rvores utilizando a base de dados do Kaggle para prever o pre√ßo final de casas residenciais em Ames, Iowa, utilizando uma variedade de aspectos</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="kaggle" class="section level1">
<h1>Kaggle</h1>
<p>Segundo o <a href="https://en.wikipedia.org/wiki/Kaggle">Wikip√©dia</a>: ‚ÄúKaggle √© a maior comunidade mundial de cientistas de dados e machine learning.‚Äù Aprendo muito estudando as resolu√ß√µes de alguns competidores pois l√° √© poss√≠vel conferir tanto as metodologias utilizadas pelos competidores quando os c√≥digos e √© not√°vel o cuidado dos participantes para que seja poss√≠vel a reprodutibilidade dos resultados, o que pode impulsionar o aprendizado.</p>
<p>O Kaggle trabalha com a ideia de <a href="https://en.wikipedia.org/wiki/Gamification">gamifica√ß√£o</a>, que √© um assunto do qual j√° escrevi em um post sobre <a href="https://gomesfellipe.github.io/post/2018-02-17-cheatsheet-gamificacao-r/cheatsheet-gamificacao-r/">gamifica√ß√£o e porque aprender R √© t√£o divertido</a> e gosto deste conceito de se criar jogos para motivar e engajar as pessoas em atividades profissionais e a ideia de se estar em um jogo possibilita doses de motiva√ß√£o especialmente a quem gosta de competir.</p>
<p>A plataforma √© focada em competi√ß√µes que envolvem modelagem preditiva, que julgam apenas o seu desempenho preditivo, embora a inteligibilidade n√£o deixe de ser importante. Neste post farei tamb√©m a modelagem descritiva com modelos de aprendizagem baseados em √°rvores, na qual o principal objetivo ser√° obter informa√ß√µes sobre os dados para o ajuste dos modelos preditivos que iremos submeter √† competi√ß√£o do Kaggle <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/">House Prices: Advanced Regression Techniques</a>.</p>
<p>A diferen√ßa entre modelos preditivos e descritivos n√£o √© t√£o rigorosa assim pois algumas das t√©cnicas podem ser utilizadas para ambos e geralmente um modelo pode servir para ambos os prop√≥sitos (mesmo que de de forma insuficiente).</p>
<p>Al√©m dos modelos de machine learning baseados em √°rvores, tamb√©m ser√° ajustado um modelo de regress√£o linear multivariado para compararmos os resultados dos ajustes e submeter nossas previs√µes no site do <a href="https://kaggle.com">kaggle</a>.</p>
<p>Os pacotes que ser√£o utilizados ser√£o os seguintes:</p>
<pre class="r"><code>library(purrr)       # Programacao funciona
library(broom)       # Arrumar outputs
library(dplyr)       # Manipulacao de dados
library(magrittr)    # pipes
library(funModeling) # df_status()
library(plyr)        # revalue()
library(gridExtra)   # Juntar ggplots
library(reshape)     # funcao melt()
library(rpart)       # Arvore de Decisoes
library(rpart.plot)  # Plot da Arvore de Decisoes
library(data.table)  # aux na manipulacao do heatmap
library(readr)       # Leitura da base de dados
library(stringr)     # Manipulacao de strings
library(ggplot2)     # Graficos elegantes
library(caret)       # Machine Learning 
library(GGally)      # up ggplot
library(ggfortify)   # autoplot()</code></pre>
<div id="base-de-dados" class="section level2">
<h2>Base de dados</h2>
<p>A base de dados deste post vem de uma competi√ß√£o √≥tima para estudantes de ci√™ncia de dados de dados com alguma experi√™ncia com R ou Python e no√ß√µes b√°sicas de machine learning e estat√≠stica.</p>
<p>Pode ser √∫til para aqueles que desejam expandir seu conjunto de habilidades em uma tarefa de regress√£o, quando a vari√°vel <span class="math inline">\(y\)</span> que desejamos estimar √© do tipo num√©rico (cont√≠nuo ou discreto).</p>
<p>Trata-se do <a href="https://ww2.amstat.org/publications/jse/v19n3/decock.pdf">conjunto de dados Ames Housing</a> que foi compilado por Dean De Cock para uso em educa√ß√£o de ci√™ncia de dados.</p>
<pre class="r"><code>train &lt;- read_csv(&quot;train.csv&quot;)
test  &lt;- read_csv(&quot;test.csv&quot;)
full  &lt;- bind_rows(train, test)

id    &lt;- test$Id
full %&lt;&gt;% select(-Id)</code></pre>
<div id="descri√ß√£o-da-competi√ß√£o" class="section level3">
<h3>Descri√ß√£o da Competi√ß√£o</h3>
<p>Traduzido do site oficial do kaggle:</p>
<p>"Pe√ßa a um comprador que descreva a casa dos seus sonhos, e eles provavelmente n√£o come√ßar√£o com a altura do teto do por√£o ou a proximidade de uma ferrovia leste-oeste. Mas o conjunto de dados desta competi√ß√£o de playground prova que muito mais influencia as negocia√ß√µes de pre√ßo do que o n√∫mero de quartos ou uma cerca branca.</p>
<p>Com 79 vari√°veis explicativas descrevendo (quase) todos os aspectos de casas residenciais em Ames, Iowa, esta competi√ß√£o desafia voc√™ a prever o pre√ßo final de cada casa."</p>
<p>Portanto, primeiramente vamos entender o comportamento da vari√°vel resposta, depois buscar quais dessas 79 vari√°veis explicativas s√£o mais importantes para representar a varia√ß√£o do pre√ßo de venda das casas atrav√©s dos m√©todos baseados em √°rvores e por fim ajustar os modelos propostos e submeter nossas estimativas no site!</p>
</div>
</div>
</div>
<div id="an√°lise-explorat√≥ria-dos-dados" class="section level1">
<h1>An√°lise explorat√≥ria dos dados</h1>
<p>Antes de pensar em ajustar algum modelo √© extremamente necess√°rio entender como se comportam os dados, portanto, tanto a vari√°vel resposta quanto as vari√°veis explicativas ser√£o avaliadas.</p>
<div id="vari√°vel-resposta" class="section level2">
<h2>Vari√°vel resposta:</h2>
<p><code>SalePrice</code> - o pre√ßo de venda da propriedade em d√≥lares. Essa √© a vari√°vel de destino que estamos tentando prever.</p>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Note que a distribui√ß√£o dos dados referentes ao pre√ßo de venda se distribui de maneira assim√©trica e n√£o possuem evid√™ncias de normalidade dos dados. Apesar dos m√©todos baseados em √°rvore se tratarem de t√©cnicas n√£o param√©tricas essa transforma√ß√£o ser√° feita pois ao final deste post desejo comparar os resultados com um modelo de regress√£o linear m√∫ltipla.</p>
</div>
</div>
<div id="√°rvore-de-decis√£o" class="section level1">
<h1>√Årvore de decis√£o</h1>
<p>Uma t√©cnica muito popular que √© mais comumente usada para resolver tarefas de classifica√ß√£o de dados por√©m a √°rvore conhecida como <a href="https://tinyurl.com/ybhlsgom">CART (Classification and Regression Trees)(Breiman, 1986)</a> lida com todos os tipos de atributos (incluindo atributos num√©ricos que s√£o tratados a partir da cria√ß√£o de intervalos). Para seu ajuste √© poss√≠vel realizar podas e produzir √°rvores bin√°rias.</p>
<p>A constru√ß√£o da √°rvore √© realizada por meio do algoritmo que iterativamente analisa os atributos descritivos de um conjunto de dados previamente rotulado. Sua popularidade como apoio para a tomada de decis√£o se deve principalmente ao fato da f√°cil visualiza√ß√£o do conhecimento gerado e o f√°cil entendimento.</p>
<p>Outra caracter√≠stica legal da √°rvore de decis√µes √© que ela permite ajustar um modelo sem um pr√©-processamento detalhado, pois √© f√°cil de ajustar, aceita valores faltantes e √© de f√°cil interpreta√ß√£o, veja:</p>
<pre class="r"><code>library(rpart)

control &lt;- rpart.control(minsplit =10, # o n√∫mero m√≠nimo de observa√ß√µes em um n√≥
                         cp = 0.006    # parametro de complexidade q controla o tamanho da arvore
)
rpartFit &lt;- rpart(exp(SalePrice) ~ . , train, method = &quot;anova&quot;, control = control) 

rpart.plot::rpart.plot(rpartFit,cex = 0.6)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-5-1.png" width="1200" /></p>
<p>No topo, vemos o primeiro n√≥ com 100% das observa√ß√µes, que representa o total da base (100%). Em seguida, vemos que a primeira vari√°vel que determina o pre√ßo de venda das casas <code>SalePrice</code> √© a vari√°vel <code>OverallQual</code>. As casas que apresentaram <code>OverallQual</code> &lt; 7.5 ocorrem em maior propor√ß√£o do que as que tiveram <code>OverallQual</code>&gt;7.5. A interpreta√ß√£o pode continuar dessa forma recursivamente.</p>
<p>√â poss√≠vel notar que as vari√°veis <code>OverallQual</code>,<code>Neighborhood</code>,<code>1stFlrSF</code>,<code>2ndFlrSF</code>,<code>GrLivArea</code>, <code>BsmtFinSF1</code> foram as que melhor representaram os dados de acordo com os par√¢metros que determinamos para ajustar esta √°rvore, vejamos com mais detalhes se existe rela√ß√£o linear e intensidade e dire√ß√£o dessa rela√ß√£o com o <a href="https://pt.wikipedia.org/wiki/Coeficiente_de_correla%C3%A7%C3%A3o_de_Pearson">coeficiente de correla√ß√£o de Pearson</a> entre estas vari√°veis dois a dois e em rela√ß√£o √† vari√°vel resposta:</p>
<pre class="r"><code>devtools::source_url(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/correlations_for_ggpairs.R&quot;)

train %&gt;% 
  select(SalePrice,OverallQual,`1stFlrSF`,`2ndFlrSF`,GrLivArea,BsmtFinSF1) %&gt;% 
  ggpairs(lower = list(continuous = my_fn))+
  theme_bw()</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Com esta figura temos muitas informa√ß√µes, destaca-se que todas essas vari√°veis possuem algum tipo de rela√ß√£o linear com a vari√°vel resposta, a menor correla√ß√£o observada foi com o <code>BsmtFinSF1</code> e a vari√°vel que apresentou a maior correla√ß√£o foi a <code>OverallQual</code>. Aten√ß√£o para a correla√ß√£o entre <code>SalePrice</code> e <code>OverallQual</code>, pois <code>Overallqual</code> parece ser uma vari√°vel ordinal e uma outra medida de correla√ß√£o que melhor representaria esta rela√ß√£o √© o <a href="https://pt.wikipedia.org/wiki/Coeficiente_de_correla%C3%A7%C3%A3o_de_postos_de_Spearman">coeficiente de correla√ß√£o de Spearman</a>, veja:</p>
<pre class="r"><code>cor(full$SalePrice, full$OverallQual, method = &quot;spearman&quot;, use = &quot;complete.obs&quot;)</code></pre>
<pre><code>## [1] 0.8098286</code></pre>
<p>Um pouco diferente do resultado da correla√ß√£o de Pearson pois avalia rela√ß√µes lineares, j√° a correla√ß√£o de Spearman avalia rela√ß√µes mon√≥tonas, sejam elas lineares ou n√£o.</p>
<div id="an√°lise-explorat√≥ria-e-input-de-nas" class="section level2 tabset">
<h2>An√°lise explorat√≥ria e input de <code>NA</code>s</h2>
<p>Arrumar a base de dados √© uma tarefa longa e que geralmente consome grande parte no tempo em um projeto de ci√™ncia de dados. N√£o adianta usar o algor√≠timo mais poderoso de machine learning se a base de dados n√£o estiver arrumada de maneira que possibilite a an√°lise dos dados.</p>
<p>Para obter informa√ß√µes da amostra, confira no <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data">link do dataset da competi√ß√£o no Kaggle</a>. Na p√°gina √© poss√≠vel conferir <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/download/data_description.txt">a descri√ß√£o da amostra</a> e nela nota-se que alguns dos valores faltantes possuem significado, ent√£o √© necess√°rio rotul√°-los para que o R possa interpretar estes valores da maneira correta.</p>
<div id="status-da-amostra" class="section level3">
<h3>Status da amostra</h3>
<p>Conferindo o status da amostra com a fun√ß√£o <code>df_status()</code> do pacote <a href="https://cran.r-project.org/web/packages/funModeling/index.html"><code>funModeling</code></a>:</p>
<pre class="r"><code>full %&gt;% 
  df_status(print_results = F) %&gt;% 
  as_tibble() %&gt;%
  arrange(-p_na, -p_zeros)</code></pre>
<pre><code>## # A tibble: 80 x 9
##    variable     q_zeros p_zeros  q_na  p_na q_inf p_inf type      unique
##    &lt;chr&gt;          &lt;int&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;int&gt;
##  1 PoolQC             0       0  2909 99.7      0     0 character      3
##  2 MiscFeature        0       0  2814 96.4      0     0 character      4
##  3 Alley              0       0  2721 93.2      0     0 character      2
##  4 Fence              0       0  2348 80.4      0     0 character      4
##  5 SalePrice          0       0  1459 50.0      0     0 numeric      663
##  6 FireplaceQu        0       0  1420 48.6      0     0 character      5
##  7 LotFrontage        0       0   486 16.6      0     0 numeric      128
##  8 GarageYrBlt        0       0   159  5.45     0     0 numeric      103
##  9 GarageFinish       0       0   159  5.45     0     0 character      3
## 10 GarageQual         0       0   159  5.45     0     0 character      5
## # ‚Ä¶ with 70 more rows</code></pre>
<p>Note que as vari√°veis problem√°ticas foram ordenadas de forma decrescente (maior n√∫mero de dados faltantes e zeros) vamos tratar uma de cada vez partindo da vari√°vel mais cr√≠tica</p>
</div>
<div id="pool" class="section level3">
<h3>Pool</h3>
<ul>
<li><code>PoolQC</code> √© a vari√°vel que possui mais <code>NA</code> e a descri√ß√£o da base informa que:</li>
</ul>
<p><code>PoolQC</code>: qualidade da piscina</p>
<ul>
<li>Ex Excelente</li>
<li>Gd Good</li>
<li>TA M√©dia / T√≠pica</li>
<li>Fa Pequena</li>
<li>NA sem piscina</li>
</ul>
<p>√â poss√≠vel observar que se trata de uma vari√°vel ordinal, portanto vamos criar uma vari√°vel auxiliar (pois esta descri√ß√£o se repete em outras vari√°veis):</p>
<pre class="r"><code># Criando vari√°vel auxilar ordinal
Qualidade &lt;- c(&#39;None&#39; = 0, &#39;Po&#39; = 1, &#39;Fa&#39; = 2, &#39;TA&#39; = 3, &#39;Gd&#39; = 4, &#39;Ex&#39; = 5)

full %&lt;&gt;%
  mutate(PoolQC =  ifelse(PoolQC %&gt;% is.na, &quot;None&quot;, PoolQC) %&gt;% as.factor() ) %&gt;% 
  mutate(PoolQC = as.integer(revalue(PoolQC, Qualidade)))</code></pre>
<p>Al√©m disso, existe outra vari√°vel relacionada √† piscina, veja:</p>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Pool&quot;)]) %&gt;% 
  table </code></pre>
<pre><code>##         PoolQC
## PoolArea    1    2    3    4
##      0      0    0    0 2906
##      144    1    0    0    0
##      228    1    0    0    0
##      368    0    0    0    1
##      444    0    0    0    1
##      480    0    0    1    0
##      512    1    0    0    0
##      519    0    1    0    0
##      555    1    0    0    0
##      561    0    0    0    1
##      576    0    0    1    0
##      648    0    1    0    0
##      738    0    0    1    0
##      800    0    0    1    0</code></pre>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Pool&quot;)]) %&gt;%
  map(~sum(is.na(.x)))</code></pre>
<pre><code>## $PoolArea
## [1] 0
## 
## $PoolQC
## [1] 0</code></pre>
<pre class="r"><code># Arrumando inconsist√´ncias:
full %&lt;&gt;% 
  mutate(PoolQC = ifelse(PoolQC == 0 &amp; PoolArea !=0, 2, PoolQC))

# Arrumando inconsist√´ncias:
full %&lt;&gt;% 
  mutate(Pool = ifelse(PoolQC == 0 &amp; PoolArea ==0, &quot;no&quot;, &quot;yes&quot;))</code></pre>
</div>
<div id="misc" class="section level3">
<h3>Misc</h3>
<p>Se referem aos recursos diversos</p>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Misc&quot;)],
         SalePrice
  ) %&gt;%
  map(~sum(is.na(.x)))</code></pre>
<pre><code>## $MiscFeature
## [1] 2814
## 
## $MiscVal
## [1] 0
## 
## $SalePrice
## [1] 1459</code></pre>
<p><code>MiscFeature</code>: recurso diverso n√£o coberto em outras categorias</p>
<ul>
<li>Elevador elev</li>
<li>Gar2 2nd Garage (se n√£o for descrito na se√ß√£o de garagem)</li>
<li>Othr Outro</li>
<li>Galp√£o derramado (mais de 100 SF)</li>
<li>TenC Campo de t√©nis</li>
<li>NA Nenhum</li>
</ul>
<p>Desta vez n√£o se trata de uma vari√°vel ordinal, vejamos:</p>
<pre class="r"><code>full %&lt;&gt;%
  mutate(MiscFeature =  if_else(MiscFeature %&gt;% is.na, &quot;None&quot;, MiscFeature) %&gt;% as.factor) 

# Breve resumo:
g1 &lt;- 
  full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Misc&quot;)], SalePrice) %&gt;% 
  ggplot(aes(y=MiscVal,x= reorder(MiscFeature, -MiscVal,FUN = median) ,fill=MiscFeature))+
  geom_boxplot()+
  theme_bw() +
  scale_fill_viridis_d() +
  labs(x = &quot;Recurso Diverso&quot;)

g2 &lt;- 
  full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Misc&quot;)], SalePrice) %&gt;% 
  ggplot(aes(y=SalePrice,x= reorder(MiscFeature, -MiscVal,FUN = median) ,fill=MiscFeature))+
  geom_boxplot()+
  theme_bw() +
  scale_fill_viridis_d() +
  labs(x = &quot;Pre√ßo de Venda&quot;)

grid.arrange(g1, g2)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<pre class="r"><code>rm(g1,g2)</code></pre>
<p>Al√©m disso, <code>MiscVal</code>: Valor do recurso variado</p>
</div>
<div id="alley" class="section level3">
<h3>Alley</h3>
<p><code>Alley</code>: Tipo de acesso ao beco para a propriedade</p>
<ul>
<li>Grvl Cascalho</li>
<li>Pave pavimentado</li>
<li>NA Nenhum acesso de beco</li>
</ul>
<p>Basta realizar o input:</p>
<pre class="r"><code>full %&lt;&gt;% 
  mutate(Alley = Alley %&gt;% str_replace_na(&quot;None&quot;)) %&gt;% 
  mutate(Alley = as.factor(Alley))</code></pre>
<pre class="r"><code>full[!is.na(full$SalePrice),] %&gt;% 
  select(Alley, SalePrice) %&gt;% 
  ggplot(aes(y=SalePrice,x= reorder(Alley, -SalePrice,FUN = median) ,fill=Alley))+
  geom_boxplot()+
  theme_bw() +
  scale_fill_viridis_d() +
  labs(x = &quot;tipo de Acesso&quot;)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
</div>
<div id="fence" class="section level3">
<h3>Fence</h3>
<p><code>Fence</code>: qualidade da cerca</p>
<ul>
<li>GdPrv Boa privacidade</li>
<li>MnPrv minima privacidade</li>
<li>GdWo boa madeira</li>
<li>MnWw M√≠nima Madeira / Fio</li>
<li>NA Sem cerca</li>
</ul>
<p>Input ser√° da seguinte forma:</p>
<pre class="r"><code>full %&lt;&gt;% 
  mutate(Fence = Fence %&gt;% str_replace_na(&quot;None&quot;))</code></pre>
<pre class="r"><code>full[1:nrow(train),] %&gt;% 
  select(Fence, SalePrice) %&gt;% 
  ggplot(aes(y=SalePrice,x= reorder(Fence, -SalePrice, median) ,fill=Fence))+
  geom_boxplot()+
  theme_bw() +
  scale_fill_viridis_d() +
  labs(x = &quot;tipo de Acesso&quot;)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<pre class="r"><code>full %&lt;&gt;% mutate(Fence = as.factor(Fence))</code></pre>
<p>Aparentemente n√£o parece existir uma rela√ß√£o ordinal sobre o tipo de cerca quanto ao pre;o de venda da casa, portanto foi convertida para fator</p>
</div>
<div id="fireplace" class="section level3">
<h3>FirePlace</h3>
<p>Vari√°veis relacionadas com lareira. Segundo a descri√ß√£o, temos:</p>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Fireplace&quot;)], SalePrice)</code></pre>
<pre><code>## # A tibble: 2,919 x 3
##    Fireplaces FireplaceQu SalePrice
##         &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;
##  1          0 &lt;NA&gt;             12.2
##  2          1 TA               12.1
##  3          1 TA               12.3
##  4          1 Gd               11.8
##  5          1 TA               12.4
##  6          0 &lt;NA&gt;             11.9
##  7          1 Gd               12.6
##  8          2 TA               12.2
##  9          2 TA               11.8
## 10          2 TA               11.7
## # ‚Ä¶ with 2,909 more rows</code></pre>
<p><code>Fireplaces</code>: Numero de lareiras</p>
<p><code>FireplaceQu</code>: Qualidade da lareira</p>
<ul>
<li>Ex Excellente - Excepcional Lareira de Alvenaria</li>
<li>Gd Boa - Lareira de alvenaria no n√≠vel principal</li>
<li>TA M√©dia - lareira pr√©-fabricada na sala principal ou Lareira de alvenaria no por√£o</li>
<li>Fa Pequena - Lareira pr√©-fabricada no por√£o</li>
<li>Po Pobre - Fog√£o Ben Franklin</li>
<li>NA sem lareira</li>
</ul>
<p>Nota-se que se trata de uma vari√°vel ordinal de acordo com a qualidade, portanto:</p>
<pre class="r"><code>full %&lt;&gt;% 
  mutate(FireplaceQu =  if_else(FireplaceQu %&gt;% is.na, &quot;None&quot;, FireplaceQu) ) %&gt;% 
  mutate(FireplaceQu = as.integer(revalue(FireplaceQu, Qualidade)))</code></pre>
<p>Conferindo se existem inconsist√™ncias:</p>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Fireplace&quot;)]) %&gt;% 
  table </code></pre>
<pre><code>##           FireplaceQu
## Fireplaces    0    1    2    3    4    5
##          0 1420    0    0    0    0    0
##          1    0   46   63  495  627   37
##          2    0    0   10   92  112    5
##          3    0    0    1    4    5    1
##          4    0    0    0    1    0    0</code></pre>
</div>
<div id="lot" class="section level3">
<h3>Lot</h3>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Lot&quot;)], SalePrice) %&gt;% 
  map_dbl(~sum(is.na(.x)))</code></pre>
<pre><code>## LotFrontage     LotArea    LotShape   LotConfig   SalePrice 
##         486           0           0           0        1459</code></pre>
<p>Segundo a descri√ß√£o:</p>
<p><code>LotFrontage</code>: Ruas linearmente conectadas √† propriedade</p>
<p><code>LotArea</code> : Tamanho do lote em p√©s quadrados</p>
<p><code>LotShape</code>: forma geral da propriedade</p>
<ul>
<li>Regue Regular<br />
</li>
<li>IR1 ligeiramente irregular</li>
<li>IR2 moderadamente irregular</li>
<li>IR3 Irregular</li>
</ul>
<p><code>LotConfig</code>: configura√ß√£o de lote</p>
<ul>
<li>Inside Lote muito para dentro</li>
<li>Corner Canto de esquina</li>
<li>CulDSac Cul-de-sac</li>
<li>FR2 Frente em 2 lados da propriedade</li>
<li>FR3 Frente em 3 lados da propriedade</li>
</ul>
<p>Input para o <code>LotFrontage</code> ser√° feito considerando a configura√ß√£o do lote, veja:</p>
<pre class="r"><code>inputsLot &lt;- full %&gt;% 
  select(LotFrontage, LotConfig) %&gt;% 
  group_by(LotConfig) %&gt;%
  dplyr::summarise(Media = mean(LotFrontage, na.rm = T),
            Mediana = median(LotFrontage, na.rm = T))

full$LotFrontage[is.na(full$LotFrontage) &amp; full$LotConfig == inputsLot$LotConfig[1]] &lt;- inputsLot$Mediana[1] 
full$LotFrontage[is.na(full$LotFrontage) &amp; full$LotConfig == inputsLot$LotConfig[2]] &lt;- inputsLot$Mediana[2] 
full$LotFrontage[is.na(full$LotFrontage) &amp; full$LotConfig == inputsLot$LotConfig[3]] &lt;- inputsLot$Mediana[3] 
full$LotFrontage[is.na(full$LotFrontage) &amp; full$LotConfig == inputsLot$LotConfig[4]] &lt;- inputsLot$Mediana[4] 
full$LotFrontage[is.na(full$LotFrontage) &amp; full$LotConfig == inputsLot$LotConfig[5]] &lt;- inputsLot$Mediana[5] </code></pre>
<p>Arrumando vari√°veis nominais e ordinais:</p>
<pre class="r"><code>full %&lt;&gt;% 
  mutate(LotShape = as.integer(revalue(full$LotShape, c(&#39;IR3&#39;=0, &#39;IR2&#39;=1, &#39;IR1&#39;=2, &#39;Reg&#39;=3))))</code></pre>
</div>
<div id="garages" class="section level3">
<h3>Garages</h3>
<p>Vari√°veis relacionadas, segundo a descri√ß√£o, temos:</p>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Garage&quot;)], SalePrice) %&gt;% 
  map_dbl(~sum(is.na(.x)))</code></pre>
<pre><code>##   GarageType  GarageYrBlt GarageFinish   GarageCars   GarageArea   GarageQual 
##          157          159          159            1            1          159 
##   GarageCond    SalePrice 
##          159         1459</code></pre>
<p><code>GarageType</code>: localiza√ß√£o da garagem</p>
<ul>
<li>2Types Mais de um tipo de garagem</li>
<li>Attchd anexa a casa</li>
<li>Basement tipo porao</li>
<li>BuiltIn (garagem parte da casa - normalmente tem sala acima da garagem)</li>
<li>CarPort Porta do carro</li>
<li>Detchd nao anexa a casa</li>
<li>NA Sem Garagem</li>
</ul>
<p><code>GarageYrBlt</code>: garagem do ano foi constru√≠da</p>
<p><code>GarageFinish</code>: acabamento interior da garagem</p>
<ul>
<li>Fin Finished</li>
<li>RFn √Åspero Finalizado<br />
</li>
<li>Unf inacabado</li>
<li>NA Sem Garagem</li>
</ul>
<p><code>GarageCars</code>: Tamanho da garagem na capacidade do carro</p>
<p><code>GarageArea</code>: Tamanho da garagem em p√©s quadrados</p>
<p><code>GarageQual</code>: GarageQuality</p>
<ul>
<li>Ex Excelente</li>
<li>Gd Good</li>
<li>TA T√≠pico / M√©dio</li>
<li>FA Justo</li>
<li>Po Poor</li>
<li>NA Sem Garagem</li>
</ul>
<p><code>GarageCond</code>: condi√ß√£o de garagem</p>
<ul>
<li>Ex Excelente</li>
<li>Gd Good</li>
<li>TA T√≠pico / M√©dio</li>
<li>Fa Justo</li>
<li>Po Poor</li>
<li>NA Sem Garagem</li>
</ul>
<pre class="r"><code>full %&lt;&gt;% 
  mutate(GarageType   =  if_else(GarageType %&gt;% is.na, &quot;None&quot;, GarageType) ) %&gt;% 
  mutate(GarageYrBlt  = if_else(GarageYrBlt %&gt;% is.na,YearBuilt, GarageYrBlt) ) %&gt;% 
  mutate(GarageFinish =  if_else(GarageFinish %&gt;% is.na, &quot;None&quot;, GarageFinish) ) %&gt;% 
  mutate(GarageFinish = as.integer(revalue(GarageFinish, c(&#39;None&#39;=0, &#39;Unf&#39;=1, &#39;RFn&#39;=2, &#39;Fin&#39;=3)))) %&gt;% 
  mutate(GarageCars   = ifelse(GarageCars %&gt;% is.na, 0, GarageCars) ) %&gt;% 
  mutate(GarageArea   = ifelse(GarageArea %&gt;% is.na, 0, GarageArea)) %&gt;% 
  mutate(GarageQual   = if_else(GarageQual %&gt;% is.na, &quot;None&quot;, GarageQual)) %&gt;% 
  mutate(GarageQual   = as.integer(revalue(GarageQual, Qualidade))) %&gt;% 
  mutate(GarageCond   = if_else(GarageCond %&gt;% is.na, &quot;None&quot;, GarageCond)) %&gt;% 
  mutate(GarageCond   = as.integer(revalue(GarageCond, Qualidade))) 
  
table(full$GarageCond)</code></pre>
<pre><code>## 
##    0    1    2    3    4    5 
##  159   14   74 2654   15    3</code></pre>
</div>
<div id="bsmt" class="section level3">
<h3>Bsmt</h3>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Bsmt&quot;)], SalePrice) %&gt;% 
  map_dbl(~sum(is.na(.x)))</code></pre>
<pre><code>##     BsmtQual     BsmtCond BsmtExposure BsmtFinType1   BsmtFinSF1 BsmtFinType2 
##           81           82           82           79            1           80 
##   BsmtFinSF2    BsmtUnfSF  TotalBsmtSF BsmtFullBath BsmtHalfBath    SalePrice 
##            1            1            1            2            2         1459</code></pre>
<p><code>BsmtQual</code>: Avalia a altura do por√£o</p>
<ul>
<li>Ex Excelente (100+ polegadas)<br />
</li>
<li>Gd Bom (90-99 polegadas)</li>
<li>TA T√≠pica (80-89 polegadas)</li>
<li>Fa Justo (70-79 polegadas)</li>
<li>Po Pobre (&lt;70 polegadas</li>
<li>NA Sem Por√£o</li>
</ul>
<p><code>BsmtCond</code>: Avalia o estado geral do por√£o</p>
<ul>
<li>Ex Excelente</li>
<li>Gd Bom</li>
<li>TA T√≠pica - umidade ligeira permitida</li>
<li>Fa Razo√°vel - umidade ou alguma rachadura ou sedimenta√ß√£o</li>
<li>Po Insuficiente - Craqueamento severo, sedimenta√ß√£o ou umidade</li>
<li>NA Sem Por√£o</li>
</ul>
<p><code>BsmtExposure</code>: Refere-se a paralisa√ß√µes ou paredes no n√≠vel do jardim</p>
<ul>
<li>Gd Good Exposi√ß√£o</li>
<li>Av M√©dia Exposi√ß√£o (n√≠veis divididos ou foyers normalmente pontua√ß√£o m√©dia ou acima)<br />
</li>
<li>Mn Exposi√ß√£o M√≠nima</li>
<li>No N√£o Exposi√ß√£o</li>
<li>NA Sem por√£o</li>
</ul>
<p><code>BsmtFinType1</code>: Avalia√ß√£o da √°rea acabada do por√£o</p>
<ul>
<li>GLQ Bons Viver</li>
<li>ALQ M√©dia Living Quarters</li>
<li>BLQ Abaixo da m√©dia Living Quarters<br />
</li>
<li>Rec M√©dia Rec Room</li>
<li>LwQ Baixa Qualidade</li>
<li>Unf unfinshed</li>
<li>NA nenhum por√£o</li>
</ul>
<p><code>BsmtFinSF1</code>: pes quadrados do tipo 1 terminado</p>
<p><code>BsmtFinType2</code>: Avalia√ß√£o do por√£o √°rea terminado (se v√°rios tipos)</p>
<ul>
<li>GLQ Bons aposentos</li>
<li>ALQ Medianos</li>
<li>BLQ abaixo da media</li>
<li>Rec Aposentos m√©dia qualidade</li>
<li>LwQ Baixa Qualidade</li>
<li>Unf</li>
<li>N√£o Sem Por√£o</li>
</ul>
<p><code>BsmtFinSF2</code>: P√©s quadrados acabados do Tipo 2</p>
<p><code>BsmtUnfSF</code>: P√©s quadrados inacabados da √°rea do por√£o</p>
<p><code>TotalBsmtSF</code>: Total p√©s quadrados da √°rea do por√£o</p>
<p>Input das vari√°veis n√£o num√©ricas com <code>None</code> e convertendo para ordinal as vari√°veis com rela√ß√£o de ordem. Para os faltantes das vari√°veis num√©ricas foram imputados o valor 0 (zeros).</p>
<pre class="r"><code># Categ√≥ricos:
full[,names(full)[names(full) %&gt;% str_detect(&quot;Bsmt&quot;)]] &lt;- 
  full[,names(full)[names(full) %&gt;% str_detect(&quot;Bsmt&quot;)]] %&gt;%
  select(names(full)[names(full) %&gt;% str_detect(&quot;Bsmt&quot;)]) %&gt;%
  mutate_if( ~ !is.numeric(.x) , ~ ifelse(is.na(.x), &quot;None&quot;, .x)) %&gt;% 
  mutate(BsmtQual = as.integer(revalue(BsmtQual, Qualidade))) %&gt;% 
  mutate(BsmtCond = as.integer(revalue(BsmtCond, Qualidade))) %&gt;% 
  mutate(BsmtExposure = as.integer(revalue(BsmtExposure, c(&#39;None&#39;=0, &#39;No&#39;=1, &#39;Mn&#39;=2, &#39;Av&#39;=3, &#39;Gd&#39;=4)))) %&gt;% 
  mutate(BsmtFinType1 = as.integer(revalue(BsmtFinType1,c(&#39;None&#39;=0, &#39;Unf&#39;=1, &#39;LwQ&#39;=2, &#39;Rec&#39;=3, &#39;BLQ&#39;=4, &#39;ALQ&#39;=5, &#39;GLQ&#39;=6)))) 

# Num√©ricos:
full[,names(full)[names(full) %&gt;% str_detect(&quot;Bsmt&quot;)]] &lt;- 
  full[,names(full)[names(full) %&gt;% str_detect(&quot;Bsmt&quot;)]] %&gt;%
  select(names(full)[names(full) %&gt;% str_detect(&quot;Bsmt&quot;)]) %&gt;%
  mutate_if( ~ is.numeric(.x) , ~ ifelse(is.na(.x), 0, .x))</code></pre>
</div>
<div id="masvnr" class="section level3">
<h3>MasVnr</h3>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;MasVnr&quot;)], SalePrice) %&gt;% 
  map_dbl(~sum(is.na(.x)))</code></pre>
<pre><code>## MasVnrType MasVnrArea  SalePrice 
##         24         23       1459</code></pre>
<p><code>MasVnrType</code>: Alvenaria tipo de verniz</p>
<ul>
<li>BrkCmn Brick Common</li>
<li>BrkFace Face de tijolos</li>
<li>CBlock Bloco cinza</li>
<li>None Nenhum</li>
<li>Stone Pedra</li>
</ul>
<p><code>MasVnrArea</code>: √Årea de folheado de alvenaria em p√©s quadrados</p>
<pre class="r"><code>full %&lt;&gt;% 
  mutate(MasVnrType = if_else(is.na(MasVnrType), &quot;None&quot;, MasVnrType)) %&gt;% 
  mutate(MasVnrType = as.integer(revalue(MasVnrType, c(&#39;None&#39;=0, &#39;BrkCmn&#39;=0, &#39;BrkFace&#39;=1, &#39;Stone&#39;=2)))) %&gt;% 
  mutate(MasVnrArea = if_else(is.na(MasVnrArea), 0, 1))</code></pre>
</div>
<div id="vari√°veis-restantes-com-poucos-na" class="section level3">
<h3>Vari√°veis restantes com poucos <code>NA</code></h3>
<p>A estrat√©gia adotada para imputar estes dados ser√° tomada de maneira arbitr√°ria. Os valores faltantes ser√£o preenchidos com o valor comum mais frequente daquela vari√°vel. As vari√°veis que restam s√£o:</p>
<pre class="r"><code>full %&gt;% 
  df_status(print_results = F) %&gt;% 
  as_tibble() %&gt;%
  arrange(-p_na, -p_zeros)</code></pre>
<pre><code>## # A tibble: 81 x 9
##    variable    q_zeros p_zeros  q_na  p_na q_inf p_inf type      unique
##    &lt;chr&gt;         &lt;int&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;int&gt;
##  1 SalePrice         0     0    1459 50.0      0     0 numeric      663
##  2 MSZoning          0     0       4  0.14     0     0 character      5
##  3 Utilities         0     0       2  0.07     0     0 character      2
##  4 Functional        0     0       2  0.07     0     0 character      7
##  5 Exterior1st       0     0       1  0.03     0     0 character     15
##  6 Exterior2nd       0     0       1  0.03     0     0 character     16
##  7 Electrical        0     0       1  0.03     0     0 character      5
##  8 KitchenQual       0     0       1  0.03     0     0 character      4
##  9 SaleType          0     0       1  0.03     0     0 character      9
## 10 PoolArea       2906    99.6     0  0        0     0 numeric       14
## # ‚Ä¶ with 71 more rows</code></pre>
<p>Vejamos:</p>
<p><code>MSZoning</code>: Identifica a classifica√ß√£o geral de zoneamento da venda.</p>
<ul>
<li>Ser√° convertida para fator, vari√°vel nominal</li>
</ul>
<p><code>KitchenQual</code>: Qualidade da cozinha</p>
<ul>
<li>Ser√° convertida para ordinal</li>
</ul>
<p><code>Utilities</code>: Tipo de utilidade dispon√≠vel</p>
<ul>
<li>Ser√° removida</li>
</ul>
<p><code>Functional</code>: Funcionalidade dom√©stica</p>
<ul>
<li>Ser√° considerada como ordinal</li>
</ul>
<p><code>Exterior1st</code>: revestimento Exterior em casa</p>
<ul>
<li>Convertida para fator, vari√°vel nominal</li>
</ul>
<p><code>Electrical</code>: Sistema el√©trico</p>
<ul>
<li>Convertida para fator, vari√°vel nominal</li>
</ul>
<p><code>SaleType</code>: Tipo de venda</p>
<ul>
<li>Convertida para fator, vari√°vel nominal</li>
</ul>
<pre class="r"><code>full &lt;- full %&gt;% 
  mutate(MSZoning    = ifelse(is.na(MSZoning),
                            full$MSZoning %&gt;% table %&gt;% sort %&gt;% names %&gt;% last, MSZoning)) %&gt;% 
  mutate(MSZoning    = as.factor(MSZoning)) %&gt;% 
  mutate(KitchenQual = ifelse(is.na(KitchenQual),
                            full$KitchenQual %&gt;% 
                              table %&gt;% sort %&gt;% names %&gt;% last, KitchenQual)) %&gt;% 
  mutate(KitchenQual = as.integer(revalue(as.character(full$KitchenQual), Qualidade))) %&gt;% 
  select(-Utilities) %&gt;% 
  mutate(Exterior1st = ifelse(is.na(Exterior1st),
                            full$Exterior1st %&gt;% table %&gt;% sort %&gt;% names %&gt;% last, Exterior1st)) %&gt;% 
  mutate(Exterior1st = as.factor(Exterior1st)) %&gt;% 
  mutate(Exterior2nd = ifelse(is.na(Exterior2nd),
                            full$Exterior2nd %&gt;% table %&gt;% sort %&gt;% names %&gt;% last, Exterior2nd)) %&gt;% 
  mutate(Exterior2nd = as.factor(Exterior2nd)) %&gt;% 
  mutate(Electrical  = ifelse(is.na(Electrical),
                            full$Electrical %&gt;% table %&gt;% sort %&gt;% names %&gt;% last, Electrical)) %&gt;% 
  mutate(Electrical  = as.factor(Electrical)) %&gt;% 
  mutate(SaleType    = ifelse(is.na(SaleType ),
                            full$SaleType  %&gt;% table %&gt;% sort %&gt;% names %&gt;% last, SaleType )) %&gt;% 
  mutate(SaleType    = as.factor(SaleType )) 


full[is.na(full$Functional),&quot;Functional&quot;] &lt;- full$Functional %&gt;% table %&gt;% sort %&gt;% names %&gt;% last
full$Functional = as.integer(revalue(full$Functional, c(&#39;Sal&#39;=0, &#39;Sev&#39;=1, &#39;Maj2&#39;=2, &#39;Maj1&#39;=3, &#39;Mod&#39;=4, &#39;Min2&#39;=5, &#39;Min1&#39;=6, &#39;Typ&#39;=7)))
full[is.na(full$KitchenQual),&quot;KitchenQual&quot;] &lt;- full$KitchenQual %&gt;% table %&gt;% sort %&gt;% names %&gt;% last %&gt;% as.numeric()
full$KitchenQual = as.integer(revalue(as.character(full$KitchenQual), Qualidade))
# full[is.na(full$Electrical),&quot;Electrical&quot;] &lt;- 3

to_remove &lt;- full %&gt;% map(~table(.x) %&gt;% length()) %&gt;% .[.== 1] %&gt;% names()
full &lt;- full %&gt;% select(-one_of(to_remove))</code></pre>
<p>Status da base no momento:</p>
<pre class="r"><code>full %&gt;% 
  df_status(print_results = F) %&gt;% 
  as_tibble() %&gt;%
  arrange(-p_na,-p_zeros, type)</code></pre>
<pre><code>## # A tibble: 79 x 9
##    variable      q_zeros p_zeros  q_na  p_na q_inf p_inf type    unique
##    &lt;chr&gt;           &lt;int&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt;
##  1 SalePrice           0     0    1459  50.0     0     0 numeric    663
##  2 PoolArea         2906    99.6     0   0       0     0 numeric     14
##  3 3SsnPorch        2882    98.7     0   0       0     0 numeric     31
##  4 LowQualFinSF     2879    98.6     0   0       0     0 numeric     36
##  5 MiscVal          2816    96.5     0   0       0     0 numeric     38
##  6 BsmtHalfBath     2744    94       0   0       0     0 numeric      3
##  7 ScreenPorch      2663    91.2     0   0       0     0 numeric    121
##  8 BsmtFinSF2       2572    88.1     0   0       0     0 numeric    272
##  9 EnclosedPorch    2460    84.3     0   0       0     0 numeric    183
## 10 HalfBath         1834    62.8     0   0       0     0 numeric      3
## # ‚Ä¶ with 69 more rows</code></pre>
<p>Transformando o <code>character</code> para <code>factor</code>:</p>
<pre class="r"><code>full %&lt;&gt;% mutate_if(is.character, as.factor)</code></pre>
<p>Transformando novamente nossa base de treino e de teste:</p>
<pre class="r"><code>train &lt;- full[1:nrow(train),] %&gt;% as.data.frame() 
test  &lt;- full[(nrow(train)+1):nrow(full),] %&gt;% select(-SalePrice) %&gt;% as.data.frame()

# # Input Missing
# train_miss_model = preProcess(train, &quot;knnImpute&quot;)
# train = predict(train_miss_model, train)
# test = predict(train_miss_model, test)
# 
# train$SalePrice &lt;- y</code></pre>
</div>
</div>
</div>
<div id="machine-learning-com-algor√≠tmos-de-aprendizagem-baseados-em-√°rvores" class="section level1">
<h1>Machine Learning com algor√≠tmos de aprendizagem baseados em √°rvores</h1>
<p>Os m√©todos baseados em √°rvores fornecem modelos preditivos de alta precis√£o, estabilidade e facilidade de interpreta√ß√£o. Ao contr√°rio dos modelos lineares, eles s√£o capazes de lidar bem com rela√ß√µes n√£o-lineares al√©m de poderem ser adaptados para resolver tanto problemas de classifica√ß√£o quanto problemas de regress√£o.</p>
<p>Algoritmos como √°rvores de decis√£o, random forest e ‚Äúgradient boosting‚Äù est√£o sendo muito usados em todos os tipos de problemas de data science e √© not√°vel o uso desses algor√≠timos para resolver os desafios do <a href="https://www.kaggle.com/">Kaggle</a>. Para resolver este problema utilizaremos estes tr√™s algoritmos e ao final, pegando carona na sele√ß√£o de vari√°veis para os algoritmos de √°rvore, ser√° ajustado um modelo de regress√£o linear para compararmos e conferirmos a signific√¢ncia estat√≠stica de cada uma das vari√°veis.</p>
<div id="varimp-com-random-forest" class="section level2">
<h2>VarImp com Random Forest</h2>
<p>Um dos benef√≠cios da floresta aleat√≥ria √© o poder de lidar com grande conjunto de dados com maior dimensionalidade e identificar as vari√°veis a import√¢ncia das vari√°veis, que pode ser uma caracter√≠stica muito √∫til por√©m deve ser feita com cautela.</p>
<p>Veja uma reflex√£o (traduzida) da <a href="https://www.stat.berkeley.edu/~breiman/RandomForests/reg_philosophy.htm">nota de Leo Breiman (Universidade da Calif√≥rnia em Berkeley)</a></p>
<blockquote>
<p>‚ÄúUma nota filos√≥fica: RF √© um exemplo de uma ferramenta que √© √∫til para fazer an√°lises de dados cient√≠ficos; Mas os algoritmos mais inteligentes n√£o substituem a intelig√™ncia humana e o conhecimento dos dados do problema; Pegue a sa√≠da de florestas aleat√≥rias n√£o como verdade absoluta, mas como suposi√ß√µes geradas por um computador inteligente que podem ser √∫teis para levar a uma compreens√£o mais profunda do problema.‚Äù</p>
</blockquote>
<p>O ajuste da √°rvore ser√° feito com o pacote <code>caret</code> e o estudo de estimativas de erro foi definido como o <a href="https://en.wikipedia.org/wiki/Out-of-bag_error">Out of bag</a> que remove a necessidade de um conjunto de teste pois √© o erro m√©dio de previs√£o em cada amostra de treinamento <span class="math inline">\(x_i\)</span> , usando apenas as √°rvores que n√£o tinham <span class="math inline">\(x_i\)</span> em sua amostra de <a href="https://www.ime.usp.br/~chang/home/mae5704/aula-bootstrap.pdf">bootstrap</a>.</p>
<pre class="r"><code>set.seed(1)
control &lt;- trainControl(method = &quot;oob&quot;,verboseIter = F)

rfFit1 &lt;- train(SalePrice ~. ,
      data=train,
      method=&quot;rf&quot;,
      metric = &quot;Rsquared&quot;,
      trControl = control,
      preProcess = c(&quot;knnImpute&quot;)
      )

randomForest::varImpPlot(rfFit1$finalModel)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<pre class="r"><code>rfFit1$finalModel$importance %&gt;% 
  as.data.frame %&gt;%
  mutate(row = rownames(.)) %&gt;% 
  arrange(desc(IncNodePurity)) %&gt;% 
  as_tibble()</code></pre>
<pre><code>## # A tibble: 217 x 2
##    IncNodePurity row        
##            &lt;dbl&gt; &lt;chr&gt;      
##  1         77.9  OverallQual
##  2         35.0  GrLivArea  
##  3         14.8  YearBuilt  
##  4         11.5  KitchenQual
##  5          9.75 TotalBsmtSF
##  6          9.29 GarageCars 
##  7          6.74 `1stFlrSF` 
##  8          6.33 GarageArea 
##  9          5.02 ExterQualTA
## 10          4.04 BsmtFinSF1 
## # ‚Ä¶ with 207 more rows</code></pre>
<p>Ap√≥s inspecionar a import√¢ncia das vari√°veis vamos selecionar as seguintes vari√°veis:</p>
<pre class="r"><code>full %&lt;&gt;% 
  select(
    SalePrice  , Neighborhood, OverallQual , GrLivArea   , YearBuilt   ,  KitchenQual, 
    GarageCars ,  GarageArea , `1stFlrSF`  , ExterQual   , BsmtFinSF1  , FireplaceQu, 
    BsmtQual   , `2ndFlrSF`  , CentralAir  , GarageFinish, YearRemodAdd, FullBath, 
    GarageYrBlt, Fireplaces  , LotFrontage , BsmtUnfSF   , TotalBsmtSF , BsmtFinType1,
    OpenPorchSF, GarageType  , BsmtExposure, OverallCond , TotalBsmtSF , LotArea
  )</code></pre>
<p>Portanto, vamos definir novamente o conjunto de dados de treino e de teste:</p>
<pre class="r"><code>train &lt;- full[1:nrow(train),] %&gt;% as.data.frame()
test  &lt;- full[(nrow(train)+1):nrow(full),-1] %&gt;% as.data.frame()</code></pre>
</div>
<div id="vari√°veis-num√©ricas" class="section level2">
<h2>Vari√°veis num√©ricas</h2>
<p>Ap√≥s a sele√ß√£o dessas vari√°veis, vamos entender como elas est√£o correlacionadas dois a dois com o <a href="https://pt.wikipedia.org/wiki/Coeficiente_de_correla%C3%A7%C3%A3o_de_Pearson">coeficiente de correla√ß√£o de pearson</a>, exibindo a matrix em um <a href="https://en.wikipedia.org/wiki/Heat_map">Heatmap</a> (ou mapa de calor ), que √© uma representa√ß√£o gr√°fica de dados em que os valores individuais contidos em uma matriz representados como cores.</p>
<pre class="r"><code>cormat &lt;- 
  full %&gt;% 
  select(SalePrice, everything()) %&gt;% 
  select_if(is.numeric) %&gt;% 
  as.data.frame() %&gt;% 
  cor(use = &quot;na.or.complete&quot;) %&gt;% 
  melt

cormat %&gt;%   
  ggplot( aes(reorder(Var1,value), reorder(Var2,value), fill=value))+
  geom_tile(color=&quot;white&quot;)+
  scale_fill_gradient2(low=&quot;blue&quot;, high=&quot;red&quot;, mid=&quot;white&quot;, midpoint=0, limit=c(-1,1), space=&quot;Lab&quot;, name=&quot;Pearson\nCorrelation&quot;)+
  theme_bw()+
  theme(axis.text.x=element_text(angle=45, vjust=1, size=10, hjust=1))+
  coord_fixed()+
  labs(x=&quot;&quot;,y=&quot;&quot;)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-37-1.png" width="1152" /></p>
<p>√â poss√≠vel notar que existem vari√°veis explicativas correlacionadas o que indica que a presen√ßa de algumas vari√°veis pode possivelmente interferir no ajuste final do modelo linear multivariado.</p>
</div>
<div id="vari√°veis-categ√≥ricas" class="section level2">
<h2>Vari√°veis categ√≥ricas</h2>
<p>J√° a rela√ß√£o das var√°veis categ√≥ricas n√£o podem ser calculada com o coeficiente de correla√ß√£o calculado anteriormente, para avaliar como elas est√£o associadas ser√° calculado a medida de associa√ß√£o <a href="https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V">V de Cram√©r</a>. Novamente a matrix dos resultados ser√£o novamente apresentados em um <a href="https://en.wikipedia.org/wiki/Heat_map">Heatmap</a> (ou mapa de calor ) que foi inspirado <a href="http://analysingstuffs.xyz/2017/12/01/visualizing-the-correlations-between-categorical-variables-with-r-a-cramers-v-heatmap/">neste post</a> (neste post tamb√©m √© apresentada uma fun√ß√£o para o c√°lculo da matrix, adaptei de forma que se tornasse mais geral e disponibilizei no meu github <a href="https://github.com/gomesfellipe/functions/blob/master/interaction_all.R">neste link</a>).</p>
<pre class="r"><code># Carrega funcao que calcula o V de Cramer:
devtools::source_url(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/cv_test.R&quot;)
# Carrega a funcao que realiza as intera√ß√µes dos calculos dois a dois:
devtools::source_url(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/interaction_all.R&quot;)</code></pre>
<p>Veja:</p>
<pre class="r"><code>cvmat &lt;- 
train %&gt;%
  select_if(~!is.numeric(.x)) %&gt;% 
  as.data.table() %&gt;%
  interaction_all(cv_test) %&gt;% 
  as_tibble() 

cvmat %&gt;% 
  ggplot( aes(variable_x, variable_y, fill=v_cramer))+
  geom_tile(color=&quot;white&quot;)+
  scale_fill_gradient2(low=&quot;blue&quot;, high=&quot;red&quot;, mid=&quot;white&quot;, midpoint=0, limit=c(-1,1), space=&quot;Lab&quot;, name=&quot;Cramer&#39;s V&quot;)+
  theme_bw()+
  theme(axis.text.x=element_text(angle=45, vjust=1, size=10, hjust=1))+
  coord_fixed()+
  labs(x=&quot;&quot;,y=&quot;&quot;)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
</div>
</div>
<div id="ajustando-modelos" class="section level1">
<h1>Ajustando modelos</h1>
<div id="arvore-de-decisao" class="section level2">
<h2>Arvore de decisao</h2>
<p>O modelo de √°rvore de decis√£o j√° foi comentado e deixei algumas refer√™ncias ao final do post portanto vejamos a seguir o ajusto no R. Segundo a <a href="https://cran.r-project.org/web/packages/rpart/rpart.pdf">documenta√ß√£o</a>:</p>
<p><code>cp</code>: par√¢metro de complexidade. No nosso caso isso significa que o <a href="https://pt.wikipedia.org/wiki/R%C2%B2"><span class="math inline">\(R^2\)</span></a> total deve aumentar em cp em cada etapa. O principal papel desse par√¢metro √© economizar tempo de computa√ß√£o removendo as divis√µes que obviamente n√£o valem a pena. Essencialmente, informamos ao programa que qualquer divis√£o que n√£o melhore o ajuste por <code>cp</code> provavelmente ser√° eliminada por <a href="https://pt.wikipedia.org/wiki/Valida%C3%A7%C3%A3o_cruzada">valida√ß√£o cruzada</a>, e que, portanto, o programa n√£o precisa busc√°-la.</p>
<p>Para pesquisa de grade existem duas maneiras de ajustar um algoritmo no pacote <code>caret</code>: permitir que o sistema fa√ßa isso automaticamente ou especificar o <code>tuneGride</code> manualmente onde cada par√¢metro do algoritmo pode ser especificado como um vetor de valores poss√≠veis. Confira o ajuste manual em R:</p>
<pre class="r"><code>set.seed(1)

control &lt;- trainControl(method = &quot;cv&quot;, number = 5,verboseIter = F)

tunegrid &lt;- expand.grid(cp=seq(0.001, 0.01, 0.001))

rpartFit2 &lt;- 
  train(y=train$SalePrice, x=train[,-1],
        method=&quot;rpart&quot;,
        trControl=control,
        tuneGrid=tunegrid,
        metric = &quot;Rsquared&quot;
  )
rpartFit2</code></pre>
<pre><code>## CART 
## 
## 1460 samples
##   28 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 1169, 1169, 1168, 1168, 1166 
## Resampling results across tuning parameters:
## 
##   cp     RMSE       Rsquared   MAE      
##   0.001  0.1918932  0.7757730  0.1386651
##   0.002  0.1943654  0.7690391  0.1410967
##   0.003  0.2016485  0.7513005  0.1457213
##   0.004  0.2029596  0.7462748  0.1457752
##   0.005  0.2098812  0.7279462  0.1534384
##   0.006  0.2090073  0.7291130  0.1539830
##   0.007  0.2110066  0.7227211  0.1544402
##   0.008  0.2120734  0.7198280  0.1555415
##   0.009  0.2142488  0.7143975  0.1570535
##   0.010  0.2148236  0.7126454  0.1575360
## 
## Rsquared was used to select the optimal model using the largest value.
## The final value used for the model was cp = 0.001.</code></pre>
<p>Podemos conferir os resultados novamente de maneira visual:</p>
<pre class="r"><code>rpart.plot(rpartFit2$finalModel, cex = 0.5)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-41-1.png" width="1200" /></p>
<p>Gerando arquivo para submiss√£o no kaggle:</p>
<pre class="r"><code>id %&gt;% cbind(predict(rpartFit2, test) %&gt;% exp) %&gt;% 
  `colnames&lt;-`(c(&quot;Id&quot;, &quot;SalePrice&quot;)) %&gt;%
  write.csv(&quot;rpartFit2.csv&quot;,row.names = F)</code></pre>
</div>
<div id="bagging" class="section level2">
<h2>Bagging</h2>
<p><a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating">‚ÄúBagging‚Äù</a> √© usado quando desejamos reduzir a varia√ß√£o de uma √°rvore de decis√£o. Ela combina o resultado de v√°rios modelos onde todas as vari√°veis s√£o considerados para divis√£o um n√≥. Em R:</p>
<pre class="r"><code>set.seed(1)

control &lt;- trainControl(method = &quot;cv&quot;, number = 5,verboseIter = F)

treebagFit &lt;- train(y=train$SalePrice, 
                    x=train[,-1], 
                    method = &quot;treebag&quot;,
                    metric = &quot;Rsquared&quot;,
                    trControl=control
)
treebagFit</code></pre>
<pre><code>## Bagged CART 
## 
## 1460 samples
##   28 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 1169, 1169, 1168, 1168, 1166 
## Resampling results:
## 
##   RMSE       Rsquared   MAE      
##   0.1831872  0.7946059  0.1288626</code></pre>
<p>Note que o <span class="math inline">\(R^2\)</span> aumentou e o <a href="https://en.wikipedia.org/wiki/Root-mean-square_deviation"><span class="math inline">\(RMSE\)</span></a> diminuiu ap√≥s o uso desta t√©cnica.</p>
<p>Resultados para enviar para o Kaggle:</p>
<pre class="r"><code>id %&gt;% cbind(predict(treebagFit, test)%&gt;% exp) %&gt;% 
  `colnames&lt;-`(c(&quot;Id&quot;, &quot;SalePrice&quot;)) %&gt;%
  write.csv(&quot;treebagFit.csv&quot;,row.names = F)</code></pre>
</div>
<div id="random-forest" class="section level2">
<h2>Random Forest</h2>
<p>A principal diferen√ßa entre ‚Äúbagging‚Äù e o algoritmo Random Forest √© que em <code>randomForest</code>, apenas um subconjunto de caracter√≠sticas √© selecionado aleatoriamente em cada divis√£o em uma √°rvore de decis√£o enquanto que no bagging todos os recursos s√£o usados.</p>
<p>Para pesquisa de grade especificaremos um vetor com os poss√≠veis valores, <a href="https://cran.r-project.org/web/packages/randomForest/randomForest.pdf">pois o default adotado para o par√¢metro</a> <code>mtry</code> √© <code>mtry</code> = p/3 (N√∫mero de vari√°veis amostradas aleatoriamente como candidatos em cada divis√£o), onde p √© o n√∫mero de vari√°veis e pode ser que o modelo se ajuste melhor aos dados ao utilizar outro valor.</p>
<p>Veja:</p>
<pre class="r"><code>set.seed(1)

tunegrid &lt;- expand.grid(mtry = seq(4, ncol(train) * 0.8, 2))

control &lt;- trainControl(method = &quot;cv&quot;, number = 5,verboseIter = F)

rfFit &lt;- train(SalePrice ~. ,
               data=train,
               method=&quot;rf&quot;,
               metric = &quot;Rsquared&quot;,
               tuneGrid=tunegrid,
               trControl=control
)
rfFit</code></pre>
<pre><code>## Random Forest 
## 
## 1460 samples
##   28 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 1169, 1169, 1168, 1168, 1166 
## Resampling results across tuning parameters:
## 
##   mtry  RMSE       Rsquared   MAE       
##    4    0.1455656  0.8781772  0.09755474
##    6    0.1417368  0.8817193  0.09435674
##    8    0.1405084  0.8826370  0.09350712
##   10    0.1395367  0.8834153  0.09290816
##   12    0.1385338  0.8845102  0.09181049
##   14    0.1386865  0.8840165  0.09223527
##   16    0.1381776  0.8846283  0.09155563
##   18    0.1384532  0.8837305  0.09222536
##   20    0.1380863  0.8840803  0.09173754
##   22    0.1383788  0.8835938  0.09189772
## 
## Rsquared was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 16.</code></pre>
<p>Note que o <span class="math inline">\(R^2\)</span> aumentou e o <span class="math inline">\(RMSE\)</span> apresentou resultados ainda mais satisfat√≥rios.</p>
<p>Veja visualmente a import√¢ncia de ada vari√°vel:</p>
<pre class="r"><code>randomForest::varImpPlot(rfFit$finalModel)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
<p>Resultados para enviar para o Kaggle:</p>
<pre class="r"><code>id %&gt;% cbind(predict(rfFit, test) %&gt;% exp) %&gt;% 
  `colnames&lt;-`(c(&quot;Id&quot;, &quot;SalePrice&quot;)) %&gt;%
  write.csv(&quot;rfFit.csv&quot;,row.names = F) </code></pre>
</div>
<div id="gbm" class="section level2">
<h2>GBM</h2>
<p>Diferentemente do ‚Äúbagging‚Äù, o ‚Äúboosting‚Äù √© uma t√©cnica de ensemble (conjunto) na qual os preditores n√£o s√£o feitos independentemente, mas sequencialmente. Na imagem a seguir √© poss√≠vel ver uma representa√ß√£o visual dessa diferen√ßa:</p>
<p><img src="https://cdn-images-1.medium.com/max/1600/1*PaXJ8HCYE9r2MgiZ32TQ2A.png" /></p>
<p>A imagem foi obtida <a href="https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d">neste artigo: Gradient Boosting from scratch</a>, recomendo a leitura pois da uma boa intui√ß√£o de como o algoritmo funciona.</p>
<p>Para a pesquisa de grade vamos permitir que o sistema fa√ßa isso automaticamente configurando apenas o <code>tuneLength</code> para indicar o n√∫mero de valores diferentes para cada par√¢metro do algoritmo.</p>
<pre class="r"><code>set.seed(1)

control &lt;- trainControl(method = &quot;cv&quot;, number = 5,verboseIter = F)

gbmFit &lt;- train(SalePrice~.,data=train,
                method = &quot;gbm&quot;,
                trControl=control,
                tuneLength=5,
                metric = &quot;Rsquared&quot;,
                verbose = FALSE
)
gbmFit</code></pre>
<pre><code>## Stochastic Gradient Boosting 
## 
## 1460 samples
##   28 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 1169, 1169, 1168, 1168, 1166 
## Resampling results across tuning parameters:
## 
##   interaction.depth  n.trees  RMSE       Rsquared   MAE       
##   1                   50      0.1736970  0.8346902  0.12145158
##   1                  100      0.1474386  0.8663694  0.10371271
##   1                  150      0.1400060  0.8775141  0.09804851
##   1                  200      0.1381902  0.8803999  0.09607709
##   1                  250      0.1375854  0.8817130  0.09502881
##   2                   50      0.1511051  0.8640075  0.10557294
##   2                  100      0.1379357  0.8815852  0.09546142
##   2                  150      0.1360260  0.8846503  0.09326628
##   2                  200      0.1355702  0.8852090  0.09248558
##   2                  250      0.1362827  0.8841734  0.09254710
##   3                   50      0.1434808  0.8743589  0.09910961
##   3                  100      0.1363881  0.8838715  0.09355652
##   3                  150      0.1346606  0.8868808  0.09163759
##   3                  200      0.1339427  0.8880370  0.09062153
##   3                  250      0.1336666  0.8886732  0.08979366
##   4                   50      0.1376575  0.8824442  0.09516571
##   4                  100      0.1334392  0.8884173  0.09192150
##   4                  150      0.1330866  0.8890336  0.09156893
##   4                  200      0.1334706  0.8886198  0.09096598
##   4                  250      0.1335809  0.8884950  0.09101981
##   5                   50      0.1384852  0.8813449  0.09535954
##   5                  100      0.1350803  0.8863344  0.09231165
##   5                  150      0.1340246  0.8878172  0.09112111
##   5                  200      0.1342892  0.8874590  0.09088714
##   5                  250      0.1349331  0.8867525  0.09104875
## 
## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1
## 
## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10
## Rsquared was used to select the optimal model using the largest value.
## The final values used for the model were n.trees = 150, interaction.depth =
##  4, shrinkage = 0.1 and n.minobsinnode = 10.</code></pre>
<p>Note que este foi o modelo que apresentou os melhores resultados quanto s√≥ <span class="math inline">\(R^2\)</span> e ao <span class="math inline">\(RMSE\)</span> em compara√ß√£o com os outros modelos.</p>
<p>Submiss√£o para Kaggle:</p>
<pre class="r"><code>id %&gt;% cbind(predict(gbmFit, test) %&gt;% exp) %&gt;%
  `colnames&lt;-`(c(&quot;Id&quot;, &quot;SalePrice&quot;)) %&gt;%
  write.csv(&quot;gbmFit.csv&quot;, row.names = F)</code></pre>
</div>
<div id="regress√£o-linear" class="section level2">
<h2>Regress√£o Linear</h2>
<p>Por fim faremos o ajuste de um modelo de regress√£o linear multivariado utilizando o pacote caret.</p>
<p>Utilizaremos valida√ß√£o cruzada separando nossa amostra em 5 e utilizaremos o m√©todo <code>lmStepAIC</code> que realiza a sele√ß√£o do modelo escalonado pelo crit√©rio de informa√ß√£o de Akaike - <a href="https://en.wikipedia.org/wiki/Akaike_information_criterion">AIC</a>.</p>
<pre class="r"><code>set.seed(1)

control &lt;- trainControl(method = &quot;cv&quot;, number = 5,verboseIter = F)

lmFit &lt;- train(SalePrice~.,data=train,
               method = &quot;lmStepAIC&quot;,
               trControl=control,
               metric = &quot;Rsquared&quot;,trace=F
)
lmFit</code></pre>
<pre><code>## Linear Regression with Stepwise Selection 
## 
## 1460 samples
##   28 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 1169, 1169, 1168, 1168, 1166 
## Resampling results:
## 
##   RMSE      Rsquared   MAE       
##   0.147716  0.8632513  0.09574552</code></pre>
<p>Note que o ajuste do modelo se apresenta de maneira satisfat√≥ria com <span class="math inline">\(R^2\)</span> e <span class="math inline">\(RMSE\)</span> semelhantes aos modelos de <code>bagging</code> e <code>boosting</code> e al√©m disso, diferente dos modelos baseados em √°rvore, com este ajuste √© poss√≠vel notar a signific√¢ncia estat√≠stica de cada par√¢metro ajustado, o que possibilita tanto o uso tanto como modelo preditivo quanto como modelo descritivo. Veja:</p>
<pre class="r"><code>ggcoef(
  lmFit$finalModel,                      #O modelo a ser conferido
  vline_color = &quot;red&quot;,          #Reta em zero  
  errorbar_color = &quot;blue&quot;,      #Cor da barra de erros
  errorbar_height = .25,
  shape = 18,                   #Altera o formato dos pontos centrais
  size=2,                      #Altera o tamanho do ponto
  color=&quot;black&quot;,
  exclude_intercept = TRUE,                #Altera a cor do ponto
  mapping = aes(x = estimate, y = term, size = p.value))+
  scale_size_continuous(trans = &quot;reverse&quot;)+ #Essa linha faz com que inverta o tamanho
  theme_bw()</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-51-1.png" width="672" /></p>
<p>Note que o intercepto <span class="math inline">\(\beta_0\)</span> foi retirado da imagem pois √© muito superior aos demais coeficientes. Note tamb√©m que <span class="math inline">\(\beta_i\)</span> informa qu√£o sens√≠vel √© <span class="math inline">\(y\)</span>, no caso <code>log(SalePrice)</code> √†s varia√ß√µes de cara umas das <span class="math inline">\(x_{i,j}\)</span> vari√°veis explicativas. Mais concretamente, se <span class="math inline">\(x_{i,j}\)</span> aumenta em uma unidade, o valor de <span class="math inline">\(y\)</span> varia em <span class="math inline">\(\beta_1\)</span> unidades.</p>
<p>Uma r√°pida <a href="http://www.portalaction.com.br/analise-de-regressao/analise-dos-residuos">An√°lise dos Res√≠duos</a>:</p>
<pre class="r"><code>lmFit$finalModel %&gt;% 
  autoplot(which = 1:2) + 
  theme_bw()</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-52-1.png" width="1500" /></p>
<p>√â poss√≠vel notar que parece haver alguns outliers em ambas as figuras. Na primeira √© poss√≠vel notar uma nuvem de pontos aleat√≥rios em torno de zero por√©m na segunda figura nota-se que alguns valores n√£o est√£o de acordo com os quantils te√≥ricos de uma distribui√ß√£o normal, o que pode prejudicar nossa interpreta√ß√£o dos coeficientes do modelo. Vamos encerrar o modelo por aqui mesmo e ver como ele se sai na competi√ß√£o do Kaggle, preparando a submiss√£o:</p>
<pre class="r"><code>id %&gt;% cbind(predict(lmFit, test) %&gt;% exp ) %&gt;% 
  `colnames&lt;-`(c(&quot;Id&quot;, &quot;SalePrice&quot;)) %&gt;%
  write.csv(&quot;lmFit.csv&quot;,row.names = F)</code></pre>
<p>O score obtido com esta submiss√£o no Kaggle foi muito pr√≥ximo dos modelos baseados e √°rvore e o tempo computacional para este ajuste foi bem menor.</p>
</div>
<div id="comparando-ajustes" class="section level2">
<h2>Comparando ajustes</h2>
<p>Vejamos a seguir uma compara√ß√£o entre estes modelos com as fun√ß√µes fornecidas pelo pacote `caret:.</p>
<pre class="r"><code>resamps &lt;- resamples(list(rpart = rpartFit2,
                          treebag = treebagFit,
                          rf = rfFit,
                          gbm = gbmFit,
                          lm = lmFit 
                          )) 
bwplot(resamps)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-54-1.png" width="672" /></p>
<p>Com este gr√°fico √© poss√≠vel notar que o modelo de regress√£o linear m√∫ltipla apresentou resultados semelhantes aos de bagging e boosting.</p>
<p>√â importante frisar que a maneira como as vari√°veis foram selecionadas para o modelo de regress√£o linear m√∫ltipla atrav√©s da import√¢ncia das vari√°veis obtida com o modelo randomForest n√£o √© um padr√£o e existem diversos outros modos estat√≠sticos de se de determinar a signific√¢ncia e a rela√ß√£o das vari√°veis para o modelo.</p>
<p>Um poss√≠vel problema neste m√©todo √© que n√£o detecta a multicolinearidade, que ocorre quando as vari√°veis explicativas est√£o fortemente correlacionadas entre si e a an√°lise de regress√£o linear pode ficar confusa e desprovida de significado, pois h√° dificuldade em distinguir o efeito de uma ou outra vari√°vel explicativa sobre a vari√°vel resposta <span class="math inline">\(Y\)</span> devido √† vari√¢ncias muito elevadas ou sinais inconsistentes.</p>
<p>Essa proposta de aprender se divertindo e de maneira produtiva me deixa muito empolgado, espero que tenham se divertido como eu me diverti fazendo este post!</p>
</div>
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias:</h1>
<ul>
<li><a href="https://www.datacamp.com/courses/machine-learning-with-tree-based-models-in-r">DataCamp Course:Machine Learning with Tree-Based Models in R</a></li>
<li><a href="https://tinyurl.com/y796aa4t">Data Science <em>for</em> Business</a></li>
<li><a href="https://lethalbrains.com/learn-ml-algorithms-by-coding-decision-trees-439ac503c9a4">Learn ML Algorithms by coding: Decision Trees</a></li>
<li><a href="https://www.datacamp.com/community/tutorials/decision-trees-R">DataCamp Tutorials: Decision Trees in R</a></li>
<li><a href="https://topepo.github.io/caret/">The caret Package - Max Kuhn</a></li>
<li><a href="https://www.vooo.pro/insights/um-tutorial-completo-sobre-a-modelagem-baseada-em-tree-arvore-do-zero-em-r-python/">Um tutorial completo sobre modelagem baseada em √°rvores de decis√£o (c√≥digos R e Python)</a></li>
<li><a href="https://machinelearningmastery.com/tuning-machine-learning-models-using-the-caret-r-package/">Tuning Machine Learning Models Using the Caret R Package</a></li>
<li><a href="https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d">Gradient Boosting from scratch</a></li>
<li><a href="https://machinelearningmastery.com/tune-machine-learning-algorithms-in-r/">Tune Machine Learning Algorithms in R (random forest case study)</a></li>
<li><a href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_manual.htm">Random Forests - Leo Breiman and Adele Cutler</a></li>
<li><a href="https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf">An Introduction to Recursive Partitioning Using the RPART Routines - CRAN</a></li>
</ul>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2018-08-31-modelos-em-arvore/modelos-em-arvore/">Um estudo sobre modelos de aprendizagem baseados em √°rvores com desafio do Kaggle</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Analise Explorat√≥ria</category>
      <category>Aprendizado Supervisionado</category>
      <category>Data mining</category>
      <category>Estatistica</category>
      <category>Machine Learning</category>
      <category>Pr√°tica</category>
      <category>Probabilidade</category>
      <category>R</category>
      <category>modelo baseado em arvores</category>
      <category>kaggle</category>
      <category>Regress√£o</category>
      <category domain="tag">Data Mining</category>
      <category domain="tag">Estatistica</category>
      <category domain="tag">gomesfellipe</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">Correlacoes</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">modelagem estatistica</category>
      <category domain="tag">Pr√°tica</category>
      <category domain="tag">R</category>
      <category domain="tag">regression</category>
      <category domain="tag">caret</category>
      <category domain="tag">xgboost</category>
      <category domain="tag">random forest</category>
      <category domain="tag">decisiontree</category>
    </item>
    <item>
      <title>Brasil x Argentina, tidytext e Machine Learning</title>
      <link>https://gomesfellipe.github.io/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml/</link>
      <pubDate>Sun, 24 Jun 2018 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml/</guid>
      <description>Aplicando t√©cnincas de Text Mining como pacote tidy text para explorar a rivalidade entre Brasil e Argentina! Veja tamb√©m como a an√°lise de sentimentos pode ser divertida al√©m de poss√≠veis aplica√ß√µes de machine learning</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="brasil-vs-argentina-e-text-mining" class="section level1">
<h1>Brasil vs Argentina e Text Mining</h1>
<p>A copa do mundo esta ai novamente e como n√£o poderia ser diferente, com ela surgem novos <a href="http://cio.com.br/noticias/2015/10/27/tome-nota-2-5-quintilhoes-de-bytes-sao-criados-todos-os-dias/">quintilh√µes de bytes todos os dias</a>, saber analisar esses dados √© um grande desafio pois a maioria dessa informa√ß√£o se encontra de forma n√£o estruturada e al√©m do desafio de captar esses dados ainda existem mais desafios que podem ser ainda maiores, como o de process√°-los e obter respostas deles.</p>
<p>Dada a rivalidade hist√≥rica entre Brasil e Argentina achei que seria interessante avaliar como anda o comportamento das pessoas do Brasil nas m√≠dias sociais em rela√ß√£o a esses dois pa√≠ses. Para o post n√£o ficar muito longo, escolhi que iria recolher informa√ß√µes apenas do Twitter devido a praticidade, foram coletados os √∫ltimos 4.000 tweets com o termo ‚Äúbrasil‚Äù e os √∫ltimos ‚Äú4.000‚Äù tweets com o termo ‚Äúargentina‚Äù no Twitter atrav√©s da sua API com o pacote os <code>twitteR</code> e <code>ROAuth</code>. O c√≥digo pode ser conferido <a href="https://github.com/gomesfellipe/functions/blob/master/getting_twitter_data.R">neste link</a>.</p>
<p>An√°lise de textos sempre foi um tema que me interessou muito, no final do ano de 2017 quando era estagi√°rio me pediram para ajudar em uma pesquisa que envolvia a an√°lise de palavras criando algumas nuvens de palavras. Pesquisando sobre t√©cnicas de textmining descobri tantas abordagens diferentes que resolvi juntar tudo que tinha encontrado em uma √∫nica fun√ß√£o (que ser√° apresentada a seguir) para a confec√ß√£o dessas nuvens, utilizarei esta fun√ß√£o para ter uma primeira impress√£o dos dados.</p>
<p>Al√©m disso, como seria um problema a tarefa de criar as nuvens de palavras s√≥ poderia ser realizada por algu√©m com conhecimento em R, na √©poca estava come√ßando meus estudo sobre shiny e como treinamento desenvolvi um app que esta hospedado no link: <a href="https://gomesfellipe.shinyapps.io/appwordcloud/" class="uri">https://gomesfellipe.shinyapps.io/appwordcloud/</a> e o c√≥digo esta aberto e dispon√≠vel para quem se interessar no meu github <a href="https://github.com/gomesfellipe/appwordcloud/blob/master/appwordcloud.Rmd">neste link</a></p>
<p>Por√©m, ap√≥s ler e estudar o livro <a href="https://www.tidytextmining.com/">Text Mining with R - A Tidy Approach</a> por <span class="citation"><a href="#ref-tidytext" role="doc-biblioref">Silge; Robinson</a> (<a href="#ref-tidytext" role="doc-biblioref">2018</a>)</span> hoje em dia eu olho para tr√°s e vejo que poderia ter feito tanto a fun√ß√£o quanto o aplicativo de maneira muito mais eficiente portanto esse post tr√°s alguns dos meus estudos sobre esse livro maravilhoso e tamb√©m algum estudo sobre Machine Learning com o pacote <a href="https://cran.r-project.org/web/packages/caret"><code>caret</code></a></p>
<div id="importando-a-dados" class="section level2">
<h2>Importando a dados</h2>
<p>Como j√° foi dito, a base de dados foi obtida atrav√©s da API do twitter e o c√≥digo pode ser obtido <a href="https://github.com/gomesfellipe/functions/blob/master/getting_twitter_data.R">neste link</a>.</p>
<pre class="r"><code>library(dplyr)
library(kableExtra)
library(magrittr)

base &lt;- read.csv(&quot;original_books.csv&quot;) %&gt;% as_tibble()</code></pre>
</div>
<div id="nuvem-de-palavras-r√°pida-com-fun√ß√£o-customizada" class="section level2">
<h2>Nuvem de palavras r√°pida com fun√ß√£o customizada</h2>
<p>Para uma primeira impress√£o dos dados, vejamos o que retorna uma nuvem de palavras criada com a fun√ß√£o <a href="https://github.com/gomesfellipe/functions/blob/master/wordcloud_sentiment.R"><code>wordcloud_sentiment()</code></a> que desenvolvi antes de conhecer a ‚ÄúA Tidy Approach‚Äù para Text Mining:</p>
<pre class="r"><code>devtools::source_url(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/wordcloud_sentiment.R&quot;)

# Obtendo nuvem e salvando tabela num objeto com nome teste:
df &lt;- wordcloud_sentiment(base$text,
                      type = &quot;text&quot;,
                      sentiment = F,
                      excludeWords = c(&quot;nao&quot;,letters,LETTERS),
                      ngrams = 2,
                      tf_idf = F,
                      max = 100,
                      freq = 10,
                      horizontal = 0.9,
                      textStemming = F,
                      print=T)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-2-1.png" width="1056" /></p>
<p>N√£o poderia esquecer, al√©m da nuvem, a fun√ß√£o tamb√©m retorna um dataframe com a frequ√™ncia das palavras:</p>
<pre class="r"><code>df %&gt;% as_tibble()</code></pre>
<pre><code>## # A tibble: 29,064 x 2
##    words          freq  
##    &lt;chr&gt;          &lt;chr&gt; 
##  1 =              &quot;2795&quot;
##  2 brasil copa    &quot;2061&quot;
##  3 copa mundo     &quot;1959&quot;
##  4 hat trick      &quot;1327&quot;
##  5 = hoje         &quot;1248&quot;
##  6 hoje brasil    &quot;1215&quot;
##  7 mundo          &quot; 852&quot;
##  8 isl ndia       &quot; 820&quot;
##  9 pra copa       &quot; 813&quot;
## 10 estreia brasil &quot; 782&quot;
## # ‚Ä¶ with 29,054 more rows</code></pre>
<p>E outra fun√ß√£o interessante √© a de criar uma nuvem a partir de um webscraping muito (muito mesmo) introdut√≥rio, para isso foi pegar todo o texto da p√°gina sobre a copa do mundo no Wikip√©dia, veja:</p>
<pre class="r"><code># Obtendo nuvem e salvando tabela num objeto com nome teste:
df_html &lt;- wordcloud_sentiment(&quot;https://pt.wikipedia.org/wiki/Copa_do_Mundo_FIFA&quot;,
                      type = &quot;url&quot;,
                      sentiment = F,
                      excludeWords = c(&quot;nao&quot;,letters,LETTERS),
                      ngrams = 2,
                      tf_idf = F,
                      max = 100,
                      freq = 6,
                      horizontal = 0.9,
                      textStemming = F,
                      print=T)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Essa fun√ß√£o √© bem ‚Äúprematura,‚Äù existem infinitas maneiras de melhorar ela e n√£o alterei ela ainda por falta de tempo.</p>
</div>
<div id="a-tidy-approach" class="section level2">
<h2>A Tidy Approach</h2>
<p>O formato tidy, em que cada linha corresponde a uma observa√ß√£o e cada coluna √† uma vari√°vel, veja:</p>
<center>
<img src="http://garrettgman.github.io/images/tidy-1.png" style="width:70.0%" />
</center>
<p>Agora a tarefa ser√° simplificada com a abordagem tidy, al√©m das fun√ß√µes do livro <a href="https://www.tidytextmining.com/">Text Mining with R</a> utilizarei a fun√ß√£o <a href="https://github.com/gomesfellipe/functions/blob/master/clean_tweets.R"><code>clean_tweets</code></a> que adaptei inspirado nesse post dessa pagina: <a href="https://sites.google.com/site/miningtwitter/home">Quick guide to mining twitter with R</a> quando estudava sobre textmining.</p>
<div id="arrumando-e-transformando-a-base-de-dados" class="section level3">
<h3>Arrumando e transformando a base de dados</h3>
<p>Utilizando as fun√ß√µes do pacote <code>tidytext</code> em conjunto com os pacotes <code>stringr</code> e <code>abjutils</code>, ser√° poss√≠vel limpar e arrumar a base de dados.</p>
<p>Al√©m disso ser√£o removidas as stop words de nossa base, com a fun√ß√£o <code>stopwords::stopwords("pt")</code> podemos obter as stopwords da nossa l√≠ngua</p>
<pre class="r"><code>library(stringr)
library(tidytext)
library(abjutils)

devtools::source_url(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/clean_tweets.R&quot;)

original_books = base %&gt;% 
  mutate(text = clean_tweets(text) %&gt;% enc2native() %&gt;% rm_accent())

#Removendo stopwords:
excludewords=c(&quot;[:alpha:]&quot;,&quot;[:alnum:]&quot;,&quot;[:digit:]&quot;,&quot;[:xdigit:]&quot;,&quot;[:space:]&quot;,&quot;[:word:]&quot;,
               LETTERS,letters,1:10,
               &quot;hat&quot;,&quot;trick&quot;,&quot;bc&quot;,&quot;de&quot;,&quot;tem&quot;,&quot;twitte&quot;,&quot;fez&quot;,
               &#39;pra&#39;,&quot;vai&quot;,&quot;ta&quot;,&quot;so&quot;,&quot;ja&quot;,&quot;rt&quot;)

stop_words = data_frame(word = c(stopwords::stopwords(&quot;pt&quot;), excludewords))

tidy_books &lt;- original_books %&gt;%
  unnest_tokens(word, text) %&gt;% 
  anti_join(stop_words)</code></pre>
<p>Portando a base de dados ap√≥s a limpeza e a remo√ß√£o das stop words:</p>
<pre class="r"><code>#Palavras mais faladas:
tidy_books %&gt;% count(word, sort = TRUE) </code></pre>
<pre><code>## # A tibble: 3,900 x 2
##    word          n
##    &lt;chr&gt;     &lt;int&gt;
##  1 copa       6993
##  2 brasil     4164
##  3 argentina  3487
##  4 mundo      2030
##  5 hoje       1825
##  6 letras     1562
##  7 messi      1493
##  8 estreia    1107
##  9 est         866
## 10 isl         828
## # ‚Ä¶ with 3,890 more rows</code></pre>
<pre class="r"><code>#Apos a limpeza, caso precise voltar as frases:
original_books = tidy_books%&gt;%
  group_by(book,line)%&gt;%
  summarise(text=paste(word,collapse = &quot; &quot;))</code></pre>
<div id="palavras-mais-frequentes" class="section level4">
<h4>Palavras mais frequentes</h4>
<p>Vejamos as palavras mais faladas nessa pesquisa:</p>
<pre class="r"><code>library(ggplot2)

tidy_books %&gt;%
  count(word, sort = TRUE) %&gt;%
  filter(n &gt; 400) %&gt;%
  mutate(word = reorder(word, n)) %&gt;%
  
  ggplot(aes(word, n, fill = I(&quot;yellow&quot;), colour = I(&quot;green&quot;))) +
  geom_col(position=&quot;dodge&quot;) +
  xlab(NULL) +
  labs(title = &quot;Frequencia total das palavras pesquisadas&quot;)+
  coord_flip()+ theme(
  panel.background = element_rect(fill = &quot;#74acdf&quot;,
                                colour = &quot;lightblue&quot;,
                                size = 0.5, linetype = &quot;solid&quot;),
  panel.grid.major = element_line(size = 0.5, linetype = &#39;solid&#39;,
                                colour = &quot;white&quot;), 
  panel.grid.minor = element_line(size = 0.25, linetype = &#39;solid&#39;,
                                colour = &quot;white&quot;)
  )</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<div id="palavras-mais-frequentes-para-cada-termo" class="section level4">
<h4>Palavras mais frequentes para cada termo</h4>
<p>Vejamos as nuvens de palavras mais frequentes de acordo com cada um dos termos pesquisados:</p>
<pre class="r"><code>#Criando nuvem de palavra:
library(wordcloud)

par(mfrow=c(1,2))
tidy_books %&gt;%
  filter(book==&quot;br&quot;)%&gt;%
  count(word) %&gt;%
  with(wordcloud(word, n, max.words = 100,random.order = F,min.freq = 15,random.color = F,colors = c(&quot;#009b3a&quot;, &quot;#fedf00&quot;,&quot;#002776&quot;),scale = c(2,1),rot.per = 0.05))

tidy_books %&gt;%
  filter(book==&quot;arg&quot;)%&gt;%
  count(word) %&gt;%
  with(wordcloud(word, n, max.words = 100,min.freq = 15,random.order = F,random.color = F,colors = c(&quot;#75ade0&quot;, &quot;#ffffff&quot;,&quot;#f6b506&quot;),scale = c(2,1),rot.per = 0.05))</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-8-1.png" width="1056" /></p>
<pre class="r"><code>par(mfrow=c(1,1))</code></pre>
</div>
</div>
<div id="an√°lise-de-sentimentos" class="section level3">
<h3>An√°lise de sentimentos</h3>
<p>A an√°lise de sentimentos utilizando a abordagem tidy foi poss√≠vel gra√ßas ao pacote <a href="https://cran.r-project.org/package=lexiconPT"><code>lexiconPT</code></a>, que esta dispon√≠vel no CRAN e que conheci ao ler o <a href="https://sillasgonzaga.github.io/2017-09-23-sensacionalista-pt01/">post: ‚ÄúO Sensacionalista e Text Mining: An√°lise de sentimento usando o lexiconPT‚Äù</a> do blog <a href="https://sillasgonzaga.github.io/">Paix√£o por dados</a> que gosto tanto de acompanhar.</p>
<pre class="r"><code># Analise de sentimentos:
library(lexiconPT)

sentiment = data.frame(word = sentiLex_lem_PT02$term ,
                       polarity = sentiLex_lem_PT02$polarity) %&gt;% 
  mutate(sentiment = if_else(polarity&gt;0,&quot;positive&quot;,if_else(polarity&lt;0,&quot;negative&quot;,&quot;neutro&quot;)),
         word = as.character(word)) %&gt;% 
  as_tibble()


library(tidyr)

book_sentiment &lt;- tidy_books %&gt;%
  inner_join(sentiment) %&gt;%
  count(book,word, index = line , sentiment) %&gt;%
  spread(sentiment, n, fill = 0) %&gt;%
  mutate(sentiment = positive - negative) %T&gt;%
  print</code></pre>
<pre><code>## # A tibble: 2,953 x 7
##    book  word      index negative neutro positive sentiment
##    &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
##  1 arg   abandonar   857        1      0        0        -1
##  2 arg   absurdo     849        1      0        0        -1
##  3 arg   absurdo    1863        1      0        0        -1
##  4 arg   afogado    2275        1      0        0        -1
##  5 arg   afogado    3659        1      0        0        -1
##  6 arg   alegria    1134        0      0        1         1
##  7 arg   almo        186        0      0        1         1
##  8 arg   almo       2828        0      0        1         1
##  9 arg   almo       3433        0      0        1         1
## 10 arg   almo       3569        0      0        1         1
## # ‚Ä¶ with 2,943 more rows</code></pre>
<p>Cada palavra possui um valor associado a sua polaridade , vejamos como ficou distribu√≠do o n√∫mero de palavras de cada sentimento de acordo com cada termo escolhido para a pesquisa:</p>
<pre class="r"><code>book_sentiment%&gt;%
  count(sentiment,book)%&gt;%
  arrange(book) %&gt;%
  
  ggplot(aes(x = factor(sentiment),y = n,fill=book))+
  geom_bar(stat=&quot;identity&quot;,position=&quot;dodge&quot;)+
  facet_wrap(~book) +
  theme_bw()+ 
    scale_fill_manual(values=c(&quot;#75ade0&quot;, &quot;#009b3a&quot;))</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<div id="comparando-sentimentos-dos-termos-de-pesquisa" class="section level4">
<h4>Comparando sentimentos dos termos de pesquisa</h4>
<p>Para termos associados a palavra ‚ÄúBrasil‚Äù no twitter:</p>
<pre class="r"><code># Nuvem de compara√ß√£o:
library(reshape2)

tidy_books %&gt;%
  filter(book==&quot;br&quot;)%&gt;%
  inner_join(sentiment) %&gt;%
  count(word, sentiment, sort = TRUE) %&gt;%
  acast(word ~ sentiment, value.var = &quot;n&quot;, fill = 0) %&gt;%
  comparison.cloud(colors = c(&quot;red&quot;, &quot;gray80&quot;,&quot;green&quot;),
                   max.words = 200)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Para termos associados a palavra ‚ÄúArgentina‚Äù no twitter:</p>
<pre class="r"><code>tidy_books %&gt;%
  filter(book==&quot;arg&quot;)%&gt;%
  inner_join(sentiment) %&gt;%
  count(word, sentiment, sort = TRUE) %&gt;%
  acast(word ~ sentiment, value.var = &quot;n&quot;, fill = 0) %&gt;%
  comparison.cloud(colors = c(&quot;red&quot;, &quot;gray80&quot;,&quot;green&quot;),
                   max.words = 200)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
<div id="propor√ß√£o-de-palavras-positivas-e-negativas-por-texto" class="section level4">
<h4>Propor√ß√£o de palavras positivas e negativas por texto</h4>
<pre class="r"><code># Propor√ß√£o de palavras negativas:
bingnegative &lt;- sentiment %&gt;% 
  filter(sentiment == &quot;negative&quot;)

bingpositive &lt;- sentiment %&gt;% 
  filter(sentiment == &quot;positive&quot;)

wordcounts &lt;- tidy_books %&gt;%
  group_by(book, line) %&gt;%
  summarize(words = n())</code></pre>
<div id="para-negativas" class="section level5">
<h5>Para negativas;</h5>
<pre class="r"><code>tidy_books %&gt;%
  semi_join(bingnegative) %&gt;%
  group_by(book, line) %&gt;%
  summarize(negativewords = n()) %&gt;%
  left_join(wordcounts, by = c(&quot;book&quot;, &quot;line&quot;)) %&gt;%
  mutate(ratio = negativewords/words) %&gt;%
  top_n(5) %&gt;%
  ungroup() %&gt;% arrange(desc(ratio)) %&gt;% filter(book==&quot;br&quot;)</code></pre>
<pre><code>## # A tibble: 5 x 5
##   book   line negativewords words ratio
##   &lt;chr&gt; &lt;int&gt;         &lt;int&gt; &lt;int&gt; &lt;dbl&gt;
## 1 br     2003             1     3 0.333
## 2 br     2775             1     3 0.333
## 3 br     2580             2     7 0.286
## 4 br      126             1     4 0.25 
## 5 br     2335             1     4 0.25</code></pre>
<p>A frase mais negativa do brasil e da argentina::</p>
<pre class="r"><code>base %&gt;%
  filter(book==&quot;br&quot;,line==2580) %&gt;% mutate(text = as.character(text))%&gt;% select(text) %&gt;% c() </code></pre>
<pre><code>## $text
## [1] &quot;um medo? \x97 de nois criar expectativa e o Brasil perder a copa https://t.co/0chcNWHh0m&quot;</code></pre>
<pre class="r"><code>base %&gt;%
  filter(book==&quot;arg&quot;,line==572) %&gt;% mutate(text = as.character(text))%&gt;% select(text) %&gt;% c()  </code></pre>
<pre><code>## $text
## [1] &quot;RT @DavidmeMelo: @SantiiSanchez16 @Flamengo Perder a copa para o time mais sujo e mais corrupto da argentina \xe9 assim mesmo https://t.co/zIC\x85&quot;</code></pre>
</div>
<div id="para-positivas" class="section level5">
<h5>Para positivas:</h5>
<pre class="r"><code>tidy_books %&gt;%
  semi_join(bingpositive) %&gt;%
  group_by(book, line) %&gt;%
  summarize(positivewords = n()) %&gt;%
  left_join(wordcounts, by = c(&quot;book&quot;, &quot;line&quot;)) %&gt;%
  mutate(ratio = positivewords/words) %&gt;%
  top_n(5) %&gt;%
  ungroup() %&gt;% arrange(desc(ratio))</code></pre>
<pre><code>## # A tibble: 22 x 5
##    book   line positivewords words ratio
##    &lt;chr&gt; &lt;int&gt;         &lt;int&gt; &lt;int&gt; &lt;dbl&gt;
##  1 arg    2120             3     9 0.333
##  2 br     2374             1     3 0.333
##  3 arg    3272             2     7 0.286
##  4 arg    2301             1     4 0.25 
##  5 br      126             1     4 0.25 
##  6 br      553             2     8 0.25 
##  7 br     1499             2     8 0.25 
##  8 br     2054             2     8 0.25 
##  9 br     2591             1     4 0.25 
## 10 arg    2130             1     5 0.2  
## # ‚Ä¶ with 12 more rows</code></pre>
<p>A frase mais positiva do brasil e da argentina:</p>
<pre class="r"><code>base %&gt;%
  filter(book==&quot;br&quot;,line==2374) %&gt;% mutate(text = as.character(text))%&gt;% select(text) %&gt;% c() </code></pre>
<pre><code>## $text
## [1] &quot;Tirei Brasil, \xe9 uma honra https://t.co/OgNCot4Wu0&quot;</code></pre>
<pre class="r"><code>base %&gt;%
  filter(book==&quot;arg&quot;,line==2120) %&gt;% mutate(text = as.character(text))%&gt;% select(text) %&gt;% c()  </code></pre>
<pre><code>## $text
## [1] &quot;@_LeoFerreiraH Quero que a Argentina passe para possivelmente enfrentar o Brasil, ganhar da Argentina j\xe1 \xe9 bom, na\x85 https://t.co/bxHJUeGVpc&quot;</code></pre>
</div>
</div>
</div>
</div>
<div id="tf-idf" class="section level2">
<h2>TF-IDF</h2>
<p>Segundo <span class="citation"><a href="#ref-tidytext" role="doc-biblioref">Silge; Robinson</a> (<a href="#ref-tidytext" role="doc-biblioref">2018</a>)</span> no livro <a href="https://www.tidytextmining.com/tfidf.html">tidytextminig</a>:</p>
<blockquote>
<p>The statistic tf-idf is intended to measure how important a word is to a document in a collection (or corpus) of documents, for example, to one novel in a collection of novels or to one website in a collection of websites.</p>
</blockquote>
<p>Traduzido pelo Google tradutor:</p>
<blockquote>
<p>A estat√≠stica tf-idf destina-se a medir a import√¢ncia de uma palavra para um documento em uma cole√ß√£o (ou corpus) de documentos, por exemplo, para um romance em uma cole√ß√£o de romances ou para um site em uma cole√ß√£o de sites.</p>
</blockquote>
<p>Matematicamente:</p>
<p><span class="math display">\[
idf(\text{term}) = \ln{\left(\frac{n_{\text{documents}}}{n_{\text{documents containing term}}}\right)}
\]</span></p>
<p>E que com o pacote <code>tidytext</code> podemos obter usando o comando <code>bind_tf_idf()</code>, veja:</p>
<pre class="r"><code># Obtendo numero de palavras
book_words &lt;- original_books %&gt;%
  unnest_tokens(word, text) %&gt;%
  count(book, word, sort = TRUE) %&gt;%
  ungroup()%&gt;%
  anti_join(stop_words)

total_words &lt;- book_words %&gt;% 
  group_by(book) %&gt;% 
  summarize(total = sum(n))

book_words &lt;- left_join(book_words, total_words)

# tf-idf:
book_words &lt;- book_words %&gt;%
  bind_tf_idf(word, book, n)

book_words %&gt;%
  arrange(desc(tf_idf))</code></pre>
<pre><code>## # A tibble: 4,773 x 7
##    book  word              n total      tf   idf  tf_idf
##    &lt;chr&gt; &lt;chr&gt;         &lt;int&gt; &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;
##  1 br    letras         1562 30429 0.0513  0.693 0.0356 
##  2 br    ansioso         688 30429 0.0226  0.693 0.0157 
##  3 arg   classificou     666 40781 0.0163  0.693 0.0113 
##  4 arg   segundo         654 40781 0.0160  0.693 0.0111 
##  5 arg   especialistas   649 40781 0.0159  0.693 0.0110 
##  6 arg   nalti           649 40781 0.0159  0.693 0.0110 
##  7 arg   repito          649 40781 0.0159  0.693 0.0110 
##  8 br    icon            248 30429 0.00815 0.693 0.00565
##  9 arg   ncio            287 40781 0.00704 0.693 0.00488
## 10 arg   penalti         284 40781 0.00696 0.693 0.00483
## # ‚Ä¶ with 4,763 more rows</code></pre>
<p>O que nos tr√°s algo como: ‚Äútermos mais relevantes.‚Äù</p>
<p>Visualmente:</p>
<pre class="r"><code>book_words %&gt;%
  arrange(desc(tf_idf)) %&gt;%
  mutate(word = factor(word, levels = rev(unique(word)))) %&gt;% 
  group_by(book) %&gt;% 
  top_n(15) %&gt;% 
  ungroup %&gt;%
  
  ggplot(aes(word, tf_idf, fill = book)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = &quot;tf-idf&quot;) +
  facet_wrap(~book, ncol = 2, scales = &quot;free&quot;) +
  coord_flip()+
  theme_bw()+ 
    scale_fill_manual(values=c(&quot;#75ade0&quot;, &quot;#009b3a&quot;))</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
</div>
<div id="bi-grams" class="section level2">
<h2>bi grams</h2>
<p>OS bi grams s√£o sequencias de palavras, a seguir ser√° procurada as sequencias de duas palavras, o que nos permite estudar um pouco melhor o contexto do seu uso.</p>
<pre class="r"><code># Bi grams
book_bigrams &lt;- original_books %&gt;%
  unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2)

book_bigrams %&gt;%
  count(bigram, sort = TRUE)</code></pre>
<pre><code>## # A tibble: 15,106 x 3
## # Groups:   book [2]
##    book  bigram                    n
##    &lt;chr&gt; &lt;chr&gt;                 &lt;int&gt;
##  1 br    brasil copa            2039
##  2 br    copa mundo             1459
##  3 br    hoje brasil            1215
##  4 arg   argentina copa         1122
##  5 arg   isl ndia                818
##  6 br    estreia brasil          764
##  7 br    ansioso estreia         684
##  8 br    est ansioso             680
##  9 arg   classificou argentina   660
## 10 arg   copa segundo            649
## # ‚Ä¶ with 15,096 more rows</code></pre>
<p>Separando as coluna de bi grams:</p>
<pre class="r"><code>bigrams_separated &lt;- book_bigrams %&gt;%
  separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;)

bigrams_filtered &lt;- bigrams_separated %&gt;%
  filter(!word1 %in% stop_words$word) %&gt;%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts &lt;- bigrams_filtered %&gt;% 
  count(word1, word2, sort = TRUE)

bigram_counts</code></pre>
<pre><code>## # A tibble: 15,106 x 4
## # Groups:   book [2]
##    book  word1       word2         n
##    &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;     &lt;int&gt;
##  1 br    brasil      copa       2039
##  2 br    copa        mundo      1459
##  3 br    hoje        brasil     1215
##  4 arg   argentina   copa       1122
##  5 arg   isl         ndia        818
##  6 br    estreia     brasil      764
##  7 br    ansioso     estreia     684
##  8 br    est         ansioso     680
##  9 arg   classificou argentina   660
## 10 arg   copa        segundo     649
## # ‚Ä¶ with 15,096 more rows</code></pre>
<p>Caso seja preciso juntar novamente:</p>
<pre class="r"><code>bigrams_united &lt;- bigrams_filtered %&gt;%
  unite(bigram, word1, word2, sep = &quot; &quot;)

bigrams_united</code></pre>
<pre><code>## # A tibble: 71,208 x 2
## # Groups:   book [2]
##    book  bigram             
##    &lt;chr&gt; &lt;chr&gt;              
##  1 arg   isl ndia           
##  2 arg   ndia pouco         
##  3 arg   pouco mil          
##  4 arg   mil habitantes     
##  5 arg   habitantes montaram
##  6 arg   montaram sele      
##  7 arg   sele est           
##  8 arg   est copa           
##  9 arg   copa fizeram       
## 10 arg   fizeram gol        
## # ‚Ä¶ with 71,198 more rows</code></pre>
<div id="analisando-bi-grams-com-tf-idf" class="section level3">
<h3>Analisando bi grams com tf-idf</h3>
<p>Tamb√©m √© poss√≠vel aplicar a transforma√ß√£o <code>tf-idf</code> em bigrams, veja:</p>
<pre class="r"><code>#bi grams com tf idf
bigram_tf_idf &lt;- bigrams_united %&gt;%
  count(book, bigram) %&gt;%
  bind_tf_idf(bigram, book, n) %&gt;%
  arrange(desc(tf_idf))

bigram_tf_idf</code></pre>
<pre><code>## # A tibble: 15,106 x 6
## # Groups:   book [2]
##    book  bigram                    n     tf   idf  tf_idf
##    &lt;chr&gt; &lt;chr&gt;                 &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;
##  1 br    hoje brasil            1215 0.0399 0.693 0.0277 
##  2 br    ansioso estreia         684 0.0225 0.693 0.0156 
##  3 br    est ansioso             680 0.0223 0.693 0.0155 
##  4 br    letras letras           620 0.0204 0.693 0.0141 
##  5 arg   classificou argentina   660 0.0162 0.693 0.0112 
##  6 arg   copa segundo            649 0.0159 0.693 0.0110 
##  7 arg   messi repito            649 0.0159 0.693 0.0110 
##  8 arg   repito classificou      649 0.0159 0.693 0.0110 
##  9 arg   segundo especialistas   649 0.0159 0.693 0.0110 
## 10 br    brasil letras           313 0.0103 0.693 0.00713
## # ‚Ä¶ with 15,096 more rows</code></pre>
</div>
<div id="analisando-contexto-de-palavras-negativas" class="section level3">
<h3>Analisando contexto de palavras negativas:</h3>
<p>Uma das abordagens interessantes ao estudar as bi-grams √© a de avaliar o contexto das palavras negativas, veja:</p>
<pre class="r"><code>bigrams_separated %&gt;%
  filter(word1 == &quot;nao&quot;) %&gt;%
  count(word1, word2, sort = TRUE)</code></pre>
<pre><code>## # A tibble: 35 x 4
## # Groups:   book [2]
##    book  word1 word2         n
##    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;
##  1 br    nao   copa         10
##  2 arg   nao   abrir         3
##  3 arg   nao   convoca       3
##  4 arg   nao   ruim          3
##  5 br    nao   acredito      2
##  6 arg   nao   achei         1
##  7 arg   nao   acordem       1
##  8 arg   nao   argentina     1
##  9 arg   nao   assisti       1
## 10 arg   nao   compara       1
## # ‚Ä¶ with 25 more rows</code></pre>
<pre class="r"><code>not_words &lt;- bigrams_separated %&gt;%
  filter(word1 == &quot;nao&quot;) %&gt;%
  inner_join(sentiment, by = c(word2 = &quot;word&quot;)) %&gt;%
  count(word2, sentiment, sort = TRUE) %&gt;%
  ungroup()

not_words</code></pre>
<pre><code>## # A tibble: 3 x 4
##   book  word2    sentiment     n
##   &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;int&gt;
## 1 arg   ruim     negative      3
## 2 arg   vencer   positive      1
## 3 br    amistoso positive      1</code></pre>
<p>A palavra n√£o antes de uma palavra ‚Äúpositiva,‚Äù como por exemplo ‚Äún√£o gosto‚Äù pode ser anulada ao somar-se suas polaridades (‚Äún√£o‚Äù = - 1, ‚Äúgosto‚Äù = +1 e ‚Äún√£o gosto‚Äù = -1 + 1) o leva a necessidade de ser tomar um cuidado especial com essas palavras em uma an√°lise de texto mais detalhada, veja de forma visual:</p>
<pre class="r"><code>not_words %&gt;%
  mutate(sentiment=ifelse(sentiment==&quot;positive&quot;,1,ifelse(sentiment==&quot;negative&quot;,-1,0)))%&gt;%
  mutate(contribution = n * sentiment) %&gt;%
  arrange(desc(abs(contribution))) %&gt;%
  head(20) %&gt;%
  mutate(word2 = reorder(word2, contribution)) %&gt;%
  
  ggplot(aes(word2, n * sentiment, fill = n * sentiment &gt; 0)) +
  geom_col() +
  xlab(&quot;Words preceded by \&quot;not\&quot;&quot;) +
  ylab(&quot;Sentiment score * number of occurrences&quot;) +
  coord_flip()+
  theme_bw()</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="machine-learning" class="section level1">
<h1>Machine Learning</h1>
<p>Estava pesquisando sobre algor√≠timos recomendados para a an√°lise de texto quando encontrei um artigo da data camp chamado: <a href="https://www.datacamp.com/community/tutorials/R-nlp-machine-learning"><em>Lyric Analysis with NLP &amp; Machine Learning with R</em></a>, do qual a autora exp√µe a seguinte tabela:</p>
<center>
<img src="http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1517331396/MLImage_cygwsb.jpg" style="width:60.0%" />
</center>
<p>Portanto resolvi fazer uma brincadeira e ajustar 4 dos modelos propostos para a tarefa supervisionada de classifica√ß√£o: K-NN, Tress (tentarei o ajuste do algor√≠timo Random Forest), Logistic Regression (Modelo estat√≠stico) e Naive-Bayes (por meio do c√°lculo de probabilidades condicionais) para ver se conseguia recuperar a classifica√ß√£o de quais os termos de pesquisa que eu utilizei para obter esses dados</p>
<p>Al√©m de t√©cnicas apresentadas no livro do pacote <code>caret</code>, por <span class="citation"><a href="#ref-caret" role="doc-biblioref">Kuhn</a> (<a href="#ref-caret" role="doc-biblioref">2018</a>)</span>, muito do que apliquei aqui foi baseado no livro ‚ÄúIntrodu√ß√£o a minera√ß√£o de dados‚Äù por <span class="citation"><a href="#ref-miner" role="doc-biblioref">Silva; Peres; Boscarioli</a> (<a href="#ref-miner" role="doc-biblioref">2016</a>)</span>, que foi bastante √∫til na minha introdu√ß√£o sobre o tema Machine Learning.</p>
<p>Vou utilizar uma fun√ß√£o chamada <code>plot_pred_type_distribution()</code>,apresentada neste post de titulo: <a href="https://www.r-bloggers.com/illustrated-guide-to-roc-and-auc/">Illustrated Guide to ROC and AUC</a> e fiz uma pequena altera√ß√£o para que ela funcionasse para o dataset deste post . A fun√ß√£o adaptada pode ser encontrada <a href="https://github.com/gomesfellipe/functions/blob/master/plot_pred_type_distribution.R">neste link</a> no meu github e a fun√ß√£o original <a href="https://github.com/joyofdata/joyofdata-articles/blob/master/roc-auc/plot_pred_type_distribution.R">neste link do github do autor</a>.</p>
<div id="pacote-caret" class="section level2">
<h2>Pacote caret</h2>
<p>Basicamente o ajuste de todos os modelos envolveram o uso do pacote <code>caret</code> e muitos dos passos aqui foram baseados nas instru√ß√µes fornecidas no <a href="https://topepo.github.io/caret/index.html">livro do pacote</a>. O pacote facilita bastante o ajuste dos par√¢metros no ajuste de modelos.</p>
</div>
<div id="transformar-e-arrumar" class="section level2">
<h2>Transformar e arrumar</h2>
<p>Uma <a href="https://www.kaggle.com/kailex/tidy-xgboost-glmnet-text2vec-lsa">solu√ß√£o do kaggle</a> para o desafio <a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge">Toxic Comment Classification Challenge</a> me chamou aten√ß√£o, do qual o participante da competi√ß√£o criou colunas que sinalizassem os caracteres especiais de cada frase, utilizarei esta t√©cnica para o ajuste e novamente utilizarei o pacote de l√©xicos do apresentado no <a href="https://sillasgonzaga.github.io/2017-09-23-sensacionalista-pt01/">post do blog Paix√£o por dados</a></p>
<p>Veja a base transformada e arrumada:</p>
<pre class="r"><code># Ref: https://cfss.uchicago.edu/text_classification.html 
# https://www.r-bloggers.com/illustrated-guide-to-roc-and-auc/
devtools::source_url(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/plot_pred_type_distribution.R&quot;)

base &lt;- base %&gt;% 
  mutate(length = str_length(text),
         ncap = str_count(text, &quot;[A-Z]&quot;),
         ncap_len = ncap / length,
         nexcl = str_count(text, fixed(&quot;!&quot;)),
         nquest = str_count(text, fixed(&quot;?&quot;)),
         npunct = str_count(text, &quot;[[:punct:]]&quot;),
         nword = str_count(text, &quot;\\w+&quot;),
         nsymb = str_count(text, &quot;&amp;|@|#|\\$|%|\\*|\\^&quot;),
         nsmile = str_count(text, &quot;((?::|;|=)(?:-)?(?:\\)|D|P))&quot;),
         text = clean_tweets(text) %&gt;% enc2native() %&gt;% rm_accent())%&gt;%
  unnest_tokens(word, text) %&gt;% 
  anti_join(stop_words)%&gt;%
  group_by(book,line,length, ncap, ncap_len, nexcl, nquest, npunct, nword, nsymb, nsmile)%&gt;%
  summarise(text=paste(word,collapse = &quot; &quot;)) %&gt;% 
  select(text,everything())%T&gt;% 
  print()</code></pre>
<pre><code>## # A tibble: 7,995 x 12
## # Groups:   book, line, length, ncap, ncap_len, nexcl, nquest, npunct, nword,
## #   nsymb [7,995]
##    text  book   line length  ncap ncap_len nexcl nquest npunct nword nsymb
##    &lt;chr&gt; &lt;chr&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt;    &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;
##  1 isl ‚Ä¶ arg       1     NA     7  NA          0      0      6    24     1
##  2 pau ‚Ä¶ arg       2    108     6   0.0556     0      0      2    20     1
##  3 mess‚Ä¶ arg       3     NA    10  NA          0      0      3    24     1
##  4 minu‚Ä¶ arg       4     NA     2  NA          0      0      2    24     1
##  5 requ‚Ä¶ arg       5    129    23   0.178      0      0     15    21     1
##  6 bras‚Ä¶ arg       6     NA    11  NA          0      0     12    20     1
##  7 dupl‚Ä¶ arg       7    123    84   0.683      0      0      8    21     1
##  8 mess‚Ä¶ arg       8     NA    10  NA          0      0      3    24     1
##  9 mess‚Ä¶ arg       9     NA    10  NA          0      0      3    24     1
## 10 mess‚Ä¶ arg      10     NA    10  NA          0      0      3    24     1
## # ‚Ä¶ with 7,985 more rows, and 1 more variable: nsmile &lt;int&gt;</code></pre>
<p>Ap√≥s arrumar e transformar as informa√ß√µes que ser√£o utilizadas na classifica√ß√£o, ser√° criado um corpus sem a abordagem tidy para obter a matriz de documentos e termos, e depois utilizar a coluna de classifica√ß√£o, veja:</p>
<pre class="r"><code>library(tm)       #Pacote de para text mining
corpus &lt;- Corpus(VectorSource(base$text))

#Criando a matrix de termos:
book_dtm = DocumentTermMatrix(corpus, control = list(minWordLength=2,minDocFreq=3)) %&gt;% 
  weightTfIdf(normalize = T) %&gt;%    # Transforma√ß√£o tf-idf com pacote tm
  removeSparseTerms( sparse = .95)  # obtendo matriz esparsa com pacote tm

#Transformando em matrix, permitindo a manipulacao:
matrix = as.matrix(book_dtm)
dim(matrix)</code></pre>
<pre><code>## [1] 7995   18</code></pre>
<p>Pronto, agora j√° podemos juntar tudo em um data frame e separa em treino e teste para a classifica√ß√£o dos textos obtidos do twitter:</p>
<pre class="r"><code>#Criando a base de dados:
full=data.frame(cbind(
  base[,&quot;book&quot;],
  matrix,
  base[,-c(1:3)]
  )) %&gt;% na.omit()</code></pre>
</div>
<div id="treino-e-teste" class="section level2">
<h2>Treino e teste</h2>
<p>Ser√° utilizado tanto o m√©todo de hold-out e de cross-validation</p>
<pre class="r"><code>set.seed(825)
particao = sample(1:2,nrow(full), replace = T,prob = c(0.7,0.3))

train = full[particao==1,] 
test = full[particao==2,] 

library(caret)</code></pre>
</div>
<div id="ajustando-modelos" class="section level2">
<h2>Ajustando modelos</h2>
<div id="knn" class="section level3">
<h3>KNN</h3>
<p>√â uma t√©cnica de aprendizado baseado em inst√¢ncia, isto quer dizer que a classifica√ß√£o de uma observa√ß√£o com a classe desconhecida √© realizada a partir da compara√ß√£o com outras observa√ß√µes cada vez que uma observa√ß√£o √© apresentado ao modelo e tamb√©m √© conhecido como ‚Äúlazy evaluation,‚Äù j√° que um modelo n√£o √© induzido previamente.</p>
<p>Diversas medidas de dist√¢ncia podem ser utilizadas, utilizarei aqui a euclideana e al√©m disso a escolha do par√¢metro <span class="math inline">\(k\)</span> (de k vizinhos mais pr√≥ximos) deve ser feita com cuidado pois um <span class="math inline">\(k\)</span> pequeno pode expor o algor√≠timo a uma alta sensibilidade a um ru√≠do.</p>
<p>Utilizarei aqui o pacote <code>caret</code> como ferramenta para o ajuste deste modelo pois ela permite que eu configure que seja feita a valida√ß√£o cruzada em conjunto com a padroniza√ß√£o, pois esses complementos beneficiam no ajuste de modelos que calculam dist√¢ncias.</p>
<pre class="r"><code># knn -------
set.seed(825)
antes = Sys.time()
book_knn &lt;- train(book ~.,
                  data=train,
                 method = &quot;knn&quot;,
                 trControl = trainControl(method = &quot;cv&quot;,number = 10), # validacao cruzada
                 preProc = c(&quot;center&quot;, &quot;scale&quot;))                      
time_knn &lt;- Sys.time() - antes 
Sys.time() - antes</code></pre>
<pre><code>## Time difference of 2.465522 secs</code></pre>
<pre class="r"><code>plot(book_knn)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/knn-1.png" width="672" /></p>
<pre class="r"><code>previsao  = predict(book_knn, test)
confusionMatrix(previsao, factor(test$book))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction arg  br
##        arg 105   8
##        br    5 371
##                                          
##                Accuracy : 0.9734         
##                  95% CI : (0.955, 0.9858)
##     No Information Rate : 0.7751         
##     P-Value [Acc &gt; NIR] : &lt;2e-16         
##                                          
##                   Kappa : 0.9245         
##                                          
##  Mcnemar&#39;s Test P-Value : 0.5791         
##                                          
##             Sensitivity : 0.9545         
##             Specificity : 0.9789         
##          Pos Pred Value : 0.9292         
##          Neg Pred Value : 0.9867         
##              Prevalence : 0.2249         
##          Detection Rate : 0.2147         
##    Detection Prevalence : 0.2311         
##       Balanced Accuracy : 0.9667         
##                                          
##        &#39;Positive&#39; Class : arg            
## </code></pre>
<pre class="r"><code>df = cbind(fit = if_else(previsao==&quot;br&quot;,1,0), class = if_else(test$book==&quot;br&quot;,1,0)) %&gt;% as.data.frame()
plot_pred_type_distribution(df,0.5)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/knn-2.png" width="672" /></p>
<p>Como podemos ver, segundo a valida√ß√£o cruzada realizada com o pacote <code>caret</code>, o n√∫mero 5 de vizinhos mais pr√≥ximos foi o que apresentou o melhor resultado. Al√©m disso o modelo apresentou uma acur√°cia de 97,18% e isto parece bom dado que a sensibilidade (taxa de verdadeiros positivos) e a especificidade (taxa de verdadeiros negativos) foram altas tamb√©m, o que foi refor√ßado com o gr√°fico ilustrado da matriz de confus√£o.</p>
<p>O tempo computacional para o ajuste do modelo foi de:2.46385908126831 segundos</p>
</div>
<div id="random-forest" class="section level3">
<h3>Random Forest</h3>
<p>O modelo de Random Forest tem se tornado muito popular devido ao seu bom desempenho e pela sua alta capacidade de se adaptar aos dados. O modelo funciona atrav√©s da combina√ß√£o de v√°rias √°rvores de decis√µes e no seu ajuste alguns par√¢metros precisam ser levados em conta.</p>
<p>O par√¢metro que sera levado em conta para o ajuste ser√° apenas o <code>ntree</code>, que representa o n√∫mero de √°rvores ajustadas. Este par√¢metro deve ser escolhido com cuidado pois pode ser t√£o grande quanto voc√™ quiser e continua aumentando a precis√£o at√© certo ponto por√©m pode ser mais limitado pelo tempo computacional dispon√≠vel.</p>
<pre class="r"><code>set.seed(824)
# Random Forest
antes = Sys.time()
book_rf &lt;- train(book ~.,
                  data=train,
                     method = &quot;rf&quot;,trace=F,
                     ntree = 200,
                     trControl = trainControl(method = &quot;cv&quot;,number = 10))
time_rf &lt;- Sys.time() - antes 
Sys.time() - antes</code></pre>
<pre><code>## Time difference of 8.994044 secs</code></pre>
<pre class="r"><code>library(randomForest)
varImpPlot(book_rf$finalModel)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/rf-1.png" width="672" /></p>
<pre class="r"><code>previsao  = predict(book_rf, test)
confusionMatrix(previsao, factor(test$book))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction arg  br
##        arg 110   0
##        br    0 379
##                                      
##                Accuracy : 1          
##                  95% CI : (0.9925, 1)
##     No Information Rate : 0.7751     
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16  
##                                      
##                   Kappa : 1          
##                                      
##  Mcnemar&#39;s Test P-Value : NA         
##                                      
##             Sensitivity : 1.0000     
##             Specificity : 1.0000     
##          Pos Pred Value : 1.0000     
##          Neg Pred Value : 1.0000     
##              Prevalence : 0.2249     
##          Detection Rate : 0.2249     
##    Detection Prevalence : 0.2249     
##       Balanced Accuracy : 1.0000     
##                                      
##        &#39;Positive&#39; Class : arg        
## </code></pre>
<pre class="r"><code># https://www.r-bloggers.com/illustrated-guide-to-roc-and-auc/
df = cbind(fit = if_else(previsao==&quot;br&quot;,1,0), class = if_else(test$book==&quot;br&quot;,1,0)) %&gt;% as.data.frame()
plot_pred_type_distribution(df,0.5)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/rf-2.png" width="672" /></p>
<p>Segundo o gr√°fico de import√¢ncia, parece que as palavras ‚Äúbrasil,‚Äù ‚Äúargentina,‚Äù ‚Äúcopa‚Äù e ‚Äúmessi‚Äù foram as que apresentaram maior impacto do preditor (lembrando que essa medida n√£o √© um efeito espec√≠fico), o que mostra que a presen√ßa das palavras que estamos utilizando para classificar tiveram um impacto na classifica√ß√£o bastante superior aos demais.</p>
<p>Quanto a acur√°cia, o random forest apresentou valor um pouco maior do que o do algor√≠timo K-NN e al√©m disso apresentou altos valores para a sensibilidade (taxa de verdadeiros positivos) e a especificidade (taxa de verdadeiros negativos), o que foi refor√ßado com o gr√°fico ilustrado da matriz de confus√£o, por√©m o tempo computacional utilizado para ajustar este modelo foi muito maior, o que leva a questionar se esse pequeno aumento na taxa de acerto vale a pena aumentando tanto no tempo de processamento (outra alternativa seria diminuir o tamanho do n√∫mero de √°rvores para ver se melhoraria na qualidade do ajuste).</p>
<p>O tempo computacional para o ajuste do modelo foi de: 8.99299788475037 segundos</p>
</div>
<div id="naive-bayes" class="section level3">
<h3>Naive Bayes</h3>
<p>Este √© um algor√≠timo que trata-se de um classificador estat√≠stico baseado no <strong>Teorema de Bayes</strong> e recebe o nome de ing√™nuo (<em>naive</em>) porque pressup√µe que o valor de um atributo que exerce algum efeito sobre a distribui√ß√£o da vari√°vel resposta √© independente do efeito que outros atributos.</p>
<p>O c√°lculo para a classifica√ß√£o √© feito por meio do c√°lculo de probabilidades condicionais, ou seja, probabilidade de uma observa√ß√£o pertencer a cada classe dado os exemplares existentes no conjunto de dados usado para o treinamento.</p>
<pre class="r"><code># Naive Bayes ----
set.seed(825)
antes = Sys.time()
book_nb &lt;- train(book ~.,
                  data=train,
                 method= &quot;nb&quot;,
                 laplace =1,       
                 trControl = trainControl(method = &quot;cv&quot;,number = 10))
time_nb &lt;- Sys.time() - antes 
Sys.time() - antes</code></pre>
<pre><code>## Time difference of 7.141471 secs</code></pre>
<pre class="r"><code>previsao  = predict(book_nb, test)
confusionMatrix(previsao, factor(test$book))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction arg  br
##        arg 108   6
##        br    2 373
##                                          
##                Accuracy : 0.9836         
##                  95% CI : (0.968, 0.9929)
##     No Information Rate : 0.7751         
##     P-Value [Acc &gt; NIR] : &lt;2e-16         
##                                          
##                   Kappa : 0.9537         
##                                          
##  Mcnemar&#39;s Test P-Value : 0.2888         
##                                          
##             Sensitivity : 0.9818         
##             Specificity : 0.9842         
##          Pos Pred Value : 0.9474         
##          Neg Pred Value : 0.9947         
##              Prevalence : 0.2249         
##          Detection Rate : 0.2209         
##    Detection Prevalence : 0.2331         
##       Balanced Accuracy : 0.9830         
##                                          
##        &#39;Positive&#39; Class : arg            
## </code></pre>
<pre class="r"><code># https://www.r-bloggers.com/illustrated-guide-to-roc-and-auc/
df = cbind(fit = if_else(previsao==&quot;br&quot;,1,0), class = if_else(test$book==&quot;br&quot;,1,0)) %&gt;% as.data.frame()
plot_pred_type_distribution(df,0.5)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/nb-1.png" width="672" /></p>
<p>Apesar a aparente acur√°cia alta, o valor calculado para a especificidade (verdadeiros negativos) foi elevado o que aponta que o ajuste do modelo n√£o se apresentou de forma eficiente</p>
<p>O tempo computacional foi de 7.1403751373291 segundos</p>
</div>
<div id="glm---logit" class="section level3">
<h3>GLM - Logit</h3>
<p>Este √© um modelo estat√≠stico que j√° abordei aqui no blog no post sobre <a href="https://gomesfellipe.github.io/post/2018-05-26-smarteademachinelearning/smarteademachinelearning/">AED de forma r√°pida e um pouco de machine learning</a> e seguindo a recomenda√ß√£o do artigo da datacamp vejamos quais resultados obtemos com o ajuste deste modelo:</p>
<pre class="r"><code># Modelo log√≠stico ----
set.seed(825)
antes = Sys.time()
book_glm &lt;- train(book ~.,
                  data=train,
                  method = &quot;glm&quot;,                                         # modelo generalizado
                  family = binomial(link = &#39;logit&#39;),                      # Familia Binomial ligacao logit
                  trControl = trainControl(method = &quot;cv&quot;, number = 10))   # validacao cruzada
time_glm &lt;- Sys.time() - antes 
Sys.time() - antes</code></pre>
<pre><code>## Time difference of 1.378149 secs</code></pre>
<pre class="r"><code>library(ggfortify)

autoplot(book_glm$finalModel, which = 1:6, data = train,
         colour = &#39;book&#39;, label.size = 3,
         ncol = 3) + theme_classic()</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/glm-1.png" width="672" /></p>
<pre class="r"><code>previsao  = predict(book_glm, test)
confusionMatrix(previsao, factor(test$book))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction arg  br
##        arg 109   0
##        br    1 379
##                                           
##                Accuracy : 0.998           
##                  95% CI : (0.9887, 0.9999)
##     No Information Rate : 0.7751          
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.9941          
##                                           
##  Mcnemar&#39;s Test P-Value : 1               
##                                           
##             Sensitivity : 0.9909          
##             Specificity : 1.0000          
##          Pos Pred Value : 1.0000          
##          Neg Pred Value : 0.9974          
##              Prevalence : 0.2249          
##          Detection Rate : 0.2229          
##    Detection Prevalence : 0.2229          
##       Balanced Accuracy : 0.9955          
##                                           
##        &#39;Positive&#39; Class : arg             
## </code></pre>
<pre class="r"><code>df = cbind(fit = if_else(previsao==&quot;br&quot;,1,0), class = if_else(test$book==&quot;br&quot;,1,0)) %&gt;% as.data.frame()
plot_pred_type_distribution(df,0.5)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/glm-2.png" width="672" /></p>
</div>
</div>
</div>
<div id="comparando-modelos" class="section level1">
<h1>Comparando modelos</h1>
<p>Agora que temos 4 modelos ajustados e cada um apresentando resultados diferentes, vejamos qual deles seria o mais interessante para caso fosse necess√°rio recuperar a classifica√ß√£o dos termos pesquisados atrav√©s da API, veja a seguir um resumo das medidas obtidas:</p>
<pre class="r"><code># &quot;Dados esses modelos, podemos fazer declara√ß√µes estat√≠sticas sobre suas diferen√ßas de desempenho? Para fazer isso, primeiro coletamos os resultados de reamostragem usando resamples.&quot; - caret
resamps &lt;- resamples(list(knn = book_knn,
                          rf = book_rf,
                          nb = book_nb,
                          glm = book_glm)) 
summary(resamps)</code></pre>
<pre><code>## 
## Call:
## summary.resamples(object = resamps)
## 
## Models: knn, rf, nb, glm 
## Number of resamples: 10 
## 
## Accuracy 
##          Min.   1st Qu.    Median      Mean   3rd Qu. Max. NA&#39;s
## knn 0.9553571 0.9821824 0.9823009 0.9831305 0.9889381    1    0
## rf  0.9823009 1.0000000 1.0000000 0.9973451 1.0000000    1    0
## nb  0.9107143 0.9623894 0.9823009 0.9768726 1.0000000    1    0
## glm 0.9910714 0.9911504 1.0000000 0.9964523 1.0000000    1    0
## 
## Kappa 
##          Min.   1st Qu.    Median      Mean   3rd Qu. Max. NA&#39;s
## knn 0.8730734 0.9500663 0.9512773 0.9530901 0.9691458    1    0
## rf  0.9513351 1.0000000 1.0000000 0.9926689 1.0000000    1    0
## nb  0.7791798 0.8998204 0.9525409 0.9398109 1.0000000    1    0
## glm 0.9752868 0.9753544 1.0000000 0.9901350 1.0000000    1    0</code></pre>
<p>Como podemos ver, o modelo que apresentou a menor acur√°cia e o menor coeficiente kappa foi o Naive Bayes enquanto que o que apresentou as maiores medidas de qualidade do ajuste foi o modelo ajustado com o algor√≠timo Random Forest e tanto o modelo ajustado pelo algor√≠timo knn quanto o modelo linear generalizado com fun√ß√£o de liga√ß√£o ‚Äúlogit‚Äù tamb√©m apresentaram acur√°cia e coeficiente kappa pr√≥ximos do apresentado no ajuste do Random Forest.</p>
<p>Portanto, apesar dos ajustes, caso dois modelos n√£o apresentem diferen√ßa estatisticamente significante e o tempo computacional gasto para o ajuste de ambos for muito diferente pode ser que ser que tenhamos um modelo candidato para:</p>
<pre class="r"><code>c( knn= time_knn,rf = time_rf,nb = time_nb,glm = time_glm)</code></pre>
<pre><code>## Time differences in secs
##      knn       rf       nb      glm 
## 2.463859 8.992998 7.140375 1.377073</code></pre>
<p>O modelo linear generalizado foi o que apresentou o menor tempo computacional e foi o que apresentou o terceiro maior registro para os as medidas de qualidade do ajuste dos modelos, portanto esse modelo ser√° avaliado com mais cuidado em seguida para saber se ele ser√° o modelo selecionado</p>
<p><strong>Obs.:</strong> Sou suspeito para falar mas dentre esses modelos eu teria prefer√™ncia por este modelo de qualquer maneira por n√£o se tratar de uma ‚Äúcaixa preta,‚Äù da qual todos os efeitos de cada par√¢metro ajustado podem ser interpretado, al√©m de obter medidas como raz√µes de chance que ajudam bastante na compreens√£o dos dados.</p>
<p>Comparando de forma visual:</p>
<pre class="r"><code>splom(resamps)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<p>Assim fica mais claro o como o ajuste dos modelos Random Forest, K-NN e GLM se destacaram quando avaliados em rela√ß√£o a acur√°cia apresentada.</p>
<p>Vejamos a seguir como foi a distribui√ß√£o dessas medidas de acordo com cada modelo atrav√©s de boxplots:</p>
<pre class="r"><code>bwplot(resamps)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>Note que al√©m de apresentar os ajustes com menor acur√°cia (e elevada taxa de falsos negativos) o algor√≠timo Naive Bayes foi o que apresentou a maior varia√ß√£o interquartil das medidas de qualidade do ajuste do modelo.</p>
<p>Para finalizar a an√°lise visual vamos obter as diferen√ßas entre os modelos com a fun√ß√£o <code>diff()</code> e em seguida conferir de maneira visual o comportamento dessas informa√ß√µes:</p>
<pre class="r"><code>difValues &lt;- diff(resamps)

# plot:
bwplot(difValues)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>Observe que tanto o modelo log√≠stico quando o ajuste com o algor√≠timo K-NN apresentaram valores muito pr√≥ximos dos valores do ajuste do Random Forest e como j√° vimos o Random Forest foi o modelo que levou maior tempo computacional para ser ajustado, portanto vamos conferir a seguir se existe diferen√ßa estatisticamente significante entre os valores obtidos atrav√©s de cada um dos ajustes e decidir qual dos modelos se apresentou de maneira mais adequada para nosso caso:</p>
<pre class="r"><code>resamps$values %&gt;% 
  select_if(is.numeric) %&gt;% 
  purrr::map(function(x) shapiro.test(x))</code></pre>
<pre><code>## $`knn~Accuracy`
## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.87602, p-value = 0.1174
## 
## 
## $`knn~Kappa`
## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.87418, p-value = 0.1118
## 
## 
## $`rf~Accuracy`
## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.53165, p-value = 8.564e-06
## 
## 
## $`rf~Kappa`
## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.53234, p-value = 8.727e-06
## 
## 
## $`nb~Accuracy`
## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.80077, p-value = 0.01482
## 
## 
## $`nb~Kappa`
## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.81793, p-value = 0.02392
## 
## 
## $`glm~Accuracy`
## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.6429, p-value = 0.0001803
## 
## 
## $`glm~Kappa`
## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.64123, p-value = 0.0001722</code></pre>
<p>Como a hip√≥tese de normalidade n√£o foi rejeitada para nenhuma das amostras de acur√°cias registradas, vejamos se existe diferen√ßa estatisticamente significante entre as m√©dias dessas medidas de qualidade para cada modelo:</p>
<pre class="r"><code>t.test(resamps$values$`rf~Accuracy`,resamps$values$`knn~Accuracy`, paired = T)  </code></pre>
<pre><code>## 
##  Paired t-test
## 
## data:  resamps$values$`rf~Accuracy` and resamps$values$`knn~Accuracy`
## t = 3.9961, df = 9, p-value = 0.003129
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.0061678 0.0222614
## sample estimates:
## mean of the differences 
##               0.0142146</code></pre>
<p>Rejeita a hip√≥tese de que as m√©dias das acur√°cias calculadas para o ajuste do algor√≠timo Random Forest e K-NN foram iguais</p>
<pre class="r"><code>t.test(resamps$values$`rf~Accuracy`,resamps$values$`glm~Accuracy`, paired = T)  </code></pre>
<pre><code>## 
##  Paired t-test
## 
## data:  resamps$values$`rf~Accuracy` and resamps$values$`glm~Accuracy`
## t = 0.43326, df = 9, p-value = 0.675
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.003768926  0.005554640
## sample estimates:
## mean of the differences 
##            0.0008928571</code></pre>
<p>Novamente, rejeita-se a hip√≥tese de que as m√©dias das acur√°cias calculadas para o ajuste do algor√≠timo Random Forest e do modelo de log√≠stico foram iguais</p>
<pre class="r"><code>t.test(resamps$values$`knn~Accuracy`,resamps$values$`glm~Accuracy`, paired = T)</code></pre>
<pre><code>## 
##  Paired t-test
## 
## data:  resamps$values$`knn~Accuracy` and resamps$values$`glm~Accuracy`
## t = -4.0077, df = 9, p-value = 0.003074
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.020841197 -0.005802292
## sample estimates:
## mean of the differences 
##             -0.01332174</code></pre>
<p>J√° para a compara√ß√£o entre as m√©dias das acur√°cias calculadas para o algor√≠timo K-NN e para o modelo log√≠stico n√£o houve evid√™ncias estat√≠sticas para se rejeitas a hip√≥tese de que ambas as m√©dias s√£o iguais, o que nos sugere o modelo log√≠stico como o segundo melhor candidato como modelo de classifica√ß√£o para este problema com estes dados.</p>
<p>Ent√£o a escolha ficar√° a crit√©rio do que √© mais importante. Caso o tempo computacional fosse uma medida que tivesse mais import√¢ncia do que a pequena superioridade de acur√°cia apresentada pelo algor√≠timo Random Forest, escolheria o modelo log√≠stico, por√©m como neste caso os 7.61592507362366 segundos a mais para ajustar o modelo n√£o fazem diferen√ßa para mim, fico com o modelo Random Forest.</p>
<p>Este post tr√°s alguns dos conceitos que venho estudado e existem muitos t√≥picos apresentados aqui que podem (e devem) ser estudados com mais profundidade, espero que tenha gostado!</p>
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias</h1>
<p>obs.: links mensionados no corpo do texto</p>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-caret" class="csl-entry">
Kuhn, Max. 2018. <em>The Caret Package</em>. <a href="https://topepo.github.io/caret/index.html">https://topepo.github.io/caret/index.html</a>.
</div>
<div id="ref-tidytext" class="csl-entry">
Silge; Robinson, Julia; David. 2018. <em>Text Mining with R</em>. <em>A Tidy Approach</em>. <a href="https://www.tidytextmining.com/">https://www.tidytextmining.com/</a>.
</div>
<div id="ref-miner" class="csl-entry">
Silva; Peres; Boscarioli, Leandro Augusto; Sarajane Marques; Clodis. 2016. <em>Introdu√ß√£o √† Minera√ß√£o de Dados</em>. <em>Com Aplica√ß√µes Em R</em>. Vol. 3. Elsevier Editora Ltda.
</div>
</div>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml/">Brasil x Argentina, tidytext e Machine Learning</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Analise Explorat√≥ria</category>
      <category>Aprendizado N√£o Supervisionado</category>
      <category>Data mining</category>
      <category>Estatistica</category>
      <category>Machine Learning</category>
      <category>Modelagem Estatistica</category>
      <category>Pr√°tica</category>
      <category>R</category>
      <category>Text Mining</category>
      <category>An√°lise de Sentimentos</category>
      <category domain="tag">Data Mining</category>
      <category domain="tag">Estatistica</category>
      <category domain="tag">gomesfellipe</category>
      <category domain="tag">twitter</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">Pr√°tica</category>
      <category domain="tag">R</category>
      <category domain="tag">text mining</category>
    </item>
    <item>
      <title>AED de forma r√°pida e um pouco de Machine Learning</title>
      <link>https://gomesfellipe.github.io/post/2018-05-26-smarteademachinelearning/smarteademachinelearning/</link>
      <pubDate>Sat, 26 May 2018 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2018-05-26-smarteademachinelearning/smarteademachinelearning/</guid>
      <description>Veja como √© poss√≠vel realizar a AED de forma muito r√°pida com o pacote SmartEAD, al√©m de uma breve aplica√ß√£o de t√©cnicas de machine learning e estat√≠stica para ilustrar alguns poss√≠veis cen√°rios da analise da dados</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/pagedtable/css/pagedtable.css" rel="stylesheet" />
<script src="/rmarkdown-libs/pagedtable/js/pagedtable.js"></script>


<style>
  .col2 {
    columns: 2 200px;         /* number of columns and width in pixels*/
    -webkit-columns: 2 200px; /* chrome, safari */
    -moz-columns: 2 200px;    /* firefox */
  }
  .col3 {
    columns: 3 100px;
    -webkit-columns: 3 100px;
    -moz-columns: 3 100px;
  }
</style>
<!-- Resumo: Neste post mostro como √© poss√≠vel realizar a AED de forma muito r√°pida com o pacote SmartEAD, e aplico algumas t√©cnicas de machine learning e estat√≠stica para ilustrar alguns poss√≠veis cen√°rios-->
<div id="a-an√°lise-explorat√≥ria-dos-dados" class="section level1">
<h1>A an√°lise explorat√≥ria dos dados</h1>
<div class="col2">
<p>A an√°lise explorat√≥ria dos dados (AED) foi um termo que ganhou bastante popularidade quando Tukey publicou o livro Exploratory Data Analysis em 1977 que tratava uma ‚Äúbusca por conhecimento antes da an√°lise de dados de fato‚Äù. Ocorre quando busca-se obter informa√ß√µes ocultas sobre os dados, tais como: varia√ß√£o, anomalias, distribui√ß√£o, tend√™ncias, padr√µes e rela√ß√µes</p>
<p>Ao iniciar uma an√°lise de dados, come√ßamos pela AED para a partir dai decidir como buscar qual solu√ß√£o para o problema. √â importante frisar que a AED e a constru√ß√£o de gr√°ficos <strong>n√£o</strong> s√£o a mesma coisa, mesmo a AED sendo altamente baseada em produ√ß√£o de gr√°ficos como de dispers√£o, histogramas, boxplots etc.</p>
<p>Por vezes a AED no R pode envolver a produ√ß√£o de longos scripts utilizando fun√ß√µes como as do pacote <code>ggplot2</code> e mesmo sabendo que desejamos sempre criar o gr√°fico de maneira mais informativa e atraente poss√≠vel, as vezes precisamos ter uma no√ß√£o geral dos dados de forma r√°pida, n√£o necessariamente t√£o detalhada e customizada de cara.</p>
<p>A vezes queremos apenas ter uma primeira impress√£o dos dados e em seguida pensar em quais os gr√°ficos mais se adequariam para a entrega dos resultados que mesmo as fun√ß√µes base do R dependendo do caso tamb√©m envolvem a confec√ß√£o de longos scripts.</p>
<p>Existem pacotes que auxiliam na hora de se fazer uma r√°pida an√°lise explorat√≥ria, como o <a href="https://github.com/ropenscilabs/skimr">skimr</a> e o <a href="https://github.com/boxuancui/DataExplorer">DataExplorer</a>. Por√©m estava pesquisando de existiam mais op√ß√µes para uma r√°pida abordagem de AED e me deparei com esta <a href="https://cran.r-project.org/web/packages/SmartEDA/vignettes/Report_r1.html">vinheta</a>, por Dayanand, Kiran, Ravi.</p>
<p>Essa vinheta apresenta o pacote <a href="https://cran.r-project.org/web/packages/SmartEDA"><code>SmartEAD</code></a> que tr√°s uma s√©rie de fun√ß√µes que auxiliam na AED de forma bem pr√°tica. O pacote est√° dispon√≠vel no CRAN.</p>
<p>Para testar o pacote foi utilizada uma base de dados do artigo <a href="http://people.stern.nyu.edu/wgreene/Lugano2013/Fair-ExtramaritalAffairs.pdf">A Theory of Extramarital Affairs</a>, publicado pela <a href="http://www.jstor.org/publisher/ucpress">The University of Chicago Press</a>.</p>
<p>Gostei tanto da proposta do pacote que resolvi preparar este post que conta com a explana√ß√£o de alguns t√≥picos apresentados pelo autor, algumas explica√ß√µes da teoria estat√≠stica apresentada na an√°lise descritiva e explorat√≥ria dos dados e al√©m da aplica√ß√£o de algumas t√©cnicas estat√≠sticas e de machine learning para o entendimento da base de dados.</p>
</div>
<p></br></p>
</div>
<div id="smarteda" class="section level1">
<h1>SmartEDA</h1>
<p>Como ele pode ajud√°-lo a criar uma an√°lise de dados explorat√≥ria? O <code>SmartEDA</code> inclui v√°rias fun√ß√µes personalizadas para executar uma an√°lise explorat√≥ria inicial em qualquer dado de entrada. A sa√≠da gerada pode ser obtida em formato resumido e gr√°fico e os resultados tamb√©m podem ser exportados como relat√≥rios.</p>
<p>O pacote SmartEDA ajuda a construir uma boa base de compreens√£o de dados, algumas de suas funcionalidades s√£o:</p>
<ul>
<li>O pacote SmartEDA far√° com que voc√™ seja capaz de aplicar diferentes tipos de EDA sem ter que lembre-se dos diferentes nomes dos pacotes R e escrever longos scripts R com esfor√ßo manual para preparar o relat√≥rio da EDA, permitindo o entendimento dos dados de maneira mais r√°pida</li>
<li>N√£o h√° necessidade de categorizar as vari√°veis em caractere, num√©rico, fator etc. As fun√ß√µes do SmartEDA categorizam automaticamente todos os recursos no tipo de dados correto (caractere, num√©rico, fator etc.) com base nos dados de entrada.</li>
</ul>
<p>O pacote SmartEDA ajuda a obter a an√°lise completa dos dados explorat√≥rios apenas executando a fun√ß√£o em vez de escrever um longo c√≥digo r.</p>
<div id="carregando-o-pacote" class="section level2">
<h2>Carregando o pacote:</h2>
<pre class="r"><code># install.packages(&quot;SmartEDA&quot;)
library(&quot;SmartEDA&quot;)</code></pre>
<p>outros pactes que ser√£o utilizados no post (incluindo um script com algumas fun√ß√µes, que estar√° dispon√≠vel no meu github <a href="https://github.com/gomesfellipe/gomesfellipe.github.io/blob/master/post/2018-05-26-smarteademachinelearning/functions.R">neste link</a>).</p>
<pre class="r"><code>library(knitr)        # Para tabelas interativas
library(DT)           # Para tabelas interativas
library(dplyr)        # Para manipulacao de dados
library(plotly)       # Para gerar uma tabela
library(psych)        # para an√°lise fatorial
source(&quot;functions.R&quot;) # script com funcoes customizadas</code></pre>
<div id="base-de-dados-utilizada" class="section level3">
<h3>Base de dados utilizada:</h3>
<div class="col2">
<p>Estava √† procura de uma base de dados para testar as funcionalidades do pacote <code>SmartEAD</code> quando um colega de trabalho me mostrou um artigo chamado <a href="http://people.stern.nyu.edu/wgreene/Lugano2013/Fair-ExtramaritalAffairs.pdf">A Theory of Extramarital Affairs</a>, publicado pela <a href="http://www.jstor.org/publisher/ucpress">The University of Chicago Press</a>. Neste artigo √© desenvolvido um <a href="https://en.wikipedia.org/wiki/Tobit_model">modelo pelo estimador de Tobit</a> que explica a aloca√ß√£o de um tempo do indiv√≠duo entre o trabalho e dois tipos de atividades de lazer: tempo passou com o c√¥njuge e tempo gasto com o amante.</p>
<p>N√£o conhecia o modelo proposto e em uma r√°pida pesquisa no Google notei que alguns dos dados utilizados nesse artigo est√£o dispon√≠veis no pacote <a href="ftp://cran.r-project.org/pub/R/web/packages/AER">AER</a> de Econometria Aplicada com R, que cont√©m fun√ß√µes, conjuntos de dados, exemplos, demonstra√ß√µes e vinhetas para o livro <a href="http://jrsyzx.njau.edu.cn/__local/C/94/F1/35C7CC5EDA214D4AAE7FE2BA0FD_0D3DFF32_3CDD40.pdf?e=.pdf">Applied Econometrics with R</a> e como esses dados j√° foram tratados e est√£o ‚Äúprontos para an√°lise‚Äù, resolvi usar essa amostra pela conveni√™ncia.</p>
<p>Portanto farei aqui uma an√°lise explorat√≥ria e ao final de cada caso (<em>sem vari√°vel reposta</em>, <em>com vari√°vel resposta num√©rica</em> e <em>com vari√°vel resposta bin√°ria</em>), para ter uma breve intui√ß√£o de como se comportam os dados irei primeiro utilizar um <em>algor√≠timo de machine learning n√£o supervisionado</em> para o agrupamento das observa√ß√µes (sem considerar q j√° conhecemos a vari√°vel resposta), depois ajustar um* modelo de regress√£o linear simples* considerando a vari√°vel resposta como num√©rica e por fim o ajuste de um <em>algor√≠timo de machine learning supervisonado de classifica√ß√£o</em> ap√≥s discretizar a vari√°vel resposta.</p>
<p>A base de dados pode ser conferida a seguir:</p>
</div>
<pre class="r"><code>library(AER)
data(Affairs)
Affairs %&gt;% rmarkdown::paged_table()</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["affairs"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["gender"],"name":[2],"type":["fct"],"align":["left"]},{"label":["age"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["yearsmarried"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["children"],"name":[5],"type":["fct"],"align":["left"]},{"label":["religiousness"],"name":[6],"type":["int"],"align":["right"]},{"label":["education"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["occupation"],"name":[8],"type":["int"],"align":["right"]},{"label":["rating"],"name":[9],"type":["int"],"align":["right"]}],"data":[{"1":"0","2":"male","3":"37.0","4":"10.000","5":"no","6":"3","7":"18","8":"7","9":"4","_rn_":"4"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"no","6":"4","7":"14","8":"6","9":"4","_rn_":"5"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"1","7":"12","8":"1","9":"4","_rn_":"11"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"5","7":"18","8":"6","9":"5","_rn_":"16"},{"1":"0","2":"male","3":"22.0","4":"0.750","5":"no","6":"2","7":"17","8":"6","9":"3","_rn_":"23"},{"1":"0","2":"female","3":"32.0","4":"1.500","5":"no","6":"2","7":"17","8":"5","9":"5","_rn_":"29"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"2","7":"12","8":"1","9":"3","_rn_":"44"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"2","7":"14","8":"4","9":"4","_rn_":"45"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"4","7":"16","8":"1","9":"2","_rn_":"47"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"no","6":"4","7":"14","8":"4","9":"5","_rn_":"49"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"2","7":"20","8":"7","9":"2","_rn_":"50"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"4","7":"18","8":"6","9":"4","_rn_":"55"},{"1":"0","2":"male","3":"47.0","4":"15.000","5":"yes","6":"5","7":"17","8":"6","9":"4","_rn_":"64"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"17","8":"5","9":"4","_rn_":"80"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"no","6":"4","7":"14","8":"5","9":"4","_rn_":"86"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"1","7":"17","8":"5","9":"5","_rn_":"93"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"2","7":"18","8":"4","9":"3","_rn_":"108"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"3","7":"16","8":"5","9":"4","_rn_":"114"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"16","8":"5","9":"5","_rn_":"115"},{"1":"0","2":"female","3":"27.0","4":"10.000","5":"yes","6":"2","7":"14","8":"1","9":"5","_rn_":"116"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"16","8":"5","9":"5","_rn_":"123"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"16","8":"5","9":"5","_rn_":"127"},{"1":"0","2":"female","3":"27.0","4":"10.000","5":"yes","6":"4","7":"16","8":"5","9":"4","_rn_":"129"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"3","7":"14","8":"1","9":"5","_rn_":"134"},{"1":"0","2":"male","3":"37.0","4":"4.000","5":"yes","6":"2","7":"20","8":"6","9":"4","_rn_":"137"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"18","8":"5","9":"5","_rn_":"139"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"no","6":"4","7":"16","8":"1","9":"5","_rn_":"147"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"5","7":"20","8":"6","9":"4","_rn_":"151"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"3","7":"16","8":"5","9":"5","_rn_":"153"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"3","7":"17","8":"5","9":"4","_rn_":"155"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"3","_rn_":"162"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"3","7":"16","8":"5","9":"5","_rn_":"163"},{"1":"0","2":"male","3":"27.0","4":"0.417","5":"no","6":"4","7":"17","8":"6","9":"4","_rn_":"165"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"5","7":"14","8":"5","9":"4","_rn_":"168"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"yes","6":"1","7":"18","8":"6","9":"4","_rn_":"170"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"4","7":"16","8":"5","9":"3","_rn_":"172"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"3","7":"12","8":"1","9":"4","_rn_":"184"},{"1":"0","2":"female","3":"22.0","4":"4.000","5":"no","6":"4","7":"17","8":"5","9":"5","_rn_":"187"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"yes","6":"1","7":"14","8":"3","9":"5","_rn_":"192"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"3","7":"16","8":"1","9":"5","_rn_":"194"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"yes","6":"5","7":"20","8":"6","9":"5","_rn_":"210"},{"1":"0","2":"male","3":"52.0","4":"15.000","5":"yes","6":"5","7":"18","8":"6","9":"3","_rn_":"217"},{"1":"0","2":"female","3":"22.0","4":"0.417","5":"no","6":"5","7":"14","8":"1","9":"4","_rn_":"220"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"2","7":"18","8":"6","9":"1","_rn_":"224"},{"1":"0","2":"female","3":"32.0","4":"7.000","5":"yes","6":"5","7":"17","8":"5","9":"3","_rn_":"227"},{"1":"0","2":"male","3":"22.0","4":"4.000","5":"no","6":"3","7":"16","8":"5","9":"5","_rn_":"228"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"18","8":"6","9":"5","_rn_":"239"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"2","7":"18","8":"5","9":"4","_rn_":"241"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"yes","6":"4","7":"16","8":"3","9":"5","_rn_":"245"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"2","7":"20","8":"6","9":"4","_rn_":"249"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"5","7":"14","8":"3","9":"5","_rn_":"262"},{"1":"0","2":"male","3":"32.0","4":"7.000","5":"yes","6":"2","7":"20","8":"6","9":"4","_rn_":"265"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"5","7":"20","8":"6","9":"5","_rn_":"267"},{"1":"0","2":"male","3":"27.0","4":"10.000","5":"yes","6":"4","7":"20","8":"6","9":"4","_rn_":"269"},{"1":"0","2":"male","3":"22.0","4":"4.000","5":"no","6":"1","7":"18","8":"5","9":"5","_rn_":"271"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"14","8":"3","9":"1","_rn_":"277"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"yes","6":"5","7":"16","8":"4","9":"4","_rn_":"290"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"17","8":"1","9":"5","_rn_":"292"},{"1":"0","2":"female","3":"27.0","4":"0.750","5":"no","6":"4","7":"17","8":"5","9":"4","_rn_":"293"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"yes","6":"4","7":"20","8":"6","9":"4","_rn_":"295"},{"1":"0","2":"female","3":"47.0","4":"15.000","5":"yes","6":"5","7":"14","8":"7","9":"2","_rn_":"299"},{"1":"0","2":"male","3":"37.0","4":"10.000","5":"yes","6":"3","7":"20","8":"6","9":"4","_rn_":"320"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"2","7":"16","8":"5","9":"5","_rn_":"321"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"no","6":"2","7":"18","8":"4","9":"5","_rn_":"324"},{"1":"0","2":"male","3":"32.0","4":"7.000","5":"no","6":"4","7":"20","8":"6","9":"4","_rn_":"334"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"2","7":"17","8":"3","9":"5","_rn_":"351"},{"1":"0","2":"male","3":"37.0","4":"10.000","5":"yes","6":"4","7":"20","8":"6","9":"4","_rn_":"355"},{"1":"0","2":"female","3":"47.0","4":"15.000","5":"yes","6":"3","7":"17","8":"6","9":"5","_rn_":"361"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"5","7":"16","8":"5","9":"5","_rn_":"362"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"no","6":"2","7":"16","8":"6","9":"4","_rn_":"366"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"no","6":"3","7":"17","8":"5","9":"5","_rn_":"370"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"5","7":"14","8":"4","9":"5","_rn_":"374"},{"1":"0","2":"female","3":"22.0","4":"0.125","5":"no","6":"2","7":"12","8":"5","9":"5","_rn_":"378"},{"1":"0","2":"male","3":"47.0","4":"15.000","5":"yes","6":"4","7":"14","8":"4","9":"3","_rn_":"381"},{"1":"0","2":"male","3":"32.0","4":"15.000","5":"yes","6":"1","7":"14","8":"5","9":"5","_rn_":"382"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"yes","6":"4","7":"16","8":"5","9":"5","_rn_":"383"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"yes","6":"3","7":"16","8":"5","9":"5","_rn_":"384"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"3","7":"17","8":"6","9":"5","_rn_":"400"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"3","7":"16","8":"5","9":"5","_rn_":"403"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"2","7":"14","8":"7","9":"2","_rn_":"409"},{"1":"0","2":"male","3":"17.5","4":"1.500","5":"yes","6":"3","7":"18","8":"6","9":"5","_rn_":"412"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"5","_rn_":"413"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"2","7":"16","8":"3","9":"4","_rn_":"416"},{"1":"0","2":"male","3":"42.0","4":"4.000","5":"no","6":"4","7":"17","8":"3","9":"3","_rn_":"418"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"yes","6":"4","7":"12","8":"1","9":"5","_rn_":"422"},{"1":"0","2":"female","3":"22.0","4":"0.417","5":"no","6":"1","7":"17","8":"6","9":"4","_rn_":"435"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"4","7":"17","8":"5","9":"5","_rn_":"439"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"no","6":"3","7":"18","8":"5","9":"2","_rn_":"445"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"yes","6":"3","7":"14","8":"1","9":"5","_rn_":"447"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"3","7":"14","8":"1","9":"4","_rn_":"448"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"4","7":"14","8":"3","9":"4","_rn_":"449"},{"1":"0","2":"male","3":"37.0","4":"10.000","5":"yes","6":"2","7":"14","8":"5","9":"3","_rn_":"478"},{"1":"0","2":"male","3":"37.0","4":"10.000","5":"yes","6":"4","7":"16","8":"5","9":"4","_rn_":"482"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"5","7":"20","8":"5","9":"3","_rn_":"486"},{"1":"0","2":"male","3":"27.0","4":"0.417","5":"no","6":"1","7":"16","8":"3","9":"4","_rn_":"489"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"5","7":"14","8":"1","9":"5","_rn_":"490"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"3","7":"16","8":"6","9":"1","_rn_":"491"},{"1":"0","2":"male","3":"37.0","4":"10.000","5":"yes","6":"1","7":"16","8":"6","9":"4","_rn_":"492"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"3","7":"17","8":"5","9":"5","_rn_":"503"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"5","_rn_":"508"},{"1":"0","2":"female","3":"27.0","4":"10.000","5":"yes","6":"5","7":"14","8":"1","9":"5","_rn_":"509"},{"1":"0","2":"male","3":"37.0","4":"10.000","5":"yes","6":"2","7":"18","8":"6","9":"4","_rn_":"512"},{"1":"0","2":"female","3":"22.0","4":"0.125","5":"no","6":"4","7":"12","8":"4","9":"5","_rn_":"515"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"5","7":"20","8":"6","9":"5","_rn_":"517"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"18","8":"6","9":"4","_rn_":"532"},{"1":"0","2":"male","3":"22.0","4":"4.000","5":"yes","6":"4","7":"14","8":"6","9":"4","_rn_":"533"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"yes","6":"4","7":"18","8":"5","9":"4","_rn_":"535"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"4","7":"20","8":"5","9":"4","_rn_":"537"},{"1":"0","2":"male","3":"32.0","4":"15.000","5":"yes","6":"3","7":"14","8":"6","9":"3","_rn_":"538"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"14","8":"5","9":"4","_rn_":"543"},{"1":"0","2":"female","3":"32.0","4":"7.000","5":"yes","6":"4","7":"17","8":"1","9":"5","_rn_":"547"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"17","8":"6","9":"5","_rn_":"550"},{"1":"0","2":"female","3":"32.0","4":"1.500","5":"no","6":"5","7":"18","8":"5","9":"5","_rn_":"558"},{"1":"0","2":"male","3":"42.0","4":"10.000","5":"yes","6":"5","7":"20","8":"7","9":"4","_rn_":"571"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"no","6":"3","7":"16","8":"5","9":"4","_rn_":"578"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"no","6":"4","7":"20","8":"6","9":"5","_rn_":"583"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"14","8":"3","9":"2","_rn_":"586"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"no","6":"5","7":"18","8":"6","9":"4","_rn_":"594"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"4","7":"16","8":"1","9":"5","_rn_":"597"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"12","8":"2","9":"4","_rn_":"602"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"2","7":"16","8":"2","9":"5","_rn_":"603"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"5","7":"18","8":"5","9":"4","_rn_":"604"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"4","7":"17","8":"5","9":"3","_rn_":"612"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"2","7":"16","8":"1","9":"2","_rn_":"613"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"3","7":"16","8":"5","9":"5","_rn_":"621"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"5","7":"20","8":"6","9":"5","_rn_":"627"},{"1":"0","2":"female","3":"22.0","4":"0.125","5":"no","6":"2","7":"14","8":"4","9":"5","_rn_":"630"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"no","6":"4","7":"16","8":"5","9":"5","_rn_":"631"},{"1":"0","2":"male","3":"32.0","4":"1.500","5":"no","6":"2","7":"18","8":"6","9":"5","_rn_":"632"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"no","6":"2","7":"17","8":"6","9":"5","_rn_":"639"},{"1":"0","2":"female","3":"27.0","4":"10.000","5":"yes","6":"4","7":"16","8":"1","9":"3","_rn_":"645"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"4","7":"18","8":"6","9":"5","_rn_":"647"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"no","6":"2","7":"16","8":"6","9":"5","_rn_":"648"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"no","6":"2","7":"18","8":"6","9":"3","_rn_":"651"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"3","7":"14","8":"5","9":"3","_rn_":"655"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"3","7":"18","8":"5","9":"4","_rn_":"667"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"2","7":"18","8":"6","9":"5","_rn_":"670"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"2","7":"16","8":"1","9":"4","_rn_":"671"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"4","7":"20","8":"5","9":"5","_rn_":"673"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"no","6":"1","7":"20","8":"5","9":"4","_rn_":"701"},{"1":"0","2":"female","3":"27.0","4":"10.000","5":"yes","6":"2","7":"12","8":"1","9":"4","_rn_":"705"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"5","7":"18","8":"6","9":"4","_rn_":"706"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"yes","6":"5","7":"12","8":"5","9":"3","_rn_":"709"},{"1":"0","2":"male","3":"52.0","4":"15.000","5":"yes","6":"2","7":"18","8":"5","9":"4","_rn_":"717"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"no","6":"3","7":"20","8":"6","9":"3","_rn_":"719"},{"1":"0","2":"male","3":"37.0","4":"4.000","5":"yes","6":"1","7":"18","8":"5","9":"4","_rn_":"723"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"4","7":"14","8":"5","9":"4","_rn_":"724"},{"1":"0","2":"female","3":"52.0","4":"15.000","5":"yes","6":"5","7":"12","8":"1","9":"3","_rn_":"726"},{"1":"0","2":"female","3":"57.0","4":"15.000","5":"yes","6":"4","7":"16","8":"6","9":"4","_rn_":"734"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"yes","6":"1","7":"16","8":"5","9":"4","_rn_":"735"},{"1":"0","2":"male","3":"37.0","4":"7.000","5":"yes","6":"4","7":"20","8":"6","9":"3","_rn_":"736"},{"1":"0","2":"male","3":"22.0","4":"0.750","5":"no","6":"2","7":"14","8":"4","9":"3","_rn_":"737"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"yes","6":"2","7":"18","8":"5","9":"3","_rn_":"739"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"3","_rn_":"743"},{"1":"0","2":"male","3":"22.0","4":"0.750","5":"yes","6":"2","7":"14","8":"4","9":"3","_rn_":"745"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"3","_rn_":"747"},{"1":"0","2":"female","3":"52.0","4":"15.000","5":"yes","6":"5","7":"17","8":"1","9":"1","_rn_":"751"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"14","8":"1","9":"2","_rn_":"752"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"yes","6":"4","7":"14","8":"5","9":"3","_rn_":"754"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"yes","6":"2","7":"16","8":"5","9":"5","_rn_":"760"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"2","7":"18","8":"6","9":"5","_rn_":"763"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"2","7":"18","8":"5","9":"5","_rn_":"774"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"5","7":"18","8":"6","9":"5","_rn_":"776"},{"1":"0","2":"female","3":"47.0","4":"15.000","5":"yes","6":"5","7":"12","8":"5","9":"4","_rn_":"779"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"3","7":"17","8":"1","9":"4","_rn_":"784"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"yes","6":"4","7":"17","8":"1","9":"2","_rn_":"788"},{"1":"0","2":"female","3":"57.0","4":"15.000","5":"yes","6":"2","7":"18","8":"5","9":"2","_rn_":"794"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"4","7":"14","8":"5","9":"4","_rn_":"795"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"3","7":"14","8":"3","9":"4","_rn_":"798"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"4","7":"9","8":"2","9":"2","_rn_":"800"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"5","_rn_":"803"},{"1":"0","2":"female","3":"22.0","4":"0.125","5":"no","6":"4","7":"14","8":"4","9":"5","_rn_":"807"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"4","7":"14","8":"1","9":"5","_rn_":"812"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"3","7":"18","8":"5","9":"4","_rn_":"820"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"no","6":"2","7":"18","8":"6","9":"5","_rn_":"823"},{"1":"0","2":"male","3":"32.0","4":"0.125","5":"yes","6":"2","7":"18","8":"5","9":"2","_rn_":"830"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"no","6":"3","7":"16","8":"5","9":"4","_rn_":"843"},{"1":"0","2":"female","3":"27.0","4":"10.000","5":"yes","6":"2","7":"16","8":"1","9":"4","_rn_":"848"},{"1":"0","2":"female","3":"32.0","4":"7.000","5":"yes","6":"4","7":"16","8":"1","9":"3","_rn_":"851"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"14","8":"5","9":"4","_rn_":"854"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"5","7":"17","8":"6","9":"2","_rn_":"856"},{"1":"0","2":"male","3":"32.0","4":"1.500","5":"yes","6":"4","7":"14","8":"6","9":"5","_rn_":"857"},{"1":"0","2":"female","3":"32.0","4":"4.000","5":"yes","6":"3","7":"17","8":"5","9":"3","_rn_":"859"},{"1":"0","2":"female","3":"37.0","4":"7.000","5":"no","6":"4","7":"18","8":"5","9":"5","_rn_":"863"},{"1":"0","2":"female","3":"22.0","4":"0.417","5":"yes","6":"3","7":"14","8":"3","9":"5","_rn_":"865"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"14","8":"1","9":"5","_rn_":"867"},{"1":"0","2":"male","3":"27.0","4":"0.750","5":"no","6":"3","7":"16","8":"5","9":"5","_rn_":"870"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"2","7":"20","8":"5","9":"5","_rn_":"873"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"yes","6":"4","7":"16","8":"4","9":"5","_rn_":"875"},{"1":"0","2":"male","3":"32.0","4":"15.000","5":"yes","6":"1","7":"14","8":"5","9":"5","_rn_":"876"},{"1":"0","2":"male","3":"22.0","4":"0.750","5":"no","6":"3","7":"17","8":"4","9":"5","_rn_":"877"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"17","8":"1","9":"4","_rn_":"880"},{"1":"0","2":"male","3":"27.0","4":"0.417","5":"yes","6":"4","7":"20","8":"5","9":"4","_rn_":"903"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"20","8":"5","9":"4","_rn_":"904"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"2","7":"14","8":"1","9":"3","_rn_":"905"},{"1":"0","2":"male","3":"22.0","4":"4.000","5":"yes","6":"1","7":"18","8":"5","9":"4","_rn_":"908"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"17","8":"5","9":"3","_rn_":"909"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"14","8":"4","9":"5","_rn_":"910"},{"1":"0","2":"male","3":"52.0","4":"15.000","5":"yes","6":"4","7":"14","8":"6","9":"2","_rn_":"912"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"4","7":"17","8":"5","9":"5","_rn_":"914"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"yes","6":"5","7":"14","8":"3","9":"5","_rn_":"915"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"yes","6":"2","7":"14","8":"3","9":"5","_rn_":"916"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"3","7":"16","8":"6","9":"5","_rn_":"920"},{"1":"0","2":"male","3":"27.0","4":"0.750","5":"no","6":"2","7":"18","8":"3","9":"3","_rn_":"921"},{"1":"0","2":"female","3":"22.0","4":"7.000","5":"yes","6":"2","7":"14","8":"5","9":"2","_rn_":"925"},{"1":"0","2":"female","3":"27.0","4":"0.750","5":"no","6":"2","7":"17","8":"5","9":"3","_rn_":"926"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"12","8":"1","9":"2","_rn_":"929"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"1","7":"14","8":"1","9":"5","_rn_":"931"},{"1":"0","2":"female","3":"37.0","4":"10.000","5":"no","6":"2","7":"12","8":"4","9":"4","_rn_":"945"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"18","8":"5","9":"3","_rn_":"947"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"3","7":"12","8":"3","9":"3","_rn_":"949"},{"1":"0","2":"male","3":"22.0","4":"4.000","5":"no","6":"2","7":"18","8":"5","9":"5","_rn_":"950"},{"1":"0","2":"male","3":"52.0","4":"7.000","5":"yes","6":"2","7":"20","8":"6","9":"2","_rn_":"961"},{"1":"0","2":"male","3":"27.0","4":"0.750","5":"no","6":"2","7":"17","8":"5","9":"5","_rn_":"965"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"no","6":"2","7":"17","8":"4","9":"5","_rn_":"966"},{"1":"0","2":"male","3":"42.0","4":"1.500","5":"no","6":"5","7":"20","8":"6","9":"5","_rn_":"967"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"no","6":"4","7":"17","8":"6","9":"5","_rn_":"987"},{"1":"0","2":"male","3":"22.0","4":"4.000","5":"no","6":"4","7":"17","8":"5","9":"3","_rn_":"990"},{"1":"0","2":"female","3":"22.0","4":"4.000","5":"yes","6":"1","7":"14","8":"5","9":"4","_rn_":"992"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"5","7":"20","8":"4","9":"5","_rn_":"995"},{"1":"0","2":"female","3":"37.0","4":"10.000","5":"yes","6":"3","7":"16","8":"6","9":"3","_rn_":"1009"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"4","7":"17","8":"6","9":"5","_rn_":"1021"},{"1":"0","2":"female","3":"47.0","4":"15.000","5":"yes","6":"4","7":"17","8":"5","9":"5","_rn_":"1026"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"no","6":"4","7":"16","8":"5","9":"4","_rn_":"1027"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"3","7":"12","8":"1","9":"4","_rn_":"1030"},{"1":"0","2":"female","3":"22.0","4":"7.000","5":"yes","6":"1","7":"14","8":"3","9":"5","_rn_":"1031"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"4","7":"17","8":"5","9":"4","_rn_":"1034"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"yes","6":"2","7":"16","8":"2","9":"4","_rn_":"1037"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"14","8":"5","9":"5","_rn_":"1038"},{"1":"0","2":"male","3":"42.0","4":"4.000","5":"yes","6":"3","7":"14","8":"4","9":"5","_rn_":"1039"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"5","7":"14","8":"5","9":"4","_rn_":"1045"},{"1":"0","2":"female","3":"32.0","4":"7.000","5":"yes","6":"4","7":"17","8":"5","9":"5","_rn_":"1046"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"4","7":"18","8":"6","9":"5","_rn_":"1054"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"no","6":"4","7":"18","8":"6","9":"4","_rn_":"1059"},{"1":"0","2":"male","3":"22.0","4":"0.750","5":"no","6":"4","7":"18","8":"6","9":"5","_rn_":"1063"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"4","7":"14","8":"5","9":"3","_rn_":"1068"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"5","7":"18","8":"1","9":"5","_rn_":"1070"},{"1":"0","2":"female","3":"52.0","4":"15.000","5":"yes","6":"5","7":"9","8":"5","9":"5","_rn_":"1072"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"yes","6":"3","7":"14","8":"5","9":"5","_rn_":"1073"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"16","8":"4","9":"4","_rn_":"1077"},{"1":"0","2":"male","3":"32.0","4":"7.000","5":"yes","6":"2","7":"20","8":"5","9":"4","_rn_":"1081"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"3","7":"18","8":"1","9":"4","_rn_":"1083"},{"1":"0","2":"male","3":"32.0","4":"15.000","5":"yes","6":"1","7":"16","8":"5","9":"5","_rn_":"1084"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"3","7":"18","8":"5","9":"5","_rn_":"1086"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"4","7":"12","8":"3","9":"4","_rn_":"1087"},{"1":"0","2":"male","3":"22.0","4":"0.750","5":"yes","6":"3","7":"14","8":"2","9":"4","_rn_":"1089"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"3","7":"16","8":"5","9":"3","_rn_":"1096"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"4","7":"14","8":"3","9":"5","_rn_":"1102"},{"1":"0","2":"female","3":"52.0","4":"15.000","5":"yes","6":"3","7":"16","8":"5","9":"4","_rn_":"1103"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"5","7":"20","8":"6","9":"4","_rn_":"1107"},{"1":"0","2":"female","3":"47.0","4":"15.000","5":"yes","6":"4","7":"12","8":"2","9":"3","_rn_":"1109"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"2","7":"20","8":"6","9":"4","_rn_":"1115"},{"1":"0","2":"male","3":"32.0","4":"7.000","5":"yes","6":"4","7":"17","8":"5","9":"5","_rn_":"1119"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"17","8":"1","9":"4","_rn_":"1124"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"no","6":"1","7":"18","8":"6","9":"5","_rn_":"1126"},{"1":"0","2":"female","3":"22.0","4":"4.000","5":"yes","6":"3","7":"9","8":"1","9":"4","_rn_":"1128"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"14","8":"1","9":"5","_rn_":"1129"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"2","7":"20","8":"6","9":"4","_rn_":"1130"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"4","7":"9","8":"2","9":"4","_rn_":"1133"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"2","7":"18","8":"1","9":"5","_rn_":"1140"},{"1":"0","2":"female","3":"22.0","4":"4.000","5":"yes","6":"3","7":"14","8":"1","9":"5","_rn_":"1143"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"14","8":"5","9":"3","_rn_":"1146"},{"1":"0","2":"male","3":"32.0","4":"7.000","5":"yes","6":"1","7":"18","8":"6","9":"4","_rn_":"1153"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"14","8":"5","9":"5","_rn_":"1156"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"yes","6":"3","7":"12","8":"1","9":"3","_rn_":"1157"},{"1":"0","2":"male","3":"52.0","4":"15.000","5":"yes","6":"2","7":"14","8":"5","9":"5","_rn_":"1158"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"2","7":"14","8":"1","9":"1","_rn_":"1160"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"2","7":"14","8":"5","9":"5","_rn_":"1161"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"4","7":"20","8":"4","9":"5","_rn_":"1166"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"3","7":"18","8":"4","9":"5","_rn_":"1177"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"5","_rn_":"1178"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"no","6":"3","7":"18","8":"5","9":"5","_rn_":"1180"},{"1":"0","2":"female","3":"22.0","4":"0.125","5":"no","6":"2","7":"16","8":"6","9":"3","_rn_":"1187"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"yes","6":"2","7":"20","8":"6","9":"3","_rn_":"1191"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"no","6":"4","7":"18","8":"5","9":"4","_rn_":"1195"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"2","7":"12","8":"5","9":"1","_rn_":"1207"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"yes","6":"5","7":"18","8":"6","9":"3","_rn_":"1208"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"2","7":"17","8":"5","9":"5","_rn_":"1209"},{"1":"0","2":"male","3":"47.0","4":"15.000","5":"no","6":"4","7":"20","8":"6","9":"4","_rn_":"1211"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"no","6":"1","7":"18","8":"5","9":"5","_rn_":"1215"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"4","_rn_":"1221"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"4","7":"18","8":"1","9":"4","_rn_":"1226"},{"1":"0","2":"female","3":"32.0","4":"7.000","5":"yes","6":"4","7":"17","8":"5","9":"4","_rn_":"1229"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"3","7":"14","8":"1","9":"3","_rn_":"1231"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"3","7":"16","8":"1","9":"4","_rn_":"1234"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"no","6":"3","7":"16","8":"4","9":"2","_rn_":"1235"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"no","6":"3","7":"16","8":"3","9":"5","_rn_":"1242"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"3","7":"16","8":"4","9":"2","_rn_":"1245"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"3","7":"12","8":"1","9":"2","_rn_":"1260"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"2","7":"18","8":"5","9":"4","_rn_":"1266"},{"1":"0","2":"female","3":"37.0","4":"7.000","5":"yes","6":"3","7":"14","8":"4","9":"4","_rn_":"1271"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"no","6":"2","7":"16","8":"5","9":"5","_rn_":"1273"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"5","7":"20","8":"5","9":"4","_rn_":"1276"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"4","7":"16","8":"5","9":"3","_rn_":"1280"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"4","7":"16","8":"1","9":"5","_rn_":"1282"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"no","6":"2","7":"17","8":"5","9":"3","_rn_":"1285"},{"1":"0","2":"female","3":"22.0","4":"0.417","5":"no","6":"4","7":"14","8":"5","9":"5","_rn_":"1295"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"no","6":"2","7":"18","8":"5","9":"5","_rn_":"1298"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"18","8":"5","9":"3","_rn_":"1299"},{"1":"0","2":"male","3":"37.0","4":"10.000","5":"yes","6":"5","7":"20","8":"7","9":"4","_rn_":"1304"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"2","7":"14","8":"4","9":"2","_rn_":"1305"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"yes","6":"2","7":"16","8":"5","9":"5","_rn_":"1311"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"yes","6":"2","7":"16","8":"6","9":"4","_rn_":"1314"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"no","6":"3","7":"18","8":"4","9":"5","_rn_":"1319"},{"1":"0","2":"female","3":"22.0","4":"4.000","5":"yes","6":"4","7":"14","8":"3","9":"4","_rn_":"1322"},{"1":"0","2":"female","3":"17.5","4":"0.750","5":"no","6":"2","7":"18","8":"5","9":"4","_rn_":"1324"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"yes","6":"4","7":"20","8":"4","9":"5","_rn_":"1327"},{"1":"0","2":"female","3":"32.0","4":"0.750","5":"no","6":"5","7":"14","8":"3","9":"3","_rn_":"1328"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"17","8":"5","9":"3","_rn_":"1330"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"no","6":"3","7":"14","8":"4","9":"5","_rn_":"1332"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"no","6":"2","7":"17","8":"3","9":"2","_rn_":"1333"},{"1":"0","2":"female","3":"22.0","4":"7.000","5":"yes","6":"4","7":"14","8":"1","9":"5","_rn_":"1336"},{"1":"0","2":"male","3":"47.0","4":"15.000","5":"yes","6":"5","7":"14","8":"6","9":"5","_rn_":"1341"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"1","7":"16","8":"4","9":"4","_rn_":"1344"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"5","7":"14","8":"1","9":"3","_rn_":"1352"},{"1":"0","2":"male","3":"42.0","4":"4.000","5":"yes","6":"4","7":"18","8":"5","9":"5","_rn_":"1358"},{"1":"0","2":"female","3":"32.0","4":"4.000","5":"yes","6":"2","7":"14","8":"1","9":"5","_rn_":"1359"},{"1":"0","2":"male","3":"52.0","4":"15.000","5":"yes","6":"2","7":"14","8":"7","9":"4","_rn_":"1361"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"16","8":"1","9":"4","_rn_":"1364"},{"1":"0","2":"male","3":"52.0","4":"15.000","5":"yes","6":"4","7":"12","8":"2","9":"4","_rn_":"1368"},{"1":"0","2":"female","3":"22.0","4":"0.417","5":"no","6":"3","7":"17","8":"1","9":"5","_rn_":"1384"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"16","8":"5","9":"5","_rn_":"1390"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"4","7":"20","8":"6","9":"4","_rn_":"1393"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"4","7":"14","8":"1","9":"5","_rn_":"1394"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"no","6":"2","7":"16","8":"3","9":"5","_rn_":"1402"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"no","6":"1","7":"20","8":"6","9":"5","_rn_":"1407"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"3","7":"20","8":"6","9":"4","_rn_":"1408"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"no","6":"2","7":"16","8":"6","9":"5","_rn_":"1412"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"5","7":"14","8":"5","9":"5","_rn_":"1413"},{"1":"0","2":"male","3":"37.0","4":"1.500","5":"yes","6":"4","7":"18","8":"5","9":"3","_rn_":"1416"},{"1":"0","2":"male","3":"32.0","4":"1.500","5":"no","6":"2","7":"18","8":"4","9":"4","_rn_":"1417"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"4","7":"14","8":"1","9":"4","_rn_":"1418"},{"1":"0","2":"female","3":"47.0","4":"15.000","5":"yes","6":"4","7":"18","8":"5","9":"4","_rn_":"1419"},{"1":"0","2":"female","3":"27.0","4":"10.000","5":"yes","6":"5","7":"12","8":"1","9":"5","_rn_":"1420"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"3","7":"16","8":"4","9":"5","_rn_":"1423"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"12","8":"4","9":"2","_rn_":"1424"},{"1":"0","2":"female","3":"27.0","4":"0.750","5":"no","6":"4","7":"16","8":"5","9":"5","_rn_":"1432"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"16","8":"1","9":"5","_rn_":"1433"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"3","7":"16","8":"1","9":"5","_rn_":"1437"},{"1":"0","2":"female","3":"27.0","4":"10.000","5":"yes","6":"2","7":"16","8":"1","9":"5","_rn_":"1438"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"no","6":"2","7":"20","8":"6","9":"5","_rn_":"1439"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"2","7":"14","8":"1","9":"3","_rn_":"1446"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"yes","6":"2","7":"17","8":"4","9":"4","_rn_":"1450"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"yes","6":"2","7":"14","8":"1","9":"5","_rn_":"1451"},{"1":"0","2":"male","3":"22.0","4":"4.000","5":"yes","6":"4","7":"14","8":"2","9":"4","_rn_":"1452"},{"1":"0","2":"male","3":"42.0","4":"0.125","5":"no","6":"4","7":"17","8":"6","9":"4","_rn_":"1453"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"yes","6":"4","7":"18","8":"6","9":"5","_rn_":"1456"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"yes","6":"3","7":"16","8":"6","9":"3","_rn_":"1464"},{"1":"0","2":"female","3":"52.0","4":"15.000","5":"yes","6":"4","7":"14","8":"1","9":"3","_rn_":"1469"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"no","6":"5","7":"20","8":"5","9":"2","_rn_":"1473"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"no","6":"2","7":"16","8":"5","9":"5","_rn_":"1481"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"no","6":"3","7":"17","8":"5","9":"5","_rn_":"1482"},{"1":"0","2":"male","3":"22.0","4":"0.125","5":"no","6":"5","7":"16","8":"4","9":"4","_rn_":"1496"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"4","7":"16","8":"1","9":"5","_rn_":"1497"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"4","7":"12","8":"1","9":"5","_rn_":"1504"},{"1":"0","2":"female","3":"47.0","4":"15.000","5":"yes","6":"2","7":"14","8":"5","9":"5","_rn_":"1513"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"3","7":"14","8":"5","9":"3","_rn_":"1515"},{"1":"0","2":"male","3":"42.0","4":"7.000","5":"yes","6":"2","7":"16","8":"5","9":"5","_rn_":"1534"},{"1":"0","2":"male","3":"22.0","4":"0.750","5":"no","6":"4","7":"16","8":"6","9":"4","_rn_":"1535"},{"1":"0","2":"male","3":"27.0","4":"0.125","5":"no","6":"3","7":"20","8":"6","9":"5","_rn_":"1536"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"yes","6":"3","7":"20","8":"6","9":"5","_rn_":"1540"},{"1":"0","2":"female","3":"22.0","4":"0.417","5":"no","6":"5","7":"14","8":"4","9":"5","_rn_":"1551"},{"1":"0","2":"female","3":"47.0","4":"15.000","5":"yes","6":"5","7":"14","8":"1","9":"4","_rn_":"1555"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"3","7":"14","8":"1","9":"5","_rn_":"1557"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"4","7":"17","8":"5","9":"5","_rn_":"1566"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"3","7":"20","8":"6","9":"5","_rn_":"1567"},{"1":"0","2":"female","3":"32.0","4":"7.000","5":"yes","6":"4","7":"17","8":"1","9":"5","_rn_":"1576"},{"1":"0","2":"female","3":"37.0","4":"10.000","5":"yes","6":"4","7":"16","8":"1","9":"5","_rn_":"1584"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"1","7":"18","8":"1","9":"4","_rn_":"1585"},{"1":"0","2":"female","3":"22.0","4":"4.000","5":"no","6":"3","7":"14","8":"1","9":"4","_rn_":"1590"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"14","8":"3","9":"2","_rn_":"1594"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"5","7":"18","8":"5","9":"2","_rn_":"1595"},{"1":"0","2":"male","3":"32.0","4":"7.000","5":"yes","6":"2","7":"18","8":"5","9":"5","_rn_":"1603"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"no","6":"4","7":"17","8":"1","9":"3","_rn_":"1608"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"no","6":"4","7":"14","8":"5","9":"5","_rn_":"1609"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"yes","6":"4","7":"14","8":"5","9":"4","_rn_":"1615"},{"1":"0","2":"female","3":"32.0","4":"7.000","5":"yes","6":"3","7":"16","8":"1","9":"5","_rn_":"1616"},{"1":"0","2":"female","3":"47.0","4":"15.000","5":"yes","6":"3","7":"16","8":"5","9":"4","_rn_":"1617"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"3","7":"16","8":"1","9":"5","_rn_":"1620"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"yes","6":"2","7":"14","8":"5","9":"5","_rn_":"1621"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"1","7":"16","8":"5","9":"5","_rn_":"1637"},{"1":"0","2":"male","3":"52.0","4":"15.000","5":"yes","6":"4","7":"16","8":"5","9":"5","_rn_":"1638"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"yes","6":"4","7":"20","8":"6","9":"5","_rn_":"1650"},{"1":"0","2":"male","3":"47.0","4":"15.000","5":"yes","6":"4","7":"16","8":"6","9":"4","_rn_":"1654"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"2","7":"14","8":"1","9":"2","_rn_":"1665"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"4","7":"14","8":"4","9":"5","_rn_":"1670"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"2","7":"16","8":"5","9":"4","_rn_":"1671"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"2","7":"16","8":"5","9":"4","_rn_":"1675"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"16","8":"5","9":"5","_rn_":"1688"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"3","7":"18","8":"6","9":"4","_rn_":"1691"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"5","7":"14","8":"4","9":"5","_rn_":"1695"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"4","7":"16","8":"4","9":"4","_rn_":"1698"},{"1":"0","2":"female","3":"57.0","4":"15.000","5":"yes","6":"3","7":"18","8":"5","9":"2","_rn_":"1704"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"3","7":"18","8":"6","9":"2","_rn_":"1705"},{"1":"0","2":"female","3":"32.0","4":"7.000","5":"yes","6":"2","7":"14","8":"1","9":"2","_rn_":"1711"},{"1":"0","2":"male","3":"22.0","4":"4.000","5":"no","6":"5","7":"12","8":"4","9":"5","_rn_":"1719"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"1","7":"16","8":"6","9":"5","_rn_":"1723"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"1","7":"14","8":"4","9":"5","_rn_":"1726"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"4","7":"12","8":"1","9":"5","_rn_":"1749"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"no","6":"2","7":"18","8":"5","9":"3","_rn_":"1752"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"5","7":"17","8":"2","9":"5","_rn_":"1754"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"4","7":"12","8":"1","9":"5","_rn_":"1758"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"5","7":"18","8":"5","9":"4","_rn_":"1761"},{"1":"0","2":"male","3":"32.0","4":"1.500","5":"no","6":"2","7":"20","8":"7","9":"3","_rn_":"1773"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"no","6":"4","7":"9","8":"3","9":"1","_rn_":"1775"},{"1":"0","2":"male","3":"37.0","4":"7.000","5":"no","6":"4","7":"18","8":"5","9":"5","_rn_":"1786"},{"1":"0","2":"male","3":"52.0","4":"15.000","5":"yes","6":"2","7":"17","8":"5","9":"4","_rn_":"1793"},{"1":"0","2":"male","3":"47.0","4":"15.000","5":"yes","6":"4","7":"17","8":"6","9":"5","_rn_":"1799"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"no","6":"2","7":"17","8":"5","9":"4","_rn_":"1803"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"14","8":"5","9":"5","_rn_":"1806"},{"1":"0","2":"female","3":"22.0","4":"4.000","5":"no","6":"2","7":"14","8":"3","9":"3","_rn_":"1807"},{"1":"0","2":"male","3":"37.0","4":"7.000","5":"yes","6":"2","7":"20","8":"6","9":"5","_rn_":"1808"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"no","6":"4","7":"12","8":"4","9":"3","_rn_":"1814"},{"1":"0","2":"male","3":"42.0","4":"10.000","5":"yes","6":"4","7":"18","8":"6","9":"4","_rn_":"1815"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"3","7":"14","8":"1","9":"5","_rn_":"1818"},{"1":"0","2":"female","3":"22.0","4":"4.000","5":"yes","6":"2","7":"14","8":"1","9":"3","_rn_":"1827"},{"1":"0","2":"female","3":"57.0","4":"15.000","5":"no","6":"4","7":"20","8":"6","9":"5","_rn_":"1834"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"14","8":"4","9":"3","_rn_":"1835"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"3","7":"18","8":"5","9":"5","_rn_":"1843"},{"1":"0","2":"female","3":"17.5","4":"10.000","5":"no","6":"4","7":"14","8":"4","9":"5","_rn_":"1846"},{"1":"0","2":"male","3":"22.0","4":"4.000","5":"yes","6":"4","7":"16","8":"5","9":"5","_rn_":"1850"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"2","7":"16","8":"1","9":"4","_rn_":"1851"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"2","7":"14","8":"5","9":"1","_rn_":"1854"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"5","7":"14","8":"1","9":"4","_rn_":"1859"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"yes","6":"2","7":"20","8":"5","9":"4","_rn_":"1861"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"4","7":"14","8":"5","9":"5","_rn_":"1866"},{"1":"0","2":"male","3":"22.0","4":"0.125","5":"no","6":"1","7":"16","8":"3","9":"5","_rn_":"1873"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"14","8":"1","9":"4","_rn_":"1875"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"5","7":"16","8":"5","9":"3","_rn_":"1885"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"yes","6":"4","7":"18","8":"5","9":"4","_rn_":"1892"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"2","7":"14","8":"3","9":"4","_rn_":"1895"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"3","7":"17","8":"5","9":"5","_rn_":"1896"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"4","7":"17","8":"4","9":"4","_rn_":"1897"},{"1":"0","2":"female","3":"52.0","4":"15.000","5":"yes","6":"5","7":"14","8":"1","9":"5","_rn_":"1899"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"2","7":"12","8":"1","9":"2","_rn_":"1904"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"3","7":"12","8":"1","9":"4","_rn_":"1905"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"2","7":"14","8":"1","9":"4","_rn_":"1908"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"4","7":"14","8":"5","9":"4","_rn_":"1916"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"yes","6":"4","7":"14","8":"3","9":"3","_rn_":"1918"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"yes","6":"2","7":"20","8":"6","9":"2","_rn_":"1920"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"3","7":"12","8":"3","9":"3","_rn_":"1930"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"3","7":"16","8":"3","9":"5","_rn_":"1940"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"3","7":"14","8":"1","9":"4","_rn_":"1947"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"14","8":"4","9":"5","_rn_":"1949"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"4","7":"14","8":"1","9":"4","_rn_":"1951"},{"1":"0","2":"female","3":"22.0","4":"4.000","5":"no","6":"4","7":"14","8":"5","9":"5","_rn_":"1952"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"16","8":"4","9":"5","_rn_":"1960"},{"1":"0","2":"male","3":"47.0","4":"15.000","5":"no","6":"4","7":"14","8":"5","9":"4","_rn_":"9001"},{"1":"0","2":"male","3":"37.0","4":"10.000","5":"yes","6":"2","7":"18","8":"6","9":"2","_rn_":"9012"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"3","7":"17","8":"5","9":"4","_rn_":"9023"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"2","7":"16","8":"1","9":"4","_rn_":"9029"},{"1":"3","2":"male","3":"27.0","4":"1.500","5":"no","6":"3","7":"18","8":"4","9":"4","_rn_":"6"},{"1":"3","2":"female","3":"27.0","4":"4.000","5":"yes","6":"3","7":"17","8":"1","9":"5","_rn_":"12"},{"1":"7","2":"male","3":"37.0","4":"15.000","5":"yes","6":"5","7":"18","8":"6","9":"2","_rn_":"43"},{"1":"12","2":"female","3":"32.0","4":"10.000","5":"yes","6":"3","7":"17","8":"5","9":"2","_rn_":"53"},{"1":"1","2":"male","3":"22.0","4":"0.125","5":"no","6":"4","7":"16","8":"5","9":"5","_rn_":"67"},{"1":"1","2":"female","3":"22.0","4":"1.500","5":"yes","6":"2","7":"14","8":"1","9":"5","_rn_":"79"},{"1":"12","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"14","8":"5","9":"2","_rn_":"122"},{"1":"7","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"14","8":"3","9":"4","_rn_":"126"},{"1":"2","2":"male","3":"37.0","4":"15.000","5":"yes","6":"2","7":"18","8":"6","9":"4","_rn_":"133"},{"1":"3","2":"female","3":"32.0","4":"15.000","5":"yes","6":"4","7":"12","8":"3","9":"2","_rn_":"138"},{"1":"1","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"14","8":"4","9":"2","_rn_":"154"},{"1":"7","2":"female","3":"42.0","4":"15.000","5":"yes","6":"3","7":"17","8":"1","9":"4","_rn_":"159"},{"1":"12","2":"female","3":"42.0","4":"15.000","5":"yes","6":"5","7":"9","8":"4","9":"1","_rn_":"174"},{"1":"12","2":"male","3":"37.0","4":"10.000","5":"yes","6":"2","7":"20","8":"6","9":"2","_rn_":"176"},{"1":"12","2":"female","3":"32.0","4":"15.000","5":"yes","6":"3","7":"14","8":"1","9":"2","_rn_":"181"},{"1":"3","2":"male","3":"27.0","4":"4.000","5":"no","6":"1","7":"18","8":"6","9":"5","_rn_":"182"},{"1":"7","2":"male","3":"37.0","4":"10.000","5":"yes","6":"2","7":"18","8":"7","9":"3","_rn_":"186"},{"1":"7","2":"female","3":"27.0","4":"4.000","5":"no","6":"3","7":"17","8":"5","9":"5","_rn_":"189"},{"1":"1","2":"male","3":"42.0","4":"15.000","5":"yes","6":"4","7":"16","8":"5","9":"5","_rn_":"204"},{"1":"1","2":"female","3":"47.0","4":"15.000","5":"yes","6":"5","7":"14","8":"4","9":"5","_rn_":"215"},{"1":"7","2":"female","3":"27.0","4":"4.000","5":"yes","6":"3","7":"18","8":"5","9":"4","_rn_":"232"},{"1":"1","2":"female","3":"27.0","4":"7.000","5":"yes","6":"5","7":"14","8":"1","9":"4","_rn_":"233"},{"1":"12","2":"male","3":"27.0","4":"1.500","5":"yes","6":"3","7":"17","8":"5","9":"4","_rn_":"252"},{"1":"12","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"14","8":"6","9":"2","_rn_":"253"},{"1":"3","2":"female","3":"42.0","4":"15.000","5":"yes","6":"4","7":"16","8":"5","9":"4","_rn_":"274"},{"1":"7","2":"female","3":"27.0","4":"10.000","5":"yes","6":"4","7":"12","8":"7","9":"3","_rn_":"275"},{"1":"1","2":"male","3":"27.0","4":"1.500","5":"no","6":"2","7":"18","8":"5","9":"2","_rn_":"287"},{"1":"1","2":"male","3":"32.0","4":"4.000","5":"no","6":"4","7":"20","8":"6","9":"4","_rn_":"288"},{"1":"1","2":"female","3":"27.0","4":"7.000","5":"yes","6":"3","7":"14","8":"1","9":"3","_rn_":"325"},{"1":"3","2":"female","3":"32.0","4":"10.000","5":"yes","6":"4","7":"14","8":"1","9":"4","_rn_":"328"},{"1":"3","2":"male","3":"27.0","4":"4.000","5":"yes","6":"2","7":"18","8":"7","9":"2","_rn_":"344"},{"1":"1","2":"female","3":"17.5","4":"0.750","5":"no","6":"5","7":"14","8":"4","9":"5","_rn_":"353"},{"1":"1","2":"female","3":"32.0","4":"10.000","5":"yes","6":"4","7":"18","8":"1","9":"5","_rn_":"354"},{"1":"7","2":"female","3":"32.0","4":"7.000","5":"yes","6":"2","7":"17","8":"6","9":"4","_rn_":"367"},{"1":"7","2":"male","3":"37.0","4":"15.000","5":"yes","6":"2","7":"20","8":"6","9":"4","_rn_":"369"},{"1":"7","2":"female","3":"37.0","4":"10.000","5":"no","6":"1","7":"20","8":"5","9":"3","_rn_":"390"},{"1":"12","2":"female","3":"32.0","4":"10.000","5":"yes","6":"2","7":"16","8":"5","9":"5","_rn_":"392"},{"1":"7","2":"male","3":"52.0","4":"15.000","5":"yes","6":"2","7":"20","8":"6","9":"4","_rn_":"423"},{"1":"7","2":"female","3":"42.0","4":"15.000","5":"yes","6":"1","7":"12","8":"1","9":"3","_rn_":"432"},{"1":"1","2":"male","3":"52.0","4":"15.000","5":"yes","6":"2","7":"20","8":"6","9":"3","_rn_":"436"},{"1":"2","2":"male","3":"37.0","4":"15.000","5":"yes","6":"3","7":"18","8":"6","9":"5","_rn_":"483"},{"1":"12","2":"female","3":"22.0","4":"4.000","5":"no","6":"3","7":"12","8":"3","9":"4","_rn_":"513"},{"1":"12","2":"male","3":"27.0","4":"7.000","5":"yes","6":"1","7":"18","8":"6","9":"2","_rn_":"516"},{"1":"1","2":"male","3":"27.0","4":"4.000","5":"yes","6":"3","7":"18","8":"5","9":"5","_rn_":"518"},{"1":"12","2":"male","3":"47.0","4":"15.000","5":"yes","6":"4","7":"17","8":"6","9":"5","_rn_":"520"},{"1":"12","2":"female","3":"42.0","4":"15.000","5":"yes","6":"4","7":"12","8":"1","9":"1","_rn_":"526"},{"1":"7","2":"male","3":"27.0","4":"4.000","5":"no","6":"3","7":"14","8":"3","9":"4","_rn_":"528"},{"1":"7","2":"female","3":"32.0","4":"7.000","5":"yes","6":"4","7":"18","8":"4","9":"5","_rn_":"553"},{"1":"1","2":"male","3":"32.0","4":"0.417","5":"yes","6":"3","7":"12","8":"3","9":"4","_rn_":"576"},{"1":"3","2":"male","3":"47.0","4":"15.000","5":"yes","6":"5","7":"16","8":"5","9":"4","_rn_":"611"},{"1":"12","2":"male","3":"37.0","4":"15.000","5":"yes","6":"2","7":"20","8":"5","9":"4","_rn_":"625"},{"1":"7","2":"male","3":"22.0","4":"4.000","5":"yes","6":"2","7":"17","8":"6","9":"4","_rn_":"635"},{"1":"1","2":"male","3":"27.0","4":"4.000","5":"no","6":"2","7":"14","8":"4","9":"5","_rn_":"646"},{"1":"7","2":"female","3":"52.0","4":"15.000","5":"yes","6":"5","7":"16","8":"1","9":"3","_rn_":"657"},{"1":"1","2":"male","3":"27.0","4":"4.000","5":"no","6":"3","7":"14","8":"3","9":"3","_rn_":"659"},{"1":"1","2":"female","3":"27.0","4":"10.000","5":"yes","6":"4","7":"16","8":"1","9":"4","_rn_":"666"},{"1":"1","2":"male","3":"32.0","4":"7.000","5":"yes","6":"3","7":"14","8":"7","9":"4","_rn_":"679"},{"1":"7","2":"male","3":"32.0","4":"7.000","5":"yes","6":"2","7":"18","8":"4","9":"1","_rn_":"729"},{"1":"3","2":"male","3":"22.0","4":"1.500","5":"no","6":"1","7":"14","8":"3","9":"2","_rn_":"755"},{"1":"7","2":"male","3":"22.0","4":"4.000","5":"yes","6":"3","7":"18","8":"6","9":"4","_rn_":"758"},{"1":"7","2":"male","3":"42.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"4","_rn_":"770"},{"1":"2","2":"female","3":"57.0","4":"15.000","5":"yes","6":"1","7":"18","8":"5","9":"4","_rn_":"786"},{"1":"7","2":"female","3":"32.0","4":"4.000","5":"yes","6":"3","7":"18","8":"5","9":"2","_rn_":"797"},{"1":"1","2":"male","3":"27.0","4":"4.000","5":"yes","6":"1","7":"16","8":"4","9":"4","_rn_":"811"},{"1":"7","2":"male","3":"32.0","4":"7.000","5":"yes","6":"4","7":"16","8":"1","9":"4","_rn_":"834"},{"1":"2","2":"male","3":"57.0","4":"15.000","5":"yes","6":"1","7":"17","8":"4","9":"4","_rn_":"858"},{"1":"7","2":"female","3":"42.0","4":"15.000","5":"yes","6":"4","7":"14","8":"5","9":"2","_rn_":"885"},{"1":"7","2":"male","3":"37.0","4":"10.000","5":"yes","6":"1","7":"18","8":"5","9":"3","_rn_":"893"},{"1":"3","2":"male","3":"42.0","4":"15.000","5":"yes","6":"3","7":"17","8":"6","9":"1","_rn_":"927"},{"1":"1","2":"female","3":"52.0","4":"15.000","5":"yes","6":"3","7":"14","8":"4","9":"4","_rn_":"928"},{"1":"2","2":"female","3":"27.0","4":"7.000","5":"yes","6":"3","7":"17","8":"5","9":"3","_rn_":"933"},{"1":"12","2":"male","3":"32.0","4":"7.000","5":"yes","6":"2","7":"12","8":"4","9":"2","_rn_":"951"},{"1":"1","2":"male","3":"22.0","4":"4.000","5":"no","6":"4","7":"14","8":"2","9":"5","_rn_":"968"},{"1":"3","2":"male","3":"27.0","4":"7.000","5":"yes","6":"3","7":"18","8":"6","9":"4","_rn_":"972"},{"1":"12","2":"female","3":"37.0","4":"15.000","5":"yes","6":"1","7":"18","8":"5","9":"5","_rn_":"975"},{"1":"7","2":"female","3":"32.0","4":"15.000","5":"yes","6":"3","7":"17","8":"1","9":"3","_rn_":"977"},{"1":"7","2":"female","3":"27.0","4":"7.000","5":"no","6":"2","7":"17","8":"5","9":"5","_rn_":"981"},{"1":"1","2":"female","3":"32.0","4":"7.000","5":"yes","6":"3","7":"17","8":"5","9":"3","_rn_":"986"},{"1":"1","2":"male","3":"32.0","4":"1.500","5":"yes","6":"2","7":"14","8":"2","9":"4","_rn_":"1002"},{"1":"12","2":"female","3":"42.0","4":"15.000","5":"yes","6":"4","7":"14","8":"1","9":"2","_rn_":"1007"},{"1":"7","2":"male","3":"32.0","4":"10.000","5":"yes","6":"3","7":"14","8":"5","9":"4","_rn_":"1011"},{"1":"7","2":"male","3":"37.0","4":"4.000","5":"yes","6":"1","7":"20","8":"6","9":"3","_rn_":"1035"},{"1":"1","2":"female","3":"27.0","4":"4.000","5":"yes","6":"2","7":"16","8":"5","9":"3","_rn_":"1050"},{"1":"12","2":"female","3":"42.0","4":"15.000","5":"yes","6":"3","7":"14","8":"4","9":"3","_rn_":"1056"},{"1":"1","2":"male","3":"27.0","4":"10.000","5":"yes","6":"5","7":"20","8":"6","9":"5","_rn_":"1057"},{"1":"12","2":"male","3":"37.0","4":"10.000","5":"yes","6":"2","7":"20","8":"6","9":"2","_rn_":"1075"},{"1":"12","2":"female","3":"27.0","4":"7.000","5":"yes","6":"1","7":"14","8":"3","9":"3","_rn_":"1080"},{"1":"3","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"12","8":"1","9":"2","_rn_":"1125"},{"1":"3","2":"male","3":"32.0","4":"10.000","5":"yes","6":"2","7":"14","8":"4","9":"4","_rn_":"1131"},{"1":"12","2":"female","3":"17.5","4":"0.750","5":"yes","6":"2","7":"12","8":"1","9":"3","_rn_":"1138"},{"1":"12","2":"female","3":"32.0","4":"15.000","5":"yes","6":"3","7":"18","8":"5","9":"4","_rn_":"1150"},{"1":"2","2":"female","3":"22.0","4":"7.000","5":"no","6":"4","7":"14","8":"4","9":"3","_rn_":"1163"},{"1":"1","2":"male","3":"32.0","4":"7.000","5":"yes","6":"4","7":"20","8":"6","9":"5","_rn_":"1169"},{"1":"7","2":"male","3":"27.0","4":"4.000","5":"yes","6":"2","7":"18","8":"6","9":"2","_rn_":"1198"},{"1":"1","2":"female","3":"22.0","4":"1.500","5":"yes","6":"5","7":"14","8":"5","9":"3","_rn_":"1204"},{"1":"12","2":"female","3":"32.0","4":"15.000","5":"no","6":"3","7":"17","8":"5","9":"1","_rn_":"1218"},{"1":"12","2":"female","3":"42.0","4":"15.000","5":"yes","6":"2","7":"12","8":"1","9":"2","_rn_":"1230"},{"1":"7","2":"male","3":"42.0","4":"15.000","5":"yes","6":"3","7":"20","8":"5","9":"4","_rn_":"1236"},{"1":"12","2":"male","3":"32.0","4":"10.000","5":"no","6":"2","7":"18","8":"4","9":"2","_rn_":"1247"},{"1":"12","2":"female","3":"32.0","4":"15.000","5":"yes","6":"3","7":"9","8":"1","9":"1","_rn_":"1259"},{"1":"7","2":"male","3":"57.0","4":"15.000","5":"yes","6":"5","7":"20","8":"4","9":"5","_rn_":"1294"},{"1":"12","2":"male","3":"47.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"4","_rn_":"1353"},{"1":"2","2":"female","3":"42.0","4":"15.000","5":"yes","6":"2","7":"17","8":"6","9":"3","_rn_":"1370"},{"1":"12","2":"male","3":"37.0","4":"15.000","5":"yes","6":"3","7":"17","8":"6","9":"3","_rn_":"1427"},{"1":"12","2":"male","3":"37.0","4":"15.000","5":"yes","6":"5","7":"17","8":"5","9":"2","_rn_":"1445"},{"1":"7","2":"male","3":"27.0","4":"10.000","5":"yes","6":"2","7":"20","8":"6","9":"4","_rn_":"1460"},{"1":"2","2":"male","3":"37.0","4":"15.000","5":"yes","6":"2","7":"16","8":"5","9":"4","_rn_":"1480"},{"1":"12","2":"female","3":"32.0","4":"15.000","5":"yes","6":"1","7":"14","8":"5","9":"2","_rn_":"1505"},{"1":"7","2":"male","3":"32.0","4":"10.000","5":"yes","6":"3","7":"17","8":"6","9":"3","_rn_":"1543"},{"1":"2","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"18","8":"5","9":"1","_rn_":"1548"},{"1":"7","2":"female","3":"27.0","4":"1.500","5":"no","6":"2","7":"17","8":"5","9":"5","_rn_":"1550"},{"1":"3","2":"female","3":"47.0","4":"15.000","5":"yes","6":"2","7":"17","8":"5","9":"2","_rn_":"1561"},{"1":"12","2":"male","3":"37.0","4":"15.000","5":"yes","6":"2","7":"17","8":"5","9":"4","_rn_":"1564"},{"1":"12","2":"female","3":"27.0","4":"4.000","5":"no","6":"2","7":"14","8":"5","9":"5","_rn_":"1573"},{"1":"2","2":"female","3":"27.0","4":"10.000","5":"yes","6":"4","7":"14","8":"1","9":"5","_rn_":"1575"},{"1":"1","2":"female","3":"22.0","4":"4.000","5":"yes","6":"3","7":"16","8":"1","9":"3","_rn_":"1599"},{"1":"12","2":"male","3":"52.0","4":"7.000","5":"no","6":"4","7":"16","8":"5","9":"5","_rn_":"1622"},{"1":"2","2":"female","3":"27.0","4":"4.000","5":"yes","6":"1","7":"16","8":"3","9":"5","_rn_":"1629"},{"1":"7","2":"female","3":"37.0","4":"15.000","5":"yes","6":"2","7":"17","8":"6","9":"4","_rn_":"1664"},{"1":"2","2":"female","3":"27.0","4":"4.000","5":"no","6":"1","7":"17","8":"3","9":"1","_rn_":"1669"},{"1":"12","2":"female","3":"17.5","4":"0.750","5":"yes","6":"2","7":"12","8":"3","9":"5","_rn_":"1674"},{"1":"7","2":"female","3":"32.0","4":"15.000","5":"yes","6":"5","7":"18","8":"5","9":"4","_rn_":"1682"},{"1":"7","2":"female","3":"22.0","4":"4.000","5":"no","6":"1","7":"16","8":"3","9":"5","_rn_":"1685"},{"1":"2","2":"male","3":"32.0","4":"4.000","5":"yes","6":"4","7":"18","8":"6","9":"4","_rn_":"1697"},{"1":"1","2":"female","3":"22.0","4":"1.500","5":"yes","6":"3","7":"18","8":"5","9":"2","_rn_":"1716"},{"1":"3","2":"female","3":"42.0","4":"15.000","5":"yes","6":"2","7":"17","8":"5","9":"4","_rn_":"1730"},{"1":"1","2":"male","3":"32.0","4":"7.000","5":"yes","6":"4","7":"16","8":"4","9":"4","_rn_":"1731"},{"1":"12","2":"male","3":"37.0","4":"15.000","5":"no","6":"3","7":"14","8":"6","9":"2","_rn_":"1732"},{"1":"1","2":"male","3":"42.0","4":"15.000","5":"yes","6":"3","7":"16","8":"6","9":"3","_rn_":"1743"},{"1":"1","2":"male","3":"27.0","4":"4.000","5":"yes","6":"1","7":"18","8":"5","9":"4","_rn_":"1751"},{"1":"2","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"20","8":"7","9":"3","_rn_":"1757"},{"1":"7","2":"male","3":"37.0","4":"15.000","5":"yes","6":"3","7":"20","8":"6","9":"4","_rn_":"1763"},{"1":"3","2":"male","3":"22.0","4":"1.500","5":"no","6":"2","7":"12","8":"3","9":"3","_rn_":"1766"},{"1":"3","2":"male","3":"32.0","4":"4.000","5":"yes","6":"3","7":"20","8":"6","9":"2","_rn_":"1772"},{"1":"2","2":"male","3":"32.0","4":"15.000","5":"yes","6":"5","7":"20","8":"6","9":"5","_rn_":"1776"},{"1":"12","2":"female","3":"52.0","4":"15.000","5":"yes","6":"1","7":"18","8":"5","9":"5","_rn_":"1782"},{"1":"12","2":"male","3":"47.0","4":"15.000","5":"no","6":"1","7":"18","8":"6","9":"5","_rn_":"1784"},{"1":"3","2":"female","3":"32.0","4":"15.000","5":"yes","6":"4","7":"16","8":"4","9":"4","_rn_":"1791"},{"1":"7","2":"female","3":"32.0","4":"15.000","5":"yes","6":"3","7":"14","8":"3","9":"2","_rn_":"1831"},{"1":"7","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"16","8":"1","9":"2","_rn_":"1840"},{"1":"12","2":"male","3":"42.0","4":"15.000","5":"yes","6":"3","7":"18","8":"6","9":"2","_rn_":"1844"},{"1":"7","2":"female","3":"42.0","4":"15.000","5":"yes","6":"2","7":"14","8":"3","9":"2","_rn_":"1856"},{"1":"12","2":"male","3":"27.0","4":"7.000","5":"yes","6":"2","7":"17","8":"5","9":"4","_rn_":"1876"},{"1":"3","2":"male","3":"32.0","4":"10.000","5":"yes","6":"4","7":"14","8":"4","9":"3","_rn_":"1929"},{"1":"7","2":"male","3":"47.0","4":"15.000","5":"yes","6":"3","7":"16","8":"4","9":"2","_rn_":"1935"},{"1":"1","2":"male","3":"22.0","4":"1.500","5":"yes","6":"1","7":"12","8":"2","9":"5","_rn_":"1938"},{"1":"7","2":"female","3":"32.0","4":"10.000","5":"yes","6":"2","7":"18","8":"5","9":"4","_rn_":"1941"},{"1":"2","2":"male","3":"32.0","4":"10.000","5":"yes","6":"2","7":"17","8":"6","9":"5","_rn_":"1954"},{"1":"2","2":"male","3":"22.0","4":"7.000","5":"yes","6":"3","7":"18","8":"6","9":"2","_rn_":"1959"},{"1":"1","2":"female","3":"32.0","4":"15.000","5":"yes","6":"3","7":"14","8":"1","9":"5","_rn_":"9010"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Neste post, a an√°lise de dados ser√° feita considerando a vari√°vel <code>affairs</code> (Quantas vezes envolvido em caso extraconjugal no √∫ltimo ano (aparentemente em 1977)) e a base de dados conta com as vari√°veis g√™nero, idade, anos de casado, se tem crian√ßas, religiosidade, educa√ß√£o, ocupa√ß√£o e como avalia o casamento.</p>
<p>Informa√ß√µes detalhadas podem ser conferidas na tabela a seguir, retirada do artigo apresentado:</p>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/tab.png" /></p>
<p>Obs.: Essa tabela foi feita com o pacote <a href="https://plot.ly/r/"><code>plotly</code></a>, o c√≥digo pode ser conferido <a href="https://gist.github.com/gomesfellipe/4d1d17ca97ac6dadfabad6baef3c5539">aqui</a>.</p>
</div>
</div>
</div>
<div id="vis√£o-geral-dos-dados" class="section level1">
<h1>Vis√£o geral dos dados</h1>
<p>Entendendo as dimens√µes do conjunto de dados, nomes de vari√°veis, resumo geral, vari√°veis ausentes e tipos de dados de cada vari√°vel com a fun√ß√£o <code>ExpData()</code>, se o argumento Type = 1, visualiza√ß√£o dos dados (os nomes das colunas s√£o ‚ÄúDescri√ß√µes‚Äù, ‚ÄúObs.‚Äù), j√° se Type = 2, estrutura dos dados (os nomes das colunas s√£o ‚ÄúS.no‚Äù, ‚ÄúVarName‚Äù, ‚ÄúVarClass‚Äù, ‚ÄúVarType‚Äù)
:</p>
<pre class="r"><code># Visao geral dos dados - Type = 1
ExpData(data=Affairs, type=1) # O tipo 1 √© uma vis√£o geral dos dados</code></pre>
<pre><code>##                                           Descriptions    Value
## 1                                   Sample size (nrow)      601
## 2                              No. of variables (ncol)        9
## 3                    No. of numeric/interger variables        7
## 4                              No. of factor variables        2
## 5                                No. of text variables        0
## 6                             No. of logical variables        0
## 7                          No. of identifier variables        0
## 8                                No. of date variables        0
## 9             No. of zero variance variables (uniform)        0
## 10               %. of variables having complete cases 100% (9)
## 11   %. of variables having &gt;0% and &lt;50% missing cases   0% (0)
## 12 %. of variables having &gt;=50% and &lt;90% missing cases   0% (0)
## 13          %. of variables having &gt;=90% missing cases   0% (0)</code></pre>
<p>Conferindo o nome das vari√°veis e os tipos de cada uma:</p>
<pre class="r"><code># Estrutura dos dados - Type = 2
ExpData(data=Affairs, type=2) # O tipo 2 √© a estrutura dos dados</code></pre>
<pre><code>##   Index Variable_Name Variable_Type Per_of_Missing No_of_distinct_values
## 1     1       affairs       numeric              0                     6
## 2     2        gender        factor              0                     2
## 3     3           age       numeric              0                     9
## 4     4  yearsmarried       numeric              0                     8
## 5     5      children        factor              0                     2
## 6     6 religiousness       integer              0                     5
## 7     7     education       numeric              0                     7
## 8     8    occupation       integer              0                     7
## 9     9        rating       integer              0                     5</code></pre>
<p>Esta fun√ß√£o fornece vis√£o geral e estrutura dos quadros de dados.</p>
</div>
<div id="an√°lise-explorat√≥ria-dos-dados" class="section level1">
<h1>An√°lise explorat√≥ria dos dados</h1>
<p>As fun√ß√µes a seguir apresentam a sa√≠da EDA para 3 casos diferentes de an√°lise explorat√≥ria dos dados, s√£o elas:</p>
<ul>
<li><p>A vari√°vel de destino n√£o est√° definida</p></li>
<li><p>A vari√°vel alvo √© cont√≠nua</p></li>
<li><p>A vari√°vel de destino √© categ√≥rica</p></li>
</ul>
<p>Para fins ilustrativos, ser√° feita inicialmente uma an√°lise considerando que n√£o existe vari√°vel resposta, em seguida ser√° considerada a vari√°vel <code>affairs</code> como vari√°vel resposta e por fim, ser√° feita uma transforma√ß√£o nesta vari√°vel resposta num√©rica de forma que ela seja discretizada da seguinte maneira:</p>
<p><span class="math display">\[
1 \text{ se j√° houve caso extraconjugal} \\
0 \text{ se n√£o houve caso extraconjugal}
\]</span></p>
</div>
<div id="relat√≥rio-em-uma-linha" class="section level1">
<h1>Relat√≥rio em uma linha</h1>
<p>Caso o interesse seja apenas ter uma no√ß√£o geral dos dados de forma extremamente r√°pida, basta rodar a linha de c√≥digo abaixo:</p>
<pre><code>ExpReport(Affairs,op_file = &quot;teste.html&quot;)</code></pre>
<p>Antes de come√ßar a explanar cada um dos casos, achei que seria legal frisar que al√©m de tudo que ser√° apresentado, existe a op√ß√£o de se obter um relat√≥rio extenso sobre a an√°lise explorat√≥ria dos dados em apenas uma linha!</p>
<div id="exemplo-para-o-caso-1-a-vari√°vel-de-destino-n√£o-est√°-definida" class="section level2">
<h2>Exemplo para o caso 1: a vari√°vel de destino n√£o est√° definida</h2>
<p>Para ilustrar o primeiro caso, onde a vari√°vel destino n√£o √© definida, vamos supor que n√£o existe uma vari√°vel alvo na nossa base de dados e estamos interessados em simplesmente obter uma vis√£o geral enquanto pensamos em quais t√©cnicas estat√≠sticas ser√£o utilizadas para avaliar nosso dataset.</p>
<div id="resumo-das-vari√°veis-num√©ricas" class="section level3">
<h3>Resumo das vari√°veis num√©ricas</h3>
<p>Resumo de de todas as vari√°veis num√©ricas:</p>
<pre class="r"><code>ExpNumStat (Affairs, 
            by = &quot;A&quot;,       # Agrupar por A (estat√≠sticas resumidas por Todos), G (estat√≠sticas resumidas por grupo), GA (estat√≠sticas resumidas por grupo e Geral)
            gp = NULL,      # vari√°vel de destino, se houver, padr√£o NULL
            MesofShape = 2, # Medidas de formas (assimetria e curtose).
            Outlier = TRUE, # Calcular o limite inferior, o limite superior e o n√∫mero de outliers
            round = 2)      # Arredondar</code></pre>
<pre><code>##   Vname Group
## 1     1   All</code></pre>
<p>Podemos ver que n√£o existem vari√°veis negativas e a √∫nica vari√°vel que apresentou ‚Äúzero‚Äù foi a vari√°vel resposta. Nenhum registro como <code>Inf</code> ou como <code>NA</code> e al√©m das medidas descritivas tamb√©m podemos notar as medidas de <code>skweness</code> e <code>kurtosis</code>. Alguns coment√°rios sobre essas medidas:</p>
<p>Medidas de forma para dar uma avalia√ß√£o detalhada dos dados. Explica a quantidade e a dire√ß√£o do desvio.</p>
<ul>
<li><strong>Kurotsis</strong> explica o qu√£o alto e afiado √© o pico central (Achatamento).</li>
<li><strong>Skewness</strong> n√£o tem unidades: mas um n√∫mero, como um escore z (medida da assimetria)</li>
</ul>
<p>Onde:</p>
<p><a href="https://pt.wikipedia.org/wiki/Curtose"><strong>Kurtose</strong></a>:</p>
<p>A curtose √© uma medida de forma que caracteriza o achatamento da curva da fun√ß√£o de distribui√ß√£o de probabilidade, Assim:</p>
<ul>
<li>Se o valor da curtose for = 0 (ou 3, pela segunda defini√ß√£o), ent√£o tem o mesmo achatamento que a distribui√ß√£o normal. Chama-se a estas fun√ß√µes de mesoc√∫rticas</li>
<li>Se o valor √© &gt; 0 (ou &gt; 3), ent√£o a distribui√ß√£o em quest√£o √© mais alta (afunilada) e concentrada que a distribui√ß√£o normal. Diz-se que esta fun√ß√£o probabilidade √© leptoc√∫rtica, ou que a distribui√ß√£o tem caudas pesadas (o significado √© que √© relativamente f√°cil obter valores que n√£o se aproximam da m√©dia a v√°rios m√∫ltiplos do desvio padr√£o)</li>
<li>Se o valor √© &lt; 0 (ou &lt; 3), ent√£o a fun√ß√£o de distribui√ß√£o √© mais ‚Äúachatada‚Äù que a distribui√ß√£o normal. Chama-se-lhe platic√∫rtica</li>
</ul>
<p><a href="https://pt.wikipedia.org/wiki/Obliquidade"><strong>Skewness</strong></a>:</p>
<p>O Skewness mede a assimetria das caudas da distribui√ß√£o. Distribui√ß√µes assim√©tricas que tem uma cauda mais ‚Äúpesada‚Äù que a outra apresentam obliquidade. Distribui√ß√µes sim√©tricas tem obliquidade zero. Assim:</p>
<ul>
<li>Se v&gt;0, ent√£o a distribui√ß√£o tem uma cauda direita (valores acima da m√©dia) mais pesada</li>
<li>Se v&lt;0, ent√£o a distribui√ß√£o tem uma cauda esquerda (valores abaixo da m√©dia) mais pesada</li>
<li>Se v=0, ent√£o a distribui√ß√£o √© aproximadamente sim√©trica (na terceira pot√™ncia do desvio em rela√ß√£o √† m√©dia).</li>
</ul>
<div id="distribui√ß√µes-de-vari√°veis-num√©ricas" class="section level4">
<h4>Distribui√ß√µes de vari√°veis num√©ricas</h4>
<p>Representa√ß√£o gr√°fica de todos os recursos num√©ricos com <strong>gr√°fico de densidade</strong> (uni variada):</p>
<pre class="r"><code># Nota: Vari√°vel exclu√≠da (se o valor √∫nico da vari√°vel for menor ou igual a 10 [im = 10])

ExpNumViz(Affairs,
          Page=c(2,2), # padr√£o de sa√≠da. 
          sample=NULL) # sele√ß√£o aleat√≥ria de plots</code></pre>
<pre><code>## $`0`</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-9-1.png" width="672" /><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-9-2.png" width="672" /></p>
<p>Exibidos os gr√°ficos com as densidades das vari√°veis num√©ricas. Como podemos ver a maioria da amostra n√£o registrou caso extraconjugal, a maioria tem de 12 ou mais anos de casado. A m√©dia amostral da idade dos indiv√≠duos √© de aproximadamente 32 anos apresentando leve assimetria com cauda a direita. As demais vari√°veis podem ser conferidas visualmente.</p>
</div>
</div>
<div id="resumo-de-vari√°veis-categ√≥ricas" class="section level3">
<h3>Resumo de vari√°veis categ√≥ricas</h3>
<p>Essa fun√ß√£o selecionar√° automaticamente vari√°veis categ√≥ricas e gerar√° frequ√™ncia ou tabelas cruzadas com base nas entradas do usu√°rio. A sa√≠da inclui contagens, porcentagens, total de linhas e total de colunas.</p>
<p>Frequ√™ncia para todas as vari√°veis independentes categ√≥ricas:</p>
<pre class="r"><code>ExpCTable(Affairs,
          Target=NULL)</code></pre>
<pre><code>##         Variable  Valid Frequency Percent CumPercent
## 1         gender female       315   52.41      52.41
## 2         gender   male       286   47.59     100.00
## 3         gender  TOTAL       601      NA         NA
## 4       children     no       171   28.45      28.45
## 5       children    yes       430   71.55     100.00
## 6       children  TOTAL       601      NA         NA
## 7        affairs      0       451   75.04      75.04
## 8        affairs      1        34    5.66      80.70
## 9        affairs     12        38    6.32      87.02
## 10       affairs      2        17    2.83      89.85
## 11       affairs      3        19    3.16      93.01
## 12       affairs      7        42    6.99     100.00
## 13       affairs  TOTAL       601      NA         NA
## 14           age   17.5         6    1.00       1.00
## 15           age     22       117   19.47      20.47
## 16           age     27       153   25.46      45.93
## 17           age     32       115   19.13      65.06
## 18           age     37        88   14.64      79.70
## 19           age     42        56    9.32      89.02
## 20           age     47        23    3.83      92.85
## 21           age     52        21    3.49      96.34
## 22           age     57        22    3.66     100.00
## 23           age  TOTAL       601      NA         NA
## 24  yearsmarried  0.125        11    1.83       1.83
## 25  yearsmarried  0.417        10    1.66       3.49
## 26  yearsmarried   0.75        31    5.16       8.65
## 27  yearsmarried    1.5        88   14.64      23.29
## 28  yearsmarried     10        70   11.65      34.94
## 29  yearsmarried     15       204   33.94      68.88
## 30  yearsmarried      4       105   17.47      86.35
## 31  yearsmarried      7        82   13.64      99.99
## 32  yearsmarried  TOTAL       601      NA         NA
## 33 religiousness      1        48    7.99       7.99
## 34 religiousness      2       164   27.29      35.28
## 35 religiousness      3       129   21.46      56.74
## 36 religiousness      4       190   31.61      88.35
## 37 religiousness      5        70   11.65     100.00
## 38 religiousness  TOTAL       601      NA         NA
## 39     education     12        44    7.32       7.32
## 40     education     14       154   25.62      32.94
## 41     education     16       115   19.13      52.07
## 42     education     17        89   14.81      66.88
## 43     education     18       112   18.64      85.52
## 44     education     20        80   13.31      98.83
## 45     education      9         7    1.16      99.99
## 46     education  TOTAL       601      NA         NA
## 47    occupation      1       113   18.80      18.80
## 48    occupation      2        13    2.16      20.96
## 49    occupation      3        47    7.82      28.78
## 50    occupation      4        68   11.31      40.09
## 51    occupation      5       204   33.94      74.03
## 52    occupation      6       143   23.79      97.82
## 53    occupation      7        13    2.16      99.98
## 54    occupation  TOTAL       601      NA         NA
## 55        rating      1        16    2.66       2.66
## 56        rating      2        66   10.98      13.64
## 57        rating      3        93   15.47      29.11
## 58        rating      4       194   32.28      61.39
## 59        rating      5       232   38.60      99.99
## 60        rating  TOTAL       601      NA         NA</code></pre>
<p>Obs.: <code>NA</code> significa <code>Not Applicable</code></p>
</div>
<div id="distribui√ß√µes-de-vari√°veis-categ√≥ricas" class="section level3">
<h3>Distribui√ß√µes de vari√°veis categ√≥ricas</h3>
<p>Essa fun√ß√£o varre automaticamente cada vari√°vel e cria um gr√°fico de barras para vari√°veis categ√≥ricas.</p>
<p>Gr√°ficos de barra para todas as vari√°veis categ√≥ricas</p>
<pre class="r"><code>ExpCatViz(Affairs,
          fname=NULL, # Nome do arquivo de saida, default √© pdf
          clim=10,# categorias m√°ximas a incluir nos gr√°ficos de barras.
          margin=2,# √≠ndice, 1 para propor√ß√µes baseadas em linha e 2 para propor√ß√µes baseadas em colunas
          Page = c(2,1), # padrao de saida
          sample=4) # sele√ß√£o aleat√≥ria de plot</code></pre>
<pre><code>## $`0`</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-11-1.png" width="672" /><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
</div>
<div id="machine-lerning-usando-algor√≠timo-n√£o-supervisionado-de-agrupamento" class="section level3">
<h3>Machine Lerning usando algor√≠timo n√£o supervisionado de agrupamento</h3>
<p>Apenas para efeitos ilustrativos, como estamos supondo que n√£o temos a vari√°vel resposta vou remover a coluna <code>affairs</code> do data set e considerarei apenas as vari√°veis num√©ricas para fazer uma an√°lise multivariada com o algor√≠timo de machine learning <a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/kmeans.html"><code>kmeans</code></a>.</p>
<p>A fun√ß√£o <a href="https://github.com/gomesfellipe/functions/blob/master/plot_kmeans.R"><code>plot_kmeans()</code></a> pode ser encontrada em <a href="github.com/gomesfellipe">meu github</a> no <a href="https://github.com/gomesfellipe/functions">reposit√≥rio aberto de fun√ß√µes</a>.</p>
<p>Vejamos os resultados:</p>
<pre class="r"><code>plot_kmeans(Affairs[,-c(1)] %&gt;% select_if(is.numeric) , 2)</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Como podemos observar, foram detectados dois grupos no conjunto de dados. O ideal agora seria fazer uma AED desses clusters identificados e avaliar qual o comportamento dos grupos formados mas como essa vari√°vel foi omitida e a seguir discutiremos a avalia√ß√£o da base diante de da vari√°vel resposta, deixo essas an√°lises aos curiosos de plant√£o.</p>
<p>Mais informa√ß√µes sobre an√°lise multivariava podem ser encontrada no meu post sobre <a href="https://gomesfellipe.github.io/post/2018-01-01-analise-multivariada-em-r/an%C3%A1lise-multivariada-em-r/">An√°lise Multivariada com r</a> e tamb√©m em um <a href="https://www.kaggle.com/gomes555/an-lise-multivariada-pca-e-kmeans">kernel que escrevi para a plataforma kaggle</a>.</p>
<p>Al√©m disso disponibilizo uma aplica√ß√£o Shiny que criei a algum tempo para PCA (An√°lise de componentes Principais) e tarefa de machine learning com agrupamento <a href="https://gomesfellipe.shinyapps.io/appPCAkmeans/">nenste link</a>.</p>
</div>
</div>
<div id="exemplo-para-o-caso-2-a-vari√°vel-de-destino-√©-cont√≠nua" class="section level2">
<h2>Exemplo para o caso 2: A vari√°vel de destino √© cont√≠nua</h2>
<p>Agora vamos considerar que estamos diante de um desfecho onde a vari√°vel alvo √© cont√≠nua, para isso ser√° considerada a vari√°vel <code>affairs</code> como vari√°vel alvo.</p>
<div id="resumo-da-vari√°vel-dependente-cont√≠nua" class="section level3">
<h3>Resumo da vari√°vel dependente cont√≠nua</h3>
<p>Descri√ß√£o da vari√°vel affairs:</p>
<pre class="r"><code>summary(Affairs[,&quot;affairs&quot;])</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   0.000   0.000   0.000   1.456   0.000  12.000</code></pre>
</div>
<div id="resumo-das-vari√°veis-num√©ricas-1" class="section level3">
<h3>Resumo das vari√°veis num√©ricas</h3>
<p>Estat√≠sticas de resumo quando a vari√°vel dependente √© cont√≠nua Pre√ßo.</p>
<pre class="r"><code>ExpNumStat(Affairs,
           by=&quot;A&quot;, # Agrupar por A (estat√≠sticas resumidas por Todos), G (estat√≠sticas resumidas por grupo), GA (estat√≠sticas resumidas por grupo e Geral)
           Qnt=seq(0,1,0.1), # padr√£o NULL. Quantis especificados [c (0,25,0,75) encontrar√£o os percentis 25 e 75]
           MesofShape=1, # Medidas de formas (assimetria e curtose)
           Outlier=TRUE, # Calcular limite superior , inferior e numero de outliers
           round=2) # Arredondamento</code></pre>
<pre><code>##   Vname Group
## 1     1   All</code></pre>
<pre class="r"><code>#Se a vari√°vel de destino for cont√≠nua, as estat√≠sticas de resumo adicionar√£o a coluna de correla√ß√£o (Correla√ß√£o entre a vari√°vel de destino e todas as vari√°veis independentes)</code></pre>
<div id="distribui√ß√µes-de-vari√°veis-num√©ricas-1" class="section level4">
<h4>Distribui√ß√µes de vari√°veis num√©ricas</h4>
<p>Representa√ß√£o gr√°fica de todas as vari√°veis num√©ricas com gr√°ficos de dispers√£o (bivariada)</p>
<p>Gr√°fico de dispers√£o entre todas as vari√°veis num√©ricas e a vari√°vel de destino affairs. Esta trama ajuda a examinar qu√£o bem uma vari√°vel alvo est√° correlacionada com vari√°veis dependentes.</p>
<p>Vari√°vel dependente √© affairs (cont√≠nuo).</p>
<pre class="r"><code>ExpNumViz(Affairs,
            target=&quot;affairs&quot;, # Variavel alvo
            nlim=4, # a vari√°vel num√©rica com valor exclusivo √© maior que 4
            Page=c(2,2), # formato de saida
            sample=NULL) # selecionado aleatoriamente 8 gr√°ficos de dispers√£o</code></pre>
<pre><code>## $`0`</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-15-1.png" width="672" /><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-15-2.png" width="672" /></p>
</div>
</div>
<div id="resumo-de-vari√°veis-categ√≥ricas-1" class="section level3">
<h3>Resumo de vari√°veis categ√≥ricas</h3>
<p>Resumo de vari√°veis categ√≥ricas de acordo com a frequ√™ncia para todas as vari√°veis independentes categ√≥ricas por Affairs</p>
<pre class="r"><code>##bin=4, descretized 4 categories based on quantiles
ExpCTable(Affairs, Target=&quot;affairs&quot;)</code></pre>
<pre><code>##         VARIABLE CATEGORY affairs:(-0.012,4] affairs:(4,8] affairs:(8,12] TOTAL
## 1         gender   female                273            22             20   315
## 2         gender     male                248            20             18   286
## 3         gender    TOTAL                521            42             38   601
## 4       children       no                157             7              7   171
## 5       children      yes                364            35             31   430
## 6       children    TOTAL                521            42             38   601
## 7        affairs        0                451             0              0   451
## 8        affairs        1                 34             0              0    34
## 9        affairs       12                  0             0             38    38
## 10       affairs        2                 17             0              0    17
## 11       affairs        3                 19             0              0    19
## 12       affairs        7                  0            42              0    42
## 13       affairs    TOTAL                521            42             38   601
## 14           age     17.5                  4             0              2     6
## 15           age       22                112             4              1   117
## 16           age       27                138             9              6   153
## 17           age       32                 95            11              9   115
## 18           age       37                 71             8              9    88
## 19           age       42                 44             6              6    56
## 20           age       47                 19             1              3    23
## 21           age       52                 17             2              2    21
## 22           age       57                 21             1              0    22
## 23           age    TOTAL                521            42             38   601
## 24  yearsmarried    0.125                 11             0              0    11
## 25  yearsmarried    0.417                 10             0              0    10
## 26  yearsmarried     0.75                 29             0              2    31
## 27  yearsmarried      1.5                 85             2              1    88
## 28  yearsmarried       10                 57             8              5    70
## 29  yearsmarried       15                165            17             22   204
## 30  yearsmarried        4                 94             9              2   105
## 31  yearsmarried        7                 70             6              6    82
## 32  yearsmarried    TOTAL                521            42             38   601
## 33 religiousness        1                 37             5              6    48
## 34 religiousness        2                138            14             12   164
## 35 religiousness        3                105            13             11   129
## 36 religiousness        4                177             6              7   190
## 37 religiousness        5                 64             4              2    70
## 38 religiousness    TOTAL                521            42             38   601
## 39     education       12                 36             2              6    44
## 40     education       14                139             6              9   154
## 41     education       16                108             5              2   115
## 42     education       17                 72             9              8    89
## 43     education       18                 94            11              7   112
## 44     education       20                 67             9              4    80
## 45     education        9                  5             0              2     7
## 46     education    TOTAL                521            42             38   601
## 47    occupation        1                101             6              6   113
## 48    occupation        2                 13             0              0    13
## 49    occupation        3                 39             5              3    47
## 50    occupation        4                 60             4              4    68
## 51    occupation        5                177            12             15   204
## 52    occupation        6                120            13             10   143
## 53    occupation        7                 11             2              0    13
## 54    occupation    TOTAL                521            42             38   601
## 55        rating        1                 11             1              4    16
## 56        rating        2                 43             8             15    66
## 57        rating        3                 80             9              4    93
## 58        rating        4                169            18              7   194
## 59        rating        5                218             6              8   232
## 60        rating    TOTAL                521            42             38   601</code></pre>
<div id="distribui√ß√µes-de-vari√°veis-categ√≥ricas-1" class="section level4">
<h4>Distribui√ß√µes de vari√°veis categ√≥ricas</h4>
<p>Essa fun√ß√£o varre automaticamente cada vari√°vel e cria um gr√°fico de barras para vari√°veis categ√≥ricas.</p>
<p>Gr√°ficos de barra para todas as vari√°veis categ√≥ricas</p>
<pre class="r"><code>ExpCatViz(Affairs,
          target=&quot;affairs&quot;, # Variavel target
          fname=NULL, # Nome do arquivo de saida, default √© pdf
          clim=10,# categorias m√°ximas a incluir nos gr√°ficos de barras.
          margin=2,# √≠ndice, 1 para propor√ß√µes baseadas em linha e 2 para propor√ß√µes baseadas em colunas
          Page = c(2,1), # padrao de saida
          sample=4) # sele√ß√£o aleat√≥ria de plot</code></pre>
<pre><code>## $`0`</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-17-1.png" width="672" /><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-17-2.png" width="672" /></p>
</div>
</div>
<div id="avaliando-a-correla√ß√£o-entre-as-vari√°veis" class="section level3">
<h3>Avaliando a correla√ß√£o entre as vari√°veis</h3>
<pre class="r"><code>library(ggplot2)
library(dplyr)
library(GGally)
data(&quot;Affairs&quot;)
#Correla√ßoes cruzadas
Affairs%&gt;%
  select(age:rating,affairs)%&gt;%
ggpairs(lower = list(continuous = my_fn,combo=wrap(&quot;facethist&quot;, binwidth=1), 
                                       continuous=wrap(my_bin, binwidth=0.25)),aes(fill=affairs))+theme_bw()</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<pre class="r"><code>ggcorr(Affairs,label = T,nbreaks = 5,label_round = 4)</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
</div>
<div id="modelo-de-regress√£o-linear-usando-stepwiseaic" class="section level3">
<h3>Modelo de regress√£o linear usando stepwiseAIC</h3>
<p>Por fim, vamos ajustar um modelo de regress√£o linear para entender quais s√£o as vari√°veis significativas para explicar a varia√ß√£o da vari√°vel resposta e qual o efeito de cada uma dessas vari√°veis explicativas no nosso desfecho.</p>
<p>Com o R base √© poss√≠vel ajustar um modelo de regress√£o linear simples utilizando a fun√ß√£o <code>lm()</code> e em seguida usar a fun√ß√£o <code>step()</code> para utilizar t√©cnicas como <a href="https://en.wikipedia.org/wiki/Stepwise_regression">stepwise</a>, por√©m como quero utilizar tamb√©m a t√©cnica de <a href="https://pt.wikipedia.org/wiki/Valida%C3%A7%C3%A3o_cruzada">valida√ß√£o cruzada</a>. Para isso vou utilizar o pacote <a href="https://cran.r-project.org/web/packages/caret/caret.pdf"><code>caret</code></a>, muito famoso por facilitar o ajuste de modelos de machine learning (ou mesmo modelos estat√≠sticos tradicionais).</p>
<p>Al√©m disso estou usando as transforma√ß√µes <a href="https://www.rdocumentation.org/packages/caret/versions/6.0-79/topics/preProcess"><code>center()</code></a>, que subtrai a m√©dia dos dados e <a href="https://www.rdocumentation.org/packages/caret/versions/6.0-79/topics/preProcess"><code>scale()</code></a> divide pelo desvio padr√£o.</p>
<pre class="r"><code>data(&quot;Affairs&quot;)
library(caret)
set.seed(123)
index &lt;- sample(1:2,nrow(Affairs),replace=T,prob=c(0.8,0.2))
train = Affairs[index==1,] %&gt;%as.data.frame()
test = Affairs[index==2,] %&gt;%as.data.frame()

# Setando os par√¢metros para o controle do ajuste do modelo:
fitControl &lt;- trainControl(method = &quot;repeatedcv&quot;,         # 10fold cross validation
                     number = 10, repeats=5                         # do 5 repititi√ß√µes of cv
                     )

# Regress√£o Linear com Stepwise
set.seed(825)
lmFit &lt;- train(affairs ~ ., data = train,
                method = &quot;lmStepAIC&quot;, 
                trControl = fitControl,
                preProc = c(&quot;center&quot;, &quot;scale&quot;),trace=F)
summary(lmFit)</code></pre>
<pre><code>## 
## Call:
## lm(formula = .outcome ~ age + yearsmarried + religiousness + 
##     occupation + rating, data = dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.1452 -1.7819 -0.7601  0.2719 11.3518 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     1.4146     0.1401  10.096  &lt; 2e-16 ***
## age            -0.6890     0.2291  -3.007 0.002779 ** 
## yearsmarried    1.1058     0.2302   4.804 2.09e-06 ***
## religiousness  -0.5121     0.1455  -3.519 0.000475 ***
## occupation      0.3858     0.1445   2.669 0.007858 ** 
## rating         -0.7830     0.1470  -5.326 1.55e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.07 on 474 degrees of freedom
## Multiple R-squared:  0.144,  Adjusted R-squared:  0.135 
## F-statistic: 15.95 on 5 and 474 DF,  p-value: 1.542e-14</code></pre>
<p>Como podemos ver as vari√°veis Idade, Anos de casado, religiosidade, ocupa√ß√£o e como avaliam o pr√≥prio relacionamento se apresentaram significantes</p>
<p>Como o <span class="math inline">\(R^2=0,144\)</span>, conclui-se que <span class="math inline">\(14,4%\)</span> da varia√ß√£o da quantidade de vezes que foi envolvida em caso extraconjugal no √∫ltimo ano √© explicada pelo modelo ajustado.</p>
<p>Observando a coluna das estimativas, podemos notar o quanto varia a quantidade de vezes que foi envolvido em caso extraconjugal ao aumentar em 1 unidade cada uma das vari√°veis explicativas.</p>
<p>Al√©m disso o valor p obtido atrav√©s da estat√≠stica F foi menor do que <span class="math inline">\(\alpha = 0.05\)</span>, o que implica que pelo menos uma das vari√°veis explicativas tem rela√ß√£o significativa com a vari√°vel resposta.</p>
<p>Selecionando apenas as vari√°veis selecionadas com o ajuste do modelo:</p>
<pre class="r"><code>train=as.data.frame(train[,c(1,3,4,6,8,9)])
test=as.data.frame(test[,c(1,3,4,6,8,9)])</code></pre>
<div id="diagn√≥stico-do-modelo" class="section level4">
<h4>Diagn√≥stico do modelo</h4>
<p>Existem varias formas e t√©cnicas de se avaliar o ajuste de um modelo e como o foco deste post √© apresentar as utilidades do pacote <code>SmartEAD</code> irei fazer uma avalia√ß√£o muito breve sobre os res√≠duos, apresento mais algumas maneiras no post sobre <a href="https://gomesfellipe.github.io/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos/">pacotes do R para avaliar o ajuste de modelos</a>.</p>
<div id="avaliando-residuos" class="section level5">
<h5>Avaliando residuos</h5>
<pre class="r"><code>library(GGally)
# calculate all residuals prior to display
residuals &lt;- lapply(train[2:ncol(train)], function(x) {
  summary(lm(affairs ~ x, data = train))$residuals
})

# add a &#39;fake&#39; column
train$Residual &lt;- seq_len(nrow(train))

# calculate a consistent y range for all residuals
y_range &lt;- range(unlist(residuals))

# plot the data
ggduo(
  train,
  2:6, c(1,7),
  types = list(continuous = lm_or_resid)
)+ theme_bw()</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<pre class="r"><code>train=train%&gt;%
  select(-Residual)</code></pre>
<p>Neste gr√°fico √© poss√≠vel observar como se comportam os ajustes de modelos lineares de cada vari√°vel explicativa em rela√ß√£o √† vari√°vel resposta e al√©m disso na segunda linha √© poss√≠vel notar o comportamento dos res√≠duos no modelo.</p>
<p>Uma das suposi√ß√µes do ajuste de um modelo linear normal √© de que <span class="math inline">\(\epsilon \sim N(0,\sigma^2)\)</span> e visualmente parece que essa condi√ß√£o n√£o deve ser atendida, pois esperar√≠amos algo como uma ‚Äúnuvem‚Äù aleat√≥ria de pontos em torno de zero.</p>
</div>
<div id="residuos-e-medidas-de-influencia" class="section level5">
<h5>Residuos e medidas de influencia</h5>
<p>Al√©m da suposi√ß√£o da normalidade dos res√≠duos, existem ainda mais detalhes do comportamento desses erros, uma breve apresenta√ß√£o no gr√°fico a seguir:</p>
<pre class="r"><code>library(ggfortify)

autoplot(lmFit$finalModel, which = 1:6, data = train,
         colour = &#39;affairs&#39;, label.size = 3,
         ncol = 3)+theme_classic()</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>Pelo que parece no gr√°fico com t√≠tulo ‚ÄúNormal Q-Q‚Äù, as vari√°veis associadas √† vari√°vel resposta com valores acima de 6 se comportam de forma inesperadas quando comparadas com os quantis te√≥ricos.</p>
</div>
</div>
</div>
</div>
<div id="exemplo-para-o-caso-3-a-vari√°vel-de-destino-√©-categ√≥rica" class="section level2">
<h2>Exemplo para o caso 3: a vari√°vel de destino √© categ√≥rica</h2>
<p>Para finalizar a avalia√ß√£o da base de dados, a Vari√°vel alvo ser√° discretizado de tal forma:</p>
<p><span class="math display">\[
1 = \text{se affairs} &gt; 0\\
0 = c.c.
\]</span></p>
<p>Essa transforma√ß√£o ser√° utilizada apenas com fins ilustrativos do algor√≠timo de √°rvore de decis√µes, que est√° ficando muito comum na ci√™ncia de dados como uma tarefa supervisionada de machine learning.</p>
<pre class="r"><code>Affairs = Affairs %&gt;% 
  mutate(daffairs = ifelse(Affairs$affairs!=0,1,0)) %&gt;% 
  mutate(daffairs = as.factor(daffairs))%&gt;% 
  select(-affairs)
levels(Affairs$daffairs) = c(&quot;N√£o&quot;, &quot;Sim&quot;)</code></pre>
<div id="resumo-das-vari√°veis-num√©ricas-2" class="section level3">
<h3>Resumo das vari√°veis num√©ricas</h3>
<p>Resumo de todas as vari√°veis num√©ricas</p>
<pre class="r"><code>ExpNumStat(Affairs,
           by=&quot;A&quot;, # Agrupar por A (estat√≠sticas resumidas por Todos), G (estat√≠sticas resumidas por grupo), GA (estat√≠sticas resumidas por grupo e Geral)
           gp=&quot;daffairs&quot;, # Variavel alvo
           Qnt=seq(0,1,0.1), # padr√£o NULL. Quantis especificados [c (0,25,0,75) encontrar√£o os percentis 25 e 75]
           MesofShape=1, # Medidas de formas (assimetria e curtose)
           Outlier=TRUE, # Calcular limite superior , inferior e numero de outliers
           round=2) # Arredondamento</code></pre>
<pre><code>##   Vname Group
## 1     1   All</code></pre>
<div id="distribui√ß√µes-de-vari√°veis-num√©ricas-2" class="section level4">
<h4>Distribui√ß√µes de vari√°veis num√©ricas</h4>
<p>Box plots para todas as vari√°veis num√©ricas vs vari√°vel dependente categ√≥rica - Compara√ß√£o bivariada apenas com categorias</p>
<p>Boxplot para todos os atributos num√©ricos por cada categoria de affair</p>
<pre class="r"><code>ExpNumViz(Affairs, target=&quot;daffairs&quot;) # amostra de variaveis para o resumo</code></pre>
<pre><code>## [[1]]</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<pre><code>## 
## [[2]]</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-26-2.png" width="672" /></p>
<pre><code>## 
## [[3]]</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-26-3.png" width="672" /></p>
<pre><code>## 
## [[4]]</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-26-4.png" width="672" /></p>
<pre><code>## 
## [[5]]</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-26-5.png" width="672" /></p>
<pre><code>## 
## [[6]]</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-26-6.png" width="672" /></p>
</div>
</div>
<div id="resumo-das-vari√°veis-categ√≥ricas" class="section level3">
<h3>Resumo das vari√°veis categ√≥ricas</h3>
<p>Tabula√ß√£o cruzada com vari√°vel de destino com tabelas customizadas entre todas as vari√°veis independentes categ√≥ricas e a vari√°vel de destino <code>daffairs</code>:</p>
<pre class="r"><code>ExpCTable(Affairs,
          Target=&quot;daffairs&quot;, # variavel alvo
          margin=1, # 1 para proporcoes por linha, 2 para colunas
          clim=10, # maximo de categorias consideradas por frequencia/ custom table
          round=2, # arredondar
          per=F) # valores percentuais. Tabela padr√£o dar√° contagens.</code></pre>
<pre><code>##         VARIABLE CATEGORY daffairs:N√£o daffairs:Sim TOTAL
## 1         gender   female          243           72   315
## 2         gender     male          208           78   286
## 3         gender    TOTAL          451          150   601
## 4       children       no          144           27   171
## 5       children      yes          307          123   430
## 6       children    TOTAL          451          150   601
## 7            age     17.5            3            3     6
## 8            age       22          101           16   117
## 9            age       27          117           36   153
## 10           age       32           77           38   115
## 11           age       37           65           23    88
## 12           age       42           38           18    56
## 13           age       47           16            7    23
## 14           age       52           15            6    21
## 15           age       57           19            3    22
## 16           age    TOTAL          451          150   601
## 17  yearsmarried    0.125           10            1    11
## 18  yearsmarried    0.417            9            1    10
## 19  yearsmarried     0.75           28            3    31
## 20  yearsmarried      1.5           76           12    88
## 21  yearsmarried       10           49           21    70
## 22  yearsmarried       15          142           62   204
## 23  yearsmarried        4           78           27   105
## 24  yearsmarried        7           59           23    82
## 25  yearsmarried    TOTAL          451          150   601
## 26 religiousness        1           28           20    48
## 27 religiousness        2          123           41   164
## 28 religiousness        3           86           43   129
## 29 religiousness        4          157           33   190
## 30 religiousness        5           57           13    70
## 31 religiousness    TOTAL          451          150   601
## 32     education       12           31           13    44
## 33     education       14          119           35   154
## 34     education       16           95           20   115
## 35     education       17           62           27    89
## 36     education       18           79           33   112
## 37     education       20           60           20    80
## 38     education        9            5            2     7
## 39     education    TOTAL          451          150   601
## 40    occupation        1           90           23   113
## 41    occupation        2           10            3    13
## 42    occupation        3           32           15    47
## 43    occupation        4           47           21    68
## 44    occupation        5          160           44   204
## 45    occupation        6          104           39   143
## 46    occupation        7            8            5    13
## 47    occupation    TOTAL          451          150   601
## 48        rating        1            8            8    16
## 49        rating        2           33           33    66
## 50        rating        3           66           27    93
## 51        rating        4          146           48   194
## 52        rating        5          198           34   232
## 53        rating    TOTAL          451          150   601</code></pre>
<div id="distribui√ß√µes-de-vari√°veis-categ√≥ricas-2" class="section level4">
<h4>Distribui√ß√µes de vari√°veis categ√≥ricas</h4>
<p>Gr√°fico de barras empilhadas com barras verticais ou horizontais para todas as vari√°veis categ√≥ricas</p>
<pre class="r"><code>ExpCatViz(Affairs,
          target=&quot;daffairs&quot;,
          fname=NULL, # Nome do arquivo de saida, default √© pdf
          clim=10,# categorias m√°ximas a incluir nos gr√°ficos de barras.
          margin=2,# √≠ndice, 1 para propor√ß√µes baseadas em linha e 2 para propor√ß√µes baseadas em colunas
          Page = c(2,1), # padrao de saida
          sample=4) # sele√ß√£o aleat√≥ria de plot</code></pre>
<pre><code>## $`0`</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-28-1.png" width="672" /><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-28-2.png" width="672" /></p>
</div>
</div>
<div id="valor-da-informa√ß√£o" class="section level3">
<h3>Valor da informa√ß√£o</h3>
<p><code>IV</code> √© o peso da evid√™ncia e valores da informa√ß√£o, <span class="math inline">\(ln(odss) \times(pct0 - pct1)\)</span> onde <span class="math inline">\(pct1 =\frac{\text{&quot;boas observa√ß√µes&quot;}}{\text{&quot;total boas observa√ß√µes&quot;}}\)</span>; <span class="math inline">\(pct0 = \frac{&quot;\text{observa√ß√µes ruins&quot;} }{ \text{&quot;total de observa√ß√µes ruins&quot;}}\)</span> e $odds =  $</p>
<pre class="r"><code>ExpCatStat(Affairs %&gt;% mutate(daffairs = if_else(daffairs==&quot;N√£o&quot;, 0, 1)) ,
           Target=&quot;daffairs&quot;,
           result = &quot;IV&quot;) %&gt;% 
  select(-one_of(&quot;Target&quot;,&quot;Ref_1&quot;,&quot;Ref_0&quot;))</code></pre>
<pre><code>##           Variable  Class Out_1 Out_0 TOTAL Per_1 Per_0 Odds   WOE   IV
## 1         gender.1 female    72   243   315  0.48  0.54 0.79 -0.12 0.01
## 2         gender.2   male    78   208   286  0.52  0.46 1.27  0.12 0.01
## 3       children.1     no    27   144   171  0.18  0.32 0.47 -0.58 0.08
## 4       children.2    yes   123   307   430  0.82  0.68 2.14  0.19 0.03
## 5            age.1   17.5     3     3     6  0.02  0.01 3.05  0.69 0.01
## 6            age.2     22    16   101   117  0.11  0.22 0.41 -0.69 0.08
## 7            age.3     27    36   117   153  0.24  0.26 0.90 -0.08 0.00
## 8            age.4     32    38    77   115  0.25  0.17 1.65  0.39 0.03
## 9            age.5     37    23    65    88  0.15  0.14 1.08  0.07 0.00
## 10           age.6     42    18    38    56  0.12  0.08 1.48  0.41 0.02
## 11           age.7     47     7    16    23  0.05  0.04 1.33  0.22 0.00
## 12           age.8     52     6    15    21  0.04  0.03 1.21  0.29 0.00
## 13           age.9     57     3    19    22  0.02  0.04 0.46 -0.69 0.01
## 14  yearsmarried.1  0.125     1    10    11  0.01  0.02 0.30 -0.69 0.01
## 15  yearsmarried.2  0.417     1     9    10  0.01  0.02 0.33 -0.69 0.01
## 16  yearsmarried.3   0.75     3    28    31  0.02  0.06 0.31 -1.11 0.04
## 17  yearsmarried.4    1.5    12    76    88  0.08  0.17 0.43 -0.76 0.07
## 18  yearsmarried.5     10    21    49    70  0.14  0.11 1.34  0.24 0.01
## 19  yearsmarried.6     15    62   142   204  0.41  0.31 1.53  0.28 0.03
## 20  yearsmarried.7      4    27    78   105  0.18  0.17 1.05  0.06 0.00
## 21  yearsmarried.8      7    23    59    82  0.15  0.13 1.20  0.14 0.00
## 22 religiousness.1      1    20    28    48  0.13  0.06 2.32  0.77 0.05
## 23 religiousness.2      2    41   123   164  0.27  0.27 1.00  0.00 0.00
## 24 religiousness.3      3    43    86   129  0.29  0.19 1.71  0.43 0.04
## 25 religiousness.4      4    33   157   190  0.22  0.35 0.53 -0.46 0.06
## 26 religiousness.5      5    13    57    70  0.09  0.13 0.66 -0.37 0.01
## 27     education.1     12    13    31    44  0.09  0.07 1.29  0.25 0.00
## 28     education.2     14    35   119   154  0.23  0.26 0.85 -0.13 0.00
## 29     education.3     16    20    95   115  0.13  0.21 0.58 -0.48 0.04
## 30     education.4     17    27    62    89  0.18  0.14 1.38  0.25 0.01
## 31     education.5     18    33    79   112  0.22  0.18 1.33  0.20 0.01
## 32     education.6     20    20    60    80  0.13  0.13 1.00  0.00 0.00
## 33     education.7      9     2     5     7  0.01  0.01 1.21  0.00 0.00
## 34    occupation.1      1    23    90   113  0.15  0.20 0.73 -0.29 0.01
## 35    occupation.2      2     3    10    13  0.02  0.02 0.90  0.00 0.00
## 36    occupation.3      3    15    32    47  0.10  0.07 1.45  0.36 0.01
## 37    occupation.4      4    21    47    68  0.14  0.10 1.40  0.34 0.01
## 38    occupation.5      5    44   160   204  0.29  0.35 0.75 -0.19 0.01
## 39    occupation.6      6    39   104   143  0.26  0.23 1.17  0.12 0.00
## 40    occupation.7      7     5     8    13  0.03  0.02 1.91  0.41 0.00
## 41        rating.1      1     8     8    16  0.05  0.02 3.12  0.92 0.03
## 42        rating.2      2    33    33    66  0.22  0.07 3.57  1.14 0.17
## 43        rating.3      3    27    66    93  0.18  0.15 1.28  0.18 0.01
## 44        rating.4      4    48   146   194  0.32  0.32 0.98  0.00 0.00
## 45        rating.5      5    34   198   232  0.23  0.44 0.37 -0.65 0.14</code></pre>
</div>
<div id="testes-estat√≠sticos" class="section level3">
<h3>Testes estat√≠sticos</h3>
<p>Al√©m de toda a informa√ß√£o visual e das estat√≠sticas descritivas, ainda contamos com alguma fun√ß√£o que fornece estat√≠sticas resumidas para todas as colunas de caracteres ou categ√≥ricas no data frame</p>
<pre class="r"><code>ExpCatStat(Affairs %&gt;% mutate(daffairs = if_else(daffairs==&quot;N√£o&quot;, 0, 1)),
           Target=&quot;daffairs&quot;, # variavel alvo
           result = &quot;Stat&quot;) # resumo de estatisticas</code></pre>
<pre><code>##        Variable   Target Unique Chi-squared p-value df IV Value Cramers V
## 1        gender daffairs      2       1.334   0.248  1     0.02      0.05
## 2      children daffairs      2      10.055   0.002  1     0.11      0.13
## 3           age daffairs      9      17.771   0.023  8     0.15      0.17
## 4  yearsmarried daffairs      8      17.177   0.016  7     0.17      0.17
## 5 religiousness daffairs      5      19.354   0.001  4     0.16      0.18
## 6     education daffairs      7       7.057   0.316  6     0.06      0.11
## 7    occupation daffairs      7       6.718   0.348  6     0.04      0.11
## 8        rating daffairs      5      41.433   0.000  4     0.35      0.26
##   Degree of Association    Predictive Power
## 1             Very Weak      Not Predictive
## 2                  Weak Somewhat Predictive
## 3                  Weak Somewhat Predictive
## 4                  Weak Somewhat Predictive
## 5                  Weak Somewhat Predictive
## 6                  Weak      Not Predictive
## 7                  Weak      Not Predictive
## 8              Moderate   Highly Predictive</code></pre>
<p>Os crit√©rios usados para classifica√ß√£o de poder preditivo vari√°vel categ√≥rico s√£o</p>
<ul>
<li><p>Se o valor da informa√ß√£o for &lt;0,03, ent√£o, poder de previs√£o = ‚ÄúN√£o Preditivo‚Äù</p></li>
<li><p>Se o valor da informa√ß√£o √© de 0,3 a 0,1, ent√£o o poder preditivo = ‚Äúum pouco preditivo‚Äù</p></li>
<li><p>Se o valor da informa√ß√£o for de 0,1 a 0,3, ent√£o, poder preditivo = ‚ÄúMedium Predictive‚Äù</p></li>
<li><p>Se o valor da informa√ß√£o for&gt; 0.3, ent√£o, poder preditivo = ‚ÄúAltamente Preditivo‚Äù</p></li>
</ul>
<p>Nota para a vari√°vel <code>rating</code> que segundo essas regras, demonstrou alto poder preditivo.</p>
</div>
<div id="machine-learning-com-random-forest" class="section level3">
<h3>Machine Learning com Random Forest</h3>
<p>O algor√≠timo supervisionado de machine learning conhecido como <a href="https://www.stat.berkeley.edu/~breiman/RandomForests/">Random Forest</a> √© uma grande caixa preta. Apresenta resultados muito robustos pois combina o resultado de v√°rias √°rvores de decis√µes e pode ser facilmente aplicada com o pacote <code>caret</code>.</p>
<p><a href="https://topepo.github.io/caret/variable-importance.html">No livro do pacote caret</a> o algor√≠timo √© apresentado da seguinte maneira: ‚Äúsegundo o pacote do R: Para cada √°rvore, a precis√£o da previs√£o na parte fora do saco dos dados √© registrada. Ent√£o, o mesmo √© feito ap√≥s a permuta√ß√£o de cada vari√°vel preditora. A diferen√ßa entre as duas precis√µes √© calculada pela m√©dia de todas as √°rvores e normalizada pelo erro padr√£o. Para a regress√£o, o MSE √© calculado nos dados fora da bolsa para cada √°rvore e, em seguida, o mesmo √© computado ap√≥s a permuta√ß√£o de uma vari√°vel. As diferen√ßas s√£o calculadas e normalizadas pelo erro padr√£o. Se o erro padr√£o √© igual a 0 para uma vari√°vel, a divis√£o n√£o √© feita.‚Äù</p>
<p>N√£o entrarei em muitos detalhes sobre o algor√≠timo pois esta parte √© apenas um demonstrativo dos diferentes cen√°rios de an√°lise explorat√≥ria dos dados. Ser√£o comentadas apenas algumas m√©tricas utilizadas.</p>
<p>Ajuste com o algor√≠timo Random Forest:</p>
<pre class="r"><code>library(caret)
set.seed(1)
index &lt;- sample(1:2,nrow(Affairs),replace=T,prob=c(0.8,0.2))
train = Affairs[index==1,] %&gt;%as.data.frame()
test = Affairs[index==2,] %&gt;%as.data.frame()


# Setando os par√¢metros para o controle do ajuste do modelo:
fitControl &lt;- trainControl(method = &quot;repeatedcv&quot;,         # 10fold cross validation
                     number = 10
                     )

# Random Forest
set.seed(825)
antes = Sys.time()
rfFit &lt;- train(daffairs ~ ., data = train,
                method = &quot;rf&quot;, 
                trControl = fitControl,
                trace=F,
                preProc = c(&quot;center&quot;, &quot;scale&quot;))

antes - Sys.time() # Para saber quanto tempo durou o ajuste</code></pre>
<pre><code>## Time difference of -13.38876 secs</code></pre>
<p>Resultados do ajuste:</p>
<pre class="r"><code>rfFit</code></pre>
<pre><code>## Random Forest 
## 
## 484 samples
##   8 predictor
##   2 classes: &#39;N√£o&#39;, &#39;Sim&#39; 
## 
## Pre-processing: centered (8), scaled (8) 
## Resampling: Cross-Validated (10 fold, repeated 1 times) 
## Summary of sample sizes: 436, 436, 435, 435, 435, 436, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##   2     0.7539966  0.1369868
##   5     0.7292942  0.1727691
##   8     0.7231718  0.1613695
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 2.</code></pre>
<p><strong>Accurary e Kappa</strong></p>
<p>Essas s√£o as m√©tricas padr√£o usadas para avaliar algoritmos em conjuntos de dados de classifica√ß√£o bin√°ria.</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Accuracy_and_precision"><strong>Accuray</strong></a>: √© a porcentagem de classificar corretamente as inst√¢ncias fora de todas as inst√¢ncias. √â mais √∫til em uma classifica√ß√£o bin√°ria do que problemas de classifica√ß√£o de v√°rias classes, porque pode ser menos claro exatamente como a precis√£o √© dividida entre essas classes (por exemplo, voc√™ precisa ir mais fundo com uma matriz de confus√£o).</li>
<li><a href="https://en.wikipedia.org/wiki/Cohen%27s_kappa"><strong>Kappa ou Kappa de Cohen</strong></a> √© como a precis√£o da classifica√ß√£o, exceto que √© normalizado na linha de base da chance aleat√≥ria em seu conjunto de dados. √â uma medida mais √∫til para usar em problemas que t√™m um desequil√≠brio nas classes (por exemplo, divis√£o de 70 a 30 para as classes 0 e 1 e voc√™ pode atingir 70% de precis√£o prevendo que todas as inst√¢ncias s√£o para a classe 0).</li>
</ul>
<p>A seguir a ‚ÄúVariable Importance‚Äù de cada vari√°vel:</p>
<pre class="r"><code>rfImp = varImp(rfFit);rfImp</code></pre>
<pre><code>## rf variable importance
## 
##               Overall
## rating         100.00
## age             94.66
## religiousness   85.77
## education       78.99
## yearsmarried    67.41
## occupation      62.48
## gendermale       6.94
## childrenyes      0.00</code></pre>
<pre class="r"><code>plot(rfImp)</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>A fun√ß√£o dimensiona automaticamente as pontua√ß√µes de import√¢ncia entre 0 e 100, os escores de import√¢ncia da vari√°vel em Random Forest s√£o medidas agregadas. Eles apenas quantificam o impacto do preditor, n√£o o efeito espec√≠fico, para isso utilizamos o ajuste um modelo param√©trico onde conseguimos estimar termos estruturais.</p>
<p>√â claro que existem muitos adentos a serem feitos tanto na forma como os dados foram apresentados no ajuste do modelo linear e no Random Forest, mas como a finalidade do post continua sendo apresentar o pacote SmartEAD, encerrarei a avalia√ß√£o por aqui.</p>
<p>Caso algu√©m queira entender com mais detalhes a avalia√ß√£o de modelos de machine learning, talvez <a href="https://topepo.github.io/caret/measuring-performance.html">o livro do pacote caret</a> seja uma alternativa interessante para ter uma no√ß√£o geral.</p>
<blockquote>
<p><em>Todos os modelos est√£o errados, alguns s√£o √∫teis - George Box</em></p>
</blockquote>
<p>N√£o conseguimos nenhum modelo √∫til que quantificasse as incertezas nas modelagens deste post mas conseguimos executar praticamente todas as fun√ß√µes do pacote <code>SmartEAD</code> e foi muito √∫til para conhecer a base em poucas linhas, obrigado Dayanand Ubrangala, Kiran R. e Ravi Prasad Kondapalli!</p>
</div>
</div>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2018-05-26-smarteademachinelearning/smarteademachinelearning/">AED de forma r√°pida e um pouco de Machine Learning</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Analise Explorat√≥ria</category>
      <category>Data mining</category>
      <category>Estatistica</category>
      <category>Modelagem Estatistica</category>
      <category>Pr√°tica</category>
      <category>R</category>
      <category>Reports</category>
      <category>Machine Learning</category>
      <category>Analise Mutivariada</category>
      <category>Aprendizado Supervisionado</category>
      <category>Aprendizado N√£o Supervisionado</category>
      <category domain="tag">analise multivariada</category>
      <category domain="tag">Correlacoes</category>
      <category domain="tag">Data Mining</category>
      <category domain="tag">Estatistica</category>
      <category domain="tag">gomesfellipe</category>
      <category domain="tag">kmeans</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">pca</category>
      <category domain="tag">R</category>
      <category domain="tag">RStudio</category>
    </item>
  </channel>
</rss>