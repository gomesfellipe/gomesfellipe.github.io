&lt;?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Teoria on Fellipe Gomes - Data Science Blog</title>
    <link>https://gomesfellipe.github.io/tags/teoria/</link>
    <description>Últimos posts sobre Data Science, Machine Learning e R</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>pt-br</language>
    <managingEditor>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</managingEditor>
    <webMaster>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</webMaster>
    <lastBuildDate>Sat, 28 Jul 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://gomesfellipe.github.io/tags/teoria/" rel="self" type="application/rss+xml" />
    <item>
      <title>modelo bayesiano do zero</title>
      <link>https://gomesfellipe.github.io/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero/</link>
      <pubDate>Sat, 28 Jul 2018 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero/</guid>
      <description>Um pouco sobre as duas grandes escolas de inferência, contas e implementação de um modelo linear bayesiano na mão para dados simulados e para dados reais</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/plotly-binding/plotly.js"></script>
<script src="/rmarkdown-libs/typedarray/typedarray.min.js"></script>
<script src="/rmarkdown-libs/jquery/jquery.min.js"></script>
<link href="/rmarkdown-libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="/rmarkdown-libs/crosstalk/js/crosstalk.min.js"></script>
<link href="/rmarkdown-libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="/rmarkdown-libs/plotly-main/plotly-latest.min.js"></script>


<div id="modelagem-estatística-e-as-duas-grandes-escolas-de-inferência" class="section level1">
<h1>Modelagem estatística e as duas grandes escolas de inferência</h1>
<p>Através da modelagem estatística é possível tomar decisões sobre diversos assuntos de interesse como por exemplo na análise de risco de crédito, previsões de quantidade de chuva em um dado local, estimativas de erros ou falhas de um novo produto ou serviço além de diversas áreas como na Educação, Economia, nas Ciências Sociais, Saúde etc.</p>
<p>Muitas vezes os parâmetros das distribuições em estudo podem ser desconhecidos e existe o desejo de se inferir sobre eles. Existem duas grandes escolas de inferência: a clássica e a bayesiana. A clássica trata esses parâmetros como quantidades fixas e não atribui distribuição a eles, a estimação desses parâmetros é dada através da função de verossimilhança, enquanto que na escola bayesiana atribui-se uma distribuição, chamada de distribuição a priori, ao conjunto de parâmetros desconhecidos quantificando a sua crença sobre esse conjunto e a estimação dos parâmetros é dada através da distribuição à posteriori, que é proporcional ao produto da função de verossimilhança com a distribuição a priori.</p>
<p>O interesse pela modelagem estatística através da abordagem bayesiana surgiu a partir de um projeto de iniciação científica quando cursava o 6º período do curso de Graduação em Estatística que tinha como objetivo o cálculo e apresentação de estatísticas descritivas para ajudar uma pesquisadora. Após obter os resultados da análise exploratória e descritiva, notei, junto com meu orientador, que havia possibilidade de dar continuidade ao estudo a partir de uma abordagem estatística mais elaborada. Sendo assim, outro projeto de iniciação científica foi iniciado em seguida com a finalidade de me preparar para utilizar um modelo linear hierárquico bayesiano sob os dados disponibilizados pela pesquisadora em minha monografia.</p>
<p>Caso tenha interesse em conferir o projeto com o estudo sobre modelos hierárquicos bayesianos, disponibilizei os resultados e os códigos em meu github <a href="https://github.com/gomesfellipe/projeto_modelos_hierarquicos_bayesianos">neste repositório</a>. Neste post farei uma breve introdução sobre o ajuste de um modelo linear bayesiano simples e os resultados obtidos (utilizando uma distribuição a priori não informativa). Os resultados obtidos serão comparados com os resultados obtidos com o ajuste de um modelo de regressão linear através da abordagem clássica.</p>
<div id="distribuição-a-priori" class="section level2">
<h2>Distribuição a priori</h2>
<p>Para o estudo, optou-se pela utilização de valores elevados para variância a priori (também consideradas como “não informativas”, fazendo uma analogia à modelos clássicos) obtendo ajustes que atribuem maior importância à informação provinda da amostra.</p>
<p>Portanto com valores elevados para variância da distribuição a priori (consideradas como “não informativas”) foram obtida a distribuição a posteriori de um parâmetro <span class="math inline">\(\theta\)</span> que contém toda a informação probabilística a respeito deste parâmetro e quando a forma analítica dessa distribuição é conhecida o gráfico da <a href="https://pt.wikipedia.org/wiki/Fun%C3%A7%C3%A3o_densidade">fdp</a> pode ilustrar o comportamento probabilístico do parâmetro de interesse e auxiliar em alguma tomada de decisão, porém, quando a forma analítica não é conhecida ou é muito custosa de ser obtida, pode-se recorrer a métodos de simulação tais como os métodos MCMC.</p>
</div>
<div id="amostrador-de-gibbs---método-mcmc" class="section level2">
<h2>Amostrador de Gibbs - método MCMC</h2>
<p>Com os avanços dos métodos de MCMC, surgiu o amostrador de Gibbs, proposto por <span class="citation">@GemanGeman</span> e tornou-se popular por <span class="citation">@GelfandSmith</span>, falo um pouco mais sobre o algoritmo no <a href="https://github.com/gomesfellipe/projeto_modelos_hierarquicos_bayesianos/blob/master/texto.pdf">texto do projeto</a>.</p>
<p>Como a convergência ocorre após o aquecimento (ou burn-in), é comum usar os valores de <span class="math inline">\(\theta^{(a)}\)</span>, <span class="math inline">\(\theta^{(a+t)}\)</span>, <span class="math inline">\(\theta^{(a+2t)}\)</span>,… para compor a amostra de <span class="math inline">\(\theta\)</span>, sendo <span class="math inline">\(a-1\)</span> o número de iterações iniciais do aquecimento e <span class="math inline">\(t\)</span> o espaçamento utilizado para diminuir a autocorrelação dos parâmetros. Maiores detalhes podem ser vistos em <span class="citation">@Gamerman06</span>.</p>
</div>
</div>
<div id="ao-que-interessa" class="section level1">
<h1>Ao que interessa</h1>
<p>O objetivo deste post é apresentar e comparar os resultados do ajuste de um modelo linear bayesiano simples utilizando uma distribuição a priori não informativa com o modelo de regressão linear simples para dados simulados e para dados reais.</p>
<p>Diversas funções foram criadas ao longo o estudo para conferir o comportamento das cadeias geradas e os resultados do ajuste do modelo, aproveitarei essas funções para este post importando do <a href="https://github.com/gomesfellipe/projeto_modelos_hierarquicos_bayesianos/blob/master/dependencies.R">repositório no github</a> da seguinte maneira:</p>
<pre class="r"><code>path_to_dep &lt;- &quot;https://raw.githubusercontent.com/gomesfellipe/projeto_modelos_hierarquicos_bayesianos/master/dependencies.R&quot;
devtools::source_url(path_to_dep, encoding=&quot;UTF-8&quot;)</code></pre>
</div>
<div id="ajuste-do-modelo-para-dados-simulados" class="section level1">
<h1>Ajuste do modelo para dados simulados</h1>
<p>Suponha então um exemplo em que a população de interesse tenha distribuição normal com média <span class="math inline">\(\beta_0 + \beta_1 X\)</span>, sendo <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span> desconhecidos e variância <span class="math inline">\(\sigma^2\)</span> desconhecida. Seja <span class="math inline">\(\tau=\frac{1}{\sigma^2}\)</span> o parâmetro chamado de precisão.</p>
<p>O parâmetro <span class="math inline">\(\beta_0\)</span> é conhecido como intercepto ou coeficiente linear e o <span class="math inline">\(\beta_1\)</span> como coeficiente angular. Além disso, suponha que as unidades dessa população sejam iid. Dessa forma, tem-se que as unidades dessa população tem a seguinte distribuição:</p>
<p><span class="math display">\[
Y_i \stackrel{iid}{\sim} N(\beta_0 + \beta_1 X_i,\frac{1}{\tau}), 
\]</span></p>
<p>onde <span class="math inline">\(i=1,...,N\)</span>.</p>
<p>Para o estudo do modelo primeiramente foi utilizado um conjunto de dados simulados utilizando uma amostra de tamanho <span class="math inline">\(N=1000\)</span> e com os seguintes parâmetros “desconhecidos” dos quais desejamos estimar: <span class="math inline">\(\beta_0 = 1\)</span>, <span class="math inline">\(\beta_1 = 0,5\)</span>, <span class="math inline">\(\tau = 2\)</span>. A amostra será simulada segundo a variável aleatória: <span class="math inline">\(X_i ~ N(0,1)\)</span> e em seguida os parâmetros deste modelo, denotados por <span class="math inline">\(\theta = (\beta_0, \beta_1, \tau)\)</span> foram estimados usando o paradigma Bayesiano.</p>
<div id="gerando-a-amostra" class="section level2">
<h2>Gerando a amostra</h2>
<p>A amostra que foi simulada foi obtida da seguinte maneira:</p>
<pre class="r"><code># Amostra que sera utilizada:

set.seed(12)
n   &lt;- 1000                 # N=1000
b0  &lt;- 1                    # \beta_0 = 1
b1  &lt;- 0.5                  # \beta_1 = 0,5
tau &lt;- 2                    # \tau = 2 e 
x   &lt;- rnorm(n)             # X_i ~ N(0,1), logo:
y   &lt;- b0 + b1 * x + rnorm(n,0,sqrt(1/tau))</code></pre>
<p>Obtendo-se uma amostra de tamanho <span class="math inline">\(n\)</span>, pode-se inferir sob os parâmetros desconhecidos <span class="math inline">\(\theta = (\beta_0, \beta_1, \tau)\)</span> através da distribuição a posteriori e para obter essa distribuição faz-se necessário calcular a função de verossimilhança, que pode ser obtida da seguinte forma:</p>
<p><span class="math display">\[
p(y| \beta_0, \beta_1 , \tau) =\prod^n_{i=1} p(y_i | \beta_0, \beta_1, \tau )  
\]</span></p>
<p>portanto</p>
<p><span class="math display">\[
p(y| \beta_0, \beta_1 , \tau) = \prod_{i=1}^n \frac{ \sqrt{\tau} }{ \sqrt{2\pi} } exp { - \frac{\tau}{2} ( y_i - \beta_0 - \beta_1 x_i )^2 }
\]</span></p>
<p>onde <span class="math inline">\(y = (y_1, ..., y_n)\)</span> é a amostra coletada. O valor p para o teste de Shapiro para conferir a suposição de normalidade da variável resposta foi de 0.6181791 enquanto que o valor p para conferir a normalidade da variável explicativa foi de 0.7413229.</p>
</div>
<div id="distribuição-a-priori-1" class="section level2">
<h2>Distribuição a priori</h2>
<p>Durante o estudo diversos valores os parâmetros a priori foram selecionados para que fosse possível avaliar a sensibilidade da qualidade da escolha da distribuição priori, aqui será apresentado os resultados obtidos com valores elevados para variância a priori (também consideradas como “não informativas”, fazendo uma analogia à modelos clássicos) que ajusta o modelo atribuindo maior importância à informação provinda da amostra.</p>
<p>Considere a priori que os parâmetros sejam independentes e que</p>
<p><span class="math display">\[
\beta_0 \sim N(m_0,\sigma_0^2),  \\
\beta_1 \sim N(m_1,\sigma_1^2) \mbox{ e }  \\
\tau    \sim G(a,b).
\]</span></p>
<p>Portanto, para a estimação foram utilizados os seguintes hiperparâmetros : <span class="math inline">\(m_0 = m_1 = 0\)</span>, <span class="math inline">\(\sigma_0^2 = \sigma_1^2 = 100\)</span>, <span class="math inline">\(a=0,1\)</span> e <span class="math inline">\(b=0,1\)</span></p>
<p>No R:</p>
<pre class="r"><code>#Parametros para b0 ~ N(mu0, sig0)
mu0 &lt;-  0
sig0 &lt;-  1000

#Parametros para b1 ~ N(mu1, sig1)
mu1 &lt;-  0
sig1 &lt;-  1000

#Parametros para tau ~ G(a,b)
a &lt;-  0.1
b &lt;-  0.1</code></pre>
<p>Dessa forma, tem-se que a distribuição conjunta a priori possui a seguinte forma:</p>
<p><span class="math display">\[
 p(\beta_0, \beta_1 , \tau) \propto exp\Big\{-\frac{1}{2\sigma_0^2}( \beta_0 - m_0)^2\Big\} exp\Big\{-\frac{1}{2\sigma_1^2}( \beta_1 - m_1)^2\Big\} \tau^{a-1}exp \{-b \tau\}.
\]</span></p>
</div>
<div id="distribuição-a-posteriori" class="section level2">
<h2>Distribuição a posteriori</h2>
<p>Combinando a função de verossimilhança com a distribuição a priori, obtêm-se a distribuição a posteriori que é proporcional a:</p>
<p><span class="math display">\[
p(\beta_0, \beta_1 , \tau|y) \propto \tau^{\frac{n}{2}+a-1} exp \left\{ -\frac{\tau}{2} \sum^n_{i=1} (y_i - \beta_0 - \beta_1 x_i)^2 - b\tau  - \frac{1}{2\sigma_0^2}(\beta_0-m_0)^2  \right\} \times   exp\left\{- \frac{1}{2\sigma_1^2}(\beta_1-m_1)^2  \right\} . 
\]</span></p>
<p>Note que essa distribuição é multivariada e não possui forma analítica conhecida. Sendo assim, recorre-se aos métodos de MCMC para se obter amostras dessa distribuição. E então faz-se necessário obter as DCCP de <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span> e <span class="math inline">\(\tau\)</span>.</p>
</div>
<div id="implementando-o-amostrador-de-gibbs" class="section level2">
<h2>Implementando o amostrador de Gibbs</h2>
<p>O tamanho da cadeia foi de 30000 simulações e o <em>burn-in</em> (ou amostra de aquecimento) utilizado considerada após o ajuste foi de 15000. no R:</p>
<pre class="r"><code>nsim           &lt;-  3*10000
burnin         &lt;-  nsim / 2 
cadeia.b0      &lt;-  rep(0,nsim)
cadeia.b1      &lt;-  rep(0,nsim)
cadeia.tau     &lt;-  rep(0,nsim)

# Chutes iniciais: 
cadeia.b0[1]    &lt;-  0
cadeia.b1[1]    &lt;-  0
cadeia.tau[1]   &lt;-  1</code></pre>
<div id="calculos-para-implementar-o-algoritimo-na-mão" class="section level3">
<h3>Calculos para implementar o algoritimo na mão</h3>
<p>Para a implementação do algoritmo, fez-se necessário o cálculo das distribuições condicionais completas a posteriori (DCCP), primeiramente veja os resultados obtidos para <span class="math inline">\(\tau\)</span>:</p>
<ul>
<li>DCCP de <span class="math inline">\(\tau\)</span>:</li>
</ul>
<p><span class="math display">\[
\tau|y_1, ...,y_n,\beta_0, \beta_1 \sim Gama ( \frac{n}{2}+a,b+\frac{1}{2} \sum^n_{i=1}(y_i-\beta_0-\beta_1 x_i)^2 ) 
\]</span></p>
<p>Em seguida, veja o resultado obtido para <span class="math inline">\(\beta_0\)</span>, o coeficiente linear da reta, isto é, a altura em que a reta de regressão intercepta o eixo dos <span class="math inline">\(Y\)</span>’s:</p>
<ul>
<li>DCCP de <span class="math inline">\(\beta_0\)</span>:</li>
</ul>
<p><span class="math display">\[
\beta_0 | y_1,...,y_n , \tau,\beta_1 \sim N(\dfrac{(\tau\sum^n_{i=1}y_i - \tau\beta_1\sum^n_{i=1}x_i  +\frac{m_0}{\sigma_0^2})}{ \tau n + \frac{1}{\sigma_0^2}},  (n\tau +   \frac{1}{\sigma_0^2} )^{-1})
\]</span></p>
<p>Por fim, veja o resultado obtido para <span class="math inline">\(\beta_1\)</span>, é o coeficiente angular da reta, ou seja, é o a variação esperada na variável <span class="math inline">\(Y\)</span> quando a variável explicativa é acrescida de 1 unidade:</p>
<ul>
<li>DCCP de <span class="math inline">\(\beta_1\)</span>:</li>
</ul>
<p><span class="math display">\[
\beta_1 | y_1,...,y_n , \tau,\beta_0 \sim N(\frac{\tau\sum^n_{i=1}x_i y_i  - \tau\beta_0\sum^n_{i=1}x_i + \frac{m_1}{\sigma_1^2}}{\tau \sum^n_{i=1}x_i^2 + \frac{1}{\sigma_1^2}}, ( \tau \sum^n_{i=1}x_i^2 + \frac{1}{\sigma_1^2} )^{-1})
\]</span></p>
<p>Agora que todas as distribuições condicionais completas estão calculadas o algorítimo já pode ser implementado, no R foi feito da seguinte maneira: (note que as linhas que foram comentadas executariam uma barra de carregamento, com ilustrado em seguida)</p>
<pre class="r"><code># pb &lt;- txtProgressBar(min = 0, max = nsim, style = 3) # iniciando barra de processo
for (k in 2:nsim){
  
  #Cadeia tau
  cadeia.tau[k]   &lt;-  rgamma(1, (n/2) + a, b + (sum((y - cadeia.b0[k-1] - (cadeia.b1[k-1]*x))^2)/2))
  
  # Cadeia B0
  c0              &lt;-  (n*cadeia.tau[k]) + (1/sig0)
  m0              &lt;-  (cadeia.tau[k]*sum(y) - (cadeia.tau[k]*cadeia.b1[k-1]*sum(x)) + (mu0/sig0))/c0
  cadeia.b0[k]    &lt;-  rnorm(1, m0, 1/sqrt(c0))
  
  # Cadeia B1
  c1              &lt;-   (sum(x^2)*cadeia.tau[k]) + (1/sig1)
  m1              &lt;-   ((cadeia.tau[k]*sum(x*y)) - (cadeia.tau[k]*cadeia.b0[k]*sum(x)) + (mu1/sig1))/c1
  cadeia.b1[k]    &lt;-   rnorm(1, m1, 1/sqrt(c1))
  
  # setTxtProgressBar(pb, k)
  
}# ;close(pb) #Encerrando barra de processo</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/loading.png" /></p>
</div>
<div id="resultados-da-cadeia" class="section level3">
<h3>Resultados da cadeia</h3>
<p>A seguir definiremos a variável <code>inds</code> que indica os valores após a amostra de aquecimento (ou <em>burn-in</em>), a variável <code>real</code> que contém os valores reais utilizados para gerar a amostra para conferir se o modelo foi capaz de recuperá-los, os nomes dos parâmetros e os resultados das cadeias foram agregados em uma matriz:</p>
<pre class="r"><code># Juntando resultados:
inds    &lt;- seq(burnin, nsim) # Definindo os indices
real    &lt;- c(b0, b1, tau)
name    &lt;- c(expression(beta[0]), expression(beta[1]), expression(tau))
results &lt;- cbind(cadeia.b0, cadeia.b1, cadeia.tau) %&gt;% as.data.frame() %&gt;% .[inds, ] %T&gt;% head</code></pre>
<div id="histograma-e-densidade" class="section level4">
<h4>Histograma e densidade</h4>
<p>A figura abaixo apresenta os histogramas junto com as densidades de três cadeias obtidas ao se inicializar o amostrador em pontos diferentes de todos os parâmetros contidos em <span class="math inline">\(\theta\)</span> e uma linha vermelha indicará o valor do real parâmetro utilizado para estimar a cadeia.</p>
<pre class="r"><code>g1 &lt;- hist_den(results[,1],name = name[1], p = real[1])
g2 &lt;- hist_den(results[,2],name = name[2], p = real[2])
g3 &lt;- hist_den(results[,3],name = name[3], p = real[3])
grid.arrange(g1,g2,g3,ncol=1)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<div id="cadeia" class="section level4">
<h4>Cadeia</h4>
<p>A figura abaixo apresenta os traços das cadeias dos parâmetros amostrados exibindo o intervalo de credibilidade com a linha pontilhada em azul e o valor verdadeiro do parâmetro em vermelho. Note que há indícios de convergência.</p>
<pre class="r"><code># Cadeia
cadeia(results, name, real)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>é possível notar que todos os intervalos de credibilidade contêm o parâmetro populacional real utilizado para gerar a amostra.</p>
</div>
<div id="autocorrelação" class="section level4">
<h4>Autocorrelação</h4>
<p>A figura abaixo apresenta os gráficos de autocorrelação, que indicam se houve a influência dos “valores vizinhos” dos parâmetros amostrados. Note que parece haver independência entre as interações.</p>
<pre class="r"><code># ACF
FAC(results)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>é possível notar que nenhuma das cadeias apresentaram estimativas autocorrelacionada</p>
</div>
<div id="estimativas" class="section level4">
<h4>Estimativas</h4>
<p>Agora que já foi verificado que a cadeia se comportou de maneira satisfatória, veja os resultados obtidos sobre as estimativas dos parâmetros através do algoritmo. apresenta os resumos a posteriori dos parâmetros amostrados.</p>
<pre class="r"><code>coef &lt;- coeficientes(results, real = real) %&gt;% as.data.frame()

tabela_coeficientes(coef)</code></pre>
<div id="htmlwidget-1" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"visdat":{"11b9832b6bfa0":["function () ","plotlyVisDat"]},"cur_data":"11b9832b6bfa0","attrs":{"11b9832b6bfa0":{"columnorder":[1,2,3,4,5],"columnwidth":[80,80,80,80,80],"header":{"values":["Média","Desv. Pad.","IC inf","IC sup","Real"],"line":{"color":"#506784"},"fill":{"color":"#1F8FFFB4"},"align":["center","center","center","center","center"],"font":{"color":"white","size":15},"height":40},"cells":{"values":[[1.0244,0.4933,1.9001],[0.023,0.0241,0.085],[0.9792,0.4464,1.7371],[1.0697,0.5409,2.0695],[1,0.5,2]],"line":{"color":"#506784"},"fill":{"color":["#1F8FFF58","white","white","white","#1F8FFF58"]},"align":["center","center","center","center","center"],"font":{"color":["white","#506784","#506784","#506784","white"],"size":12},"height":30},"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"table"}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"hovermode":"closest","showlegend":false},"source":"A","config":{"showSendToCloud":false},"data":[{"columnorder":[1,2,3,4,5],"columnwidth":[80,80,80,80,80],"header":{"values":["Média","Desv. Pad.","IC inf","IC sup","Real"],"line":{"color":"#506784"},"fill":{"color":"#1F8FFFB4"},"align":["center","center","center","center","center"],"font":{"color":"white","size":15},"height":40},"cells":{"values":[[1.0244,0.4933,1.9001],[0.023,0.0241,0.085],[0.9792,0.4464,1.7371],[1.0697,0.5409,2.0695],[1,0.5,2]],"line":{"color":"#506784"},"fill":{"color":["#1F8FFF58","white","white","white","#1F8FFF58"]},"align":["center","center","center","center","center"],"font":{"color":["white","#506784","#506784","#506784","white"],"size":12},"height":30},"type":"table","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>Como se trata de uma amostra simulada é possível comparar as estimativas com os valores reais que geraram a amostra e os valores estão muito próximos da média (todos eles estão incluídos no intervalo de credibilidade).</p>
</div>
</div>
<div id="comparando-com-o-modelo-linear-clássico" class="section level3">
<h3>Comparando com o modelo linear clássico</h3>
<p>Agora que os resultados sob o paradigma bayesiano já foram conferidos será ajustado um modelo de regressão linear simples pelo método dos mínimos quadrados através da função <code>lm()</code> sob o paradigma clássico para comparar com os resultados de um modelo de regressão linear simples sob o paradigma bayesiano utilizando os resultados calculados.</p>
<pre class="r"><code># Reta do modelo classico
plot(x, y)
modelo.classico &lt;- lm(y ~ 1 + x)
a.classico      &lt;- modelo.classico$coefficients[1]
b.classico      &lt;- modelo.classico$coefficients[2]
abline(a        &lt;- a.classico, b = b.classico, col = &quot;blue&quot;)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>O modelo estimado para estes dados sob o paradigma da inferência clássica foi o seguinte: <span class="math inline">\(\hat{y} = 1.0245 x + 0,4933\)</span>, o que mostra que as estimativas de <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span> foram muito parecidas com as estimativas sob o paradigma da inferência bayesiana.</p>
<pre class="r"><code># Reta do modelo bayesiano
plot(x, y)
a.bayes  &lt;-  mean(results[, 1])
b.bayes  &lt;-  mean(results[, 2])
abline(a = a.bayes, b = b.bayes, col = &quot;red&quot;)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>A figura apresenta o gráfico de dispersão entre as variáveis da amostra simulada e as retas dos ajustes de ambos os modelos:</p>
<pre class="r"><code>library(stringr)
library(ggplot2)
library(ggExtra)

# Texto da imagem
text.classico &lt;- str_c(&quot;Modelo Classico: &quot;,&quot;y = &quot;,round(a.classico,4),&quot; x + &quot;,round(b.classico,4))
text.bayes    &lt;- str_c(&quot;Modelo Bayesiano: &quot;,&quot;y = &quot;,round(a.bayes,4),&quot; x + &quot;,round(b.bayes,4))

# Gerando o e ambos:
cbind(y, x) %&gt;%
  as.data.frame %&gt;%
    ggplot(aes(y = y, x = x)) +
    geom_point() +
    geom_smooth(method = &quot;lm&quot;, se = F, col = &quot;red&quot;) +
    theme_classic() +
    geom_abline(slope = b.bayes,
    intercept = a.bayes,
    col = &quot;blue&quot;) +
    labs(title = &quot;&quot;,
    x = &quot;Covariável&quot;,
    y = &quot;Reposta&quot;)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>Agora que os resultados no algoritmo já foram conferidos e avaliados de maneira satisfatória utilizando os dados simulados, é a vez de fazer o ajuste para dados reais.</p>
</div>
</div>
</div>
<div id="ajuste-do-modelo-para-dados-reais" class="section level1">
<h1>Ajuste do modelo para dados reais</h1>
<p>O conjunto de dados que será utilizado como exemplo foi disponibilizado por <span class="citation">@Ezekiel_cars</span> e hoje faz parte do conjunto de banco de dados nativos do R (a base de dados pode ser obtida ao escrever <code>cars</code> no console). Os dados informam a velocidade dos carros e as distâncias tomadas para parar, esses dados foram registrados na década de 1920 e são de grande utilidade didática até os dias de hoje.</p>
<p>Considere que deseja-se modelar a velocidade dos carros de acordo com as distâncias tomadas para parar, portanto a variável resposta será a velocidade e a variável explicativa do modelo será a distância tomada para parar.</p>
<div id="amostra-utilizada" class="section level2">
<h2>Amostra utilizada</h2>
<pre class="r"><code>y    &lt;-  cars$speed
x    &lt;-  cars$dist
n    &lt;-  nrow(cars)</code></pre>
<p>o valor p para o teste de Shapiro para conferir a suposição de normalidade da variável resposta foi de 0.4576319 enquanto que o valor p para conferir a normalidade da variável explicativa foi de 0.0390997</p>
</div>
<div id="distribuição-a-priori-2" class="section level2">
<h2>Distribuição a priori</h2>
<p>Serão utilizados os mesmos valores que foram propostos na simulação como hiperparametros e chutes iniciais para a cadeia, o código usado foi exatamente o mesmo.</p>
</div>
<div id="resultados-da-cadeia-1" class="section level2">
<h2>Resultados da cadeia</h2>
<p>Definiremos novamente a variável <code>inds</code> que indica os valores após a amostra de aquecimento (ou <em>burn-in</em>), desta vez não haverá a variável <code>real</code> pois não conhecemos os valores reais utilizados para gerar a amostra para conferir se o modelo foi capaz de recuperá-los. Desta vez utilizaremos a variável <code>classico</code>, que guarda os valores obtidos com o ajuste do modelo linear pela abordagem clássica.</p>
<pre class="r"><code># Juntando resultados:
inds     &lt;- seq(burnin, nsim) # Definindo os indices
results  &lt;- cbind(cadeia.b0, cadeia.b1, cadeia.tau) %&gt;% as.data.frame() %&gt;% .[inds, ]
classico &lt;- c(coefficients(lm(cars)), 1 / var(lm(cars)$residuals))
name     &lt;- c(expression(beta[0]), expression(beta[1]), expression(tau))</code></pre>
<div id="histograma-e-densidade-1" class="section level4">
<h4>Histograma e densidade</h4>
<p>A figura abaixo exibe os histogramas com as densidades de três cadeias obtidas ao se iniciar o amostrador em pontos diferentes de todos os parâmetros <span class="math inline">\(\theta\)</span> mas dessa vez sem a linha vermelha que indicava o valor do parâmetro real pois agora ele é desconhecido.</p>
<pre class="r"><code>g1 &lt;- hist_den(results[, 1], name = name[1])
g2 &lt;- hist_den(results[, 2], name = name[2])
g3 &lt;- hist_den(results[, 3], name = name[3])
grid.arrange(g1, g2, g3, ncol = 1)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>Nota-se que ambas as cadeias convergiram uma mesma distribuição e que as últimas três cadeias apresentaram valores próximos.</p>
</div>
<div id="cadeias" class="section level4">
<h4>Cadeias</h4>
<p>A figura abaixo apresenta os traços das cadeias dos parâmetros amostrados. Note que há indícios de convergência.</p>
<pre class="r"><code>cadeia(results,name)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
</div>
<div id="autocorrelação-1" class="section level4">
<h4>Autocorrelação</h4>
<p>A Figura abaixo apresenta os gráficos de autocorrelação dos parâmetros amostrados.</p>
<pre class="r"><code>FAC(results)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>É possível notar que apenas nas primeiras defasagens das cadeias das estimativas para os parâmetros <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span> se apresentaram de forma autocorrelacionada e que a partir dessa defasagem o gráfico de autocorrelação se apresentou de forma desejável.</p>
</div>
<div id="estimativas-1" class="section level4">
<h4>Estimativas</h4>
<p>Como todas as características da cadeia gerada foram avaliadas de maneira satisfatória agora será possível conferir o ajuste dos parâmetros de maneira mais segura pois já foi constatada a convergência da cadeia</p>
</div>
<div id="comparando-com-o-modelo-linear-clássico-1" class="section level4">
<h4>Comparando com o modelo linear clássico</h4>
<p>Agora que os resultados sob o paradigma bayesiano já foram conferidos novamente será ajustado um modelo de regressão linear simples pelo método dos mínimos quadrados sob o paradigma clássico para comparar com os resultados do um modelo de regressão linear simples sob o paradigma bayesiano utilizando os resultados calculados na seção.</p>
<pre class="r"><code># Reta do modelo classico 
plot(x, y)
modelo.classico &lt;- lm(y ~ 1 + x)
a.classico      &lt;- modelo.classico$coefficients[1]
b.classico      &lt;- modelo.classico$coefficients[2]
abline(a        &lt;- a.classico, b = b.classico, col = &quot;blue&quot;)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<pre class="r"><code># Reta do modelo bayesiano
plot(x, y)
a.bayes &lt;- mean(results[, 1])
b.bayes &lt;- mean(results[, 2])
abline(a = a.bayes, b = b.bayes, col = &quot;red&quot;)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>A Tabela abaixo apresenta o resumo a posteriori dos parâmetros estimados da cadeia e note que esta tabela não conta com a coluna dos valores reais como no exemplo anterior e sim as estimativas sob o paradigma clássico.</p>
<pre class="r"><code>coef &lt;- 
  coeficientes(results,real = classico) %&gt;% as.data.frame()

tabela_coeficientes(coef)</code></pre>
<div id="htmlwidget-2" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-2">{"x":{"visdat":{"11b981cae4251":["function () ","plotlyVisDat"]},"cur_data":"11b981cae4251","attrs":{"11b981cae4251":{"columnorder":[1,2,3,4,5],"columnwidth":[80,80,80,80,80],"header":{"values":["Média","Desv. Pad.","IC inf","IC sup","Real"],"line":{"color":"#506784"},"fill":{"color":"#1F8FFFB4"},"align":["center","center","center","center","center"],"font":{"color":"white","size":15},"height":40},"cells":{"values":[[8.2374,0.1663,0.1083],[0.8481,0.017,0.0214],[6.5848,0.1326,0.0699],[9.9239,0.1997,0.1542],[8.2839,0.1656,0.1025]],"line":{"color":"#506784"},"fill":{"color":["#1F8FFF58","white","white","white","#1F8FFF58"]},"align":["center","center","center","center","center"],"font":{"color":["white","#506784","#506784","#506784","white"],"size":12},"height":30},"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"table"}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"hovermode":"closest","showlegend":false},"source":"A","config":{"showSendToCloud":false},"data":[{"columnorder":[1,2,3,4,5],"columnwidth":[80,80,80,80,80],"header":{"values":["Média","Desv. Pad.","IC inf","IC sup","Real"],"line":{"color":"#506784"},"fill":{"color":"#1F8FFFB4"},"align":["center","center","center","center","center"],"font":{"color":"white","size":15},"height":40},"cells":{"values":[[8.2374,0.1663,0.1083],[0.8481,0.017,0.0214],[6.5848,0.1326,0.0699],[9.9239,0.1997,0.1542],[8.2839,0.1656,0.1025]],"line":{"color":"#506784"},"fill":{"color":["#1F8FFF58","white","white","white","#1F8FFF58"]},"align":["center","center","center","center","center"],"font":{"color":["white","#506784","#506784","#506784","white"],"size":12},"height":30},"type":"table","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>O modelo estimado sob este paradigma pode ser escrito da seguinte maneira: <span class="math inline">\(\hat{y} = 8,2839 x + 0,1656\)</span>, ou seja, os valores de <span class="math inline">\(\beta_0\)</span> e de <span class="math inline">\(\beta_1\)</span> novamente foram muito próximos dos parâmetros obtidos ao estimar sob o paradigma clássico.</p>
</div>
<div id="comparando-de-forma-visual" class="section level4">
<h4>Comparando de forma visual</h4>
<p>A Figura ilustra o gráfico de dispersão dos dados citados acima, com a intenção de exibir quanto uma variável é afetada por outra, onde no eixo vertical representa a velocidade do carro e no eixo horizontal a distância tomada para parar.</p>
<p>Além do comportamento das variáveis, neste gráfico é exibido também os resultados obtidos do ajuste ao se utilizar o método de mínimos quadrados (representada pela linha em vermelho) para estimar os parâmetros e o ajuste do modelo ao se utilizar o método apresentado acima em (representada pela linha azul).</p>
<pre class="r"><code># Texto da imagem
text.classico &lt;- str_c(&quot;Modelo Classico: &quot;,&quot;y = &quot;,round(a.classico,4),&quot; x + &quot;,round(b.classico,4))
text.bayes    &lt;- str_c(&quot;Modelo Bayesiano: &quot;,&quot;y = &quot;,round(a.bayes,4),&quot; x + &quot;,round(b.bayes,4))

#Gerando o scatter.plot
cbind(y, x) %&gt;%
  as.data.frame %&gt;%
  ggplot(aes(y = y, x = x)) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, se = F, col = &quot;red&quot;) +
  theme_classic() +
  geom_abline(slope = b.bayes,
              intercept = a.bayes,
              col = &quot;blue&quot;) +
  labs(title = &quot;Relação entre a Distância e a Velocidade com \nreta do modelo linear clássico vs bayesiano&quot;,
       x = &quot;Distância&quot;,
       y = &quot;Velocidade&quot;)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>É possível notar que os coeficientes calculados foram muito parecidos, mesmo apresentando pequenas diferenças decimais no valor dos coeficientes ainda é possível notar que as retas estão basicamente sobrepostas, ou seja, os valores estimados em ambas as abordagens foram praticamente os mesmos.</p>
<p>Apesar dos valores dos ajustes terem apresentado basicamente os mesmo resultados, a maneira de se conferir a qualidade do ajuste é diferente em ambas as abordagens. Enquanto sob o paradigma clássico o ajuste do modelo pode ser checado ao avaliar os pre-supostos quanto à distribuição dos resíduos, como recomenda <span class="citation">@GaussClarice</span>, ao utilizar um método de MCMC faz-se necessário conferir também outros aspectos como por exemplo se houve convergência da cadeias além do comportamento das autocorrelações, vide <span class="citation">@migon</span>.</p>
</div>
</div>
</div>
<div id="conclusão" class="section level1">
<h1>Conclusão</h1>
<p>O uso do algorítmo para simular os dados da implementação do modelo hierárquico bayesiano envolveu diversas etapas. Inicialmente foi necessária a revisão de literatura para a compreensão dos métodos que seriam utilizados na implementação do algoritmo, bem como em seu desenvolvimento. Essa pesquisa funcionou de maneira muito didática, de forma que a cada semana a abordagem pudesse envolver maior grau de complexidade.</p>
<p>Durante o estudo, diversos valores de parâmetros a priori foram selecionados para que fosse possível avaliar a sensibilidade da qualidade da escolha da distribuição a priori. Observou-se que valores elevados para variância a priori (também consideradas como “não informativas” - fazendo uma analogia à modelos clássicos) obtiveram melhores ajustes atribuindo maior importância à informação provinda da amostra.</p>
<p>O estudo com dados simulados facilitou o entendimento do algoritmo pois foi possível notar com facilidade a inadequabilidade das escolhas das prioris, que resultavam em estimativas muito distante do parâmetro populacional que gerou a amostra.</p>
</div>
<div id="referências" class="section level1">
<h1>Referências</h1>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero/">modelo bayesiano do zero</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Aprendizado Não Supervisionado</category>
      <category>Bayes</category>
      <category>Inferência Bayesiana</category>
      <category>Estatistica</category>
      <category>Modelagem Estatistica</category>
      <category>Probabilidade</category>
      <category>R</category>
      <category>Simulação</category>
      <category>Teoria</category>
      <category domain="tag">bayes</category>
      <category domain="tag">Estatistica</category>
      <category domain="tag">gomesfellipe</category>
      <category domain="tag">jags</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">modelagem estatistica</category>
      <category domain="tag">modelos generalizados</category>
      <category domain="tag">modelos lineares</category>
      <category domain="tag">probabilidade</category>
      <category domain="tag">R</category>
      <category domain="tag">regression</category>
      <category domain="tag">Teoria</category>
    </item>
    <item>
      <title>O paradoxo dos aniversários com simulação e probabilidade</title>
      <link>https://gomesfellipe.github.io/post/2018-01-20-simulacao-e-probabilidade/simulacao-e-probabilidade/</link>
      <pubDate>Sat, 20 Jan 2018 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2018-01-20-simulacao-e-probabilidade/simulacao-e-probabilidade/</guid>
      <description>Quanto você acha que é a probabiliddade num grupo de 23 pessoas escolhidas aleatoriamente que duas delas farão aniversário no mesmo dia? Acreditaria se eu te dissesse que essa chance é maior do que 50%? A probabilidade é contra intuitiva e neste post vamos demonstrar de forma analitica e atraves de simulação esse e outros resultados além de dissertar um pouco sobre a história e conceitos importantes de probabilidade</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="curiosidades-sobre-a-teoria-das-probabilidades" class="section level1">
<h1>Curiosidades sobre a teoria das probabilidades</h1>
<p>O uso de cálculo de probabilidades para avaliar incertezas já é utilizado a centenas de anos. Foram tantas áreas que se encontraram aplicações (como na medicina, jogos de azar, previsão do tempo…) que hoje não restam dúvidas de que os dados são onipresentes, ainda mais em plena era da informação.</p>
<p>Os conceitos de chances e de incertezas são tão antigos quando a própria civilização. Pessoas sempre tiveram que lidar com incertezas sobre o clima, suprimento de alimentos, suprimentos de água, risco de vida e tantas outras ameaças ao ser humano que o esforço para reduzir essas incertezas e seus efeitos passou a ser muito importante.</p>
<p>A ideia do jogo tem uma longa história,já no egito antigo em 2000 a.c foram encontrados em tumbas (<a href="https://pt.wikipedia.org/wiki/Jogo_de_azar#Hist%C3%B3ria">dados cúbicos com marcações praticamente idênticas às de dados modernos (wikipedia)</a>).</p>
<p>Segundo <span class="citation"><a href="#ref-DeGroot" role="doc-biblioref">DeGroot</a> (<a href="#ref-DeGroot" role="doc-biblioref">n.d.</a>)</span>, a teoria da probabilidade foi desenvolvida de forma constante desde o século XVII e tem sido amplamente aplicada em diversos campos de estudo. Hoje, a teoria da probabilidade é uma ferramenta importante na maioria das áreas de engenharia, ciência e gestão.</p>
<p>Muitos pesquisadores estão ativamente envolvidos na descoberta e no estabelecimento de novas aplicações de probabilidade em campos de química, meteorologia, fotografia de satélites, marketing, previsão de terremoto, comportamento humano, design de sistemas informáticos, finanças, genética e lei.</p>
<div id="conceitos-e-interpretações-para-probabilidades" class="section level2">
<h2>Conceitos e interpretações para probabilidades</h2>
<p>Além das muitas aplicações formais da teoria da probabilidade, o conceito de probabilidade entra em nossa vida cotidiana e conversa.</p>
<p>Muitas vezes ouvimos e usamos expressões como “<em>Provavelmente vai chover a amanhã à noite</em>,” “<em>É muito provável que o onibus atrase</em>,” ou “<em>As chances são altas de não poder se juntar a nós para almoçar esta tarde</em>.” Cada uma dessas expressões é baseada no conceito da probabilidade de que algum evento específico ocorrerá.</p>
<p>Existem três abordagens atualmente, as duas primeiras são:</p>
<div id="clássica" class="section level4">
<h4>Clássica</h4>
<ul>
<li><p>Se refere à subconjuntos unitários equiprováveis</p></li>
<li><p><span class="math inline">\(P(A)=\dfrac{\text{Número de elementos de }A}{\text{Número de elementos de }\Omega}\)</span></p></li>
</ul>
</div>
<div id="frequentista-ou-estatística" class="section level4">
<h4>Frequentista ou Estatística</h4>
<ul>
<li><p>Considera o limite de frequências relativas como o valor de probabilidade</p></li>
<li><p><span class="math inline">\(P(A)=lim_{n \rightarrow \infty} \frac{n_A}{n}\)</span></p></li>
</ul>
<p>onde <span class="math inline">\(n_A\)</span> é o nº de ocorrências de <span class="math inline">\(A\)</span> em <span class="math inline">\(n\)</span> repetições independentes do experimento</p>
</div>
<div id="definição-de-probabilidade" class="section level4">
<h4>Definição de probabilidade</h4>
<p>Segundo <span class="citation"><a href="#ref-Magalhaes" role="doc-biblioref">Magalhães</a> (<a href="#ref-Magalhaes" role="doc-biblioref">n.d.</a>)</span>, as definições acima possuem o apelo da intuição e permanecem sendo usadas para resolver inúmeros problemas, entretanto elas não são suficientes para uma formulação matemática rigorosa da probabilidade.</p>
<p>Aproximadamente em 1930 A. N. Kolmogorov apresentou um conjunto de axiomas matemáticos para definir probabilidade, permitindo incluir as definições anteriores como casos particulares.</p>
<p>Porém, como o verdadeiro significado da probabilidade ainda é um assunto altamente polêmico e está envolvido em muitas discussões filosóficas atuais sobre as bases da estatística e quando se trata de probabilidades, não adianta utilizar apenas a intuição pois nosso cérebro vai da bug!</p>
<p>A probabilidade é extremamente contra intuitiva e seu estudo deve sempre envolver uma vasta gama de exercícios para treinar nosso raciocínio analítico. Existem diversos problemas práticos que já ilustraram isso e um ótimo exemplo que todo mundo que já fez um curso básico de probabilidade já conhece, o <a href="https://pt.wikipedia.org/wiki/Paradoxo_do_anivers%C3%A1rio">Paradóxo do aniversário</a></p>
</div>
</div>
</div>
<div id="o-paradoxo-do-aniversário-ou-problema-dos-aniversários---feller68" class="section level1">
<h1>O paradoxo do aniversário (ou problema dos aniversários - Feller[68])</h1>
<p>Exemplo retirado do livro do <span class="citation"><a href="#ref-Feller" role="doc-biblioref">Feller</a> (<a href="#ref-Feller" role="doc-biblioref">n.d.</a>)</span>, questiona:</p>
<p>“Num grupo de <span class="math inline">\(n\)</span> pessoas, qual é a probabilidade de pelo menos duas delas fazerem aniversário no mesmo dia?”</p>
<p>Esse problema surpreende todo mundo porque dependendo do valor de <span class="math inline">\(n\)</span> pessoas, a probabilidade é bastante alta! Segundo veremos a probabilidade de isso ocorrer em uma turma de 23 pessoas ou mais escolhidas <strong>aleatoriamente</strong> é maior que <strong>50%</strong>!</p>
<p>Qual aluno de qualquer turma de probabilidade que nunca foi desafiado numa aposta pelo professor que tinha dois alunos com mesma data de aniversário na sala de aula e se deu conta que perderia em poucos minutos?</p>
<p>Vamos resolver esse problema tanto pela abordagem clássica quanto pela abordagem frequentista, para utilizar a segunda abordagem dados de muitas turmas de variados tamanhos serão simulados utilizando o <strong>R</strong> e podemos comparar os resultados e buscar alguma evidência de que os dados se distribuem de forma semelhante!</p>
<p><strong>Obs</strong>: Simular dados permitem imitar o funcionamento de, praticamente, qualquer tipo de operação ou processo (sistemas) do mundo real!</p>
</div>
<div id="probabilidade" class="section level1">
<h1>Probabilidade</h1>
<p>Considerando o ano com 365 dias, podemos assumir que <span class="math inline">\(n&lt;365\)</span> primeiramente devemos definir o espaço amostral <span class="math inline">\(\Omega\)</span> que será o conjunto de todas as sequências formadas com as datas dos aniversários (associamos cada data a um dos 365 dias do ano), defini-se:</p>
<p><em>experimento</em>: observar o aniversário de n pessoas</p>
<p><span class="math display">\[
\Omega = \{ (1,1,...,1),(1,2,53,...,201),(24,27,109,...,200),... \}
\]</span></p>
<p>portanto, sua cardinalidade será:</p>
<p><span class="math display">\[
\#\Omega = 365^n
\]</span></p>
<p>Definindo o evento:</p>
<p><span class="math display">\[
A = \text{pelo meno 2 alunos fazendo aniversário no mesmo dia em uma turma de tamanho }n
\]</span>
Observa-se que é um evento complicado de se calcular. Uma prática muito comum na teoria das probabilidades nestes casos é estudar o complementar do evento de interesse, veja:</p>
<p><span class="math display">\[
A^c = \text{nenhum dos alunos fazenndo aniversário no mesmo dia em uma turma de tamanho }n
\]</span></p>
<p>Agora basta fazer a conta:</p>
<p><span class="math display">\[
P(A^c)=\frac{\#A^c}{\#\Omega}=\frac{365 \times 364 \times ... \times (365-n+1)}{365^n}=\frac{365!}{365^n (365-n)!}
\]</span></p>
<p>segundo propriedades , se o evento é o complementar de todos n serem diferentes consequentemente o seguinte resultado é verdadeiro:</p>
<p><span class="math display">\[
p(A)=1- \frac{365!}{365^n (365-n)!}
\]</span></p>
<p>Agora que já sabemos a probabilidade de pelo menos duas pessoas fazerem aniversário no mesmo dia em uma turma de <span class="math inline">\(n\)</span> alunos, vejamos o comportamento deste ajuste e uma tabela com possíveis valores de <span class="math inline">\(n\)</span>:</p>
<p>Em R:</p>
<p>Utilizando expansão em série de Taylor (<a href="https://pt.wikipedia.org/wiki/Paradoxo_do_anivers%C3%A1rio#Aproxima%C3%A7%C3%B5es">mais informações</a>):</p>
<pre class="r"><code>birthday=function(x){
  a=1-exp(-(x^2)/(2*365))
  return(a)
}
birthday(23)</code></pre>
<pre><code>## [1] 0.5155095</code></pre>
<table class="table table-condensed">
<thead>
<tr>
<th style="text-align:right;">
n
</th>
<th style="text-align:right;">
P
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FFF5EB; width: 20.00%">5</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FFF5EB; width: 20.00%">0.0336668</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FEE6CE; width: 30.00%">15</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FEE6CE; width: 39.17%">0.2652457</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FDD0A2; width: 40.00%">25</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FDD0A2; width: 64.84%">0.5752117</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FDAE6B; width: 50.00%">35</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FDAE6B; width: 84.54%">0.8132683</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FD8D3C; width: 60.00%">45</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FD8D3C; width: 94.84%">0.9375864</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #F16913; width: 70.00%">55</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #F16913; width: 98.69%">0.9841381</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #D94801; width: 80.00%">65</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #D94801; width: 99.75%">0.9969349</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #A63603; width: 90.00%">75</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #A63603; width: 99.97%">0.9995496</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #7F2704; width: 100.00%">85</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #7F2704; width: 100.00%">0.9999497</span>
</td>
</tr>
</tbody>
</table>
<p>Em Python (função retirada do <a href="https://pt.wikipedia.org/wiki/Paradoxo_do_anivers%C3%A1rio#Implementa%C3%A7%C3%A3o_em_Python">wikpédia</a> para comparar os resultados):</p>
<pre class="python"><code>def birthday(x):
    p = (1.0/365)**x
    for i in range((366-x),366):
        p *= i
    return 1-p
    
print(&quot;%1.7f&quot; %(birthday(23))) #Arredondando para o mesmo numero de casas decimais default do R</code></pre>
<pre><code>## 0.5072972</code></pre>
<p>Tanto a aproximação do R quanto a do Python obtiveram resultados semelhantes</p>
<p>Vejamos como é o comportamento da curva teórica e as estimações:</p>
<p><img src="/post/2018-01-20-simulacao-e-probabilidade/simulacao-e-probabilidade_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Note que segundo a distribuição teórica, confirmamos que a probabilidade do evento ocorrer em uma turma de 23 pessoas ou mais escolhidas <strong>aleatoriamente</strong> é maior que <strong>50%</strong>!</p>
</div>
<div id="simulação" class="section level1">
<h1>Simulação</h1>
<p>Segundo o <a href="https://pt.wikipedia.org/wiki/Simula%C3%A7%C3%A3o">wikipédia</a>, a simulação “consiste em empregar formalizações em computadores, como expressões matemáticas ou especificações mais ou menos formalizadas, com o propósito de imitar um processo ou operação do mundo real”</p>
<p>Nossa simulação irá consistir em imitar o comportamento de um processo do mundo real utilizando o seguinte código para simular o experimento de <em>observar o aniversário de <span class="math inline">\(n\)</span> pessoas</em> milhares de vezes:</p>
<pre class="r"><code>N&lt;- 5000                                    #Numero de simulacoes do experimento

prob=0
for(n in 2:100){                            #Para n variand de 2 até 50
  cont_a=0                                  #Inicia o contador
  M=matrix(NA, N, n)                        #Delara uma matriz varia com as dimensoes desejadas  
  for(i in 1:N){                            #indice i que percorre todas as N linhas simuladas
    M[i,] = sample(1:365, n, replace = T)   #Sorteio de uma amosra de tamanho n de numeros de 1 a 365 
    linha=M[i,]                             #objeto linha recebe a linha simulada
    tab=table(linha)                        #objeto tab guarda a tabela de frequencias dessa amostra
    if(length(tab)&lt;n){                      #se o tamanho da tabela de frequencias for menor que o tamanho da turma
      cont_a=cont_a+1                       #contador recebe 1 pois duas pessoas fizeram aniversario no mesmo dia
    } 
  }
  prob[n]=cont_a/N                          #a probabilidade será a proporcao de pessoas que fazem aniversario no mesmo dia observadas em N amostra simuladas
}

prob[23]</code></pre>
<pre><code>## [1] 0.5088</code></pre>
<p>Notamos que o resultado observado é muito próximo d resultado calculado de acordo com a probabilidade teoria para a chance de se se encontrar pelo menos 2 pessoas que fazem aniversário em uma turma de 23 anos (<em>novamente ultrapassou os 50%!!!</em>)</p>
<p>Para efeito de comparação visual com a resolução anterior:</p>
<table class="table table-condensed">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
n
</th>
<th style="text-align:right;">
P
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
5
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FFF5EB; width: 20.00%">5</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FFF5EB; width: 20.00%">0.0236</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
15
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FEE6CE; width: 30.00%">15</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FEE6CE; width: 38.30%">0.2470</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
25
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FDD0A2; width: 40.00%">25</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FDD0A2; width: 65.21%">0.5754</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
35
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FDAE6B; width: 50.00%">35</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FDAE6B; width: 85.02%">0.8172</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
45
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FD8D3C; width: 60.00%">45</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FD8D3C; width: 95.00%">0.9390</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
55
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #F16913; width: 70.00%">55</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #F16913; width: 98.87%">0.9862</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
65
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #D94801; width: 80.00%">65</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #D94801; width: 99.85%">0.9982</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
75
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #A63603; width: 90.00%">75</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #A63603; width: 99.97%">0.9996</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
85
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #7F2704; width: 100.00%">85</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #7F2704; width: 100.00%">1.0000</span>
</td>
</tr>
</tbody>
</table>
<p><img src="/post/2018-01-20-simulacao-e-probabilidade/simulacao-e-probabilidade_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<div id="comparando" class="section level1">
<h1>Comparando</h1>
<p>Por fim, vejamos de forma visual se o comportamento dos resultados simulados estão de acordo com o resultado teórico calculado:</p>
<p><img src="/post/2018-01-20-simulacao-e-probabilidade/simulacao-e-probabilidade_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Como podemos ver o comportamento dos dados simulados foi muito similar ao da curva teórica calculada.</p>
</div>
<div id="modelagem-e-simulação-em-probabilidade" class="section level1">
<h1>Modelagem e simulação em probabilidade</h1>
<p>Existe uma vasta gama de aplicações de simulações como em projetos de análises de sistemas de manufatura, avaliação de requisitos não funcionais de hardware e software, avaliação de novas armas e táticas militares, reposição de estoque, projeto de sistemas de transporte, avaliações de serviços, aplicações estatísticas de cadeias MCMC…</p>
<p>Um simulador permite testar várias alternativas a um custo <strong>geralmente</strong> mais baixo do que no mundo real, possibilitando o melhor entendimento sobre o problema!</p>
</div>
<div id="referências" class="section level1 unnumbered">
<h1>Referências</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-DeGroot" class="csl-entry">
DeGroot, Morris H. n.d. <em>Probability and Statistics</em>. Vol. 4.
</div>
<div id="ref-Feller" class="csl-entry">
Feller, William. n.d. <em>An Introduction to Probability Theory and Its Applications</em>. Vol. 3.
</div>
<div id="ref-Magalhaes" class="csl-entry">
Magalhães, Mascos N. n.d. <em>Probabilidade e Variáveis Aleatóriasa</em>. Vol. 1.
</div>
</div>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2018-01-20-simulacao-e-probabilidade/simulacao-e-probabilidade/">O paradoxo dos aniversários com simulação e probabilidade</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Estatistica</category>
      <category>Modelagem Estatistica</category>
      <category>Analise Exploratória</category>
      <category>Teoria</category>
      <category>Simulação</category>
      <category>Probabilidade</category>
      <category domain="tag">gomesfellipe</category>
      <category domain="tag">Estatistica</category>
      <category domain="tag">Teoria</category>
      <category domain="tag">analise multivariada</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">modelagem estatistica</category>
      <category domain="tag">simulacao</category>
      <category domain="tag">probabilidade</category>
    </item>
    <item>
      <title>Análise Multivariada com R</title>
      <link>https://gomesfellipe.github.io/post/2018-01-04-analise-multivariada-em-r/analise-multivariada-em-r/</link>
      <pubDate>Thu, 04 Jan 2018 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2018-01-04-analise-multivariada-em-r/analise-multivariada-em-r/</guid>
      <description>Análise Multivariada Imagem do Wikpedia
Esse é o primeiro post do ano e como no ano de 2017 falou-se tanto das maravilhas computacionais desta onda do Big Data e em contra partida, identificamos que deste 2004 a popularidade pelo termo “estatística” vem diminuindo como mostrei em uma breve pesquisa neste post sobre a API do googletrends sinto que existe uma necessidade de se ampliar também a divulgação dos métodos estatísticos pois o aprofundamento na teoria é fundamental (é muito fácil achar resultados sem fundamento apenas “apertando botão”), como as ferramentas da estatística multivariada que muitas vezes servem como soluções para essas grandes quantidades de dados</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="análise-multivariada" class="section level1">
<h1>Análise Multivariada</h1>
<p><a href="https://commons.wikimedia.org/wiki/File:Multivariate_Gaussian.png">Imagem do Wikpedia</a></p>
<p>Esse é o primeiro post do ano e como no ano de 2017 falou-se tanto das maravilhas computacionais desta onda do Big Data e em contra partida, <a href="https://gomesfellipe.github.io/post/2017-12-12-google-trends-e-r/google-trends-e-r/">identificamos que deste 2004 a popularidade pelo termo “estatística” vem diminuindo como mostrei em uma breve pesquisa neste post sobre a API do googletrends</a> sinto que existe uma necessidade de se ampliar também a divulgação dos métodos estatísticos pois o aprofundamento na teoria é fundamental (é muito fácil achar resultados sem fundamento apenas “apertando botão”), como as ferramentas da estatística multivariada que muitas vezes servem como soluções para essas grandes quantidades de dados</p>
<p>Diversas vezes nos deparamos com bases de dados que envolvem além de muitas observações, muitas variáveis, especialmente nas análises de fenômenos ou processos sociais, psicológicos, educacionais e econômicos bem como na área da química, biologia, geologia, marketing, medicina, medicina veterinária, dentre muitas outras.</p>
<p>Com as ferramentas estatísticas da análise multivariada somos capazes de identificar muitos elementos que podem ter relevância na análise dos dados, dois exemplos de ferramentas importantes são as que permitem encontrar fatores que não são diretamente observáveis com base em um conjunto de variáveis observáveis e as que permitem agrupar conjuntos de dados que possuem características semelhantes com algorítimos computacionais (chamados de aprendizados não-supervisionados ou semi-supervisionados em machine learning) e a partir dai estudar as novas classificações.</p>
<p>Neste post será apresentado algumas soluções para o caso em que existe a necessidade de avaliar um grande conjunto de dados com muitas variáveis e não temos muitas informações a respeito.</p>
</div>
<div id="análise-fatorial" class="section level1">
<h1>Análise Fatorial</h1>
<p>Na análise fatorial buscamos fatores que explicam parte da variância total dos dados, os fatores são as somas das variâncias originais.</p>
<div id="objetivo-da-análise-fatorial" class="section level2">
<h2>Objetivo da análise fatorial:</h2>
<ul>
<li><p>Procura identificar fatores que não são diretamente observáveis, com base em um conjunto de variáveis observáveis.</p></li>
<li><p>Explicar a correlação ou covariância, entre um conjunto de variáveis, em termos de um número limitado de variáveis não-observáveis, chamadas de fatores ou variáveis latentes.</p></li>
<li><p>Em casos nos quais se tem um número grande de variáveis medidas e correlacionadas entre si, seria possível identificar-se um número menor de variáveis alternativas, não correlacionadas e que de algum modo sumarizassem as informações principais das variáveis originais.</p></li>
<li><p>A partir do momento em que os fatores são identificados, seus valores numéricos, chamados de escores, podem ser obtidos para cada elemento amostral. Conseqüentemente, estes escores podem ser utilizados em outras análises que envolvam outras técnicas estatísticas, como análise de regressão ou análise de variância, por exemplo.</p></li>
</ul>
</div>
<div id="etapas-para-realização" class="section level2">
<h2>Etapas para realização</h2>
<ul>
<li><p>Computação da matriz de correlações para as variáveis originais;</p></li>
<li><p>Extração de fatores</p></li>
<li><p>Rotação dos fatores para tonar a interpretação mais fácil;</p></li>
<li><p>Cálculo dos escores dos fatores</p></li>
</ul>
<div id="matriz-de-correlação" class="section level3">
<h3>Matriz de Correlação:</h3>
<ul>
<li>Teste de Bartlett - a hipótese nula da matriz de correlação ser uma matriz identidade ( <span class="math inline">\(| R | = 1\)</span> ), isto é, avalia se os componentes fora da diagonal principal são zero. O resultado significativo indica que existem algumas relações entre as variáveis.</li>
</ul>
<p>No R:</p>
<pre class="r"><code>Bartlett.sphericity.test &lt;- function(x)
{
  method &lt;- &quot;Teste de esfericidade de Bartlett&quot;
  data.name &lt;- deparse(substitute(x))
  x &lt;- subset(x, complete.cases(x)) # Omitindo valores faltantes
  n &lt;- nrow(x)
  p &lt;- ncol(x)
  chisq &lt;- (1-n+(2*p+5)/6)*log(det(cor(x)))
  df &lt;- p*(p-1)/2
  p.value &lt;- pchisq(chisq, df, lower.tail=FALSE)
  names(chisq) &lt;- &quot;X-squared&quot;
  names(df) &lt;- &quot;df&quot;
  return(structure(list(statistic=chisq, parameter=df, p.value=p.value,
                        method=method, data.name=data.name), class=&quot;htest&quot;))
}
Bartlett.sphericity.test(dados)</code></pre>
<pre><code>## 
##  Teste de esfericidade de Bartlett
## 
## data:  dados
## X-squared = 2590.3, df = 55, p-value &lt; 2.2e-16</code></pre>
<ul>
<li>Teste KMO (Kaiser-Meyer-Olkin) - avalia a adequação do tamanho amostra. Varia entre 0 e 1, onde: zero indica inadequado para análise fatorial, aceitável se for maior que 0.5, recomendado acima de 0.8.</li>
</ul>
<p>No R:</p>
<pre class="r"><code>kmo = function(x)
{
  x = subset(x, complete.cases(x))
  r = cor(x)
  r2 = r^2 
  i = solve(r) 
  d = diag(i) 
  p2 = (-i/sqrt(outer(d, d)))^2 
  diag(r2) &lt;- diag(p2) &lt;- 0 
  KMO = sum(r2)/(sum(r2)+sum(p2))
  MSA = colSums(r2)/(colSums(r2)+colSums(p2))
  return(list(KMO=KMO, MSA=MSA))
}

kmo(dados)</code></pre>
<pre><code>## $KMO
## [1] 0.5942236
## 
## $MSA
##         A         B         C         D         E         F         G         H 
## 0.6789278 0.9151657 0.6897541 0.3385536 0.8699746 0.3632508 0.5172135 0.4878681 
##         I         J         K 
## 0.4901580 0.4895023 0.4686937</code></pre>
<div id="tipos-de-correlação" class="section level4">
<h4>Tipos de correlação:</h4>
<p>Nem sempre é possível utilizar a correlação de pearson, porém, existem diversas outras maneiras de se saber qual a correlação dos dados. Podemos utilizar correlações como de Spearman, Policórica, etc.. Já fiz um post onde explico os <a href="https://gomesfellipe.github.io/post/tipos-de-relacoes-entre-variaveis/">diferentes tipos de relações entre os tipos de variáveis</a> e os <a href="https://gomesfellipe.github.io/post/tipos-de-correlacoes/">tipos de correlações</a> possíveis para avaliar a relação dessas variáveis.</p>
<p>Aqui um outro exemplo de como utilizar a correlação parcial</p>
<pre class="r"><code>partial.cor &lt;- function (x)
{
R &lt;- cor(x)
RI &lt;- solve(R)
D &lt;- 1/sqrt(diag(RI))
Rp &lt;- -RI * (D %o% D)
diag(Rp) &lt;- 0
rownames(Rp) &lt;- colnames(Rp) &lt;- colnames(x)
Rp
}
mat_anti_imagem &lt;- -partial.cor(dados[,1:10])
mat_anti_imagem</code></pre>
<pre><code>##              A           B            C           D           E            F
## A  0.000000000 -0.25871349 -0.872167400  0.01668860 -0.04245837 -0.011036934
## B -0.258713494  0.00000000 -0.204228090  0.05621580  0.09801359  0.060189694
## C -0.872167400 -0.20422809  0.000000000 -0.02459342 -0.00482831 -0.008201211
## D  0.016688604  0.05621580 -0.024593418  0.00000000 -0.18602037  0.806258927
## E -0.042458373  0.09801359 -0.004828310 -0.18602037  0.00000000 -0.140784264
## F -0.011036934  0.06018969 -0.008201211  0.80625893 -0.14078426  0.000000000
## G  0.006994874 -0.12746560  0.045529480 -0.75073120 -0.32160010 -0.792991683
## H -0.071792191  0.03375700  0.059785980 -0.03196914  0.08903510  0.001496987
## I  0.055698412 -0.03720345 -0.040061304  0.08243832 -0.03925037  0.084673934
## J -0.042609593  0.06775755  0.004145341 -0.06024239  0.05806892 -0.001404078
##              G            H           I            J
## A  0.006994874 -0.071792191  0.05569841 -0.042609593
## B -0.127465596  0.033756995 -0.03720345  0.067757550
## C  0.045529480  0.059785980 -0.04006130  0.004145341
## D -0.750731196 -0.031969142  0.08243832 -0.060242391
## E -0.321600101  0.089035097 -0.03925037  0.058068923
## F -0.792991683  0.001496987  0.08467393 -0.001404078
## G  0.000000000 -0.025016441 -0.05943429  0.009961615
## H -0.025016441  0.000000000 -0.55229599  0.044929237
## I -0.059434295 -0.552295987  0.00000000  0.056248550
## J  0.009961615  0.044929237  0.05624855  0.000000000</code></pre>
</div>
</div>
<div id="extração-de-fatores-via-componentes-principais" class="section level3">
<h3>Extração de fatores via componentes principais</h3>
<ul>
<li><p>Determinado o número de fatores necessários para representar os dados</p></li>
<li><p>Também é determinado o método que será utilizado, o mais utilizado é a análise de componentes principais</p></li>
</ul>
<div id="estimação-do-número-de-fatores-m" class="section level4">
<h4>Estimação do número de fatores m</h4>
<ul>
<li><p>Estimação do número de fatores m</p></li>
<li><p>Para a estimação de m, bastará extrair-se os autovalores da matriz de correlação amostral.</p></li>
<li><p>Observa-se quais autovalores são os mais importantes em termos de grandeza numérica.</p></li>
<li><p>os autovalores refletem a importância do fator se o número de fatores for igual ao número de variáveis então a soma dos autovetores é igual a soma das variâncias (pois cada variância será igual a 1).</p></li>
<li><p>Portanto a razão $ / 2 var $ indica proporção da variabilidade total explicada pelo fator</p></li>
</ul>
<p><strong>Critérios:</strong></p>
<ol style="list-style-type: decimal">
<li><p>A análise da proporção da variância total relacionada com cada autovalor (<span class="math inline">\(\lambda_i\)</span>). Permanecem aqueles autovalores que maiores proporções da variância total e, portanto, o valor de m será igual ao número de autovalores retidos;</p></li>
<li><p>A comparação do valor numérico de (<span class="math inline">\(\lambda_i\)</span>) com o valor 1. O valor de m será igual ao número de autovalores maiores ou iguais a 1.</p></li>
<li><p>Observação do gráfico scree-plot, que dispõe os valores de (<span class="math inline">\(\lambda_i\)</span>) ordenados em ordem decrescente. Por este critério, procura-se no gráfico um “ponto de salto”, que estaria representando um decréscimo de importância em relação à variância total. O valor de m seria então igual ao número de autovalores anteriores ao “ponto de salto”.</p></li>
</ol>
</div>
</div>
<div id="análise-de-componentes-principais" class="section level3">
<h3>Análise de componentes Principais:</h3>
<ul>
<li><p>Fatores são obtidos através da decomposição espectral da matriz de correlações, resultado em cargas fatoriais que indicam o quanto cada variável está associada a cada fator e os autovalores associados a cada um dos fatores envolvidos</p></li>
<li><p>São formadas combinações lineares das variáveis observadas.</p></li>
<li><p>O primeiro componente principal consiste na combinação que responde pela maior quantidade de variância na amostra.</p></li>
<li><p>O segundo componente responde pela segunda maior variância na amostra e não é correlacionado com o primeiro componente.</p></li>
<li><p>Sucessivos componentes explicam progressivamente menores porções de variância total da amostra e todos são não correlacionados uns aos outros.</p></li>
</ul>
<p>No R a análise de componentes principais pode ser realizada com as funções nativas <code>prcomp()</code> e a visualização pode ser realizada com a função <code>biplot</code> nativa do R ou com a função <code>autoplot()</code> do pacote <code>ggfortify</code> apresentado em um posto que eu <a href="https://gomesfellipe.github.io/post/2017-12-26-diagnostico-de-modelos/diagnostico-de-modelos/">comento sobre ajustes de modelos lineares</a>.</p>
<p>Neste exemplo utilizaremos <a href="https://github.com/vqv/ggbiplot/blob/master/R/ggscreeplot.r">as funções de código aberto encontrei nesse github</a> que permite elaborar o gráfico baseado em funções do <code>ggplot</code>, além disso também carregaremos o pacote deste Github. Veja:</p>
<pre class="r"><code>library(ggplot2)
library(ggfortify)
library(ggbiplot)
#Componentes principais:
acpcor=prcomp(dados, scale = TRUE)
summary(acpcor)</code></pre>
<pre><code>## Importance of components:
##                           PC1    PC2    PC3    PC4     PC5     PC6    PC7
## Standard deviation     1.7205 1.5835 1.2745 1.2107 1.02156 0.72100 0.6634
## Proportion of Variance 0.2691 0.2280 0.1477 0.1333 0.09487 0.04726 0.0400
## Cumulative Proportion  0.2691 0.4971 0.6447 0.7780 0.87285 0.92011 0.9601
##                           PC8     PC9    PC10    PC11
## Standard deviation     0.4953 0.33061 0.25079 0.14588
## Proportion of Variance 0.0223 0.00994 0.00572 0.00193
## Cumulative Proportion  0.9824 0.99235 0.99807 1.00000</code></pre>
<pre class="r"><code>ggbiplot(acpcor, obs.scale = 1, var.scale = 1,
   ellipse = TRUE, circle = TRUE) +
  scale_color_discrete(name = &#39;&#39;) +
  theme(legend.direction = &#39;horizontal&#39;, legend.position = &#39;top&#39;)</code></pre>
<p><img src="/post/2018-01-04-analise-multivariada-em-r/analise-multivariada-em-r_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code># autoplot(acpcor, label = TRUE, label.size = 1,
#          loadings = TRUE, loadings.label = TRUE, loadings.label.size  = 3)</code></pre>
<p>Para a observação do gráfico scree-plot podemos utilizar os comandos a seguir (com funções nativas do R ou mesmo com funções personalizadas como a que eu acabei de comentar <a href="https://github.com/vqv/ggbiplot/blob/master/R/ggscreeplot.r">disponivel nesse github</a></p>
<pre class="r"><code>#Com Funcao nativa do R:
# plot(1:ncol(dados), acpcor$sdev^2, type = &quot;b&quot;, xlab = &quot;Componente&quot;,
#      ylab = &quot;Variância&quot;, pch = 20, cex.axis = 1.3, cex.lab = 1.3)

#Ou funcao personalizada com ggplot2:
ggscreeplot(acpcor)</code></pre>
<p><img src="/post/2018-01-04-analise-multivariada-em-r/analise-multivariada-em-r_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<div id="rotação" class="section level4">
<h4>Rotação</h4>
<ul>
<li><p>Algumas variáveis são mais correlacionadas com alguns fatores do que outras.</p></li>
<li><p>Em alguns casos, a interpretação dos fatores originais pode não ser tarefa muito fácil devido à aparição de coeficientes de grandeza numérica similar, e não desprezível, em vários fatores diferentes.</p></li>
<li><p>O propósito da rotação é obter uma estrutura simples.</p></li>
<li><p>Em uma estrutura simples, cada fator tem carga alta somente para algumas variáveis, tornando mais fácil a sua identificação.</p></li>
<li><p>Tipos: Varimax, Quartimax, Equamax</p></li>
</ul>
<p>Aplicando a Varimax:</p>
<pre class="r"><code>k &lt;- 6 #6 fatores selecionados
carfat = acpcor$rotation[, 1:k] %*% diag(acpcor$sdev[1:k])
carfatr = varimax(carfat)</code></pre>
</div>
<div id="comunalidade" class="section level4">
<h4>Comunalidade</h4>
<ul>
<li><p>Índices atribuídos a variável original que expressam em % o quanto da variabilidade de cada variável é explicada pelo modelo</p></li>
<li><p>Designa-se por comunalidade (<span class="math inline">\(h^{2}_i\)</span>)a proporção da variância de cada variável explicada pelos fatores comuns.</p></li>
<li><p>As comunalidades variam entre 0 e 1, sendo 0 quando os fatores comuns não explicam nenhuma variância da variável e 1 quando explicam toda a sua variância.</p></li>
<li><p>Quando o valor das comunalidades é menor que 0,6 deve-se
pensar em: aumentar a amostra, eliminar as variáveis.</p></li>
</ul>
</div>
</div>
<div id="interpretar-o-modelo" class="section level3">
<h3>Interpretar o modelo</h3>
<ul>
<li><p>Feito pelas cargas fatoriais que são os parâmetros do modelo</p></li>
<li><p>Fatores expressam as covariâncias entre cada fator e as variáveis originais</p></li>
<li><p>Varimax ajuda a interpretar o modelo</p></li>
<li><p>Rotações ortogonais (para dependente) ; Rotações oblíquas (para independentes)</p></li>
</ul>
</div>
</div>
</div>
<div id="clusters" class="section level1">
<h1>Clusters</h1>
<p>Técnica estatística multivariada que tem como objetivo organizar um conjunto de objetos em um determinado nº de subconjuntos mutuamente exclusivos (clusters), de tal forma que os objetos em um mesmo cluster sejam semelhantes entre si,porém diferentes dos objetos nos outros clusters</p>
<p>Etapas para análise de clusters, que são comuns em qualquer análise (KDD):</p>
<ul>
<li>Seleção dos objetos a serem agrupados</li>
<li>Definir conjunto de atributos que caracterizam os objetos</li>
<li>Medida de dissimilaridade</li>
<li>Seleção de um algoritmo de agregação</li>
<li>Definição do número de clusters</li>
<li>Interpretação e validação dos clusters</li>
</ul>
<p>Critérios para a seleção:</p>
<ul>
<li>Selecionar variáveis diferentes entre si</li>
<li>Variáveis padronizadas (padronização mais comum é a Z-score)</li>
</ul>
<p>Existem algumas abordagens para a utilização das técnicas de análises de clusters, as diferenças entre os métodos hierárquicos e os não hierárquicos são as seguintes:</p>
<p>Métodos Hierárquicos são preferidos quando:</p>
<ul>
<li>Serão analisadas varias alternativas de agrupamento.</li>
<li>O tamanho da amostra é moderado ( de 300 a 1000 objetos )</li>
</ul>
<p>Métodos não-hierárquicos são preferidos quando:</p>
<ul>
<li>O número de grupos é conhecido.</li>
<li>Presença dos outliers, desde que os métodos não-hierárquicos são
menos influenciados por outliers.</li>
<li>Há um grande nº de objetos a serem agrupados.</li>
</ul>
<div id="método-hierárquico-de-agrupamento" class="section level2">
<h2>Método hierárquico de agrupamento</h2>
<p>É realizado em dois passos, o primeiro deles calcula-se a matriz de similaridade com o uso da função <em>dist()</em> (existem diversos tipos de distâncias que podem ser utilizadas aqui), o método utilizado será o de <strong>Ward</strong> (também poderíamos escolher o método da menor distância, maior distância ou a distância média).</p>
<p>Vantagens:</p>
<ul>
<li>Rápidos e exigem menos tempo de processamento.</li>
<li>Apresentam resultados para diferentes níveis de agregação.</li>
</ul>
<p>Desvantagens:</p>
<ul>
<li>Alocação de um objeto em um cluster é irrevogável</li>
<li>Impacto substancial dos outliers ( apesar do Ward ser o menos susceptível)</li>
<li>Não apropriados para analisar uma amostra muito extensa, pois a medida que o tamanho da amostra aumenta, a necessidade de armazenamento das distâncias cresce drasticamente</li>
</ul>
<p>Para bases grandes é melhor não usar este método pois precisa da matriz de distâncias.</p>
<p>Dentre os métodos, a menor distância pode ser ruim em muitas situações, pois coloca muitos objetos no mesmo cluster.</p>
<p>Geralmente utiliza-se o dendograma para a visualização dos clusters.</p>
<pre class="r"><code>#Construindo a matriz de similaridade:
matriz_similaridade = dist(iris[,-5],             #Conjunto de dados utilizados
                           &quot;euclidean&quot;            #medida de distância utilizada
                           )

#Construindo o agrupamento hierárquico aglomerativo:
agrupamento = hclust(matriz_similaridade,     #Matriz de similaridade calculada
                     &quot;ward.D&quot;                 #Método de agrupamento
                     )
#Converte hclust em dendrograma e plot:
hcd &lt;- as.dendrogram(agrupamento)

library(ggdendro)
# Tipo pode ser &quot;rectangle&quot; ou &quot;triangle&quot;
dend_data &lt;- dendro_data(hcd, type = &quot;rectangle&quot;)
# o que esta contido em dend_data:
names(dend_data)</code></pre>
<pre><code>## [1] &quot;segments&quot;    &quot;labels&quot;      &quot;leaf_labels&quot; &quot;class&quot;</code></pre>
<pre class="r"><code>plot(agrupamento,xlab=&quot;Matriz de similaridade&quot;,main = &quot;Dendograma&quot;, cex = 0.3)
#Construindo representacao de grupos - geração de vetores:
grupos = cutree(agrupamento,             #Variável calculada em hclust
                3                        #Quantidade de grupos desejados
                )

#Construindo o dendograma:
rect.hclust(agrupamento, k=3, border=&quot;red&quot;)</code></pre>
<p><img src="/post/2018-01-04-analise-multivariada-em-r/analise-multivariada-em-r_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Existem diversas outras maneiras de se visualizar dendogramas, veja a seguir um outro exemplo utilizando o pacote <code>ape</code>:</p>
<pre class="r"><code>library(ape)
plot(as.phylo(agrupamento), type = &quot;unrooted&quot;, cex = 0.6,
     no.margin = TRUE)</code></pre>
<p><img src="/post/2018-01-04-analise-multivariada-em-r/analise-multivariada-em-r_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Para mais informações de métodos de plot de dendogramas,talvez <a href="http://www.sthda.com/english/wiki/beautiful-dendrogram-visualizations-in-r-5-must-known-methods-unsupervised-machine-learning">essa página</a> possa ser útil.</p>
</div>
<div id="método-não-hierárquico-de-agrupamento-k-means" class="section level2">
<h2>Método não hierárquico de agrupamento K-means</h2>
<p>Esta é uma das mais populares abordagens de agrupamento de dados por partição. A partir de uma escolha inicial para os centroides, o algoritmo procede verificando quais exemplares são mais similares a quais centroides.</p>
<p>Vantagens:</p>
<ul>
<li>Tendem a maximizar a dispersão entre os centros de gravidade dos clusters (mantem os clusters bem separados)</li>
<li>Simplicidade de cálculo, calcula somente as distâncias entre os objetos e os centros de gravidade dos clusters</li>
</ul>
<p>Desvantagens:</p>
<ul>
<li>Depende dos conjuntos de sementes iniciais, principalmente se a seleção das sementes é aleatória</li>
<li>Não há garantias de um agrupamento ótimo dos objetos</li>
</ul>
<pre class="r"><code>#Construindo o agrupamento por particionamento:
c = kmeans(iris[,-5],          #Conjunto de dados utilizados
                 2,            #Número de  grupos a ser descoberto
                 iter.max=5    #Número máximo de iterações permitido no algorítmo
                 )</code></pre>
<p>Para efeito de visualização, podemos utilizar a seguinte função que encontra dois fatores principais a partir da análise fatorial e às utiliza como eixos</p>
<pre class="r"><code>plot_kmeans = function(df, clusters, runs) {
  suppressMessages(library(psych))
  suppressMessages(library(ggplot2))
  
  #cluster
  tmp_k = kmeans(df, centers = clusters, nstart = 100)
  
  #factor
  tmp_f = fa(df, 2, rotate = &quot;none&quot;)
  
  #collect data
  tmp_d = data.frame(matrix(ncol=0, nrow=nrow(df)))
  tmp_d$cluster = as.factor(tmp_k$cluster)
  tmp_d$fact_1 = as.numeric(tmp_f$scores[, 1])
  tmp_d$fact_2 = as.numeric(tmp_f$scores[, 2])
  tmp_d$label = rownames(df)
  
  #plot
  g = ggplot(tmp_d, aes(fact_1, fact_2, color = cluster)) + geom_point() + geom_text(aes(label = label), size = 3, vjust = 1, color = &quot;black&quot;)
  return(g)
}
plot_kmeans(iris[,-5], 3)</code></pre>
<p><img src="/post/2018-01-04-analise-multivariada-em-r/analise-multivariada-em-r_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<div id="análise-exploratória-dos-clusters" class="section level3">
<h3>Análise exploratória dos clusters</h3>
<p>Não vou me estender nessa parte, mas é bom esclarecer que após encontrar os clusters e de extrema importância realizar a análise exploratória deles para entender os comportamentos dos grupos identificados.</p>
<pre class="r"><code>#Conferindo os grupos formados:
c$cluster%&gt;%
  table()</code></pre>
<pre><code>## .
##  1  2 
## 53 97</code></pre>
<pre class="r"><code>c$cluster%&gt;%
  table()%&gt;%
  barplot(main=&quot;Frequências dos clusters&quot;, names.arg=c(&quot;Cluster 1&quot;, &quot;Cluster 2&quot;))</code></pre>
<p><img src="/post/2018-01-04-analise-multivariada-em-r/analise-multivariada-em-r_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
</div>
</div>
<div id="medidas-de-validação-e-estabilidade" class="section level2">
<h2>Medidas de validação e estabilidade</h2>
<div id="pseudo-f" class="section level4">
<h4>pseudo-F</h4>
<p>O número adequado de clusters (k) deve ser maximizar o pseudo-F:</p>
<p><span class="math display">\[
pseudo-F = \dfrac{  \dfrac{BSS}{k-1} } { \dfrac{WSS}{N-k}} =\dfrac{\textrm{Quadrado médio entre clusters}}{\textrm{Quadrado médio dentro dos clusters}}
\]</span></p>
</div>
<div id="libraryclvalid" class="section level4">
<h4>library(clvalid)</h4>
<p>Este pacote faz os cálculos das medidas que avaliam se os clusters são compactos, bem separados e estáveis.</p>
<p>Vejamos os tipos de medidas:</p>
<p><strong>Medidas de validação</strong>:</p>
<ol style="list-style-type: decimal">
<li>conectividade: relativa ao grau de vizinhança entre objetos em um mesmo cluster, varia
entre 0 e infinito e quanto menor melhor.</li>
<li>silhueta: homogeneidade interna, assume valores entre -1 e 1 e quanto mais próximo de 1
melhor.</li>
<li>índice de Dunn: quantifica a separação entre os agrupamentos, assume valores entre 0 e 1 e
quanto maior melhor.</li>
</ol>
<p><strong>Medidas de estabilidade</strong>:</p>
<ol style="list-style-type: decimal">
<li>APN - average proportion of non-overlap: proporção média de observações não
classificadas no mesmo cluster nos casos com dados completos e incompletos. Assume valor
no intervalo [0,1], próximos de 0 indicam agrupamentos consistentes.</li>
<li>AD - average distance: distância média entre observações classificadas no mesmo cluster
nos casos com dados completos e incompletos. Assume valores não negativos, sendo
preferíveis valores próximos de zero.</li>
<li>ADM - average distance between means: distância média entre os centroides quando as
observações estão em um mesmo cluster. Assume valores não negativos, sendo preferíveis
valores próximos de zero.</li>
<li>FOM - figure of merit: medida do erro cometido ao usar os centroides como estimativas das
observações na coluna removida. Assume valores não negativos, sendo preferíveis valores
próximos de zero.</li>
</ol>
<pre class="r"><code>library(clValid)

#Medidas de validação:
valida=clValid(iris[1:4],3,clMethods=c(&quot;hierarchical&quot;,&quot;kmeans&quot;),validation=&quot;internal&quot;)
summary(valida)</code></pre>
<pre><code>## 
## Clustering Methods:
##  hierarchical kmeans 
## 
## Cluster sizes:
##  3 
## 
## Validation Measures:
##                                  3
##                                   
## hierarchical Connectivity   4.4770
##              Dunn           0.1378
##              Silhouette     0.5542
## kmeans       Connectivity  10.0917
##              Dunn           0.0988
##              Silhouette     0.5528
## 
## Optimal Scores:
## 
##              Score  Method       Clusters
## Connectivity 4.4770 hierarchical 3       
## Dunn         0.1378 hierarchical 3       
## Silhouette   0.5542 hierarchical 3</code></pre>
<pre class="r"><code>#Medidas de estabilidade;
valida=clValid(iris[1:4],3,clMethods=c(&quot;hierarchical&quot;,&quot;kmeans&quot;),validation=&quot;stability&quot;)
summary(valida)</code></pre>
<pre><code>## 
## Clustering Methods:
##  hierarchical kmeans 
## 
## Cluster sizes:
##  3 
## 
## Validation Measures:
##                        3
##                         
## hierarchical APN  0.0912
##              AD   1.0596
##              ADM  0.3680
##              FOM  0.4209
## kmeans       APN  0.0630
##              AD   0.9390
##              ADM  0.1131
##              FOM  0.3935
## 
## Optimal Scores:
## 
##     Score  Method Clusters
## APN 0.0630 kmeans 3       
## AD  0.9390 kmeans 3       
## ADM 0.1131 kmeans 3       
## FOM 0.3935 kmeans 3</code></pre>
</div>
<div id="gráfico-da-silhueta" class="section level4">
<h4>Gráfico da silhueta:</h4>
<pre class="r"><code>library(cluster)
#Construindo a matriz de similaridade:
matriz_similaridade = dist(iris[,-5],             #Conjunto de dados utilizados
                           &quot;euclidean&quot;            #medida de distância utilizada
                           )

#Construindo o agrupamento hierárquico aglomerativo:
agrupamento = hclust(matriz_similaridade,     #Matriz de similaridade calculada
                     &quot;ward.D&quot;                 #Método de agrupamento
                     )

silhueta =silhouette(cutree(agrupamento,k=3),dist(iris[,-5]))
plot(silhueta,main=&quot;&quot;)</code></pre>
<p><img src="/post/2018-01-04-analise-multivariada-em-r/analise-multivariada-em-r_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<pre class="r"><code>summary(silhueta)</code></pre>
<pre><code>## Silhouette of 150 units in 3 clusters from silhouette.default(x = cutree(agrupamento, k = 3), dist = dist(iris[,  from     -5])) :
##  Cluster sizes and average silhouette widths:
##        50        64        36 
## 0.7994998 0.4115006 0.4670305 
## Individual silhouette widths:
##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -0.09013  0.39933  0.56701  0.55416  0.77690  0.85493</code></pre>
</div>
<div id="muitas-opções" class="section level3">
<h3>Muitas opções</h3>
<p>Como podemos observar, a análise de agrupamentos é um método exploratório. É útil para organizar conjuntos de dados que contam com características semelhantes.</p>
<p>É uma das principais técnicas da mineração de dados e já conta com grande variedade de algoritmos.</p>
</div>
</div>
</div>
<div id="referência" class="section level1">
<h1>Referência</h1>
<p><a href="https://www1.udel.edu/oiss/pdf/617.pdf">I Johnson e Wichern (2007). Applied Multivariate Statistical Analysis, 6th
Edition. Prentice-Hal</a></p>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2018-01-04-analise-multivariada-em-r/analise-multivariada-em-r/">Análise Multivariada com R</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Estatistica</category>
      <category>R</category>
      <category>Teoria</category>
      <category>Analise Mutivariada</category>
      <category>Nao supervisionado</category>
      <category>Clustering</category>
      <category domain="tag">Estatistica</category>
      <category domain="tag">gomesfellipe</category>
      <category domain="tag">R Markdown</category>
      <category domain="tag">R</category>
      <category domain="tag">Teoria</category>
      <category domain="tag">pca</category>
      <category domain="tag">kmeans</category>
      <category domain="tag">clustering</category>
      <category domain="tag">analise multivariada</category>
    </item>
    <item>
      <title>Pacotes do R para avaliar o ajuste de modelos</title>
      <link>https://gomesfellipe.github.io/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos/</link>
      <pubDate>Sun, 24 Dec 2017 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos/</guid>
      <description>Alguns pacotes úteis para avaliar o ajuste do modelo de forma rápida, precisa e elegante</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="funções-do-r-para-avaliar-o-ajuste-de-modelos" class="section level1">
<h1>Funções do R para avaliar o ajuste de modelos</h1>
<p>Traduzindo:</p>
<p>“<em>Essencialmente, todos os modelos estão errados, mas alguns são úteis</em>” - George E. P. Box</p>
<p>Se você estuda estatística provavelmente já deve saber quem é este simpático senhor. Box teve grande contribuição para a estatística. Foi aluno do Ronald Aylmer Fisher e ainda se casou com a filha dele!</p>
<p>Lendo um <a href="http://jaguar.fcav.unesp.br/RME/fasciculos/v27/v27_n4/A10_Millor.pdf">artigo sobre a vida de Fisher</a> um parágrafo me chamou atenção com uma fala de sua filha, que dizia o seguinte:</p>
<p>“Joan Fisher Box, filha de Fisher, em seu livro sobre a vida do pai, se referindo à péssima classificação dele em francês, escreveu: “… ele nunca teve muita paciência com irrelevâncias.” (Box, 1978)"</p>
<p>Fico imaginando o tamanho da contribuição desdes crânios para a comunidade se tivessem acesso a tantos mecanismos que temos hoje em dia e o que eles achariam relevantes..</p>
<p>Para o bom ajuste de um modelo, certamente; a inferência, as análises de desvios, os critérios de seleção de um modelo, conferir comportamento dos resíduos e avaliação das estatísticas de diagnósticos são muito relevantes.</p>
<p>No <a href="https://cran.r-project.org/">CRAN</a> já contamos com muitos pacotes disponíveis para nos auxiliar nessas avaliações, portanto vou mostrar aqui alguns pacotes com funções que já me ajudaram muito em avaliações de modelos indo além das funções nativas do R e do pacote <code>ggplot2</code> (Um excelente pacote para apresentações elegantes e práticas de resultados visuais).</p>
</div>
<div id="ggally" class="section level1">
<h1>GGally</h1>
<p>Este pacote é sensacional, existem funções muito relevantes nele para melhorar a nossa experiência com ajuste de modelos, as funções apresentadas aqui são baseadas na <a href="http://ggobi.github.io/ggally/#ggally">página de documentação GGally</a>, lá você pode conferir a documentação completa.</p>
<p>Primeiramente vamos carregar o pacote:</p>
<pre class="r"><code>library(GGally)</code></pre>
<p>Carregado o pacote, vejamos as principais funções que podem nos auxiliar.</p>
<div id="ggallyggcoef" class="section level2">
<h2><code>GGally::ggcoef</code></h2>
<p>O objetivo da função <code>GGally::ggcoef</code> é traçar rapidamente os coeficientes de um modelo.</p>
<p>Para um modelo linear:</p>
<pre class="r"><code>reg &lt;- lm(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width, data = iris)
ggcoef(reg)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Para um modelo logístico podemos utilizar o argumento <code>exponentiate = TRUE</code> e além disso, somos capazes de fazer diversas alterações no gráfico utilizando o <code>ggcoef()</code> veja alguns exemplo de argumentos que podem ser usados para personalizar como barras de erro e a linha vertical são plotadas:</p>
<pre class="r"><code>#Ajustando o modelo:
d &lt;- as.data.frame(Titanic)
log.reg &lt;- glm(Survived ~ Sex + Age + Class, family = binomial, data = d, weights = d$Freq)

#Elaborando o gráfico
ggcoef(
  log.reg,                      #O modelo a ser conferido
  exponentiate = TRUE,          #Para avaliar o modelo logístico
  vline_color = &quot;red&quot;,          #Reta em zero  
  #vline_linetype =  &quot;solid&quot;,   #Altera a linha de referência
  errorbar_color = &quot;blue&quot;,      #Cor da barra de erros
  errorbar_height = .25,
  shape = 18,                   #Altera o formato dos pontos centrais
  #size=3,                      #Altera o tamanho do ponto
  color=&quot;black&quot;,                #Altera a cor do ponto
  mapping = aes(x = estimate, y = term, size = p.value))+
  scale_size_continuous(trans = &quot;reverse&quot;) #Essa linha faz com que inverta o tamanho                 </code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="ggallyggduo" class="section level2">
<h2><code>GGally::ggduo</code></h2>
<p>O objetivo desta função é exibir dois dados agrupados em uma matriz de plotagem. Isso é útil para análise de correlação canônica, análise de séries temporais múltiplas e análise de regressão.</p>
<p>Os dados do exemplo apresentados aqui podem ser encontrados neste <a href="http://www.stats.idre.ucla.edu/r/dae/canonical-correlation-analysis">link</a></p>
<pre class="r"><code>data(psychademic)
head(psychademic)</code></pre>
<pre><code>##   locus_of_control self_concept motivation read write math science    sex
## 1            -0.84        -0.24          4 54.8  64.5 44.5    52.6 female
## 2            -0.38        -0.47          3 62.7  43.7 44.7    52.6 female
## 3             0.89         0.59          3 60.6  56.7 70.5    58.0   male
## 4             0.71         0.28          3 62.7  56.7 54.7    58.0   male
## 5            -0.64         0.03          4 41.6  46.3 38.4    36.3 female
## 6             1.11         0.90          2 62.7  64.5 61.4    58.0 female</code></pre>
<pre class="r"><code>psych_variables &lt;- attr(psychademic, &quot;psychology&quot;)
academic_variables &lt;- attr(psychademic, &quot;academic&quot;)</code></pre>
<pre class="r"><code>ggduo(
  psychademic, psych_variables, academic_variables,
  types = list(continuous = &quot;smooth_lm&quot;),
  title = &quot;Correlação entre as variáveis psicológicas e academicas&quot;,
  xlab = &quot;Psicológicos&quot;,
  ylab = &quot;Academicas&quot;
)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Uma vez que o <code>ggduo</code> não tem uma seção superior para exibir os valores de correlação, podemos usar uma função personalizada para adicionar a informação nas parcelas contínuas.</p>
<p>Criando uma função personalizada para informar a correlação entre as observações:</p>
<pre class="r"><code>lm_with_cor &lt;- function(data, mapping, ..., method = &quot;pearson&quot;) {
  x &lt;- eval(mapping$x, data)
  y &lt;- eval(mapping$y, data)
  cor &lt;- cor(x, y, method = method)
  ggally_smooth_lm(data, mapping, ...) +
    ggplot2::geom_label(
      data = data.frame(
        x = min(x, na.rm = TRUE),
        y = max(y, na.rm = TRUE),
        lab = round(cor, digits = 3)
      ),
      mapping = ggplot2::aes(x = x, y = y, label = lab),
      hjust = 0, vjust = 1,
      size = 5, fontface = &quot;bold&quot;,
      inherit.aes = FALSE # do not inherit anything from the ...
    )
}</code></pre>
<p>Portanto:</p>
<pre class="r"><code>ggduo(
  psychademic, psych_variables, academic_variables,
  types = list(continuous = &quot;smooth_lm&quot;),
  title = &quot;Correlação entre variáveis acadêmica e psicológica&quot;,
  xlab = &quot;Psicológica&quot;,
  ylab = &quot;Academica&quot;
)+
  theme(legend.position = &quot;bottom&quot;)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Para avaliar resíduos da uma regressão ajustada para cada uma das variáveis explanatórias vs. as variáveis explanatórias:</p>
<pre class="r"><code>dados &lt;- datasets::swiss

# Criando uma coluna &quot;fake&quot;:
dados$Residual &lt;- seq_len(nrow(dados))

# Calculando todos os resíduos que serão exibidos:
colunas=2:6  #Informe as colunas que contem as variaveis explanatorias
residuals &lt;- lapply(dados[colunas], function(x) {
  summary(lm(Fertility ~ x, data = dados))$residuals
})
# Calculando um intervalo constante para todos os resíduos
y_range &lt;- range(unlist(residuals))

# Função modificada para mostrar os resíduos:

lm_or_resid &lt;- function(data, mapping, ..., line_color = &quot;red&quot;, line_size = 1) {
  if (as.character(mapping$y) != &quot;Residual&quot;) {
    return(ggally_smooth_lm(data, mapping, ...))
  }

  # Criando os resíduos para apresentar:
  resid_data &lt;- data.frame(
    x = data[[as.character(mapping$x)]],
    y = residuals[[as.character(mapping$x)]]
  )

  ggplot(data = data, mapping = mapping) +
    geom_hline(yintercept = 0, color = line_color, size = line_size) +
    ylim(y_range) +
    geom_point(data = resid_data, mapping = aes(x = x, y = y), ...)

}

# Plote os dados:
ggduo(
  dados,
  2:6, c(1,7),
  types = list(continuous = lm_or_resid)
)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
</div>
<div id="ggallyggnostic" class="section level2">
<h2><code>GGally::ggnostic</code></h2>
<p>O <code>ggnostic</code> é um wrapper de exibição para <code>ggduo</code> que exibe diagnósticos de modelo completo para cada variável explicativa dada.</p>
<p>Por padrão, o ggduo exibe os valores residuais, o sigma do modelo de “leave-one-out”, os pontos de alavanca e a distância de Cook em relação a cada variável explicativa.</p>
<p>As linhas da matriz de plotagem podem ser expandidas para incluir valores ajustados, erro padrão dos valores ajustados, resíduos padronizados e qualquer uma das variáveis de resposta.</p>
<p>Se o modelo for um modelo linear, os asteriscos (*) são adicionados de acordo com a significância anova de cada variável explicativa.</p>
<p>A maioria das parcelas diagnósticas contêm linhas de referência para ajudar a determinar se o modelo está adequadamente instalado</p>
<p>Olhando para os conjuntos de dados do conjunto de dados <code>state.x77</code> ajustaremos um modelo de regressão múltipla para a expectativa de vida.</p>
<pre class="r"><code>#Dados que serão utilizados no exemplos:
state &lt;- as.data.frame(state.x77)
#Arrumando o nome das variaveis:
colnames(state)[c(4, 6)] &lt;- c(&quot;Life.Exp&quot;, &quot;HS.Grad&quot;)
# Ajustando o modelo completo:
model &lt;- lm(Life.Exp ~ ., data = state)
# Executando o stepwise para encontrar o melhor ajuste
model &lt;- step(model, trace = FALSE)</code></pre>
<p>Executando o diagnóstico deste modelo com a função <code>ggnostic()</code>:</p>
<pre class="r"><code># look at model diagnostics
ggnostic(model)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Para acessar as variáveis influentes do modelo podemos utilizar a função <code>influence.measures()</code>, veja:</p>
<pre class="r"><code>summary(influence.measures(model))</code></pre>
<pre><code>## Potentially influential observations of
##   lm(formula = Life.Exp ~ Population + Murder + HS.Grad + Frost,      data = state) :
## 
##            dfb.1_ dfb.Pplt dfb.Mrdr dfb.HS.G dfb.Frst dffit   cov.r   cook.d
## Alaska      0.41   0.18    -0.40    -0.35    -0.16    -0.50    1.36_*  0.05 
## California  0.04  -0.09     0.00    -0.04     0.03    -0.12    1.81_*  0.00 
## Hawaii     -0.03  -0.57    -0.28     0.66    -1.24_*   1.43_*  0.74    0.36 
## Nevada      0.40   0.14    -0.42    -0.29    -0.28    -0.52    1.46_*  0.05 
## New York    0.01  -0.06     0.00     0.00    -0.01    -0.07    1.44_*  0.00 
##            hat    
## Alaska      0.25  
## California  0.38_*
## Hawaii      0.24  
## Nevada      0.29  
## New York    0.23</code></pre>
<p>Esta função retorna as seguintes estatísticas:</p>
<table>
<colgroup>
<col width="23%" />
<col width="25%" />
<col width="17%" />
<col width="17%" />
<col width="17%" />
</colgroup>
<thead>
<tr class="header">
<th>DFBeta</th>
<th>DFFit</th>
<th>CovRatio</th>
<th>D.Cook</th>
<th>h</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Alteração no vetor estimado <span class="math inline">\(\hat \beta\)</span> ao se retirar o i-ésimo ponto da análise</td>
<td>Alteração provocada no valor ajustado pela retirada da observação <span class="math inline">\(i\)</span></td>
<td>Expressa o relação de covariancia</td>
<td>Medida de afastamento das estimativas ao retirar <span class="math inline">\(i\)</span> e também considera o resíduo estudentizado internamente</td>
<td>Elementos da diagonal da matriz H</td>
</tr>
</tbody>
</table>
<p>Vejamos então um exemplo de matriz de matriz de diagnóstico completo.</p>
<p>As seguintes linhas de código exibirão uma matriz de diagnóstico para o mesmo modelo:</p>
<pre class="r"><code>#Ajustando um modelo de exemplo:
flea_model &lt;- step(lm(head ~ ., data = flea), trace = FALSE)</code></pre>
<p>Todas as colunas possíveis e usando <code>ggally_smooth()</code> para exibir os pontos ajustados e as variáveis de resposta temos:</p>
<pre class="r"><code># default output
ggnostic(flea_model,
 #        mapping = ggplot2::aes(color = species),  #Para colorir segundo um fator
         columnsY = c(&quot;head&quot;, &quot;.fitted&quot;, &quot;.se.fit&quot;, &quot;.resid&quot;, &quot;.std.resid&quot;, &quot;.hat&quot;, &quot;.sigma&quot;, &quot;.cooksd&quot;),
        continuous = list(default = ggally_smooth, .fitted = ggally_smooth)
)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
<div id="ggallyggpairs" class="section level2">
<h2><code>GGally::ggpairs</code></h2>
<p>O <code>ggpairs</code> é uma forma especial de uma ggmatrix que produz uma comparação pairwise de dados multivariados. Por padrão, o ggpairs fornece duas comparações diferentes de cada par de colunas e exibe a densidade ou a contagem da variável respectiva ao longo da diagonal. Com diferentes configurações de parâmetros, a diagonal pode ser substituída pelos valores do eixo e rótulos variáveis.</p>
<pre class="r"><code>#Funcao de correlacoes
my_fn &lt;- function(data, mapping, method=&quot;lm&quot;, ...){
  p &lt;- ggplot(data = data, mapping = mapping) + 
    geom_point() + 
    geom_smooth(method=method, ...)
  p
}
data(tips, package = &quot;reshape&quot;)
#Correlaçoes cruzadas
ggpairs(tips, lower = list(continuous = my_fn))</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Existem muitos recursos ocultos dentro dos <code>ggpairs()</code> e muitos exemplos podem ser conferidos na internet para obter o máximo do <code>ggpairs()</code>.</p>
</div>
<div id="ggallyggscatmat" class="section level2">
<h2><code>GGally::ggscatmat</code></h2>
<p>A principal função é <code>ggscatmat</code>. É semelhante a <code>ggpairs()</code>, mas funciona apenas para dados multivariados puramente numéricos.</p>
<p>É mais rápido que ggpairs, porque é necessário fazer menos escolhas.</p>
<p>Ele cria uma matriz com diagramas de dispersão na diagonal inferior, densidades na diagonal e correlações escritas na diagonal superior.</p>
<p>A sintaxe é inserir o conjunto de dados, as colunas que deseja traçar, uma coluna de cores e um nível alfa.</p>
<pre class="r"><code>data(flea)
ggscatmat(flea, columns = 2:4, color=&quot;species&quot;, alpha=0.8)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
</div>
</div>
<div id="ggfottify" class="section level1">
<h1>ggfottify</h1>
<p>Outra opção interessante para avaliar o ajuste dos modelos é o pacote <a href="https://cran.r-project.org/web/packages/ggfortify/index.html">ggfottify</a>. Ele disponibiliza uma interface de traçado (como a função <code>plot(modelo_ajustado)</code>) de análise e gráficos em um estilo unificado, porém usando <code>ggplot2</code>.</p>
<p>Vamos então dar início carregando o pacote:</p>
<pre class="r"><code>library(ggfortify)</code></pre>
<p>Veja a seguir alguns dos gráficos disponíveis no R para a análise de resíduos:</p>
<pre class="r"><code>autoplot(flea_model, which = 1:6, ncol = 3, label.size = 3)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>Especificando as opções de plot</p>
<p>Algumas propriedades desses gráficos podem ser alteradas. Por exemplo, a opção <code>colour = 'dodgerblue3'</code> é para pontos de dados, o <code>smooth.colour = 'black'</code> é para linhas de suavização e <code>ad.colour = 'blue'</code> é para opções adicionais.</p>
<p>Veja ainda que ncol e nrow controlam o layout.</p>
<pre class="r"><code>autoplot(flea_model, which = 1:6, colour = &#39;dodgerblue3&#39;,
         smooth.colour = &#39;black&#39;, smooth.linetype = &#39;dashed&#39;,
         ad.colour = &#39;blue&#39;,
         label.size = 3, label.n = 5, label.colour = &#39;blue&#39;,
         ncol = 3)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>Além disso, você pode usar nomes de colunas para essas propriedades, vamos separar os grupos de machos e fêmeas por cores:</p>
<pre class="r"><code>autoplot(flea_model, which = 1:6, data = flea,
         colour = &#39;species&#39;, label.size = 3,
         ncol = 3)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>O que será que os crânios da estatística fariam diante de tantos recursos?</p>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos/">Pacotes do R para avaliar o ajuste de modelos</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Estatistica</category>
      <category>Modelagem Estatistica</category>
      <category>R</category>
      <category>Teoria</category>
      <category>Tidyverse</category>
      <category domain="tag">gomesfellipe</category>
      <category domain="tag">R</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">modelagem estatistica</category>
      <category domain="tag">Estatistica</category>
      <category domain="tag">Correlacoes</category>
      <category domain="tag">R Markdown</category>
      <category domain="tag">regression</category>
      <category domain="tag">Teoria</category>
      <category domain="tag">modelos lineares</category>
      <category domain="tag">modelos generalizados</category>
      <category domain="tag">ggfortify</category>
      <category domain="tag">GGally</category>
    </item>
  </channel>
</rss>