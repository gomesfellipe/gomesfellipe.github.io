&lt;?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>modelagem on Fellipe Gomes - Data Science Blog</title>
    <link>https://gomesfellipe.github.io/tags/modelagem/</link>
    <description>√öltimos posts sobre Data Science, Machine Learning e R</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>pt-br</language>
    <managingEditor>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</managingEditor>
    <webMaster>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</webMaster>
    <lastBuildDate>Tue, 30 May 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://gomesfellipe.github.io/tags/modelagem/" rel="self" type="application/rss+xml" />
    <item>
      <title>Solu√ß√£o Final - ML Olympiad [1¬∫ lugar]</title>
      <link>https://gomesfellipe.github.io/post/2023-05-30-solucao-final-ensure-healthy-lives-kaggle-competition/</link>
      <pubDate>Tue, 30 May 2023 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2023-05-30-solucao-final-ensure-healthy-lives-kaggle-competition/</guid>
      <description>Confira a estrat√©gia aplicada para esta competi√ß√£o</description>
      <content:encoded>&lt;![CDATA[
        
<link href="https://gomesfellipe.github.io/post/2023-05-30-solucao-final-ensure-healthy-lives-kaggle-competition/index_files/vembedr/css/vembedr.css" rel="stylesheet" />

<div id="TOC">
<ul>
<li><a href="#introdu%C3%A7%C3%A3o" id="toc-introdu√ß√£o">Introdu√ß√£o</a></li>
<li><a href="#defini%C3%A7%C3%A3o-do-problema-de-neg%C3%B3cio" id="toc-defini√ß√£o-do-problema-de-neg√≥cio">Defini√ß√£o do problema de neg√≥cio</a></li>
<li><a href="#solu%C3%A7%C3%B5es" id="toc-solu√ß√µes">Solu√ß√µes</a></li>
<li><a href="#estrat%C3%A9gia-anal%C3%ADtica" id="toc-estrat√©gia-anal√≠tica">Estrat√©gia anal√≠tica</a>
<ul>
<li><a href="#decis%C3%B5es-sobre-a-target" id="toc-decis√µes-sobre-a-target">Decis√µes sobre a target</a></li>
<li><a href="#processamento-dos-dados" id="toc-processamento-dos-dados">Processamento dos Dados</a></li>
<li><a href="#dados-externos" id="toc-dados-externos">Dados Externos</a></li>
<li><a href="#feature-engineering" id="toc-feature-engineering">Feature Engineering</a></li>
<li><a href="#modelos" id="toc-modelos">Modelos</a></li>
<li><a href="#ensemble" id="toc-ensemble">Ensemble</a></li>
<li><a href="#post-processing" id="toc-post-processing">Post Processing</a></li>
</ul></li>
<li><a href="#considera%C3%A7%C3%B5es-finais" id="toc-considera√ß√µes-finais">Considera√ß√µes Finais</a></li>
<li><a href="#sobre-o-autor" id="toc-sobre-o-autor">Sobre o Autor</a></li>
</ul>
</div>

<div id="introdu√ß√£o" class="section level1">
<h1>Introdu√ß√£o</h1>
<p>O <a href="https://www.meetup.com/TensorFlowSP/events/284607061/">TFUG - TensorFlow Users Group de S√£o Paulo</a> lan√ßou uma nova <a href="https://www.kaggle.com/competitions/ml-olympiad-ensure-healthy-lives">competi√ß√£o no Kaggle</a> onde o objetivo era desenvolver modelos para previs√£o de diagn√≥stico de s√≠ndromes respirat√≥rias, que √© um tema relacionado com um dos 17 t√≥picos de Desenvolvimento Sustent√°vel das Na√ß√µes Unidas - <em>Boa sa√∫de e bem-estar</em>.</p>
<p>Como um cientista de dados, acredito que seja muito importante continuarmos aprimorando nossas habilidades e conhecimentos. Competi√ß√µes como essa s√£o muito divertidas e possibilitam que testemos nossos limites em um ambiente competitivo e colaborativo, al√©m de ser uma grande oportunidade para nos desafiarmos e aprender uns com os outros.</p>
<p>Tive o enorme prazer de conquistar o primeiro lugar, dessa vez com meu grande amigo <a href="https://www.linkedin.com/in/kaike-wesley-reis">Kaike</a>, parceiro de competi√ß√µes de longa data que trouxe grande sinergia para a <a href="https://www.kaggle.com/code/gomes555/ml-olypiads-1-lugar-blending">solu√ß√£o final</a> com a contribui√ß√£o de seu modelo (compartilhado abertamente no Kaggle).</p>
<p>Aqui est√£o alguns dos pr√™mios recebidos:</p>
<center>
<img src="/post/2023-05-30-solucao-final-ensure-healthy-lives-kaggle-competition/premio2.png" style="width:80.0%" />
</center>
<p>Como nesta competi√ß√£o havia bastante trabalho a ser feito e tivemos apenas 1 m√™s para trabalhar na solu√ß√£o, foi preciso fazer uma boa gest√£o do c√≥digo e do tempo de desenvolvimento.</p>
</div>
<div id="defini√ß√£o-do-problema-de-neg√≥cio" class="section level1">
<h1>Defini√ß√£o do problema de neg√≥cio</h1>
<p>O objetivo desta competi√ß√£o consistiu em predizer qual o agente causador da s√≠ndrome respirat√≥ria aguda grave com base nos dados e sintomas dos pacientes.</p>
<p>Esta tarefa pode ser enquadrada como um problema supervisionado de classifica√ß√£o multinomial (com m√∫ltiplos outputs) na qual as previs√µes s√£o, de certa forma, dependentes da entrada umas das outras (o paciente s√≥ pode ter registrado uma das doen√ßas).</p>
<p>A valida√ß√£o da solu√ß√£o foi feita utilizando a m√©trica Macro (or Mean) F1-Score, que √© basicamente a m√©dia do F1 calculado sobre as previs√µes de cada nota.</p>
</div>
<div id="solu√ß√µes" class="section level1">
<h1>Solu√ß√µes</h1>
<p>Ambas solu√ß√µes (minha e do Kaike) foram compartilhadas no Kaggle:</p>
<ul>
<li><a href="https://www.kaggle.com/code/gomes555/ml-olympiad-1-lugar-catboost-pos-process">ML Olympiad - 1¬∫ Lugar - Catboost + Pos Process</a> (Fellipe)</li>
<li><a href="https://www.kaggle.com/code/kaikewreis/ml-olypiads-1-lugar-lightgbm-binary-ensemble">ML Olypiads - 1¬∫ Lugar - LightGBM Binary Ensemble</a> (Kaike)</li>
<li><a href="https://www.kaggle.com/code/gomes555/ml-olympiad-1-lugar-blending">ML Olympiad - 1¬∫ Lugar - Blending</a> (combina√ß√£o das solu√ß√µes em um emsemble)</li>
</ul>
<p>Disponibilizamos tamb√©m a solu√ß√£o em formato de v√≠deo, gravado em um meetup com dura√ß√£o de 1 hora e meia para o canal do <a href="https://www.youtube.com/@tensorflowugsp">TensorFlow UGSP</a> no Youtube no link: <a href="https://youtu.be/6HPJn38NF3w" class="uri">https://youtu.be/6HPJn38NF3w</a></p>
<center>
<div class="vembedr">
<div>
<iframe src="https://www.youtube.com/embed/6HPJn38NF3w" width="533" height="300" frameborder="0" allowfullscreen="" data-external="1"></iframe>
</div>
</div>
</center>
</div>
<div id="estrat√©gia-anal√≠tica" class="section level1">
<h1>Estrat√©gia anal√≠tica</h1>
<p>Nas se√ß√µes abaixo apresento o racional por tr√°s da minha solu√ß√£o, como chegamos nos 5 melhores modelos individuais (para cada doen√ßa respirat√≥ria) que utilizei em um ensemble para chegar ao primeiro lugar, bem como a estrat√©gia de p√≥s processamento que com que o score melhorasse significativamente.</p>
<div id="decis√µes-sobre-a-target" class="section level2">
<h2>Decis√µes sobre a target</h2>
<p>A primeira decis√£o importante era definir como enquadrar o problema; se utilizar√≠amos 1 modelo multiclasse ou diferentes modelos para cada classe.</p>
<p>Em todos os testes que fizemos, os modelos individuais superaram o F1-Score Macro de um modelo √∫nico. Como 3 das classes eram bastante desbalanceadas, acredito que modelos especializados nesses casos conseguiram captar melhor suas nuances.</p>
</div>
<div id="processamento-dos-dados" class="section level2">
<h2>Processamento dos Dados</h2>
<p>Como optamos por unificar os resultados apenas na reta final, meu pr√©-processamento foi muito diferente do feito pelo Kaike e isso foi fundamental para que as estimativas dos nossos modelos tivessem baixa correla√ß√£o. N√£o focarei aqui no meu pr√©-processamento, pois n√£o acho que foi o diferencial para atingir um score superior a 0.6 (quem tiver curiosidade est√° tudo bem documentado nos notebooks compartilhados).</p>
</div>
<div id="dados-externos" class="section level2">
<h2>Dados Externos</h2>
<p>O fato de n√£o termos as informa√ß√µes do ano em que esses dados foram coletados dificultou na busca de bases externas, pois indicadores socioecon√¥micos e de sa√∫de variam bastante ao longo do tempo.</p>
<p>Fizemos alguns testes utilizando o <a href="https://basedosdados.org/dataset/mundo-onu-adh">Atlas do Desenvolvimento Humano (ADH)</a>, mas n√£o tivemos muito sucesso, pois esses dados est√£o muito defasados (1991-2010). Tamb√©m tentamos acrescentar a informa√ß√£o de <a href="https://github.com/kelvins/Municipios-Brasileiros/">latitude e longitude de cada munic√≠pio</a>, mas isso n√£o trouxe uma melhora substancial no nosso score.</p>
</div>
<div id="feature-engineering" class="section level2">
<h2>Feature Engineering</h2>
<p>Outra etapa em que investimos bastante tempo foi para criar novas vari√°veis.</p>
<p>Novamente, nossa engenharia de recursos foi feita de maneira separada para que nossos modelos aprendessem aspectos diferentes dos dados. Abaixo, compartilho algumas das features que desenvolvi apenas para o meu modelo:</p>
<ul>
<li>Presen√ßa de sintomas relacionados √† Target;</li>
<li>Se tomografia era t√≠pica do COVID;</li>
<li>Intervalo de idade com mais casos;</li>
<li>Idade discretizada;</li>
<li>Diferen√ßa entre a semana de notifica√ß√£o e primeiros sintomas;</li>
<li>Novas features baseadas nas contagens de algumas features categ√≥ricas;</li>
<li>etc.</li>
</ul>
</div>
<div id="modelos" class="section level2">
<h2>Modelos</h2>
<p>Al√©m de pr√©-processamentos e feature engineering diferentes, tamb√©m utilizamos modelos e mecanismos de tunning diferentes, o que ajudou para que nossas estimativas tivessem baixa correla√ß√£o. Eu usei o Catboost como modelo final, j√° o Kaike optou por um LightGBM com tuning de hiperparametros.</p>
</div>
<div id="ensemble" class="section level2">
<h2>Ensemble</h2>
<p>Calculamos a m√©dia das probabilidades previstas de cada modelo para cada classe antes de selecionar a classe que tivesse a maior probabilidade.</p>
<p>Como nossas previs√µes tinham baixa correla√ß√£o, conseguimos ser bem sucedidos no ensemble combinando nossas submiss√µes com score ~0.6 alcan√ßando ~0.61 na tabela p√∫blica.</p>
</div>
<div id="post-processing" class="section level2">
<h2>Post Processing</h2>
<p>Acredito que o <strong>diferencial</strong> dessa competi√ß√£o estava no p√≥s processamento.</p>
<p>Quando avaliamos o score do modelo de cada classe, tamb√©m calculamos um threshold que maximizava os respectivos F1.</p>
<p>Observamos que nosso modelo para a classe 5 apresentava um F1 muito superior √†s demais classes com esse threshold otimizado, ent√£o fizemos o seguinte:</p>
<ol style="list-style-type: decimal">
<li>Calculamos as probabilidades individuais para cada classe;</li>
<li>Selecionamos a classe que tinha maior probabilidade estimada em cada inst√¢ncia;</li>
<li>Pegamos a classifica√ß√£o bin√°ria da classe 5 com o threshold otimizado e aplicamos a seguinte condi√ß√£o: Se o modelo da classe 5 estimou que y5[i]==1, ent√£o yfinal[i] √© 5, caso contr√°rio, use a classe de maior probabilidade entre as outras 4. (Em outras palavras: <code>np.where(y5_test_class==1, 5, sub.CLASSI_FIN)</code>)</li>
</ol>
</div>
</div>
<div id="considera√ß√µes-finais" class="section level1">
<h1>Considera√ß√µes Finais</h1>
<p>Foi uma competi√ß√£o muito interessante e desafiadora. Agrade√ßo imensamente ao <a href="https://www.meetup.com/TensorFlowSP/events/284607061/">TFUG</a> por organizar o evento e a todos os participantes que contribu√≠ram para o aprendizado coletivo.Foi uma √≥tima oportunidade de aprendizado e troca de experi√™ncias.</p>
<p>Espero que minha solu√ß√£o possa ser √∫til para outros projetos e desafios futuros.</p>
</div>
<div id="sobre-o-autor" class="section level1">
<h1>Sobre o Autor</h1>
<p>Me chamo Fellipe Gomes, sou cientista de dados e apaixonado por aprendizado de m√°quina. Compartilho meu conhecimento por meio de artigos, tutoriais e projetos de c√≥digo aberto. Se quiser saber mais sobre meu trabalho, sinta-se √† vontade para conferir meu <a href="https://www.linkedin.com/in/fellipe-gomes-06943264/">LinkedIn</a> e <a href="https://github.com/fellipe-gomes">GitHub</a>.</p>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2023-05-30-solucao-final-ensure-healthy-lives-kaggle-competition/">Solu√ß√£o Final - ML Olympiad [1¬∫ lugar]</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">catboost</category>
      <category domain="tag">classification</category>
      <category domain="tag">data-mining</category>
      <category domain="tag">data-science</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">machine-learning</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">pratica</category>
      <category domain="tag">r</category>
    </item>
    <item>
      <title>Solu√ß√£o Final - ML Olympiad [2¬∫ lugar]</title>
      <link>https://gomesfellipe.github.io/post/2022-04-20-solucao-final-education-quality-kaggle-competition/</link>
      <pubDate>Tue, 19 Apr 2022 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2022-04-20-solucao-final-education-quality-kaggle-competition/</guid>
      <description>Confira a estrat√©gia aplicada para esta competi√ß√£o</description>
      <content:encoded>&lt;![CDATA[
        

<div id="TOC">
<ul>
<li><a href="#introdu%C3%A7%C3%A3o" id="toc-introdu√ß√£o">Introdu√ß√£o</a></li>
<li><a href="#defini%C3%A7%C3%A3o-do-problema-de-neg%C3%B3cio" id="toc-defini√ß√£o-do-problema-de-neg√≥cio">Defini√ß√£o do problema de neg√≥cio</a></li>
<li><a href="#an%C3%A1lise-explorat%C3%B3ria-em-r" id="toc-an√°lise-explorat√≥ria-em-r">An√°lise Explorat√≥ria (em R)</a>
<ul>
<li><a href="#estrutura-da-base" id="toc-estrutura-da-base">Estrutura da base</a></li>
<li><a href="#ano-da-base-de-dados" id="toc-ano-da-base-de-dados">Ano da base de dados</a></li>
<li><a href="#target" id="toc-target">Target</a></li>
</ul></li>
<li><a href="#machine-learning-em-python" id="toc-machine-learning-em-python">Machine Learning (em Python)</a>
<ul>
<li><a href="#importar-dependencias" id="toc-importar-dependencias">Importar dependencias</a></li>
<li><a href="#carregar-dados" id="toc-carregar-dados">Carregar dados</a></li>
<li><a href="#modelagem" id="toc-modelagem">Modelagem</a></li>
</ul></li>
<li><a href="#submiss%C3%A3o" id="toc-submiss√£o">Submiss√£o</a></li>
<li><a href="#considera%C3%A7%C3%B5es-finais" id="toc-considera√ß√µes-finais">Considera√ß√µes Finais</a></li>
</ul>
</div>

<div id="introdu√ß√£o" class="section level1">
<h1>Introdu√ß√£o</h1>
<p>No final de Janeiro desde ano (2022) o <a href="https://www.meetup.com/TensorFlowSP/events/284607061/">TFUG - TensorFlow Users Group de S√£o Paulo</a> lan√ßou uma competi√ß√£o no Kaggle para prever as notas do enem que tem rela√ß√£o com um dos 17 t√≥picos de Desenvolvimento Sustent√°vel das Na√ß√µes Unidas - <em>Educa√ß√£o de Qualidade</em>.</p>
<p>Al√©m de divertido, o desafio foi repleto de possibilidades e bastante desafiador! Todos os competidores que trabalharam duro em pleno m√™s de carnaval est√£o de parab√©ns! üòÖ üòÇ</p>
<p>Aqui est√£o alguns dos pr√™mios recebidos:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/premio.png" style="width:80.0%" />
</center>
<p>Como nesta competi√ß√£o havia bastante trabalho a ser feito e tivemos apenas 1 m√™s para trabalhar na solu√ß√£o, foi preciso fazer uma boa gest√£o do c√≥digo e do tempo de desenvolvimento.</p>
<p>Nas se√ß√µes abaixo apresento o racional por tr√°s da minha solu√ß√£o bem como os 5 melhores modelos individuais (para cada nota) que utilizei em um ensemble para chegar ao segundo lugar.</p>
</div>
<div id="defini√ß√£o-do-problema-de-neg√≥cio" class="section level1">
<h1>Defini√ß√£o do problema de neg√≥cio</h1>
<p>O objetivo desta competi√ß√£o consistiu em prever as notas dos alunos(as) nas provas: Ci√™ncias da Natureza, Ci√™ncias Humanas, Linguagens e C√≥digos, Matem√°tica e Reda√ß√£o.</p>
<p>Apesar das notas serem calculadas de maneira independente, a partir de modelos de <a href="http://portal.mec.gov.br/ultimas-noticias/389-ensino-medio-2092297298/17319-teoria-de-resposta-ao-item-avalia-habilidade-e-minimiza-o-chute">TRI (Teoria de Resposta ao Item)</a> que levam em considera√ß√£o a performance em um caderno espec√≠fico e na dificuldade de cada quest√£o, o mesmo aluno realiza todas as provas em um curto per√≠odo de tempo.</p>
<p>Portanto, esta tarefa pode ser enquadrada como um problema supervisionado de regress√£o com m√∫ltiplos outputs na qual as previs√µes s√£o, de certa forma, dependentes da entrada umas das outras.</p>
<p>A valida√ß√£o da solu√ß√£o foi feita utilizando a m√©trica Mean Columnwise Root Mean Squared Error ‚Äì MCRMSE, que √© basicamente a m√©dia do RMSE calculado sobre as previs√µes de cada nota.</p>
</div>
<div id="an√°lise-explorat√≥ria-em-r" class="section level1">
<h1>An√°lise Explorat√≥ria (em R)</h1>
<p>Convido o leitor a conferir o <a href="https://gomesfellipe.github.io/post/2021-11-01-solucao-final-porto-seguro-data-challenge/">notebook publicado no Kaggle</a> com a an√°lise explorat√≥ria completa. Aqui irei trazer apenas alguns dos principais insights que encontrei durante a etapa de an√°lise explorat√≥ria.</p>
<div id="estrutura-da-base" class="section level2">
<h2>Estrutura da base</h2>
<p>Veja a seguir qual a estrutura geral da base de dados:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/02_df_status.png" style="width:95.0%" />
</center>
<p>√â not√≥rio que existem dados faltantes e que parece haver algum padr√£o. Vejamos com mais detalhse:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/03_missing.png" style="width:95.0%" />
</center>
<div class="w3-panel w3-pale-blue w3-border">
<p>¬† üí° Insights!</p>
<p>Existem dados <em>missing</em> nas 5 targets que queremos prever e note que existe uma rela√ß√£o tanto entre as provas de Matem√°tica e Ci√™ncias da Natuerza quanto nas de Ci√™ncias Humanas, Linguagens e C√≥digos e Reda√ß√£o, o que parece ocorrer devido a aus√™ncia do aluno incrito em comparecer a realiza√ß√£o da prova no respectivo dia.</p>
</div>
</div>
<div id="ano-da-base-de-dados" class="section level2">
<h2>Ano da base de dados</h2>
<p>Essa informa√ß√£o n√£o estava explicitamente dispon√≠vel, mas ap√≥s analisar a idade dos participantes em rela√ß√£o ao ano em que conclu√≠ram o ensino m√©dio, foi poss√≠vel identificar que tratavam-se dos dados de 2019, veja:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/05_ano_concluiu.png" style="width:95.0%" />
</center>
<p>Essa informa√ß√£o poderia ser √∫til na hora de buscar dados externos (permitido nesta competi√ß√£o).</p>
<div class="w3-panel w3-pale-blue w3-border">
<p>¬† üí° Insights!</p>
<p>‚Üí Aten√ß√£o aos outliers: √â no m√≠nimo estranho uma pessoa que formou em 2007 ter 17 anos;</p>
<p>‚Üí Como ningu√©m concluiu a escola no ano de 2019 e a m√©dia das idades vai diminuindo quanto mais pr√≥ximo de 2018, parece que estes dados s√£o de 2019. Essa inform√ß√£o poderia ser √∫til na hora de procurar por bases externas.</p>
</div>
</div>
<div id="target" class="section level2">
<h2>Target</h2>
<p>A primeira decis√£o importante era definir como enquadrar o problema; se seriam m√∫ltiplos modelos independentes ou modelos com sa√≠das dependentes.</p>
<p>Primeiramente vejamos como eram as distribui√ß√µes das notas por caderno:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/07_distribuicao_target.png" style="width:95.0%" />
</center>
<p>Ao olhar estas distribui√ß√µes foram surgindo v√°rias id√©ias! Cheguei at√© a tentar modelos estat√≠sticos GAM considerando a resposta como uma distribui√ß√£o Beta (transformando as targets no intervalo [0,1]) mas acabou n√£o apresentando bons resultados para a competi√ß√£o.. acho que seria necess√°rio um pouco mais de prepara√ß√£o nos dados.</p>
<p>Apesar das notas do enem serem calculadas via TRI (Teoria de Resposta ao Item) que considera as notas independentes, parece existir alguma correla√ß√£o entre as notas, veja:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/08_correlacao_notas.png" style="width:95.0%" />
</center>
<p>As targets da nota de L√≠nguas e C√≥digos e Ci√™ncias Humanas pareciam possuir uma correla√ß√£o ‚Äúinteressante‚Äù, mas, ap√≥s testar modelos de m√∫ltiplas respostas dependentes para cada dia (com e sem a nota da reda√ß√£o), em nenhum de meus testes superou (de maneira consistente) o desempenho de modelos que considerassem as sa√≠das independentes. Portanto foquei em criar 5 modelos independentes.</p>
</div>
</div>
<div id="machine-learning-em-python" class="section level1">
<h1>Machine Learning (em Python)</h1>
<p>Toda a rotina de pr√©-processamento dos dados, feature engineering, modelagem, ensamble e p√≥s-processamento foi realizada utilizando a linguagem Python para cada uma das 5 notas. Trouxe apenas o modelo final neste post mas, para chegar at√© aqui foram necess√°rio muitos testes!</p>
<div id="importar-dependencias" class="section level2">
<h2>Importar dependencias</h2>
<p>Carregar pacotes Python:</p>
<pre class="python"><code># data prep
import numpy as np 
import pandas as pd 
# pre process
from sklearn.preprocessing import MinMaxScaler
# modeling
from sklearn.model_selection import train_test_split
from catboost import CatBoostRegressor
# plots
import seaborn as sns
import matplotlib.pyplot as plt</code></pre>
<p>Confira a baixo as fun√ß√µes desenvolvidas para a solu√ß√£o deste problema</p>
<details>
<summary>
(<em>Clique aqui para expandir as fun√ß√µes</em>)
</summary>
<pre class="python"><code>def prep_data_questionarios(df):
  &#39;&#39;&#39;
  Converte dados de questionario para ordinal
  &#39;&#39;&#39;
    # escolaridade pai
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}
    df.loc[:, &#39;Q001&#39;] = df.loc[:, &#39;Q001&#39;].map(to_map).astype(int)

    # escolaridade mae
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}
    df.loc[:, &#39;Q002&#39;] = df.loc[:, &#39;Q002&#39;].map(to_map).astype(int) 

    # ocupacao pai
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: -1}
    df.loc[:, &#39;Q003&#39;] = df.loc[:, &#39;Q003&#39;].map(to_map).astype(int) 

    # ocupacao mae
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: -1}
    df.loc[:, &#39;Q004&#39;] = df.loc[:, &#39;Q004&#39;].map(to_map).astype(int) 

    # renda da familia
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;:5, &#39;G&#39;:6, &#39;H&#39;:7, &#39;I&#39;:8,
              &#39;J&#39;:9, &#39;K&#39;:10,&#39;L&#39;:11, &#39;M&#39;:12, &#39;N&#39;:13, &#39;O&#39;:14, &#39;P&#39;:15, &#39;Q&#39;:16}
    df.loc[:, &#39;Q006&#39;] = df.loc[:, &#39;Q006&#39;].map(to_map).astype(int) 

    # empregado domestico
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3}
    df.loc[:, &#39;Q007&#39;] = df.loc[:, &#39;Q007&#39;].map(to_map).astype(int) 

    # banheiro
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q008&#39;] = df.loc[:, &#39;Q008&#39;].map(to_map).astype(int) 

    # qnt de quartos
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q009&#39;] = df.loc[:, &#39;Q009&#39;].map(to_map).astype(int) 

    # qnt de carros
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q010&#39;] = df.loc[:, &#39;Q010&#39;].map(to_map).astype(int) 

    # qnt de motocicleta
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q011&#39;] = df.loc[:, &#39;Q011&#39;].map(to_map).astype(int) 

    # qnt de geladeira
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q012&#39;] = df.loc[:, &#39;Q012&#39;].map(to_map).astype(int) 

    # qnt de freezer
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q013&#39;] = df.loc[:, &#39;Q013&#39;].map(to_map).astype(int) 

    # qnt de maquina de lavar roupa
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q014&#39;] = df.loc[:, &#39;Q014&#39;].map(to_map).astype(int) 

    # qnt de maquina de secar roupa
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q015&#39;] = df.loc[:, &#39;Q015&#39;].map(to_map).astype(int) 

    # qnt de microondas
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q016&#39;] = df.loc[:, &#39;Q016&#39;].map(to_map).astype(int) 

    # qnt de maquina de lavar louca
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q017&#39;] = df.loc[:, &#39;Q017&#39;].map(to_map).astype(int) 

    # tem aspirador de po
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1}
    df.loc[:, &#39;Q018&#39;] = df.loc[:, &#39;Q018&#39;].map(to_map).astype(int) 

    # qtd tv colorida
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q019&#39;] = df.loc[:, &#39;Q019&#39;].map(to_map).astype(int) 

    # tem dvd
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1}
    df.loc[:, &#39;Q020&#39;] = df.loc[:, &#39;Q020&#39;].map(to_map).astype(int) 

    # tem tv por assinatura
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1}
    df.loc[:, &#39;Q021&#39;] = df.loc[:, &#39;Q021&#39;].map(to_map).astype(int) 

    # qtd telefone celular
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q022&#39;] = df.loc[:, &#39;Q022&#39;].map(to_map).astype(int) 

    # qtd telefone fixo
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1}
    df.loc[:, &#39;Q023&#39;] = df.loc[:, &#39;Q023&#39;].map(to_map).astype(int) 

    # qtd computador
    to_map =  {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q024&#39;] = df.loc[:, &#39;Q024&#39;].map(to_map).astype(int) 

    # tem acesso a internet
    to_map =  {&#39;A&#39;:0, &#39;B&#39;:1}
    df.loc[:, &#39;Q025&#39;] = df.loc[:, &#39;Q025&#39;].map(to_map).astype(int) 
    
    return(df)
  
def fe_questionario(df):
  &#39;&#39;&#39;
  Gerar novas features artificiais baseadas nos dados de questionario
  &#39;&#39;&#39;
    df.loc[:, &quot;Q021+Q006&quot;] = df[&quot;Q021&quot;] + df[&quot;Q006&quot;]
    df.loc[:, &quot;Q018+Q006&quot;] = df[&quot;Q018&quot;] + df[&quot;Q006&quot;]
    df.loc[:, &quot;Q018+Q008&quot;] = df[&quot;Q018&quot;] + df[&quot;Q008&quot;]
    df.loc[:, &quot;Q010+Q018&quot;] = df[&quot;Q010&quot;] + df[&quot;Q018&quot;]
    df.loc[:, &quot;Q018+Q024&quot;] = df[&quot;Q018&quot;] + df[&quot;Q024&quot;]
    
    df.loc[:, &quot;Q018*Q006&quot;] = df[&quot;Q018&quot;] * df[&quot;Q006&quot;]
    df.loc[:, &quot;Q010*Q018&quot;] = df[&quot;Q010&quot;] * df[&quot;Q018&quot;]
    
    return df
  
def fe_mun(data):
    &#39;&#39;&#39;
    Gerar novas features a partir das localizacoes de municipio
    &#39;&#39;&#39;
    for c in list(data.columns[data.dtypes==&#39;category&#39;]):
        data.loc[:, c] = data.loc[:, c].astype(&#39;object&#39;)
    
    data.loc[:, &#39;FE_MUNICIPIO_PROVA_x_MUNICIPIO_RESIDENCIA&#39;] = np.where(data.NO_MUNICIPIO_PROVA == data.NO_MUNICIPIO_RESIDENCIA , 1, 0)
    data.loc[:, &#39;FE_MUNICIPIO_PROVA_x_MUNICIPIO_NASCIMENTO&#39;] = np.where(data.NO_MUNICIPIO_PROVA == data.NO_MUNICIPIO_NASCIMENTO , 1, 0)
    data.loc[:, &#39;FE_MUNICIPIO_PROVA_x_MUNICIPIO_ESC&#39;] = np.where(data.NO_MUNICIPIO_PROVA == data.NO_MUNICIPIO_ESC , 1, 0)
    data.loc[:, &#39;FE_MUNICIPIO_RESIDENCIA_x_MUNICIPIO_NASCIMENTO&#39;] = np.where(data.NO_MUNICIPIO_RESIDENCIA == data.NO_MUNICIPIO_NASCIMENTO , 1, 0)
    data.loc[:, &#39;FE_MUNICIPIO_RESIDENCIA_x_MUNICIPIO_ESC&#39;] = np.where(data.NO_MUNICIPIO_RESIDENCIA == data.NO_MUNICIPIO_ESC , 1, 0)
    data.loc[:, &#39;FE_MUNICIPIO_NASCIMENTO_x_MUNICIPIO_ESC&#39;] = np.where(data.NO_MUNICIPIO_RESIDENCIA == data.NO_MUNICIPIO_ESC , 1, 0)
    
    for c in list(data.columns[data.dtypes==&#39;object&#39;]):
        data.loc[:, c] = data.loc[:, c].astype(&#39;category&#39;)
    
    return data
  
def fe_in(df):
    &#39;&#39;&#39;
    Gerar features a partir das indicadoras
    &#39;&#39;&#39;
    df.loc[:, &#39;IN_DEFICIT_ATENCAO+IN_TEMPO_ADICIONAL&#39;] = df[&quot;IN_DEFICIT_ATENCAO&quot;] + df[&quot;IN_TEMPO_ADICIONAL&quot;]
    df.loc[:, &#39;IN_LEDOR+IN_TRANSCRICAO&#39;] = df[&quot;IN_LEDOR&quot;] + df[&quot;IN_TRANSCRICAO&quot;]

    return df
  
def prep_co_escola(df):
    &#39;&#39;&#39;
    Converter codigo da escola para categorico
    &#39;&#39;&#39;
    df.loc[:, &#39;CO_ESCOLA&#39;] = [str(x) for x in df.CO_ESCOLA]
    df.loc[:, &#39;CO_ESCOLA&#39;] = np.where(df[&#39;CO_ESCOLA&#39;]==&#39;nan&#39;, np.nan, df[&#39;CO_ESCOLA&#39;])
    df.loc[:, &#39;CO_ESCOLA&#39;] = df.loc[:, &#39;CO_ESCOLA&#39;].astype(&#39;category&#39;)
    
    return df
  
def fe_extra(df):
    &#39;&#39;&#39;
    Gerar novas features 
    &#39;&#39;&#39;
    df.loc[:, &quot;FE_IDADE_DISCRETA&quot;] = pd.cut(df.NU_IDADE, (0, 15, 18, 23, 36, 60, 120), labels=[&#39;ADOLESCENTE&#39;,&#39;ADOLESCENTE_2&#39;, &#39;JOVEM&#39;,&#39;JOVEM_2&#39;, &#39;ADULTO&#39;, &#39;IDOSO&#39;]).astype(&#39;category&#39;)
    df.loc[:, &#39;FE_OCUPACAO_PAIS&#39;] = df.Q003 + df.Q004
    df.loc[:, &#39;FE_ESCOLARIDADE_PAIS&#39;] = df.Q001 + df.Q002
    df.loc[:, &#39;FE_RENDA_POR_PESSOA&#39;] = df.Q006 / df.Q005
    df.loc[:, &#39;FE_CELULAR_POR_PESSOA&#39;] = df.Q022 / df.Q005
    df.loc[:, &#39;FE_COMPUTADOR_POR_PESSOA&#39;] = df.Q024 / df.Q005
    df.loc[:, &#39;FE_VISAO_RUIM&#39;] = df[[&#39;IN_BAIXA_VISAO&#39;, &#39;IN_CEGUEIRA&#39;, &#39;IN_VISAO_MONOCULAR&#39;, &#39;IN_SURDO_CEGUEIRA&#39;]].max(axis=1)
    df.loc[:, &#39;FE_AUDICAO_RUIM&#39;] = df[[&#39;IN_SURDEZ&#39;, &#39;IN_DEFICIENCIA_AUDITIVA&#39;, &#39;IN_SURDO_CEGUEIRA&#39;]].max(axis=1)
    df.loc[:, &#39;FE_TDAH_MAIS_TEMPO&#39;] = df.IN_TEMPO_ADICIONAL + df.IN_DEFICIT_ATENCAO
    df.loc[:, &#39;FE_TDAH_MEDICADO&#39;] = np.where((df.IN_DEFICIT_ATENCAO==1)&amp;(df.IN_MEDICAMENTOS==1), 1, 0)
    df.loc[:, &#39;FE_RECURSO_VISAO&#39;] =  df[[&#39;IN_BRAILLE&#39;, &#39;IN_AMPLIADA_24&#39;, &#39;IN_AMPLIADA_18&#39;, &#39;IN_LEDOR&#39;, &#39;IN_MAQUINA_BRAILE&#39;, &#39;IN_LAMINA_OVERLAY&#39;]].max(axis=1)
    df.loc[:, &#39;FE_RECURSO_SURDEZ&#39;] =  df[[&#39;IN_LIBRAS&#39;, &#39;IN_LEITURA_LABIAL&#39;, &#39;IN_TRANSCRICAO&#39;]].max(axis=1)
    acess = [&#39;IN_ACESSO&#39;, &#39;IN_MESA_CADEIRA_RODAS&#39;, &#39;IN_MESA_CADEIRA_SEPARADA&#39;, &#39;IN_APOIO_PERNA&#39;, &#39;IN_CADEIRA_ESPECIAL&#39;, &#39;IN_CADEIRA_CANHOTO&#39;, &#39;IN_CADEIRA_ACOLCHOADA&#39;, &#39;IN_MOBILIARIO_OBESO&#39;, &#39;IN_SALA_INDIVIDUAL&#39;, &#39;IN_SALA_ESPECIAL&#39;, &#39;IN_SALA_ACOMPANHANTE&#39;, &#39;IN_MOBILIARIO_ESPECIFICO&#39;, &#39;IN_MATERIAL_ESPECIFICO&#39;]
    df.loc[:, &#39;FE_ACESSIBILIDADE&#39;] =  df[acess].max(axis=1)

    return df</code></pre>
</details>
<p>¬†</p>
<p>Carregar features artificiais extra√≠das atrav√©s de um modelo KNN. N√£o apresentarei o c√≥digo aqui (talvez fique para um pr√≥ximo post) mas a id√©ia √© basicamente a seguinte:</p>
<div class="w3-panel w3-pale-yellow w3-border">
<p>¬† üß™ Feature Extraction com KNN</p>
<p>Ajuste um <code>KNeighborsRegressor</code> encontrando os K-vizinhos mais pr√≥ximos de cada inst√¢ncia out-of-fold via valida√ß√£o cruzada (para evitar data leak) nos dados de treino e depois ajuste um modelo em todos os dados de treino para obter os K-vizinhos mais pr√≥ximos nos dados de teste.</p>
</div>
<p>Quem sabe no futuro fa√ßo um post compartilhando esta estrat√©gia com mais detalhes.</p>
<pre class="python"><code>knn_train = pd.read_csv(&quot;../input/knn/KNN_feat_train_CH_LC.csv&quot;)
knn_test = pd.read_csv(&quot;../input/knn/KNN_feat_test_CH_LC.csv&quot;)

knn_train_cn_mt = pd.read_csv(&quot;../input/knn/KNN_feat_train_CN_MT.csv&quot;)
knn_test_cn_mt = pd.read_csv(&quot;../input/knn/KNN_feat_test_CN_MT.csv&quot;)

knn_train_rd = pd.read_csv(&quot;../input/knn/KNN_feat_train_RD.csv&quot;)
knn_test_rd = pd.read_csv(&quot;../input/knn/KNN_feat_test_RD.csv&quot;)</code></pre>
</div>
<div id="carregar-dados" class="section level2">
<h2>Carregar dados</h2>
<p>Importar uma vers√£o do dataset no formato <code>.parquet</code> que foi compactada com um truque para otimizar o consumo de mem√≥ria disponibilizada pelos organizadores <a href="https://www.kaggle.com/code/caneiro/mlo-make-parquet">neste notebook</a>.</p>
<pre class="python"><code>train = pd.read_parquet(&#39;train.parquet&#39;)
test = pd.read_parquet(&#39;test.parquet&#39;)
sub = pd.read_csv(&#39;../input/qualityeducation/sample_submission.csv&#39;)</code></pre>
<p>Definir objetos com targets</p>
<pre class="python"><code>targets = [&#39;NU_NOTA_LC&#39;, &#39;NU_NOTA_CH&#39;, &#39;NU_NOTA_CN&#39;,  &#39;NU_NOTA_MT&#39;, &#39;NU_NOTA_REDACAO&#39;]
presencas = [&#39;TP_PRESENCA_LC&#39;, &#39;TP_PRESENCA_CH&#39;, &#39;TP_PRESENCA_CN&#39;, &#39;TP_PRESENCA_MT&#39;, &#39;TP_STATUS_REDACAO&#39;]</code></pre>
<div class="w3-panel w3-pale-yellow w3-border">
<p>¬† ‚ö†Ô∏è Aten√ß√£o:</p>
<p>A feature de presen√ßa √© muito importante no p√≥s-processamento para atribuir nota zero aos alunos que n√£o foram realizar a prova mas n√£o faz sentido mant√™-la nos dados de treino pois ser√° sempre constante.</p>
</div>
<div id="dados-externos" class="section level3">
<h3>Dados externos</h3>
<p>Dados Externos utilizados:</p>
<ol style="list-style-type: decimal">
<li><a href="https://basedosdados.org/dataset/mundo-onu-adh">Atlas do Desenvolvimento Humano (ADH)</a></li>
</ol>
<p>Esta base tinha muita informa√ß√£o legal mas sua cobertura temporal estava bastante defasada (1991 - 2010) o que pode adicionar algum ru√≠do ao modelo.</p>
<p>As features selecionadas (sem muito crit√©rio) desta base foram:</p>
<pre class="python"><code>extra1 = pd.read_csv(&quot;municipio.csv&quot;)

extra1 = extra1[extra1.ano==2010]

features_extra1 = [&#39;expectativa_vida&#39;, &#39;razao_dependencia&#39;, &#39;expectativa_anos_estudo&#39;,
&#39;taxa_analfabetismo_11_a_14&#39;, &#39;taxa_analfabetismo_15_a_17&#39;, &#39;taxa_analfabetismo_18_mais&#39;,
&#39;taxa_atraso_0_basico&#39;, &#39;taxa_atraso_0_fundamental&#39;, &#39;taxa_atraso_0_medio&#39;,
&#39;taxa_freq_bruta_medio&#39;, &#39;taxa_freq_liquida_medio&#39;,
&#39;taxa_freq_medio_18_24&#39;, &#39;taxa_freq_medio_6_14&#39;, &#39;indice_gini&#39;,&#39;prop_pobreza_extrema&#39;, &#39;prop_pobreza&#39;,
&#39;prop_renda_10_ricos&#39;, &#39;prop_renda_20_pobres&#39;, &#39;razao_10_ricos_40_pobres&#39;,&#39;renda_pc&#39; , &#39;renda_pc_quintil_1&#39;,
&#39;indice_theil&#39;, &#39;prop_trabalhadores_conta_proria&#39;, 
&#39;prop_empregadores&#39;, &#39;prop_ocupados_agropecuaria&#39;, &#39;prop_ocupados_comercio&#39;,
&#39;prop_ocupados_construcao&#39;, &#39;prop_ocupados_formalizacao&#39;, &#39;prop_ocupados_medio&#39;,
&#39;prop_ocupados_servicos&#39;, &#39;prop_ocupados_superior&#39;,
&#39;prop_ocupados_renda_0&#39;, &#39;renda_media_ocupados&#39;, &#39;indice_treil_trabalho&#39;,
&#39;taxa_ocupados_carteira&#39;, &#39;taxa_agua_encanada&#39;, 
&#39;taxa_banheiro_agua_encanada&#39;, &#39;taxa_coleta_lixo&#39;, &#39;taxa_energia_eletrica&#39;,
&#39;taxa_agua_esgoto_inadequados&#39;, &#39;taxa_criancas_dom_sem_fund&#39;,
&#39;pea&#39;, &#39;indice_escolaridade&#39;, &#39;indice_frequencia_escolar&#39;, 
&#39;idhm&#39;, &#39;idhm_e&#39;, &#39;idhm_l&#39;, &#39;idhm_r&#39;]
extra1 = extra1[[&#39;id_municipio&#39;]+features_extra1]

train = pd.merge(train, extra1, how=&#39;left&#39;, left_on=&#39;CO_MUNICIPIO_RESIDENCIA&#39;, right_on=&#39;id_municipio&#39;)
test = pd.merge(test, extra1, how=&#39;left&#39;, left_on=&#39;CO_MUNICIPIO_RESIDENCIA&#39;, right_on=&#39;id_municipio&#39;)</code></pre>
<ol start="2" style="list-style-type: decimal">
<li><a href="https://www.gov.br/inep/pt-br/acesso-a-informacao/dados-abertos/microdados/censo-escolar">Microdados do Censo Escolar da Educaca√ß√£o B√°sica</a></li>
</ol>
<p>Base dispon√≠vel no mesmo site dos dados da competi√ß√£o e que tr√°s informa√ß√µes muito ricas das escolas do Brasil. Infelizmente quase 75% da informa√ß√£o da escola do aluno era missing ent√£o esta base n√£o conseguiu alavancar os ganhos do modelo de maneira consider√°vel.</p>
<p>Nesta base foquei principalmente nas features utilizadas para calcular o IIE (√çndice de Estrutura da Escola) que se baseia nos seguintes componentes:</p>
<table>
<colgroup>
<col width="32%" />
<col width="24%" />
<col width="42%" />
</colgroup>
<thead>
<tr class="header">
<th>Componente 1: Pedag√≥gica (IEE_Pedag√≥gico):</th>
<th>Componente 2: B√°sica (IEE_B√°sico):</th>
<th>Componente 3: Tecnol√≥gica (IEE_Tecnol√≥gico):</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Qualifica√ß√£o do docente (forma√ß√£o acad√™mica dos professores)</td>
<td>√Ågua filtrada (bin√°ria)</td>
<td>N√∫mero de computadores por aluno (computadores dispon√≠veis para uso dos alunos)</td>
</tr>
<tr class="even">
<td>N√∫mero de alunos por sala</td>
<td>Acesso √† rede p√∫blica de energia (bin√°ria)</td>
<td>N√∫mero de equipamentos multim√≠dia por aluno</td>
</tr>
<tr class="odd">
<td>N√∫mero de funcion√°rios por aluno</td>
<td>Acesso √† rede p√∫blica de esgoto (bin√°ria)</td>
<td>Acesso a internet (bin√°ria)</td>
</tr>
<tr class="even">
<td>Quadra de esportes coberta (bin√°ria)</td>
<td>Coleta peri√≥dica de lixo (bin√°ria)</td>
<td>Laborat√≥rio de Ci√™ncias (bin√°ria)</td>
</tr>
<tr class="odd">
<td>Biblioteca (bin√°ria)</td>
<td>Banheiro dentro do pr√©dio (bin√°ria)</td>
<td>Laborat√≥rio de Inform√°tica (bin√°ria)</td>
</tr>
</tbody>
</table>
<ul>
<li><a href="https://leosalesblog.wordpress.com/2018/02/03/escola-ruim-aluno-ruim-entendendo-a-relacao-entre-estrutura-escolar-e-desempenho-no-enem/">Fonte</a></li>
</ul>
<pre class="python"><code># Importar dados
extra2 = pd.read_csv(&#39;microdados_ed_basica_2021.csv&#39;, error_bad_lines=False, sep=&#39;;&#39;, encoding=&#39;latin1&#39;, dtype={&#39;CO_ORGAO_REGIONAL&#39;: &#39;str&#39;})
extra2 = extra2[extra2.isnull().sum(axis=1) / extra2.shape[1] &lt; .9]

# Tratamento nas features
extra2.loc[:, &#39;QT_TOTAL_ALUNOS&#39;] = extra2[[&#39;QT_MAT_BAS_ND&#39;, &#39;QT_MAT_BAS_BRANCA&#39;, &#39;QT_MAT_BAS_PRETA&#39;, &#39;QT_MAT_BAS_PARDA&#39;, &#39;QT_MAT_BAS_AMARELA&#39;, &#39;QT_MAT_BAS_INDIGENA&#39;]].sum(axis=1).fillna(0)
extra2.loc[:, &#39;QT_TOTAL_PROFESSORES&#39;] = (extra2.QT_DOC_BAS + extra2.QT_DOC_INF + extra2.QT_DOC_INF_CRE + extra2.QT_DOC_INF_PRE + extra2.QT_DOC_FUND + extra2.QT_DOC_FUND_AI + extra2.QT_DOC_FUND_AF + extra2.QT_DOC_MED + extra2.QT_DOC_PROF + extra2.QT_DOC_PROF_TEC + extra2.QT_DOC_EJA + extra2.QT_DOC_EJA_FUND + extra2.QT_DOC_EJA_MED + extra2.QT_DOC_ESP + extra2.QT_DOC_ESP_CC + extra2.QT_DOC_ESP_CE).fillna(0)
extra2.loc[:, &#39;QT_SALAS_UTILIZADAS&#39;] = (extra2.loc[:, &#39;QT_TOTAL_ALUNOS&#39;] / extra2.QT_SALAS_UTILIZADAS).fillna(0)
extra2.loc[:, &#39;QT_COMP_DISP_ALUNO&#39;] = extra2.QT_DESKTOP_ALUNO + extra2.QT_COMP_PORTATIL_ALUNO + extra2.QT_TABLET_ALUNO

# Selecao de faetures importantes
features_extra2 = [&#39;CO_ENTIDADE&#39;, &#39;QT_SALAS_UTILIZADAS&#39;, &#39;QT_TOTAL_PROFESSORES&#39;, &#39;IN_QUADRA_ESPORTES_COBERTA&#39;, &#39;IN_BIBLIOTECA&#39;,
       &#39;IN_AGUA_POTAVEL&#39;, &#39;IN_ENERGIA_REDE_PUBLICA&#39;, &#39;IN_ESGOTO_REDE_PUBLICA&#39;, &#39;IN_LIXO_SERVICO_COLETA&#39;, &#39;IN_BANHEIRO&#39;,
       &#39;QT_COMP_DISP_ALUNO&#39;, &#39;QT_EQUIP_MULTIMIDIA&#39;, &#39;IN_INTERNET&#39;, &#39;IN_LABORATORIO_CIENCIAS&#39;, &#39;IN_LABORATORIO_INFORMATICA&#39;]
extra2 = extra2[features_extra2]

# Remover outliers
for c in list(extra2.iloc[:, 1:].columns):
    trs = extra2.loc[extra2[c]!=88888, c].quantile(.99)
    extra2.loc[(extra2[c]==88888)|(extra2[c]&gt;trs), c] = trs
    
#Normalizar para calcular IEE
scaler = MinMaxScaler()
to_iee = scaler.fit_transform(extra2.iloc[:, 1:])
to_iee = pd.DataFrame(to_iee, columns=extra2.iloc[:, 1:].columns)

# Calcular IEE e componentes
extra2.loc[:, &#39;COMP1&#39;] = to_iee[[&#39;QT_SALAS_UTILIZADAS&#39;, &#39;QT_TOTAL_PROFESSORES&#39;, &#39;IN_QUADRA_ESPORTES_COBERTA&#39;, &#39;IN_BIBLIOTECA&#39;]].sum(axis=1)
extra2.loc[:, &#39;COMP2&#39;] = to_iee[[&#39;IN_AGUA_POTAVEL&#39;, &#39;IN_ENERGIA_REDE_PUBLICA&#39;, &#39;IN_ESGOTO_REDE_PUBLICA&#39;, &#39;IN_LIXO_SERVICO_COLETA&#39;, &#39;IN_BANHEIRO&#39;]].sum(axis=1)
extra2.loc[:, &#39;COMP3&#39;] = to_iee[[&#39;QT_COMP_DISP_ALUNO&#39;, &#39;QT_EQUIP_MULTIMIDIA&#39;, &#39;IN_INTERNET&#39;, &#39;IN_LABORATORIO_CIENCIAS&#39;, &#39;IN_LABORATORIO_INFORMATICA&#39;]].sum(axis=1)
extra2.loc[:, &#39;IEE&#39;] = extra2.COMP1 + extra2.COMP2 + extra2.COMP3

train = pd.merge(train, extra2, how=&#39;left&#39;, left_on=&#39;CO_ESCOLA&#39;, right_on=&#39;CO_ENTIDADE&#39;).drop(&#39;CO_ENTIDADE&#39;, axis=1)
test = pd.merge(test, extra2, how=&#39;left&#39;, left_on=&#39;CO_ESCOLA&#39;, right_on=&#39;CO_ENTIDADE&#39;).drop(&#39;CO_ENTIDADE&#39;, axis=1)</code></pre>
</div>
</div>
<div id="modelagem" class="section level2">
<h2>Modelagem</h2>
<p>Testei muitos modelos e muitas abordagens (inclusive com finalidade de estudo). Foram modelos estat√≠sticos (GAM considerando a distribui√ß√£o Beta(0,1)), redes neurais (TabNet) e √°rvores mas no final das contas os que tiveram melhor custo/benef√≠cio foram o LightGBM e o CatBoost.</p>
<p>Sobre o tuning, tomei a decis√£o de n√£o investir muito em otimiza√ß√£o autom√°tica de hiperpar√¢metros pois o tempo era curto e os ganhos seriam pequenos comparados com o potencial ganho com a variedade de features que poderiam ser geradas, ent√£o fiz apenas alguns testes manuais conforme via necessidade.</p>
<div id="pre-processing" class="section level4">
<h4>Pre processing</h4>
<p>A etapa que investi bastante tempo foi para criar novas vari√°veis. A seguir trago algumas features constru√≠das que foram utilizadas em determinados modelos, a partir dos dados dispon√≠veis:</p>
<ul>
<li>Renda somada dos pais;</li>
<li>N√≠vel de ocupa√ß√£o somado dos pais;</li>
<li>Renda dividido pelo n√∫mero de pessoas na casa;</li>
<li>Quantidade de celulares por pessoa na casa;</li>
<li>Quantidade de computadores por pessoa na casa;</li>
<li>Se a pessoa possui vis√£o ruim (se possui baixa vis√£o, cegueira ou monocular);</li>
<li>Se a pessoa possui audi√ß√£o ruim (Surdez, defici√™ncia auditiva);</li>
<li>Se o aluno possui TDAH e toma medicamento controlado;</li>
<li>Se o aluno possui TDAH e teve mais tempo de prova;</li>
<li>Se precisou de recurso de vis√£o ou audi√ß√£o (libras, baile, etc);</li>
<li>Se o munic√≠pio que nasceu √© o mesmo da escola;</li>
<li>Se o munic√≠pio que fez a prova √© o mesmo da escola;</li>
<li>Se o munic√≠pio da prova √© o mesmo da resid√™ncia;</li>
<li>Nota m√©dia dos alunos da respectiva escola nas outras provas (*);</li>
<li>Renda m√©dia dos alunos da respectiva escola (*).</li>
</ul>
<p>(*) Estas features precisaram ser calculadas de maneira muito cuidadosa para n√£o causar algum tipo de data leak!</p>
</div>
<div id="post-processing" class="section level4">
<h4>Post Processing</h4>
<p>Essa base tinha uma pegadinha que fazia muita diferen√ßa no resultado final. Existem duas possibilidades de um aluno tirar zero em uma prova: errar tudo ou n√£o comparecer.</p>
<p>Como temos a informa√ß√£o da presen√ßa do aluno na prova (o que na pr√°tica seria meio estranho) bastava dar zero para os alunos faltantes na hora de prever nos dados de teste para submeter.</p>
</div>
<div id="linguagens-e-c√≥digos" class="section level3">
<h3>Linguagens e C√≥digos</h3>
<p>Definir finalidade de algumas colunas:</p>
<pre class="python"><code># colunas que serao dropadas
to_drop = [&#39;IN_PROVA_DEITADO&#39;,
            &#39;NU_INSCRICAO&#39;,
            &#39;CO_MUNICIPIO_ESC&#39;,
            &#39;CO_UF_NASCIMENTO&#39;,
            &#39;CO_UF_RESIDENCIA&#39;,
            &#39;CO_UF_ESC&#39;,
            &#39;CO_UF_PROVA&#39;,
            &#39;CO_MUNICIPIO_PROVA&#39;,
            &#39;CO_MUNICIPIO_RESIDENCIA&#39;,
            &#39;CO_MUNICIPIO_NASCIMENTO&#39;]

# definir target e presenca
target = &quot;NU_NOTA_LC&quot;
presenca = &quot;TP_PRESENCA_LC&quot;

# demais notas para dropar (menos ch)
notas = list(set(targets)-set([target, &#39;NU_NOTA_CH&#39;]))</code></pre>
<p>Pr√©-processamento nos dados de treino</p>
<pre class="python"><code>X = train.copy()
X.loc[:, &#39;knn_feature&#39;] = knn_train.knn_oof
X = X.drop(to_drop, axis=1) 
X = X[X[presenca]==1]
X = X[~X[target].isnull()]

X = X.loc[:, ~X.columns.isin([target]+[presenca]+notas)]
X.loc[:, &#39;FE_RENDA&#39;] = X.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000, &#39;C&#39;:1500, &#39;D&#39;:2000,
&#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, &#39;I&#39;:6000, &#39;J&#39;:7000,&#39;K&#39;:8000,&#39;L&#39;:9000,
&#39;M&#39;:10000, &#39;N&#39;:12000, &#39;O&#39;:15000, &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
X = prep_data_questionarios(X)
X = fe_mun(X)
X = fe_questionario(X)
X = fe_in(X)
X = prep_co_escola(X)
X = fe_extra(X)

y = train.loc[(train[presenca]==1)&amp;(~train[target].isnull()), target].astype(np.float64)</code></pre>
<p>Pr√©-processamento nos dados de teste</p>
<pre class="python"><code>X_test = test.copy()
X_test.loc[:, &#39;knn_feature&#39;] = knn_test.knn_test
X_test = X_test.drop(to_drop, axis=1) 

X_test = X_test.loc[:, ~X_test.columns.isin([presenca])]
X_test.loc[:, &#39;FE_RENDA&#39;] = X_test.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000, &#39;C&#39;:1500, &#39;D&#39;:2000,
&#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, &#39;I&#39;:6000, &#39;J&#39;:7000, &#39;K&#39;:8000,&#39;L&#39;:9000,
&#39;M&#39;:10000, &#39;N&#39;:12000, &#39;O&#39;:15000, &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
X_test = prep_data_questionarios(X_test)
X_test = fe_mun(X_test)
X_test = fe_questionario(X_test)
X_test = fe_in(X_test)
X_test = prep_co_escola(X_test)
X_test = fe_extra(X_test)</code></pre>
<p>Feature engineering separada para evitar data leak:</p>
<pre class="python"><code># calcular estatisticas nos dados de treino
co_escola_renda_media = X.groupby(&#39;CO_ESCOLA&#39;).FE_RENDA.mean()
co_escola_idade_media = X.groupby(&#39;CO_ESCOLA&#39;).NU_IDADE.mean()
co_escola_nota_ch = X.groupby(&#39;CO_ESCOLA&#39;).NU_NOTA_CH.mean()
X = X.drop(&#39;NU_NOTA_CH&#39;, axis=1)

# instanciar objeto com as estatisticas por escola
co_escola_aux = pd.DataFrame({
    &#39;CO_ESCOLA&#39;: co_escola_renda_media.index,
    &#39;FE_ESCOLA_RENDA_MEDIA&#39;: co_escola_renda_media,
    &#39;FE_IDADE_MEDIA&#39;: co_escola_idade_media,
    &#39;FE_NOTA_CH&#39;: co_escola_nota_ch
}).reset_index(drop=True)

# Concatenar estatisticas nas bases de treino e teste
X = pd.merge(X, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)
X_test = pd.merge(X_test, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)

# Codigo da escola para categorico
X.loc[:, &#39;CO_ESCOLA&#39;] = X.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)
X_test.loc[:, &#39;CO_ESCOLA&#39;] = X_test.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)

# Features de contagem
X.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X_test.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X.NO_MUNICIPIO_RESIDENCIA.map({x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X_test.NO_MUNICIPIO_RESIDENCIA.map({ x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X.NO_MUNICIPIO_NASCIMENTO.map({ x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X_test.NO_MUNICIPIO_NASCIMENTO.map({x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})

X.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X_test.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})</code></pre>
<p>Ajustar modelo:</p>
<pre class="python"><code>cat_feat = X.columns[X.dtypes==&#39;category&#39;]
cat_indices = [X.columns.get_loc(x) for x in cat_feat]

for c in list(cat_feat):
    X.loc[:, c] = X.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)
    X_test.loc[:, c] = X_test.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)

X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.1, random_state=SEED)
    
clf = CatBoostRegressor(random_state=314,
                            cat_features=cat_indices,
                            verbose=0,
                            loss_function = &quot;RMSE&quot;,
                            od_type = &quot;Iter&quot;,
                            od_wait = 100,
                            iterations=3000,
                            use_best_model=True)

clf.fit(X, y, eval_set = (X_eval, y_eval), verbose=False, plot=True)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/lc_catboost.png" style="width:95.0%" />
</center>
<p>Salvar previs√µes:</p>
<pre class="python"><code>sub.loc[:, &#39;NU_NOTA_LC&#39;] = clf.predict(X_test)
# alunos que nao foram fazer a prova tiraram zero
sub.loc[test.TP_PRESENCA_LC!=1, &#39;NU_NOTA_LC&#39;] = 0</code></pre>
<p>Comparar distribui√ß√£o da target nos dados de treino com rela√ß√£o √†s previs√µes do modelo:</p>
<pre class="python"><code>sns.kdeplot(train.loc[:, target], shade=True, color=&#39;r&#39;, clip=[0,1000])
sns.kdeplot(sub.loc[:, target], shade=True, color=&#39;b&#39;, clip=[0,1000])
plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
plt.title(target)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/lc_pred.png" style="width:50.0%" />
</center>
</div>
<div id="ci√™ncias-humanas" class="section level3">
<h3>Ci√™ncias Humanas</h3>
<p>Novas features desenvolvidas especificamente para este modelo:</p>
<pre class="python"><code>def fe_ch(df):
    
    df.loc[:, &#39;FE_RENDA&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000,
    &#39;C&#39;:1500, &#39;D&#39;:2000, &#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, &#39;I&#39;:6000,
    &#39;J&#39;:7000, &#39;K&#39;:8000,&#39;L&#39;:9000, &#39;M&#39;:10000, &#39;N&#39;:12000, &#39;O&#39;:15000, 
    &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
    df.loc[:, &#39;FE_NU_IDADE*TP_ANO_CONCLUIU&#39;] = df.TP_ANO_CONCLUIU * df.NU_IDADE
    df.loc[:, &#39;FE_Q002+Q024&#39;] = df.loc[:, &#39;Q002&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, 
    &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}).astype(int) + 
    df.loc[:, &#39;Q024&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}).astype(int) 
    df.loc[:, &#39;FE_SCORE&#39;] = (1/df.TP_ANO_CONCLUIU) + np.sqrt(df.NU_IDADE) +
    np.where(df.TP_ESCOLA==3, 1, 0)
    
    return df</code></pre>
<p>Definir finalidade de algumas colunas:</p>
<pre class="python"><code># colunas que serao dropadas
to_drop = [&#39;IN_PROVA_DEITADO&#39;,
           &#39;NU_INSCRICAO&#39;,
           &#39;CO_MUNICIPIO_ESC&#39;,
           &#39;CO_UF_NASCIMENTO&#39;,
           &#39;CO_UF_RESIDENCIA&#39;,
           &#39;CO_UF_ESC&#39;,
           &#39;CO_UF_PROVA&#39;,
           &#39;CO_MUNICIPIO_PROVA&#39;,
           &#39;CO_MUNICIPIO_RESIDENCIA&#39;,
          &#39;CO_MUNICIPIO_NASCIMENTO&#39;]

# definir target e presenca
target = &quot;NU_NOTA_CH&quot;
presenca = &quot;TP_PRESENCA_CH&quot;

# demais notas para dropar (menos lc)
notas = list(set(targets)-set([target, &#39;NU_NOTA_LC&#39;]))</code></pre>
<p>Pr√©-processamento nos dados de treino</p>
<pre class="python"><code>X = train.copy()
X.loc[:, &#39;knn_feature&#39;] = knn_train.knn_oof
X = X.drop(to_drop, axis=1)
X = X[X[presenca]==1]
X = X[~X[target].isnull()]

X = X.loc[:, ~X.columns.isin([target]+[presenca]+notas)]
X = fe_ch(X)
X = prep_data_questionarios(X)
X = fe_mun(X)
X = fe_questionario(X)
#X = fe_in(X)
X = prep_co_escola(X)
X = fe_extra(X)

y = train.loc[(train[presenca]==1)&amp;(~train[target].isnull()), target].astype(np.float64)</code></pre>
<p>Pr√©-processamento nos dados de teste</p>
<pre class="python"><code>X_test = test.copy()
X_test.loc[:, &#39;knn_feature&#39;] = knn_test.knn_test
X_test = X_test.drop(to_drop, axis=1) 

X_test = X_test.loc[:, ~X_test.columns.isin([presenca])]
X_test = fe_ch(X_test)
X_test = prep_data_questionarios(X_test)
X_test = fe_mun(X_test)
X_test = fe_questionario(X_test)
#X_test = fe_in(X_test)
X_test = prep_co_escola(X_test)
X_test = fe_extra(X_test)</code></pre>
<p>Feature engineering separada para evitar data leak:</p>
<pre class="python"><code># calcular estatisticas nos dados de treino
co_escola_renda_media = X.groupby(&#39;CO_ESCOLA&#39;).FE_RENDA.mean()
co_escola_idade_media = X.groupby(&#39;CO_ESCOLA&#39;).NU_IDADE.mean()
co_escola_nota_lc = X.groupby(&#39;CO_ESCOLA&#39;).NU_NOTA_LC.mean()
X = X.drop(&#39;NU_NOTA_LC&#39;, axis=1)

# instanciar objeto com as estatisticas por escola
co_escola_aux = pd.DataFrame({
    &#39;CO_ESCOLA&#39;: co_escola_renda_media.index,
    &#39;FE_ESCOLA_RENDA_MEDIA&#39;: co_escola_renda_media,
    &#39;FE_IDADE_MEDIA&#39;: co_escola_idade_media,
    &#39;FE_NOTA_LC&#39;: co_escola_nota_lc
}).reset_index(drop=True)

# Concatenar estatisticas nas bases de treino e teste
X = pd.merge(X, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)
X_test = pd.merge(X_test, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)

# Codigo da escola para categorico
X.loc[:, &#39;CO_ESCOLA&#39;] = X.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)
X_test.loc[:, &#39;CO_ESCOLA&#39;] = X_test.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)

# Features de contagem
X.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X_test.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X.NO_MUNICIPIO_RESIDENCIA.map({x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X_test.NO_MUNICIPIO_RESIDENCIA.map({ x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X.NO_MUNICIPIO_NASCIMENTO.map({ x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X_test.NO_MUNICIPIO_NASCIMENTO.map({x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})

X.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X_test.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})</code></pre>
<p>Ajustar modelo:</p>
<pre class="python"><code>%%time

cat_feat = X.columns[X.dtypes==&#39;category&#39;]
cat_indices = [X.columns.get_loc(x) for x in cat_feat]

for c in list(cat_feat):
    X.loc[:, c] = X.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)
    X_test.loc[:, c] = X_test.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)

X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.1, random_state=SEED)

clf = CatBoostRegressor(random_state=314,
                            cat_features=cat_indices,
                            verbose=0,
                            loss_function = &quot;RMSE&quot;,
                            od_type = &quot;Iter&quot;,
                            od_wait = 100,iterations=3000,
                            use_best_model=True)

clf.fit(X, y, eval_set = (X_eval, y_eval), verbose=False, plot=True)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/ch_catboost.png" style="width:95.0%" />
</center>
<p>Salvar previs√µes:</p>
<pre class="python"><code>sub.loc[:, &#39;NU_NOTA_CH&#39;] = clf.predict(X_test)
# alunos que nao foram fazer a prova tiraram zero
sub.loc[test.TP_PRESENCA_CH!=1, &#39;NU_NOTA_CH&#39;] = 0</code></pre>
<p>Comparar distribui√ß√£o da target nos dados de treino com rela√ß√£o √†s previs√µes do modelo:</p>
<pre class="python"><code>sns.kdeplot(train.loc[:, target], shade=True, color=&#39;r&#39;, clip=[0,1000])
sns.kdeplot(sub.loc[:, target], shade=True, color=&#39;b&#39;, clip=[0,1000])
plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
plt.title(target)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/ch_pred.png" style="width:50.0%" />
</center>
</div>
<div id="ci√™ncias-da-natureza" class="section level3">
<h3>Ci√™ncias da Natureza</h3>
<p>Novas features desenvolvidas especificamente para este modelo:</p>
<pre class="python"><code>def fe_cn(df):
    df.loc[:, &#39;FE_RENDA&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000,
    &#39;C&#39;:1500, &#39;D&#39;:2000, &#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, 
    &#39;I&#39;:6000, &#39;J&#39;:7000, &#39;K&#39;:8000,&#39;L&#39;:9000, &#39;M&#39;:10000, &#39;N&#39;:12000, 
    &#39;O&#39;:15000, &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
    df.loc[:, &#39;FE_NU_IDADE*TP_ANO_CONCLUIU&#39;] = df.TP_ANO_CONCLUIU * df.NU_IDADE
    df.loc[:, &#39;FE_Q002+Q024&#39;] = df.loc[:, &#39;Q002&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2,
    &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}).astype(int) + 
    df.loc[:, &#39;Q024&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}).astype(int) 
    df.loc[:, &#39;FE_SCORE&#39;] = (1/df.TP_ANO_CONCLUIU) + np.sqrt(df.NU_IDADE) + np.where(df.TP_ESCOLA==3, 1, 0)
    
    df.loc[:, &#39;FE_UF_ESCOLA&#39;] = df.SG_UF_ESC.map({
      &#39;AM&#39;:&#39;Norte&#39;, &#39;RR&#39;:&#39;Norte&#39;, &#39;AP&#39;:&#39;Norte&#39;, &#39;PA&#39;:&#39;Norte&#39;, &#39;TO&#39;:&#39;Norte&#39;, &#39;RO&#39;:&#39;Norte&#39;, &#39;AC&#39;:&#39;Norte&#39;,
      &#39;MA&#39;:&#39;Nordeste&#39;, &#39;PI&#39;:&#39;Nordeste&#39;, &#39;CE&#39;:&#39;Nordeste&#39;, &#39;RN&#39;:&#39;Nordeste&#39;, &#39;PE&#39;:&#39;Nordeste&#39;, &#39;PB&#39;:&#39;Nordeste&#39;, &#39;SE&#39;:&#39;Nordeste&#39;, &#39;AL&#39;:&#39;Nordeste&#39;, &#39;BA&#39;:&#39;Nordeste&#39;,
      &#39;MT&#39;: &#39;CentroOeste&#39;, &#39;MS&#39;: &#39;CentroOeste&#39;, &#39;GO&#39;: &#39;CentroOeste&#39;,
      &#39;SP&#39;: &#39;Sudeste&#39;, &#39;RJ&#39;: &#39;Sudeste&#39;, &#39;ES&#39;: &#39;Sudeste&#39;, &#39;MG&#39;: &#39;Sudeste&#39;,
      &#39;PR&#39;: &#39;Sul&#39;, &#39;RS&#39;: &#39;Sul&#39;, &#39;SC&#39;: &#39;Sul&#39;}).astype(&#39;category&#39;)
    return df</code></pre>
<p>Definir finalidade de algumas colunas:</p>
<pre class="python"><code># colunas que serao dropadas
to_drop = [&#39;IN_PROVA_DEITADO&#39;,
           &#39;NU_INSCRICAO&#39;,
           &#39;CO_MUNICIPIO_ESC&#39;,
           &#39;CO_UF_NASCIMENTO&#39;,
           &#39;CO_UF_RESIDENCIA&#39;,
           &#39;CO_UF_ESC&#39;,
           &#39;CO_UF_PROVA&#39;,
           &#39;CO_MUNICIPIO_PROVA&#39;,
           &#39;CO_MUNICIPIO_RESIDENCIA&#39;,
          &#39;CO_MUNICIPIO_NASCIMENTO&#39;]

# definir target e presenca
target = &quot;NU_NOTA_CN&quot;
presenca = &quot;TP_PRESENCA_CN&quot;

# demais notas para dropar (menos mt)
notas = list(set(targets)-set([target, &#39;NU_NOTA_MT&#39;]))</code></pre>
<p>Pr√©-processamento nos dados de treino</p>
<pre class="python"><code>X = train.copy()
X = X.drop(to_drop, axis=1) 
X = X[X[presenca]==1]
X = X[~X[target].isnull()]

X = X.loc[:, ~X.columns.isin([target]+[presenca]+notas)]
X = fe_cn(X)
X = prep_data_questionarios(X)
X = fe_mun(X)
X = fe_questionario(X)
X = fe_in(X)
X = prep_co_escola(X)
X = fe_extra(X)

y = train.loc[(train[presenca]==1)&amp;(~train[target].isnull()), target].astype(np.float64)</code></pre>
<p>Pr√©-processamento nos dados de teste</p>
<pre class="python"><code>X_test = test.copy()
X_test = X_test.drop(to_drop, axis=1) 

X_test = X_test.loc[:, ~X_test.columns.isin([presenca])]
X_test = fe_cn(X_test)
X_test = prep_data_questionarios(X_test)
X_test = fe_mun(X_test)
X_test = fe_questionario(X_test)
X_test = fe_in(X_test)
X_test = prep_co_escola(X_test)
X_test = fe_extra(X_test)</code></pre>
<p>Feature engineering separada para evitar data leak:</p>
<pre class="python"><code># calcular estatisticas nos dados de treino
co_escola_renda_media = X.groupby(&#39;CO_ESCOLA&#39;).FE_RENDA.mean()
co_escola_idade_media = X.groupby(&#39;CO_ESCOLA&#39;).NU_IDADE.mean()
co_escola_nota_mt = X.groupby(&#39;CO_ESCOLA&#39;).NU_NOTA_MT.mean()
X = X.drop(&#39;NU_NOTA_MT&#39;, axis=1)

# instanciar objeto com as estatisticas por escola
co_escola_aux = pd.DataFrame({
    &#39;CO_ESCOLA&#39;: co_escola_renda_media.index,
    &#39;FE_ESCOLA_RENDA_MEDIA&#39;: co_escola_renda_media,
    &#39;FE_IDADE_MEDIA&#39;: co_escola_idade_media,
    &#39;FE_NOTA_MT&#39;: co_escola_nota_mt
}).reset_index(drop=True)

# Concatenar estatisticas nas bases de treino e teste
X = pd.merge(X, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)
X_test = pd.merge(X_test, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)

# Codigo da escola para categorico
X.loc[:, &#39;CO_ESCOLA&#39;] = X.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)
X_test.loc[:, &#39;CO_ESCOLA&#39;] = X_test.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)

# Features de contagem
X.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X_test.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X.NO_MUNICIPIO_RESIDENCIA.map({x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X_test.NO_MUNICIPIO_RESIDENCIA.map({ x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X.NO_MUNICIPIO_NASCIMENTO.map({ x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X_test.NO_MUNICIPIO_NASCIMENTO.map({x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})

X.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X_test.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})</code></pre>
<p>Ajustar modelo:</p>
<pre class="python"><code>%%time

cat_feat = X.columns[X.dtypes==&#39;category&#39;]
cat_indices = [X.columns.get_loc(x) for x in cat_feat]

for c in list(cat_feat):
    X.loc[:, c] = X.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)
    X_test.loc[:, c] = X_test.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)

X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.1, random_state=SEED)

clf = CatBoostRegressor(random_state=314,
                            cat_features=cat_indices,
                            verbose=0,
                            loss_function = &quot;RMSE&quot;,
                            od_type = &quot;Iter&quot;,
                            od_wait = 100,iterations=3000,
                            use_best_model=True)

clf.fit(X, y, eval_set = (X_eval, y_eval), verbose=False, plot=True)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/cn_catboost.png" style="width:95.0%" />
</center>
<p>Salvar previs√µes:</p>
<pre class="python"><code>sub.loc[:, &#39;NU_NOTA_CN&#39;] = clf.predict(X_test)
# alunos que nao foram fazer a prova tiraram zero
sub.loc[test.TP_PRESENCA_CN!=1, &#39;NU_NOTA_CN&#39;] = 0</code></pre>
<p>Comparar distribui√ß√£o da target nos dados de treino com rela√ß√£o √†s previs√µes do modelo:</p>
<pre class="python"><code>sns.kdeplot(train.loc[:, target], shade=True, color=&#39;r&#39;, clip=[0,1000])
sns.kdeplot(sub.loc[:, target], shade=True, color=&#39;b&#39;, clip=[0,1000])
plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
plt.title(target)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/cn_pred.png" style="width:50.0%" />
</center>
</div>
<div id="matem√°tica" class="section level3">
<h3>Matem√°tica</h3>
<p>Novas features desenvolvidas especificamente para este modelo:</p>
<pre class="python"><code>def fe_mt(df):
    
    df.loc[:, &#39;FE_RENDA&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000, &#39;C&#39;:1500, &#39;D&#39;:2000, &#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, &#39;I&#39;:6000, &#39;J&#39;:7000, &#39;K&#39;:8000,&#39;L&#39;:9000, &#39;M&#39;:10000, &#39;N&#39;:12000, &#39;O&#39;:15000, &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
    df.loc[:, &#39;FE_NU_IDADE*TP_ANO_CONCLUIU&#39;] = df.TP_ANO_CONCLUIU * df.NU_IDADE
    df.loc[:, &#39;FE_Q002+Q024&#39;] = df.loc[:, &#39;Q002&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}).astype(int) + df.loc[:, &#39;Q024&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}).astype(int) 
    df.loc[:, &#39;FE_SCORE&#39;] = (1/df.TP_ANO_CONCLUIU) + np.sqrt(df.NU_IDADE) + np.where(df.TP_ESCOLA==3, 1, 0)
    
    df.loc[:, &#39;FE_UF_ESCOLA&#39;] = df.SG_UF_ESC.map({&#39;AM&#39;:&#39;Norte&#39;, &#39;RR&#39;:&#39;Norte&#39;, &#39;AP&#39;:&#39;Norte&#39;, &#39;PA&#39;:&#39;Norte&#39;, &#39;TO&#39;:&#39;Norte&#39;, &#39;RO&#39;:&#39;Norte&#39;, &#39;AC&#39;:&#39;Norte&#39;,
                &#39;MA&#39;:&#39;Nordeste&#39;, &#39;PI&#39;:&#39;Nordeste&#39;, &#39;CE&#39;:&#39;Nordeste&#39;, &#39;RN&#39;:&#39;Nordeste&#39;, &#39;PE&#39;:&#39;Nordeste&#39;, &#39;PB&#39;:&#39;Nordeste&#39;, &#39;SE&#39;:&#39;Nordeste&#39;, &#39;AL&#39;:&#39;Nordeste&#39;, &#39;BA&#39;:&#39;Nordeste&#39;,
                &#39;MT&#39;: &#39;CentroOeste&#39;, &#39;MS&#39;: &#39;CentroOeste&#39;, &#39;GO&#39;: &#39;CentroOeste&#39;,
                &#39;SP&#39;: &#39;Sudeste&#39;, &#39;RJ&#39;: &#39;Sudeste&#39;, &#39;ES&#39;: &#39;Sudeste&#39;, &#39;MG&#39;: &#39;Sudeste&#39;,
                &#39;PR&#39;: &#39;Sul&#39;, &#39;RS&#39;: &#39;Sul&#39;, &#39;SC&#39;: &#39;Sul&#39;}).astype(&#39;category&#39;)
    
    
    return df</code></pre>
<p>Definir finalidade de algumas colunas:</p>
<pre class="python"><code># colunas que serao dropadas
to_drop = [&#39;IN_PROVA_DEITADO&#39;,
           &#39;NU_INSCRICAO&#39;,
           &#39;CO_MUNICIPIO_ESC&#39;,
           &#39;CO_UF_NASCIMENTO&#39;,
           &#39;CO_UF_RESIDENCIA&#39;,
           &#39;CO_UF_ESC&#39;,
           &#39;CO_UF_PROVA&#39;,
           &#39;CO_MUNICIPIO_PROVA&#39;,
           &#39;CO_MUNICIPIO_RESIDENCIA&#39;,
          &#39;CO_MUNICIPIO_NASCIMENTO&#39;]

# definir target e presenca
target = &quot;NU_NOTA_MT&quot;
presenca = &quot;TP_PRESENCA_MT&quot;

# demais notas para dropar (menos cn)
notas = list(set(targets)-set([target, &#39;NU_NOTA_CN&#39;]))</code></pre>
<p>Pr√©-processamento nos dados de treino</p>
<pre class="python"><code>X = train.copy()
X = X.drop(to_drop, axis=1) 
X = X[X[presenca]==1]
X = X[~X[target].isnull()]

X = X.loc[:, ~X.columns.isin([target]+[presenca]+notas)]
X = fe_mt(X)
X = prep_data_questionarios(X)
X = fe_mun(X)
#X = fe_questionario(X)
#X = fe_in(X)
X = prep_co_escola(X)
X = fe_extra(X)

y = train.loc[(train[presenca]==1)&amp;(~train[target].isnull()), target].astype(np.float64)</code></pre>
<p>Pr√©-processamento nos dados de teste</p>
<pre class="python"><code>X_test = test.copy()
X_test = X_test.drop(to_drop, axis=1) 

X_test = X_test.loc[:, ~X_test.columns.isin([presenca])]
X_test = fe_mt(X_test)
X_test = prep_data_questionarios(X_test)
X_test = fe_mun(X_test)
#X_test = fe_questionario(X_test)
#X_test = fe_in(X_test)
X_test = prep_co_escola(X_test)
X_test = fe_extra(X_test)</code></pre>
<p>Feature engineering separada para evitar data leak:</p>
<pre class="python"><code># calcular estatisticas nos dados de treino
co_escola_renda_media = X.groupby(&#39;CO_ESCOLA&#39;).FE_RENDA.mean()
co_escola_idade_media = X.groupby(&#39;CO_ESCOLA&#39;).NU_IDADE.mean()
co_escola_nota_cn = X.groupby(&#39;CO_ESCOLA&#39;).NU_NOTA_CN.mean()
X = X.drop(&#39;NU_NOTA_CN&#39;, axis=1)

# instanciar objeto com as estatisticas por escola
co_escola_aux = pd.DataFrame({
    &#39;CO_ESCOLA&#39;: co_escola_renda_media.index,
    &#39;FE_ESCOLA_RENDA_MEDIA&#39;: co_escola_renda_media,
    &#39;FE_IDADE_MEDIA&#39;: co_escola_idade_media,
    &#39;FE_NOTA_CN&#39;: co_escola_nota_cn
}).reset_index(drop=True)

# Concatenar estatisticas nas bases de treino e teste
X = pd.merge(X, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)
X_test = pd.merge(X_test, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)

# Codigo da escola para categorico
X.loc[:, &#39;CO_ESCOLA&#39;] = X.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)
X_test.loc[:, &#39;CO_ESCOLA&#39;] = X_test.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)

# Features de contagem
X.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X_test.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X.NO_MUNICIPIO_RESIDENCIA.map({x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X_test.NO_MUNICIPIO_RESIDENCIA.map({ x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X.NO_MUNICIPIO_NASCIMENTO.map({ x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X_test.NO_MUNICIPIO_NASCIMENTO.map({x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})

X.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X_test.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})</code></pre>
<p>Ajustar modelo:</p>
<pre class="python"><code>%%time

cat_feat = X.columns[X.dtypes==&#39;category&#39;]
cat_indices = [X.columns.get_loc(x) for x in cat_feat]

for c in list(cat_feat):
    X.loc[:, c] = X.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)
    X_test.loc[:, c] = X_test.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)

X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.1, random_state=SEED)
    
clf = CatBoostRegressor(random_state=314,
                            cat_features=cat_indices,
                            verbose=0,
                            loss_function = &quot;RMSE&quot;,
                            od_type = &quot;Iter&quot;,
                            od_wait = 100,iterations=3000,
                            use_best_model=True)

clf.fit(X, y, eval_set = (X_eval, y_eval), verbose=False, plot=True)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/mt_catboost.png" style="width:95.0%" />
</center>
<p>Salvar previs√µes:</p>
<pre class="python"><code>sub.loc[:, &#39;NU_NOTA_MT&#39;] = clf.predict(X_test)
# alunos que nao foram fazer a prova tiraram zero
sub.loc[test.TP_PRESENCA_CN!=1, &#39;NU_NOTA_MT&#39;] = 0</code></pre>
<p>Comparar distribui√ß√£o da target nos dados de treino com rela√ß√£o √†s previs√µes do modelo:</p>
<pre class="python"><code>sns.kdeplot(train.loc[:, target], shade=True, color=&#39;r&#39;, clip=[0,1000])
sns.kdeplot(sub.loc[:, target], shade=True, color=&#39;b&#39;, clip=[0,1000])
plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
plt.title(target)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/mt_pred.png" style="width:50.0%" />
</center>
</div>
<div id="reda√ß√£o" class="section level3">
<h3>Reda√ß√£o</h3>
<p>Novas features desenvolvidas especificamente para este modelo:</p>
<pre class="python"><code>def fe_rd(df):
    
    df.loc[:, &#39;FE_RENDA&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000, &#39;C&#39;:1500, &#39;D&#39;:2000, &#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, &#39;I&#39;:6000, &#39;J&#39;:7000, &#39;K&#39;:8000,&#39;L&#39;:9000, &#39;M&#39;:10000, &#39;N&#39;:12000, &#39;O&#39;:15000, &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
    df.loc[:, &#39;FE_NU_IDADE*TP_ANO_CONCLUIU&#39;] = df.TP_ANO_CONCLUIU * df.NU_IDADE
    df.loc[:, &#39;FE_Q002+Q024&#39;] = df.loc[:, &#39;Q002&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}).astype(int) + df.loc[:, &#39;Q024&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}).astype(int) 
    df.loc[:, &#39;FE_SCORE&#39;] = (1/df.TP_ANO_CONCLUIU) + np.sqrt(df.NU_IDADE) + np.where(df.TP_ESCOLA==3, 1, 0)
    
    df.loc[:, &#39;FE_RENDA_FAMILIA_+_IDADE&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;:5, &#39;G&#39;:6, &#39;H&#39;:7, &#39;I&#39;:8, &#39;J&#39;:9, &#39;K&#39;:10,&#39;L&#39;:11, &#39;M&#39;:12, &#39;N&#39;:13, &#39;O&#39;:14, &#39;P&#39;:15, &#39;Q&#39;:16}).astype(int) + df.NU_IDADE        
    df.loc[:, &#39;FE_RENDA_FAMILIA_+_ANO_CONCLUIU&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;:5, &#39;G&#39;:6, &#39;H&#39;:7, &#39;I&#39;:8, &#39;J&#39;:9, &#39;K&#39;:10,&#39;L&#39;:11, &#39;M&#39;:12, &#39;N&#39;:13, &#39;O&#39;:14, &#39;P&#39;:15, &#39;Q&#39;:16}).astype(int)+ df.TP_ANO_CONCLUIU  
    
    return df</code></pre>
<p>Definir finalidade de algumas colunas:</p>
<pre class="python"><code># colunas que serao dropadas
to_drop = [&#39;IN_PROVA_DEITADO&#39;,
           &#39;NU_INSCRICAO&#39;,
           &#39;CO_MUNICIPIO_ESC&#39;,
           &#39;CO_UF_NASCIMENTO&#39;,
           &#39;CO_UF_RESIDENCIA&#39;,
           &#39;CO_UF_ESC&#39;,
           &#39;CO_UF_PROVA&#39;,
           &#39;CO_MUNICIPIO_PROVA&#39;,
           &#39;CO_MUNICIPIO_RESIDENCIA&#39;,
          &#39;CO_MUNICIPIO_NASCIMENTO&#39;]

# definir target e presenca
target = &quot;NU_NOTA_REDACAO&quot;
presenca = &quot;TP_STATUS_REDACAO&quot;

# demais notas para dropar 
notas = list(set(targets)-set([target]))</code></pre>
<p>Pr√©-processamento nos dados de treino</p>
<pre class="python"><code>X = train.copy()
X = X.drop(to_drop, axis=1) 
X = X[X[presenca]==1]
X = X[~X[target].isnull()]


X = X.loc[:, ~X.columns.isin([target]+[presenca]+notas)]
X = fe_rd(X)
X = prep_data_questionarios(X)
X = fe_mun(X)
#X = fe_questionario(X)
X = fe_in(X)
X = prep_co_escola(X)
X = fe_extra(X)

y = train.loc[(train[presenca]==1)&amp;(~train[target].isnull()), target].astype(np.float64)</code></pre>
<p>Pr√©-processamento nos dados de teste</p>
<pre class="python"><code>X_test = test.copy()
X_test = X_test.drop(to_drop, axis=1) 

X_test = X_test.loc[:, ~X_test.columns.isin([presenca])]
X_test = fe_rd(X_test)
X_test = prep_data_questionarios(X_test)
X_test = fe_mun(X_test)
#X_test = fe_questionario(X_test)
X_test = fe_in(X_test)
X_test = prep_co_escola(X_test)
X_test = fe_extra(X_test)</code></pre>
<p>Feature engineering separada para evitar data leak:</p>
<pre class="python"><code># calcular estatisticas nos dados de treino
co_escola_renda_media = X.groupby(&#39;CO_ESCOLA&#39;).FE_RENDA.mean()
co_escola_idade_media = X.groupby(&#39;CO_ESCOLA&#39;).NU_IDADE.mean()

# instanciar objeto com as estatisticas por escola
co_escola_aux = pd.DataFrame({
    &#39;CO_ESCOLA&#39;: co_escola_renda_media.index,
    &#39;FE_ESCOLA_RENDA_MEDIA&#39;: co_escola_renda_media,
    &#39;FE_IDADE_MEDIA&#39;: co_escola_idade_media
}).reset_index(drop=True)

# Concatenar estatisticas nas bases de treino e teste
X = pd.merge(X, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)
X_test = pd.merge(X_test, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)

# Codigo da escola para categorico
X.loc[:, &#39;CO_ESCOLA&#39;] = X.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)
X_test.loc[:, &#39;CO_ESCOLA&#39;] = X_test.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)

# Features de contagem
X.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X_test.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X.NO_MUNICIPIO_RESIDENCIA.map({x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X_test.NO_MUNICIPIO_RESIDENCIA.map({ x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X.NO_MUNICIPIO_NASCIMENTO.map({ x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X_test.NO_MUNICIPIO_NASCIMENTO.map({x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})

X.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X_test.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})</code></pre>
<p>Ajustar modelo:</p>
<pre class="python"><code>%%time

cat_feat = X.columns[X.dtypes==&#39;category&#39;]
cat_indices = [X.columns.get_loc(x) for x in cat_feat]

for c in list(cat_feat):
    X.loc[:, c] = X.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)
    X_test.loc[:, c] = X_test.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)

X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.1, random_state=SEED)

clf = CatBoostRegressor(random_state=314,
                            cat_features=cat_indices,
                            verbose=0,
                            loss_function = &quot;RMSE&quot;,
                            od_type = &quot;Iter&quot;,
                            od_wait = 100,iterations=3000,
                            use_best_model=True)

clf.fit(X, y, eval_set = (X_eval, y_eval), verbose=False, plot=True)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/redacao_catboost.png" style="width:95.0%" />
</center>
<p>Salvar previs√µes:</p>
<pre class="python"><code>sub.loc[:, &#39;NU_NOTA_REDACAO&#39;] = clf.predict(X_test)
# alunos que nao foram fazer a prova tiraram zero
sub.loc[test.TP_STATUS_REDACAO!=1, &#39;NU_NOTA_REDACAO&#39;] = 0</code></pre>
<p>Comparar distribui√ß√£o da target nos dados de treino com rela√ß√£o √†s previs√µes do modelo:</p>
<pre class="python"><code>sns.kdeplot(train.loc[:, target], shade=True, color=&#39;r&#39;, clip=[0,1000])
sns.kdeplot(sub.loc[:, target], shade=True, color=&#39;b&#39;, clip=[0,1000])
plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
plt.title(target)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/redacao_pred.png" style="width:50.0%" />
</center>
</div>
</div>
</div>
<div id="submiss√£o" class="section level1">
<h1>Submiss√£o</h1>
<p>Veja a seguir como ficou a distribui√ß√£o das previs√µes comparada √† distribui√ß√£o da target nos dados de treino:</p>
<pre class="python"><code>plt.figure(figsize=(16, 5))

notas = [&#39;NU_NOTA_CH&#39;, &#39;NU_NOTA_CN&#39;, &#39;NU_NOTA_MT&#39;, &#39;NU_NOTA_LC&#39;, &#39;NU_NOTA_REDACAO&#39;]

for i in range(len(notas)):

    plt.subplot(1, 5, i+1)
    sns.kdeplot(train.loc[:, notas[i]], shade=True, color=&#39;r&#39;, clip=[0,1000])
    sns.kdeplot(sub.loc[:, notas[i]], shade=True, color=&#39;b&#39;, clip=[0,1000])
    plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
    plt.title(notas[i])
plt.tight_layout()
plt.show()</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/all_pred.png" style="width:95.0%" />
</center>
<p>Acredito que talvez um tuning do modelo poderia trazer mais qualidade √†s previs√µes mas com o tempo limitado n√£o pude investir muito nesta etapa.</p>
</div>
<div id="considera√ß√µes-finais" class="section level1">
<h1>Considera√ß√µes Finais</h1>
<p>Em resumo, essas foram as principais id√©ias para a solu√ß√£o da competi√ß√£o e acredito que um dos segredos era focar em feature engineering por 2 motivos:</p>
<ul>
<li>A base era muito grande e o processo de tuning seria muito custoso (a n√£o ser que tenha um √≥timo computador a disposi√ß√£o);</li>
<li>Os atributos n√£o eram an√¥nimos, o que d√° muita informa√ß√£o de contexto.</li>
</ul>
<p>Agrade√ßo aos organizadores e √† todos os participantes que tornaram esta competi√ß√£o t√£o divertida! Por mais competi√ß√µes como esta, que valorizam a comunidade brasileira de Data Science!</p>
<p>Espero que tenham gostado e at√© logo!</p>
<p>Abra√ßos!</p>
<p>Fellipe Gomes</p>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2022-04-20-solucao-final-education-quality-kaggle-competition/">Solu√ß√£o Final - ML Olympiad [2¬∫ lugar]</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">catboost</category>
      <category domain="tag">data-mining</category>
      <category domain="tag">data-science</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">machine-learning</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">pratica</category>
      <category domain="tag">r</category>
      <category domain="tag">regressao</category>
    </item>
    <item>
      <title>Desenvolva um bot e receba resultados de Machine Learning no seu Smartphone para ajudar nos investimentos</title>
      <link>https://gomesfellipe.github.io/post/2020-03-25-investment-alert/investment-alert/</link>
      <pubDate>Wed, 25 Mar 2020 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2020-03-25-investment-alert/investment-alert/</guid>
      <description>Entenda a l√≥gica de como montar uma carteira, coletar dados de finan√ßa em tempo real, treinar um modelo de Machine Learning com Prophet (Facebook Open Source) e receber an√°lises automatizadas no Smartphone</description>
      <content:encoded>&lt;![CDATA[
        


<style>
.column {
  float: left;
  width: 50%;
  padding: 10px;
}

.column4 {
  float: left;
  width: 33%;
  padding: 10px;
}

.column8 {
  float: left;
  width: 66%;
  padding: 10px;
}

.row:after {
  content: "";
  display: table; 
  clear: both;
}
</style>
<div id="por-que-investir" class="section level1">
<h1>Por que investir?</h1>
<p>Como esta sua situa√ß√£o financeira? Caso tenha alguma reserva pode ser interessante pensar em investimentos pois a poupan√ßa j√° n√£o √© mais garantia de lucro no longo prazo, n√£o acredita?</p>
<p>Estamos no final do primeiro trimestre de 2020 e desde 2019 j√° liam-se not√≠cias como esta abaixo que levam √† reflex√£o sobre reeduca√ß√£o financeira pois alertam sobre a necessidade da busca por novas oportunidades de investimento.</p>
<center>
<img src="/post/2020-03-25-investment-alert/noticia_poupanca.png" style="width:50.0%" /> <br><small>Fonte: <a href="https://noticias.r7.com/economia/economize/poupanca-em-baixa-exige-busca-por-novos-investimentos-em-2020-25122019" class="uri">https://noticias.r7.com/economia/economize/poupanca-em-baixa-exige-busca-por-novos-investimentos-em-2020-25122019</a></small>
</center>
<p></br></p>
<p>Com a finalidade de fomentar um pouco a discuss√£o sobre investimentos, trouxe nesse post algumas sugest√µes e id√©ias de como elaborar uma carteira e otimizar as escolhas para equilibrar risco em novos investimentos combinando elementos de estat√≠stica, machine learning e programa√ß√£o em R.</p>
<p>Ao final do post criaremos um <a href="https://core.telegram.org/bots">rob√¥ no telegram</a> que coletar√° os dados das cota√ß√µes adquiridas, aplicar√° o modelo <a href="https://facebook.github.io/prophet/docs/quick_start.html">Prophet do Facebook</a> para forecast e analisar√° a desmontagem da carteira segundo os crit√©rios estabelecidos e enviar√° mensagens para n√≥s com uma tabela financeira automatizada via Telegram como mostra na anima√ß√£o:</p>
<p></br></p>
<div class="row">
<div class="column4">
<center>
<img src="/post/2020-03-25-investment-alert/watch.png" />
</center>
</div>
<div class="column8">
<p><img src="/post/2020-03-25-investment-alert/report_stocks.gif" style="width:90.0%" /></p>
</div>
</div>
<p></br></p>
<div class="w3-panel w3-pale-red w3-border">
<p><i class="fa fa-exclamation-triangle" aria-hidden="true"></i> <strong>AVISO</strong>: Este post <strong>n√£o</strong> tem como finalidade ser um guia de investimentos (J√° existem muitas consultorias especializadas nisso por ai). Todos as decis√µes tomadas como diversifica√ß√£o da carteira, sele√ß√£o de a√ß√µes e crit√©rios para desmontagem da carteira s√£o <strong>exemplos</strong> e servem para ilustrar algumas <strong>possibilidades</strong> que um cientista de dados t√™m na hora de desenvolver ferramentas para auxiliar √† tomada de decis√£o.</p>
</div>
<p></br></p>
<div id="diferen√ßa-de-poupar-e-investir" class="section level2">
<h2>Diferen√ßa de poupar e investir</h2>
<p>De acordo com um <a href="https://www.infomoney.com.br/minhas-financas/brasileiros-nao-sabem-a-diferenca-entre-poupar-e-investir-afirma-especialista-2/">especialista entrevistado pela InfoMoney</a>:</p>
<blockquote>
‚ÄúPoupar √© guardar dinheiro para usar no futuro, comprar alguma coisa com ele. Investimento √© juntar dinheiro, n√£o mexer nele, para que este gere rendimentos e a√≠ sim, usar os lucros mais para frente. √â o recomendado para quem quer viver de renda no futuro, por exemplo‚Äù (‚Ä¶)
<div align="right">
<font size="1">InfoMoney - Ago 2015</font>
</div>
</blockquote>
<p>Ou seja, poupar √© acumular agora para utilizar depois, e normalmente envolve mudan√ßa de h√°bitos, pois requer uma redu√ß√£o nos gastos pessoais e familiares, j√° investir √© usar esse dinheiro poupado em aplica√ß√µes que rendam.</p>
<p>Como todo mundo sabe, n√£o existe investimento sem risco e este risco deve ser controlado e utilizado a nosso favor de forma que gere alguma seguran√ßa financeira.</p>
</div>
</div>
<div id="montagem-da-carteira" class="section level1">
<h1>Montagem da carteira</h1>
<div class="row">
<div class="column8">
<p>N√£o colocar todos os ovos na mesma cesta significa que voc√™ deve diversificar o seu investimento. Provavelmente voc√™ j√° ouviu essa frase alguma vez na vida e ela certamente faz sentido!</p>
<p>Diversificar a carteira ir√° proteger seus investimentos diminuindo o risco pois, imagine s√≥, voc√™ investe todo o seu dinheiro em uma empresa e ela passa por alguma crise assim seu dinheiro estar√° todo comprometido!</p>
<p>Existem diversos motivos para se diversificar a carteira mas acho que essa met√°fora dos ovos j√° resume bem pois acredito que ningu√©m queira perder tudo em uma queda.</p>
</div>
<div class="column4">
<p><img src="/post/2020-03-25-investment-alert/ovos_mesm_cesta.png" style="width:90.0%" /></p>
</div>
</div>
<div id="como-dividir-a-carteira" class="section level2">
<h2>Como dividir a carteira?</h2>
<p>Dependendo do risco que voc√™ deseja se expor existem muitas formas de preparar a carteira mas a id√©ias principal consiste em atingir um equil√≠brio entre dois tipos de investimento:</p>
<ul>
<li>Renda fixa: Menor exposi√ß√£o, menor risco (ex.: CDI, Selic e TR)</li>
<li>Renda vari√°vel: Maior exposi√ß√£o, maior risco (ex: A√ß√µes, Commodities, Im√≥veis)</li>
</ul>
<p>Para ajudar a dividir a carteira de investimentos neste post utilizaremos a chamada <a href="https://www.btgpactualdigital.com/blog/coluna-gustavo-cerbasi/defina-sua-estrategia-entre-renda-fixa-ou-variavel">Regra (ou Lei) dos 80</a>. A estrat√©gia √© a seguinte: subtraia da sua idade o n√∫mero 80. O resultado dessa conta vai indicar o percentual a ser investido em <a href="https://pt.wikipedia.org/wiki/Renda_vari%C3%A1vel">renda vari√°vel</a>. Por exemplo, no meu caso: tenho 26 anos, portanto <span class="math inline">\(80-26 = 54\%\)</span> dever√° ser investido em renda vari√°vel. Aos 53 anos esse percentual vai cair√° para a metade, <span class="math inline">\(27\%\)</span>.</p>
<p>A id√©ias principal por tr√°s desta regra que √© que a cada ano que passa, 1% do montante da renda vari√°vel deva ser direcionado para a <a href="https://pt.wikipedia.org/wiki/Renda_fixa">renda fixa</a>.</p>
<p>Para testar diferentes valores seguindo esta regra desenvolvi uma fun√ß√£o que se chama <a href="https://gist.github.com/gomesfellipe/a74710a63b3c8637166b538ad2f460f5"><code>montagem80()</code> (que j√° est√° dispon√≠vel no github)</a>. Vejamos alguns resultados para diferentes cen√°rios e vamos selecionar um para seguir com a montagem da carteira:</p>
<pre class="r"><code># https://gist.github.com/gomesfellipe/a74710a63b3c8637166b538ad2f460f5
devtools::source_gist(&quot;a74710a63b3c8637166b538ad2f460f5&quot;, quiet = T)</code></pre>
<div class="row">
<div class="column">
<ol style="list-style-type: decimal">
<li>Entrada de R$20.000,00 <big>üëà</big></li>
</ol>
<pre class="r"><code>montagem80(entrada = 20000, idade = 26)</code></pre>
<pre><code>## ‚Ä¢ Entrada: R$R$20.000,00
##   ‚îî‚îÄ Renda fixa:     R$9.200,00
##   ‚îî‚îÄ Renda variavel: R$10.800,00
##        (Acao + Cryptomoeda): 
##        R$9.720,00 + R$1.080,00</code></pre>
</div>
<div class="column">
<ol start="2" style="list-style-type: decimal">
<li>R$ 5.000,00 em acoes</li>
</ol>
<pre class="r"><code>montagem80(variavel = 10800, idade = 53)</code></pre>
<pre><code>## ‚Ä¢ Entrada: R$R$40.000,00
##   ‚îî‚îÄ Renda fixa:     R$29.200,00
##   ‚îî‚îÄ Renda variavel: R$10.800,00
##        (Acao + Cryptomoeda): 
##        R$9.720,00 + R$1.080,00</code></pre>
</div>
</div>
<p>Utilizaremos a primeira (1.) configura√ß√£o que esta assinalada em vermelho como exemplo, onde:</p>
<ul>
<li>Entrada: R$20.000,00</li>
<li>Renda fixa: R$9.200,00</li>
<li>Renda Vari√°vel (Acoes): R$9.720,00</li>
<li>Renda Vari√°vel (Crypt): R$1.080,00</li>
</ul>
<p>Note que a entrada deve ser o dobro na segunda (2.) configura√ß√£o caso deseje investir R$10.800,00 (que √© o valor sugerido aos 26 anos para uma entrada de R$20.000,00)</p>
<p>Ap√≥s definir a quantidade a ser investida √© hora de planejar a pr√≥xima etapa: a diversifica√ß√£o.</p>
</div>
<div id="diversifica√ß√£o-da-carteira" class="section level2">
<h2>Diversifica√ß√£o da carteira</h2>
<div class="row">
<div class="column8">
<p>A diversifica√ß√£o √© uma t√©cnica para gest√£o do risco que visa distribuir o capital investido em uma variedade de investimentos dentro de da nossa carteira.</p>
<p>Assim, o risco do portf√≥lio √© consideravelmente reduzido pois reduzimos a volatividade e criamos um equil√≠brio onde um desempenho positivo de um ativo neutraliza as baixas ocorridas em outras aplica√ß√µes. Al√©m disso a diversifica√ß√£o pode ser tanto coma renda vari√°vel quanto com a renda fixa.</p>
<p>Mas lembre-se, n√£o existe uma receita mais eficiente!</p>
</div>
<div class="column4">
</br>
<iframe src="https://giphy.com/embed/qJkRbWM1MfVjq" width="100%" height="150" frameBorder="0" class="giphy-embed" allowFullScreen></iframe>
<p>
<a href="https://giphy.com/gifs/eggs-qJkRbWM1MfVjq">via GIPHY</a>
</p>
</div>
</div>
<div id="renda-fixa" class="section level3">
<h3>Renda fixa</h3>
<p>Normalmente, tamb√©m diversificamos nossa renda fixa por√©m como gostaria de focar nas an√°lises de renda vari√°vel utilizaremos o simulador dispon√≠vel no site <a href="https://verios.com.br/" class="uri">https://verios.com.br/</a> neste <a href="https://simulador-tesouro-direto.verios.com.br/">link</a> para selecionar apenas um t√≠tulo:</p>
<center>
<img src="/post/2020-03-25-investment-alert/tesouro_direto.png" style="width:80.0%" />
<small></br>Fonte: <a href="https://simulador-tesouro-direto.verios.com.br/" class="uri">https://simulador-tesouro-direto.verios.com.br/</a></small>
</center>
<p></br></p>
<p>Como exemplo, escolhi o Tesouro Prefixado 2015 (LTN), note que com essa escolha o lucro planejado seria de quase 3 mil reais nos pr√≥ximos 5 anos.</p>
<div class="w3-panel w3-pale-red w3-border">
<p><i class="fa fa-exclamation-triangle" aria-hidden="true"></i> <strong>AVISO</strong>: Lembrando que esta sele√ß√£o √© apenas um exemplo e existem diversas informa√ß√µes a serem levadas em conta ao se fazer esta escolha al√©m de se poder diversificar tamb√©m. Convido o leitor a procurar saber mais sobre os tipos, pr√≥s e contras do Tesouro Direto.</p>
</div>
</div>
<div id="renda-vari√°vel" class="section level3">
<h3>Renda vari√°vel</h3>
<p>Para elabora√ß√£o da parte da renda vari√°vel da carteira selecionei duas a√ß√µes que s√£o inversamente correlacionadas (de forma totalmente arbitr√°ria) baseado no excelente post "<a href="https://www.tradingcomdados.com/post/2017/07/09/estudo-de-correla%C3%A7%C3%A3o-entre-a%C3%A7%C3%B5es-da-bolsa-de-valores-de-s%C3%A3o-paulo">Estudo de correla√ß√£o entre a√ß√µes da Bolsa de Valores de S√£o Paulo</a>" escrito por Victor Gomes onde o autor faz um estudo de correla√ß√µes das s√©ries hist√≥ricas de a√ß√µes de diferentes setores.</p>
<p>Al√©m disso, o Bitcoin ser√° selecionado para completar a carteira de renda vari√°vel como um ativo de alto risco com bastante volatividade. Mas voc√™ deve estar se perguntando, por que assumir este risco?</p>
<p>Existem muitas hist√≥rias de pessoas que ficaram milion√°rias com o Bitcoin pela sua valoriza√ß√£o inesperada ao longo do tempo, como por exemplo o <a href="https://www.infomoney.com.br/mercados/adolescente-fica-milionario-aos-18-anos-usando-bitcoins-apos-fazer-aposta-com-os-pais/">adolescente que ficou milion√°rio aos 18 anos usando bitcoins ap√≥s fazer aposta com os pais</a>.</p>
<p>Ent√£o <strong>eu</strong> acho que 10% dessa nossa carteira (n√£o esque√ßa que podemos ter mais de uma carteira) √© um risco que pode valer a pena correr e por isso vou inclu√≠-lo.</p>
<p>Portanto, para este exemplo consideramos:</p>
<ul>
<li>TUPY3.SA: <a href="https://www.google.com/search?q=tupy3&amp;oq=tupy3&amp;aqs=chrome..69i57.7273j0j4&amp;sourceid=chrome&amp;ie=UTF-8">Tupy</a></li>
<li>ELET3.SA: <a href="https://www.google.com/search?ei=-dxuXtuyNIbR5OUPxY2m8AE&amp;q=ELET3.SA&amp;oq=ELET3.SA&amp;gs_l=psy-ab.3..0.40045.41040..41701...0.2..0.123.237.0j2......0....2j1..gws-wiz.......0i71.a2fjObIM6cM&amp;ved=0ahUKEwibk86a8p3oAhWGKLkGHcWGCR4Q4dUDCAs&amp;uact=5">Centrais Eletricas Brasileiras SA</a></li>
<li>BTC-USD: <a href="https://www.google.com/search?ei=JN1uXsDJLJm55OUPj8-c8As&amp;q=bitcoin&amp;oq=bitcoin&amp;gs_l=psy-ab.3.0.0i131i70i258j0i131i67j0i131j0i67j0j0i67j0i131j0i67j0j0i131.25383.26281..27215...0.0..0.175.1147.1j8......0....1..gws-wiz.xu0EQx1CnXI">Bitcoin</a></li>
</ul>
<pre class="r"><code>portifolio = c(&quot;TUPY3.SA&quot;,&quot;ELET3.SA&quot;, &quot;BTC-USD&quot;)</code></pre>
<div class="w3-panel w3-pale-red w3-border">
<p><i class="fa fa-exclamation-triangle" aria-hidden="true"></i> <strong>AVISO</strong>: Volto a frisar que as escolhas das a√ß√µes foram feitas de forma arbitr√°ria. Estamos em tempos de incerteza atualmente por conta do corona v√≠rus (espero que todos fiquem bem) o que leva algumas escolhas √† serem ainda mais complexas e imprevis√≠veis.</p>
</div>
<div id="obter-dados" class="section level4">
<h4>Obter dados</h4>
<p>A aquisi√ß√£o das s√©rie hist√≥ricas das cota√ß√µes destes ativos desde 01/01/2016 foram obtidas utilizando o pacote <a href="https://github.com/business-science/tidyquant"><code>tidyquant</code></a> que nos retorna os dados das cota√ß√µes das a√ß√µes informadas em formato ‚Äúarrumado‚Äù (ou seja, familiar com fun√ß√µes do <a href="https://www.tidyverse.org/">tidyverse</a>), veja:</p>
<pre class="r"><code>library(tidyquant) # aquisicao de dados financeiros
stocks &lt;-map_df(portifolio, ~tq_get(.x, get = &quot;stock.prices&quot;, from = &quot; 2016-01-01&quot;))
saveRDS(stocks, &quot;stocks.rds&quot;)</code></pre>
<p>Veja as linhas iniciais do dataset obtido:</p>
<pre class="r"><code>stocks %&gt;% head() %&gt;% kable2()</code></pre>
<center>
<img src="/post/2020-03-25-investment-alert/img1.png" style="width:80.0%" />
</center>
<p>Existem uma s√©rie de vantagens de se trabalhar com os dados neste formato em R, veremos o porque nas pr√≥ximas se√ß√µes.</p>
</div>
<div id="arrumar-dataset" class="section level4">
<h4>Arrumar dataset</h4>
<p>O formato <a href="https://github.com/tidyverts/tsibble"><code>tsibble</code></a> √© um formato moderno para se trabalhar com s√©ries temporais trazendo a filosofia do <code>tidyverse</code> para os dados de s√©ries temporais facilitando o <a href="https://blog.earo.me/2018/12/20/reintro-tsibble/">fluxo de trabalho</a>.</p>
<p>Diversos outros pacotes podem ser combinados utilizando os dados no formato do pacote <code>tsibble</code> como os pacotes <a href="https://robjhyndman.com/hyndsight/fable/"><code>fable</code></a> e o <a href="https://github.com/mitchelloharawild/fable.prophet"><code>prophet</code></a> (sugiro a leitura para quem nao conhece) para aplica√ß√£o de modelagem de s√©ries temporais.</p>
<p>Veja como √© o fluxo ao trabalhar com objetos do tipo <code>tsibble</code></p>
<center>
<img src="/post/2020-03-25-investment-alert/ds-pipeline.png" style="width:60.0%" />
</br>
<small>Fonte: <a href="https://blog.earo.me/2018/12/20/reintro-tsibble/" class="uri">https://blog.earo.me/2018/12/20/reintro-tsibble/</a></small>
</center>
<p></br></p>
<pre class="r"><code>library(tsibble) # series temporais arrumadas
tbl_stocks &lt;- stocks %&gt;% as_tsibble(key = symbol, index = date) </code></pre>
<p>Ap√≥s importar e arrumar o dataset, seguimos para os pr√≥ximos passos.</p>
</div>
<div id="transformar" class="section level4">
<h4>Transformar</h4>
<p>Ap√≥s converter para <code>tsibble</code>, vamos preencher alguns gaps da bolsa como por exemplo os finais de semana (quando a bolsa de valores fica fechada) com o mesmo valor do dia anterior (Para este exemplo vamos fazer esse preenchimento dos gaps do final de semana mas n√£o √© sempre √© necess√°rio):</p>
<pre class="r"><code>tbl_stocks &lt;- 
  tbl_stocks %&gt;% 
  fill_gaps() %&gt;% 
  tidyr::fill(c(open, high, low, close, volume, adjusted),.direction = &quot;down&quot;)</code></pre>
<p>Com os dados arrumados vamos a algumas visualiza√ß√µes.</p>
</div>
<div id="visualizar" class="section level4">
<h4>Visualizar</h4>
<p>Vejamos qual foi o comportamento das s√©ries hist√≥ricas que coletamos desde o in√≠cio de 2016:</p>
<pre class="r"><code>library(forecast) # series temporais
library(fpp3)     # series temporais
d2 &lt;- 
  tbl_stocks %&gt;% 
  group_by(symbol) %&gt;% 
  summarise(y = mean(close))

autoplot(tbl_stocks)+
  geom_line(aes(group = symbol), color = &quot;black&quot;, show.legend = F) + 
  facet_wrap(~symbol, scales = &quot;free_y&quot;, ncol = 1)+
  geom_ma(n=6*30, color = &quot;red&quot;) + 
  theme(legend.position = &quot;none&quot;)+
  labs(subtitle = &quot;m√©dia m√≥vel n = 6 meses&quot;,
       caption = &quot;gomesfellipe.github.io&quot;)</code></pre>
<center>
<img src="/post/2020-03-25-investment-alert/unnamed-chunk-10-1.png" style="width:80.0%" />
</center>
<p>Note que a s√©rie do Bitcoin √© a mais imprevis√≠vel, houve um pico em 2018 mas ap√≥s isso n√£o houve nenhum grande pico como aquele. Em breve ocorrer√° o <a href="https://www.infomoney.com.br/onde-investir/halving-conheca-o-processo-que-pode-levar-o-bitcoin-a-uma-nova-explosao-de-preco/">Halving</a> (a contagem regressiva pode acompanhada <a href="https://www.bitcoinblockhalf.com/">neste link</a>) que √© um processo de choque de oferta e ocorre aproximadamente a cada 4 anos e pode ser uma boa oportunidade de retorno.</p>
<p>As duas s√©ries da bolsa de valores n√£o parecem ter uma correla√ß√£o muito forte, os picos ocorrem de forma alternada e isto pode ser uma caracter√≠stica boa para a carteira pois caso uma delas entre em crise a outra poder√° estar em uma fase boa.</p>
<div class="w3-panel w3-pale-blue w3-border">
<p><i class="fa fa-ambulance" aria-hidden="true"></i> Nota: Esta queda abrupta que ocorreu na bolsa no in√≠cio de 2020 √© o reflexo da <a href="https://pt.wikipedia.org/wiki/Pandemia_de_COVID-19_no_Brasil">Pandemia CODVID-19</a> que j√° come√ßou a apresentar alguns casos no Brasil e isso certamente tem gerando muita incerteza na bolsa de valores. Nem eu nem ningu√©m sabe o que pode acontecer, estou na torcida para que todos fiquem bem e pelo sucesso na conten√ß√£o desse v√≠rus! <i class="fas fa-praying-hands"></i></p>
</div>
</div>
<div id="correla√ß√µes" class="section level4">
<h4>Correla√ß√µes</h4>
<p>Para estudar melhor as conjecturas formadas ao observar o comportamento das s√©ries hist√≥rias (de que os picos e vales se alternam) vamos conferir olhada nos gr√°ficos de dispers√£o e coeficientes de <a href="https://pt.wikipedia.org/wiki/Coeficiente_de_correla%C3%A7%C3%A3o_de_postos_de_Spearman">correla√ß√£o de Spearman</a>:</p>
<pre class="r"><code>points_loess &lt;- function(data, mapping){
  ggplot(data = data, mapping = mapping) + 
    geom_point(alpha = 0.3,size=0.5) + 
    geom_smooth(method = &quot;loess&quot;)
}

tbl_stocks %&gt;%
  as_tibble() %&gt;% 
  select(symbol, date, close) %&gt;% 
  spread(key = symbol, value = close) %&gt;%
  GGally::ggpairs(columns = 2:4,
                  upper = list(continuous = GGally::wrap(&quot;cor&quot;, method = &quot;spearman&quot;)),
                  lower = list(continuous =  points_loess))</code></pre>
<center>
<img src="/post/2020-03-25-investment-alert/unnamed-chunk-11-1.png" style="width:80.0%" />
</center>
<p>Note que a rela√ß√£o entre TUPY3 e ELET3 n√£o √© linear e al√©m disso essa correla√ß√£o √© fraca, o que pode ser ben√©fico para o portf√≥lio pois uma poss√≠vel queda em uma n√£o parece n√£o ter tanto impacto na outra.</p>
<p>Al√©m disso note que a correla√ß√£o do BTC-USD com TUPY3 e ELET3 √© moderada e mesmo apresentando este valores num√©ricos, o Bitcoin √© um ativo negociado em escala global e √© de um setor totalmente diferente das outras a√ß√µes.</p>
<div class="w3-panel w3-pale-red w3-border">
<p><i class="fa fa-exclamation-triangle" aria-hidden="true"></i> <strong>AVISO</strong>: Essa an√°lise meramente num√©rica n√£o √© o suficiente para detectar uma rela√ß√£o de causa, pois correla√ß√£o n√£o implica causalidade. Isto que dizer que poda haver uma causa em comum para ambas ou que seja uma <a href="https://pt.wikipedia.org/wiki/Regress%C3%A3o_esp%C3%BAria">correla√ß√£o esp√∫ria</a>. Quando se trata da bolsa de valores √© necess√°rio tamb√©m conhecer um pouco sobre a situa√ß√£o e hist√≥ria da empresa na qual se investe.</p>
</div>
</div>
<div id="forecast-com-prophet-do-facebook" class="section level4">
<h4>Forecast com Prophet do Facebook</h4>
<p>Poder√≠amos utilizar uma s√©rie de modelos estat√≠sticos, econom√©tricos e de machine learning utilizando tanto as fun√ß√µes nativas do R base ou pacote forecast quanto as fun√ß√µes desenvolvidas para trabalhar de maneira ‚Äúarrumada‚Äù com tsibble mas resolvi fazer uma abordagem diferente neste post escolhendo o modelo disponibilizado pelo Facebook.</p>
<div class="row">
<div class="column">
<p></br>
O <a href="https://facebook.github.io/prophet/">Prophet</a> √© um software de c√≥digo aberto disponibilizado pela equipe de <a href="https://research.fb.com/category/data-science/">Data Science do Facebook</a> que fornece um procedimento para realiza√ß√£o de previs√µes de dados de s√©ries temporais.</p>
</div>
<div class="column">
<p><img src="/post/2020-03-25-investment-alert/prophet_logo.png" style="width:80.0%" />
</br><small>Fonte: <a href="https://facebook.github.io/prophet/" class="uri">https://facebook.github.io/prophet/</a></small></p>
</div>
</div>
<p>Segundo a <a href="https://facebook.github.io/prophet/">documenta√ß√£o oficial</a>, (em tradu√ß√£o livre):</p>
<blockquote>
<p>O Prophet tem como ‚Äúbase em um modelo aditivo no qual tend√™ncias n√£o lineares se ajustam √† sazonalidade anual, semanal e di√°ria, al√©m de efeitos de f√©rias. Funciona melhor com s√©ries temporais que t√™m fortes efeitos sazonais e v√°rias temporadas de dados hist√≥ricos. O Profeta √© robusto para a falta de dados e mudan√ßas na tend√™ncia, e geralmente lida bem com outliers‚Äù.</p>
</blockquote>
<p>Este modelo me pareceu uma boa op√ß√£o para exemplificar a etapa da modelagem de s√©ries temporais deste post. Vamos ver o que o modelo do Facebook tem a nos dizer sobre o futuro das nossas a√ß√µes.</p>
<div id="divis√£o-entre-treino-e-teste-em-s√©ries-temporais" class="section level5">
<h5>Divis√£o entre treino e teste em s√©ries temporais</h5>
<p>Assim como em uma tarefa de machine learning que n√£o envolvem dados temporais, no caso de s√©ries temporais, quando desejamos avaliar o ajuste do nosso modelo tamb√©m dividimos o dataset em treino e teste por√©m utilizamos a data como √≠ndice.</p>
<p>Veja como ser√£o divididas nossas s√©ries hist√≥ricas:</p>
<ul>
<li>treino: inicio da s√©rie at√© 18/11/20;</li>
<li>teste: de 18/11/2020 at√© o final da s√©rie hist√≥rica (2 meses atr√°s)</li>
</ul>
<pre class="r"><code>h = 30 * 2
data_split &lt;- Sys.Date() - h

tbl_stocks_train &lt;-  
  tbl_stocks %&gt;% 
  filter(date &lt;= data_split) %&gt;% 
  select(symbol, date, close)

tbl_stocks_test &lt;- 
  tbl_stocks %&gt;%
  filter(date &gt; data_split) %&gt;% 
  select(symbol, date, close)</code></pre>
<p>Ap√≥s dividir os dados em treino e teste j√° estamos habilitados √† utilizar o modelo. Note que, por default, o modelo espera duas colunas nomeadas como <code>ds</code>: data da s√©rie e <code>y</code>: vari√°vel target da s√©rie.</p>
<p>Al√©m disso faremos uma previs√£o 6 meses a frente dos dados de teste para entender qual a tend√™ncia o modelo estaria adotando.</p>
<pre class="r"><code>library(prophet)

prophet_results &lt;- 
  tbl_stocks_train %&gt;% 
  rename(ds = date, y = close) %&gt;% 
  nest(data = c(ds, y)) %&gt;% 
  mutate(pmodel = map(data, ~ prophet(.x, daily.seasonality=TRUE))
  )%&gt;% 
  mutate(pprediction = map(pmodel, ~.x %&gt;%  
                             make_future_dataframe(periods = h + 30*6) %&gt;%
                             predict(.x,.))) </code></pre>
<p>Veja os resultados do ajuste do modelo:</p>
<pre class="r"><code>library(patchwork)

pmap(list(
  x = prophet_results$pprediction,
  y = split(tbl_stocks_train, tbl_stocks_train$symbol),
  z = split(tbl_stocks_test, tbl_stocks_test$symbol),
  w = prophet_results$symbol
),function(x, y, z, w){
  x %&gt;% 
    as_tibble() %&gt;%
    mutate(ds=as_date(ds)) %&gt;%
    select(ds, trend, yhat, yhat_lower, yhat_upper) %&gt;% 
    ggplot() + 
    geom_line(aes(x=ds, y=yhat, color=&quot;blue&quot;), show.legend = F) +
    geom_line(data=y, aes(x=date, y=close, color=&quot;black&quot;), show.legend = F) +
    geom_line(data=z, aes(x=date, y=close, color=&quot;red&quot;), show.legend = T) +
    geom_ribbon(aes(x=ds, ymin=yhat_lower, ymax=yhat_upper), alpha=0.2)  +
    scale_x_date(#limits = c(as.Date(&quot;2018-06-01&quot;), Sys.Date() + 30*12), 
      date_breaks = &quot;6 month&quot;, date_labels = &quot;%m/%Y&quot;) +
    theme_bw()+
    labs(y = w, x = &quot;&quot;, caption = &quot;gomesfellipe.github.io&quot;)+
    scale_colour_manual(values=c(&quot;black&quot;,&quot;blue&quot;,&quot;red&quot;), name=&quot;&quot;, labels=c(&quot;treino&quot;,&quot;modelo&quot;,&quot;teste&quot;))+
    theme(legend.position = c(1-0.8,1-0.2), 
          legend.background = element_rect(fill=alpha(&#39;lightgrey&#39;, 0.2)),
          legend.direction = &quot;horizontal&quot;)
}
) %&gt;% {.[[1]] / .[[2]] / .[[3]]}

ggsave(&quot;prophet.png&quot;) # salvar para o bot retornar este resultado!</code></pre>
<center>
<img src="/post/2020-03-25-investment-alert/unnamed-chunk-15-1.png" style="width:80.0%" />
</center>
<p>Parece interessante..</p>
<p>Veja que a linha azul (modelo ajustado) se ajusta bem √† linha preta (dados de treino) acompanhando a s√©rie hist√≥rica e captando algumas tend√™ncias n√£o lineares.</p>
<p>Por√©m note que a linha azul se perde completamente da linha vermelha (dados de teste) no in√≠cio de 2020 e acho isso muito razo√°vel pois dificilmente algum modelo iria prever os efeitos de uma Pandemia utilizando apenas a s√©rie hist√≥rica do ativo.</p>
<p>Note ainda que a linha azul se estende at√© o final de 2020 (previs√µes para os pr√≥ximos 6 meses) o que sugere que a s√©rie possu√≠a esta tendencia positiva ao longo dos anos, segundo o modelo Prophet .</p>
<div class="w3-panel w3-pale-yellow w3-border">
<p><i class="fa fa-exclamation-triangle" aria-hidden="true"></i> <strong>Aviso</strong>: Existem v√°rias maneiras de estudar e melhorar a qualidade do ajuste deste modelo mas como o objetivo deste post n√£o √© este deixo como aviso para o leitor.</p>
</div>
<p>Com a interpreta√ß√£o dos resultados conclu√≠da.. vamos √†s compras!</p>
</div>
</div>
</div>
</div>
</div>
<div id="comprando-a√ß√µes" class="section level1">
<h1>Comprando a√ß√µes</h1>
<p>Ap√≥s toda essa exemplifica√ß√£o de como podem ser feitas as an√°lises para a elabora√ß√£o da carteira chegou a hora das compras.</p>
<p>Suponha que tiv√©ssemos realizado nossas compra no fechamento do dia <strong>2018-12-01</strong>, quando os valores de <font color="blue"><strong>fechamento</strong></font> das cota√ß√µes eram as seguintes:</p>
<pre class="r"><code>tbl_stocks %&gt;% 
  filter(date == &quot;2018-12-01&quot;) %&gt;% 
  mutate(close = cell_spec(moeda_real(close), &quot;html&quot;, color = &quot;blue&quot;, bold = T)) %&gt;% 
  mutate_at(c(3:5, 8), ~moeda_real(.x)) %&gt;% 
  kable2()</code></pre>
<center>
<img src="/post/2020-03-25-investment-alert/img2.png" style="width:80.0%" />
</center>
<p>Neste dia a cota√ß√£o para ELET3 era R$24,42 e TUPY3 era R$19,73 e suponha que tenhamos comprado 200 lotes de ELET3 e 150 fracion√°rios de TUPY, totalizando R$7.843,64 (pr√≥ximo ao que tinhaamos planejado no inicio do estudo)</p>
<p>Vamos guardar estes valores:</p>
<pre class="r"><code>cot_inicio = filter(tbl_stocks, date == &quot;2018-12-01&quot;, symbol != &quot;BTC-USD&quot;) %&gt;% pull(close)
qtd_inicio = c(elet = 200, tupy = 150)</code></pre>
<p>Note que o valor do Bitcoin est√° em d√≥lares, para obter o valor em reais (R$) daquele dia vamos utilizar a <a href="https://www.mercadobitcoin.net/api/BTC/day-summary/2020/01/09/">API do Mercado Bitcoin</a>. Como n√£o existe nenhum pacote que forne√ßa estes dados diretamente no R, a requisi√ß√£o ser√° feita normalmente via API com o pacote <code>jsonlite</code>:</p>
<pre class="r"><code>library(jsonlite) # requisicao de api
url &lt;- glue::glue(&quot;https://www.mercadobitcoin.net/api/BTC/day-summary/2019/12/01/&quot;)
safe_fromJSON &lt;- purrr:::safely(fromJSON, as.numeric(NA)) 
consulta &lt;- safe_fromJSON(url)$result %&gt;% map_dfc(~.x)

consulta %&gt;%
  select(-date) %&gt;% 
  mutate_all(~moeda_real(.x)) %&gt;%
  mutate(closing = cell_spec(closing, &quot;html&quot;, color = &quot;blue&quot;)) %&gt;% 
  kable2()</code></pre>
<center>
<img src="/post/2020-03-25-investment-alert/img3.png" style="width:80.0%" />
</center>
<p>O pre√ßo de fechamento foi de R$31.747,38 e suponhamos que tenha sido este o valor pago no dia. (Parece que neste dia o d√≥lar estava em torno de R$4,03)</p>
<p>Para completar esta carteira fict√≠cia vamos adquirir 0,0032 do valor de um Bitcoin</p>
<pre class="r"><code>cot_inicio[3] &lt;- consulta$closing
qtd_inicio[3] &lt;- 0.032</code></pre>
<p>Portanto, ao valor de R$31.747,38 compramos 0.032 Bitcoin totalizando R$1.015,92 completando nossa carteira.</p>
<div id="tabela-financeira" class="section level2">
<h2>Tabela financeira</h2>
<p>Semelhante a uma planilha financeira, criaremos uma tabela financeira automatizada que receber√° como input os valores da montagem e calcular√° automaticamente os valores do desmontagem no tempo atual utilizando dados de APIs abertas.</p>
<blockquote>
‚ÄúAquilo que n√£o se pode medir, n√£o se pode melhorar‚Äù.
<div align="right">
<font size="1">F√≠sico irland√™s William Thomson</font>
</div>
</blockquote>
<p>Primeiro √© necess√°rio obter os valores mais recentes das cota√ß√µes das acoes que compramos na bolsa e para isto ser√° necess√°rio utilizar outro pacote pois o <code>tidyquant</code> s√≥ fornece os dados em frequ√™ncia di√°ria.</p>
<p>Utilizaremos portanto, o pacote <a href="https://www.business-science.io/code-tools/2017/09/03/alphavantager-0-1-0.html">alphavantager</a> que fornece dados de finan√ßas da API gratuita <a href="https://www.alphavantage.co/">Alpha Vantage</a> no formato arrumados e tamb√©m foi desenvolvida pela <a href="https://github.com/business-science">Business Science</a> (mesmo criados do pacote <code>tidyquant</code>).</p>
<div class="w3-panel w3-pale-yellow w3-border">
<p><i class="fa fa-exclamation-triangle" aria-hidden="true"></i> <strong>Aviso</strong>: Na <a href="https://www.alphavantage.co/support/#api-key">documenta√ß√£o da api do Alphavantage</a> recomenda-se a solicita√ß√µes de API com modera√ß√£o, suportando at√© 5 solicita√ß√µes de API por minuto e 500 solicita√ß√µes por dia para obter o melhor desempenho no servidor. Caso deseje segmentar um volume maior de chamadas da API, confira a <a href="https://www.alphavantage.co/premium/">associa√ß√£o premium.</a>.</p>
</div>
<p>J√° para a coleta da cota√ß√£o em tempo real do Bitcoin em reais (R$), utilizaremos novamente a <a href="https://www.mercadobitcoin.com.br/api-doc/?">api do mercado bitcoin</a>.</p>
<pre class="r"><code># Importar dados da bolsa de valores ==================================

library(alphavantager) # api streaming bovespa

AV_API_KEY = Sys.getenv(&quot;AV_API_KEY&quot;)
av_api_key(AV_API_KEY)

consulta_acoes &lt;- map_df(portifolio[1:2], ~{
  alphavantager::av_get(symbol = .x,
         av_fun = &quot;TIME_SERIES_INTRADAY&quot;,
         interval = &quot;1min&quot;,  # &quot;1min&quot;, &quot;5min&quot;, &quot;15min&quot;, &quot;30min&quot; ou &quot;60min&quot;
         outputsize = &quot;compact&quot;) %&gt;%  # &quot;full&quot;
    bind_cols(stock = rep(.x, nrow(.)))
})

# Impotar dados do bitcoin ============================================

coin &lt;- &quot;BTC&quot;
method &lt;- &quot;ticker&quot;
url &lt;- glue::glue(&quot;https://www.mercadobitcoin.net/api/{coin}/{method}/&quot;)

safe_fromJSON &lt;- safely(fromJSON, as.numeric(NA)) 
consulta_bitcoin &lt;- 
  safe_fromJSON(url)$result$ticker %&gt;% 
  as_tibble() %&gt;% 
  transmute(timestamp = lubridate::ymd_hms(as.POSIXct(date, origin=&quot;1970-01-01&quot;)),
            open, high, low, close = sell, volume = NA, stock = &quot;BTC.BR&quot;) %&gt;% 
  mutate_at(c(&#39;open&#39;, &#39;high&#39;, &#39;low&#39;, &#39;close&#39;), ~as.numeric(.x))

# Combinar resultados das consultas ==================================

consulta_atual &lt;- 
  bind_rows(
    consulta_acoes %&gt;% 
      group_by(stock) %&gt;% 
      filter(timestamp == last(timestamp)),
    consulta_bitcoin
  ) 

# Salvar consulta ====================================================
saveRDS(consulta_atual, &quot;consulta_atual.rds&quot;)</code></pre>
<p>Resultados da consulta atual (ap√≥s combinar a requisi√ß√£o da bolsa de valores e do Mercado Bitcoin):</p>
<pre class="r"><code>consulta_atual  %&gt;%
  mutate_at(2:4, ~moeda_real(.x)) %&gt;% 
  mutate_if(is.numeric, ~ifelse(is.na(.x), &quot;-&quot;, format(.x, digits = 2))) %&gt;% 
  mutate(close = cell_spec(moeda_real(close), &quot;html&quot;, color = &quot;blue&quot;)) %&gt;% 
  kable2()</code></pre>
<center>
<img src="/post/2020-03-25-investment-alert/img4.png" style="width:80.0%" />
</center>
<p>Com os valores da Montagem organizados e os valores da Desmontagem coletados em tempo real j√° podemos construir nossa tabela financeira automatizada.</p>
<p>A tabela cont√©m:</p>
<ul>
<li>Valores de cota√ß√£o e quantidade adquiridas de cada uma no momento da montagem da carteira;</li>
<li>Valores de cada cota√ß√£o no momento atual com suas respectivas quantidades dispon√≠veis;</li>
<li>Resultados de o ganho (ou perda) seguido do resultado bruto caso realize a venda agora;</li>
<li>√öltima coluna indica se vale a pena vender ou n√£o aquela cota√ß√£o considerando que o valor de venda √© superior ao valor de compra.</li>
</ul>
<div class="w3-panel w3-pale-yellow w3-border">
<p><i class="fa fa-exclamation-triangle" aria-hidden="true"></i> <strong>Aviso</strong>: Essa opera√ß√£o de vender o ativo caso o pre√ßo da venda seja maior que o da compra √© apenas um exemplo. Poder√≠amos utilizar diversas estat√≠sticas para determinar o momento da opera√ß√£o por√©m adotei esta apenas para ilustrar o funcionamento da tabela financeira, o limite de op√ß√µes √© a sua criatividade!</p>
</div>
<p>Veja a tabela final com os resultados atualizados em tempo real:</p>
<pre class="r"><code>porcentagem &lt;- function(x){paste0(round(x,2), &quot;%&quot;)} # Funcao auxiliar

# Tabela resultado
financas &lt;- 
  tibble(
    ativo = portifolio,
    cot_inicio = cot_inicio,
    qtd_inicio = qtd_inicio,
    vol_inicio = cot_inicio * qtd_inicio,
    cot_atual = consulta_atual$close,
    qtd_atual = qtd_inicio,
    vol_atual = cot_atual * qtd_atual,
    ganho_perda = vol_atual - vol_inicio,
    resultado_bruto = ganho_perda / vol_inicio * 100
  ) 


tabela &lt;- 
  financas %&gt;% 
  mutate(
    cot_inicio = moeda_real(cot_inicio),
    cot_atual = moeda_real(cot_atual),
    vol_inicio = moeda_real(vol_inicio),
    vol_atual = moeda_real(vol_atual),
    qtd_inicio = round(qtd_inicio,4),
    qtd_atual = round(qtd_atual,4),
    ` ` = ifelse(ganho_perda &gt; 0,&quot;\u2713&quot;, &quot;\u2718&quot;) ,
    cot_atual = cell_spec(cot_atual, &quot;html&quot;, color = &quot;blue&quot;),
    ganho_perda = cell_spec(moeda_real(ganho_perda), &quot;html&quot;,
                            color = ifelse(ganho_perda &gt; 0, 
                                           &quot;green&quot;, &quot;red&quot;)),
    resultado_bruto = cell_spec(porcentagem(resultado_bruto), &quot;html&quot;,
                                color = ifelse(resultado_bruto &gt; 0, 
                                               &quot;green&quot;, &quot;red&quot;))) %&gt;% 
  `colnames&lt;-`(stringr::str_replace_all(colnames(.), &quot;(_|[[:space:]])&quot;, &quot;\n&quot;)) %&gt;% 
  # Exibicao
  kable(format = &quot;html&quot;, escape = F) %&gt;%
  kable_styling(c(&quot;striped&quot;, &quot;bordered&quot;, &quot;hover&quot;, &quot;responsive&quot;), full_width = T, font_size = 12) %&gt;%
  add_header_above(c(&quot; &quot;, &quot;Montagem&quot; = 3,
                     &quot;Desmontagem / Atual&quot; = 3, &quot;Resultado&quot; = 3))

tabela</code></pre>
<center>
<img src="/post/2020-03-25-investment-alert/img5.png" style="width:80.0%" />
</center>
<p>Agora que j√° possu√≠mos nossa requisi√ß√£o completa e arrumada em tempo real precisamos ter acesso a esta informa√ß√£o de forma din√¢mica tamb√©m e para isso utilizaremos o bot do Telegram.</p>
</div>
</div>
<div id="bot-telegram" class="section level1">
<h1>Bot Telegram</h1>
<p>Depois de muitas decis√µes tomadas enfim chegamos ao bot! Espero que tenha notado que montar uma carteira n√£o √© uma tarefa f√°cil pois envolve exposi√ß√£o ao risco e tamb√©m exige certo acompanhamento do mercado.</p>
<div class="row">
<div class="column8">
<p></br>
Para criar um bot no Telegram basta seguir os passos do <a href="https://github.com/lbraglia/telegram">readme</a> do pacote <a href="https://github.com/lbraglia/telegram"><code>telegram</code></a> disponibilizado no Github ou seguir os passos desse excelente <a href="https://www.curso-r.com/blog/2017-08-19-r-telegram-bitcoin/">post do curso-r</a> que me inspirou a uns anos atras e hoje me auxiliou novamente para criar este bot. Ao concluir a etapa de configura√ß√£o teremos um novo contado no Telegram, o nosso bot!</p>
<p>Com a configura√ß√£o no aplicativo do Telegram conclu√≠da, o primeiro passo para configurar as a√ß√µes do bot no R √© iniciar um objeto TGBot declarando o id do seu bot. No meu caso o bot se chama <em>Stocks</em> e o id √© <em>fgstockbot</em>.</p>
</div>
<div class="column4">
</br>
<center>
<img src="/post/2020-03-25-investment-alert/bot_telegram.png" style="width:99.0%" />
</center>
</div>
</div>
<p>Com o R conectado ao bot do Telegram j√° somos capazes de criar um conjunto de regras de forma que o bot nos retorne as informa√ß√µes que desejamos.</p>
<p>Desenvolveremos a fun√ß√£o <code>report_stocks()</code> que programa o bot para realizar o seguinte algoritmo:</p>
<ol start="0" style="list-style-type: decimal">
<li>Carregar depend√™ncias e conectar chaves de acesso</li>
<li>Conferir se a bolsa de valores esta aberta</li>
<li>Requisi√ß√£o das cota√ß√µes da bolsa de valores em <em>real-time</em></li>
<li>Requisi√ß√£o da cota√ß√£o do Bitcoin em <em>real-time</em></li>
<li>Combinar resultados</li>
<li>Inserir resultados da coleta na tabela financeira</li>
<li>Se o valor de algum volume atual seja maior que o volume inicial:
<ul>
<li>Calcular valores de desmontagem</li>
<li>Preparar layout da tabela financeira</li>
<li>Salvar resultados</li>
<li>Enviar via Telegram</li>
</ul></li>
<li>Aguardar 20 minutos para a pr√≥xima requisi√ß√£o</li>
<li>Repetir todo o processo</li>
</ol>
<p>Parece complicado mas √© tranquilo pois todos os c√≥digos de cada uma destas tarefas j√° foram desenvolvidos nas se√ß√µes anteriores e ser√£o apenas combinados. A fun√ß√£o <a href="https://gist.github.com/gomesfellipe/357af0735d2aedca60146a7655e33929"><code>report_stocks()</code> j√° esta dispon√≠vel no github</a>, veja a baixo:</p>
<p>(A frequ√™ncia adotata como default pela fun√ß√£o tenta fazer a requisi√ß√£o com a maior frequ√™ncia poss√≠vel na api do Alphavantage)</p>
<script src="https://gist.github.com/gomesfellipe/357af0735d2aedca60146a7655e33929.js"></script>
<p>Ap√≥s todas as devidas configura√ß√µes, basta carregar a fun√ß√£o e executar para obter o seguinte resultado:</p>
<pre class="r"><code># https://gist.github.com/gomesfellipe/357af0735d2aedca60146a7655e33929
devtools::source_gist(&quot;357af0735d2aedca60146a7655e33929&quot;,quiet = T)

# Executar
bot_report_stocks(portifolio = portifolio, 
                  cot_inicio = cot_inicio,
                  qtd_inicio = qtd_inicio)</code></pre>
<center>
<img src="/post/2020-03-25-investment-alert/report_stocks.gif" style="width:70.0%" />
</center>
<p></br>
E assim obtemos um feedback atrav√©s do nosso Smartphone ou Smartwatch em tempo real sobre o desempenho da nossa carteira!</p>
</div>
<div id="conclus√£o-e-pr√≥ximos-passos" class="section level1">
<h1>Conclus√£o e pr√≥ximos passos</h1>
<p>Neste post criamos um bot que coleta os dados e faz an√°lises disponibilizando-as em tempo real no Telegram. Por√©m vimos tamb√©m o qu√£o dif√≠cil pode ser a montagem de uma carteira e as an√°lises envolvendo s√©ries hist√≥ricas de ativos.</p>
<p>Diante dos resultados obtidos aqui existe uma grande gama de op√ß√µes de inova√ß√µes para trabalhos futuros como:</p>
<ul>
<li>Programar o bot para responder a uma <a href="https://support.apple.com/pt-br/guide/watch/apd92a90f882/watchos">menssagem r√°pida no smartwatch</a> com o valor de uma previs√£o;</li>
<li>Desenvolver um Shiny parametrizando o bot para consultar an√°lises em tempo real;</li>
<li>Hospedar a rotina em um servidor para operacionalizar o bot (caso tenha d√∫vidas de como se iniciar um RStudio Server u um Shiny Server <a href="https://gomesfellipe.github.io/post/2018-10-27-server-cloud/server-cloud/">consulte este post do blog</a>);</li>
<li>Criar uma API com <a href="https://www.rplumber.io">plumber</a> para fornecer os resultados;</li>
<li>Treinar modelo utilizando mais dados, mais vari√°veis explicativas e tuning dos par√¢metros;</li>
<li>Criar pacote com um rob√¥ mais geral para responder diferentes consultas.</li>
</ul>
<p>Espero que tenha gostado qualquer d√∫vida deixe nos coment√°rios!</p>
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias</h1>
<ul>
<li><a href="https://www.infomoney.com.br/minhas-financas/brasileiros-nao-sabem-a-diferenca-entre-poupar-e-investir-afirma-especialista-2/" class="uri">https://www.infomoney.com.br/minhas-financas/brasileiros-nao-sabem-a-diferenca-entre-poupar-e-investir-afirma-especialista-2/</a></li>
<li><a href="https://www.btgpactualdigital.com/blog/investimentos/diversificacao-de-investimentos" class="uri">https://www.btgpactualdigital.com/blog/investimentos/diversificacao-de-investimentos</a></li>
<li><a href="https://www.btgpactualdigital.com/blog/coluna-gustavo-cerbasi/defina-sua-estrategia-entre-renda-fixa-ou-variavel" class="uri">https://www.btgpactualdigital.com/blog/coluna-gustavo-cerbasi/defina-sua-estrategia-entre-renda-fixa-ou-variavel</a></li>
<li><a href="https://blog.earo.me/2018/12/20/reintro-tsibble/" class="uri">https://blog.earo.me/2018/12/20/reintro-tsibble/</a></li>
<li><a href="https://www.tradingcomdados.com/post/2017/07/09/estudo-de-correla%C3%A7%C3%A3o-entre-a%C3%A7%C3%B5es-da-bolsa-de-valores-de-s%C3%A3o-paulo" class="uri">https://www.tradingcomdados.com/post/2017/07/09/estudo-de-correla%C3%A7%C3%A3o-entre-a%C3%A7%C3%B5es-da-bolsa-de-valores-de-s%C3%A3o-paulo</a></li>
<li><a href="https://www.business-science.io/code-tools/2017/10/28/demo_week_h2o.html" class="uri">https://www.business-science.io/code-tools/2017/10/28/demo_week_h2o.html</a></li>
<li><a href="https://www.infomoney.com.br/mercados/adolescente-fica-milionario-aos-18-anos-usando-bitcoins-apos-fazer-aposta-com-os-pais/" class="uri">https://www.infomoney.com.br/mercados/adolescente-fica-milionario-aos-18-anos-usando-bitcoins-apos-fazer-aposta-com-os-pais/</a></li>
<li><a href="https://www.infomoney.com.br/onde-investir/halving-conheca-o-processo-que-pode-levar-o-bitcoin-a-uma-nova-explosao-de-preco/" class="uri">https://www.infomoney.com.br/onde-investir/halving-conheca-o-processo-que-pode-levar-o-bitcoin-a-uma-nova-explosao-de-preco/</a></li>
<li><a href="https://cran.r-project.org/web/packages/telegram/README.html" class="uri">https://cran.r-project.org/web/packages/telegram/README.html</a></li>
<li><a href="https://otexts.com/fpp2/" class="uri">https://otexts.com/fpp2/</a>
<a href="https://www.curso-r.com/blog/2017-08-19-r-telegram-bitcoin/" class="uri">https://www.curso-r.com/blog/2017-08-19-r-telegram-bitcoin/</a></li>
<li><a href="https://www.curso-r.com/blog/2017-08-19-r-telegram-bitcoin/" class="uri">https://www.curso-r.com/blog/2017-08-19-r-telegram-bitcoin/</a></li>
</ul>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2020-03-25-investment-alert/investment-alert/">Desenvolva um bot e receba resultados de Machine Learning no seu Smartphone para ajudar nos investimentos</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">bitcoin</category>
      <category domain="tag">bolsa-de-valores</category>
      <category domain="tag">correlacao</category>
      <category domain="tag">dplyr</category>
      <category domain="tag">estatistica</category>
      <category domain="tag">machine-learning</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">prophet</category>
      <category domain="tag">r</category>
      <category domain="tag">reports</category>
    </item>
    <item>
      <title>An√°lise de sobreviv√™ncia com dados do jogo PUBG dispon√≠veis no Kaggle</title>
      <link>https://gomesfellipe.github.io/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle/</link>
      <pubDate>Wed, 28 Nov 2018 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle/</guid>
      <description>O que interefere na probabilidade de um indiv√≠duo sobreviver? Quais fatores apresentam efeito no risco de morte em um intervalo de tempo? Neste post buscaremos evid√™ncias estat√≠sticas para responder estas perguntas em dados abertos do PUBG hospedados no Kaggle</description>
      <content:encoded>&lt;![CDATA[
        


<div id="an√°lise-de-sobreviv√™ncia-e-pubg" class="section level1">
<h1>An√°lise de sobreviv√™ncia e PUBG</h1>
<p>An√°lise de sobreviv√™ncia √© um termo que se refere a situa√ß√µes m√©dicas e √© caracterizada pela sua vari√°vel resposta, que pode ser apresentada de tr√™s formas: probabilidade de sobreviv√™ncia, taxa de incid√™cia e taxa de incid√™ncia acumulada.</p>
<p>Na engenharia este termo tamb√©m √© conhecido como confiabilidade, no entanto, condi√ß√µes parecidas podem ocorrer em (inusitadas) outras √°reas.</p>
<p>PUBG √© um jogo online multiplayer de batalha em que 100 jogadores s√£o lan√ßados em uma ilha e tem como objetivo principal <strong>sobreviver</strong>, a √°rea de jogo diminui progressivamente, confinando os sobreviventes a um espa√ßo cada vez menor e for√ßando encontros e o vencedor √© o √∫ltimo jogador (ou time) a permanecer vivo.</p>
<p>Um √∫nico jogo dura aproximadamente de 30-35 minutos e neste tempo o jogador coleta itens (arma, cura, boost), abate outros jogadores, comete e leva dano de seus advers√°rios, pode dirigir ve√≠culos dentre outras a√ß√µes enquanto tentam sobrevier ao mesmo tempo.</p>
<p>Quest√µes que surgiram em mente ap√≥s um per√≠odo de estudos de an√°lise de sobreviv√™ncia e confiabilidade e ouvindo pessoas falarem sobre esta modalidade de jogo:</p>
<ul>
<li>O que interefere na probabilidade de um indiv√≠duo sobreviver?</li>
<li>O que tem efeito no risco de um jogador ser abatido em um intervalo de tempo?</li>
</ul>
<p>Faremos uma abordagem estat√≠stica aqui, ap√≥s uma breve an√°lise explorat√≥ria os dados ser√£o avaliados utilizando o modelo de Kaplan-Meier, que √© um estimador de forma n√£o param√©trica para a fun√ß√£o de sobreviv√™ncia e o modelo semiparam√©trico de regress√£o de riscos proporcionais de Cox.</p>
</div>
<div id="a-base-de-dados" class="section level1">
<h1>A Base de dados</h1>
<p>A base de dados utilizada foi obtida atrav√©s do Kaggle em ‚ÄúPUBG Match Deaths and Statistics‚Äù: <a href="https://www.kaggle.com/skihikingkevin/pubg-match-deaths" class="uri">https://www.kaggle.com/skihikingkevin/pubg-match-deaths</a> que conta com mais de 65 milh√µes de registros de mortes no jogo PlayerUnknown Battleground‚Äôs matches - PUBG.</p>
<p><a href="https://www.kaggle.com/gomes555/analise-de-sobrevivencia-km-e-cox/">Existe uma vers√£o deste post no kaggle</a> e al√©m desta base, existe uma competi√ß√£o em andamento que vai at√© o dia 30 de Janeiro no link:<a href="https://www.kaggle.com/c/pubg-finish-placement-prediction" class="uri">https://www.kaggle.com/c/pubg-finish-placement-prediction</a> que desafia os jogadores a prever o posicionamento do vencedor em percentil, onde 1 corresponde ao 1¬∫ lugar e 0 corresponde ao √∫ltimo lugar do jogo. Fiz uma participa√ß√£o com um <a href="https://www.kaggle.com/gomes555/xgboost-caret-for-fun">script testando os resultados do algor√≠tmo xgboost com caret</a> e tamb√©m testei uns <a href="https://www.kaggle.com/gomes555/tidyverse-machine-learning-for-fun">ajustes com random forest utilizando o tidyverse</a>. Esses scripts s√£o abertos e est√£o prontos para uso, <a href="https://www.kaggle.com/gomes555">n√£o me renderam a melhor posi√ß√£o</a> mas a intens√£o aqui √©, principalmente, aprender e testar os m√©todos pois S√£o muitas possibilidade para aprender e praticar. Voltando a base de dados:</p>
<p>Segundo a <a href="https://www.kaggle.com/skihikingkevin/pubg-match-deaths#aggregate.zip">descri√ß√£o da base no kaggle</a>:</p>
<p><code>agg_match_stats_x.csv</code> fornece informa√ß√µes de correspond√™ncia mais agregadas sobre os dados de mortes, como tamanho da fila, fpp/tpp, morte do jogador, etc.</p>
<p>As colunas s√£o as seguintes:</p>
<div class="col2">
<ul>
<li><code>match_id</code> : O id √∫nico de correspond√™ncia gerado por pubg.op.gg. √â poss√≠vel fazer uma jun√ß√£o disso com os dados das mortes para ver todas as informa√ß√µes</li>
<li><code>party_size</code> : o n√∫mero m√°ximo de jogadores por equipe. por exemplo, 2 implica que era um sistema de fila dupla</li>
<li><code>player_dist_ride</code> : unidades de distancia total (metros?) que o jogador percorreu em um ve√≠culo</li>
<li><code>player_dist_walk</code> : unidades de distancia total (metros?) percorrida pelo jogador a p√©</li>
<li><code>match_mode</code> : se o jogo foi jogado em primeira pessoa (fpp) ou em terceira pessoa (tpp)</li>
<li><code>team_placement</code> : a classifica√ß√£o final da equipe dentro da partida</li>
<li><code>player_dmg</code> : Total de pontos de vida que o jogador distribuiu</li>
<li><code>player_assists</code> : N√∫mero de assist√™ncias que o jogador marcou</li>
<li><code>game_size</code> : o n√∫mero total de equipes que estavam no jogo</li>
<li><code>player_dbno</code> : N√∫mero de knockdowns que o jogador marcou</li>
<li><code>player_kills</code> : N√∫mero de mortes que o jogador marcou</li>
<li><code>team_id</code> : o ID da equipe √† qual o jogador pertencia</li>
<li><code>date</code> : a data e a hora em que a partida ocorreu</li>
<li><code>player_name</code> : nome do jogador</li>
</ul>
<hr />
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/img.png" /></p>
</div>
<p>A rotinas abaixo carregam os pacotes, fun√ß√µes customizadas e salva em extens√£o <code>.rds</code>uma amostra da base de dados utilizadas ao longo do post:</p>
<pre class="r"><code># Carregar pacotes --------------------------------------------------------
packages &lt;- c(&quot;data.table&quot;, &quot;dplyr&quot;, &quot;purrr&quot;, &quot;survival&quot;  , &quot;survminer&quot;,
              &quot;ggfortify&quot;,&quot;GGally&quot;, &quot;ggplot2&quot;,&quot;moments&quot;, &quot;gridExtra&quot;,&quot;ggExtra&quot;,
              &quot;cowplot&quot;,&quot;lubridate&quot;, &quot;scales&quot;, &quot;knitr&quot;, &quot;kableExtra&quot;, &quot;grid&quot;,
              &quot;broom&quot;, &quot;formattable&quot;, &quot;grid&quot;)
purrr::walk(packages,library, character.only = TRUE, warn.conflicts = FALSE)
rm(packages)

# Funcoes customizadas do github ------------------------------------------
source(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/inicio_e_fim_da_base.R&quot;)
source(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/grafico_descritivo.R&quot;)
source(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/sumario_custom_num.R&quot;)

# Opcoes do documento -----------------------------------------------------
# options(scipen = 99999)

# Tema dos graficos -------------------------------------------------------
theme_set(theme_bw()+
            theme(axis.text.x = element_text(size=17),
                  axis.text.y = element_text(size=17),
                  axis.title.y = element_text(size=20), legend.position = &quot;bottom&quot;))

# Tema das tabelas kable --------------------------------------------------
kable2 &lt;- function(x,linhas=NULL,colunas=NULL, ...){
  k &lt;- 
    kable(x,digits = 4,...) %&gt;%
    kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) %&gt;%
    kable_styling(c(&quot;striped&quot;, &quot;bordered&quot;)) 
  
  if (!is.null(linhas)) {
    # destque na linha:
    k &lt;-  k %&gt;% row_spec(linhas, bold = T, color = &quot;white&quot;, background = &quot;#FFE8BD&quot;)
  }
  
  if (!is.null(colunas)) {
    # destque na colunas:
    k &lt;-  k %&gt;% column_spec(colunas,bold=T, color=&quot;white&quot;, background = &quot;#FFE8BD&quot;)
  }
  k %&gt;%
    scroll_box(width = &quot;850px&quot;)
}</code></pre>
<p>Em uma an√°lise de sobreviv√™ncia √© comum a presen√ßa de observa√ß√µes censuradas, (isto √©, quando ocorre a perda de informa√ß√£o decorrente de n√£o se ter observado a data de ocorr√™ncia do desfecho). No caso dessa base de dados n√£o existe uma vari√°vel que define a censura, pois apenas a morte do jogador √© registrada e √© poss√≠vel que se os jogadores se desconectarem do jogo mesmo que n√£o sejam mortos seja contado como morte de qualquer jeito. Os detalhes por tr√°s da aquisi√ß√£o de dados n√£o trazem essa informa√ß√£o portanto pode n√£o ser poss√≠vel distinguir a censura do desfecho e isso √© um detalhe relevante que deve ser levado em conta.</p>
<pre class="r"><code># Carregar base -----------------------------------------------------------
set.seed(2)   # reprodutivel
pubg_tpp1 &lt;-  # Informacoes dos criterios de selecao no corpo do texto
  map_df(paste0(&quot;agg_match_stats_&quot;,0:4,&quot;.csv&quot;), 
         ~ fread(.x, showProgress = T,
                 data.table = T)[match_mode == &quot;tpp&quot; &amp; party_size == 1 &amp; year(date) == 2018 &amp; player_dist_walk&gt;10 &amp; player_dmg != 0 ][, !c(&quot;match_mode&quot;,&quot;party_size&quot;,&quot;game_size&quot;,&quot;date&quot;, &quot;team_id&quot;,&quot;player_dbno&quot;, &quot;team_placement&quot;), with=FALSE][,player_survive_time := player_survive_time/60] %&gt;% 
           group_by(match_id) %&gt;%
           do(sample_n(.,1)) %&gt;% 
           ungroup() 
  )

# Salvar base coletada ----------------------------------------------------
saveRDS(pubg_tpp1,&quot;pubg_tpp1.rds&quot;)</code></pre>
<!-- <iframe src="https://giphy.com/embed/3oKIPmaM8aFolCcuI0" width="100%" height="270" frameBorder="0" class="giphy-embed" allowFullScreen></iframe> -->
<div class="col2">
<p>Descri√ß√£o da rotina acima e os crit√©rios para a sele√ß√£o da amostra:</p>
<ol style="list-style-type: decimal">
<li>percorre as 5 bases dispon√≠veis: <code>paste0("agg_match_stats_",0:4,".csv")</code></li>
<li>seleciona partidas em terceira pessoa: <code>match_mode == "tpp"</code></li>
<li>com tamanho da equipe = 1 (individual): <code>party_size == 1</code></li>
<li>do ano de 2018: <code>year(date) == 2018</code></li>
<li>andaram mais que 10 unidades de distancia (metros?): <code>player_dist_walk&gt;10</code></li>
<li>fizeram algum dano (evitar jogadores ausentes): <code>player_dmg != 0</code><br />
</li>
<li>remove colunas n√£o utilizadas na analise</li>
<li>converte do tempo para minutos: <code>player_survive_time := player_survive_time/60</code></li>
<li>agrupa por partida: <code>group_by(match_id)</code></li>
<li>seleciona um jogador de cada partida: <code>do(sample_n(.,1))</code></li>
</ol>
<iframe src="https://giphy.com/embed/g4OqNwXDrnfOcbaaUM" width="240" height="300" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
</div>
<p>Note que apenas um jogador de cada partida √© selecionado na inten√ß√£o de obter independ√™ncia entre observa√ß√µes, isso reduziu drasticamente seu tamanho. Agora que a base j√° foi importada e filtrada, faremos a leitura de 200 linhas aleat√≥rias com a finalidade de diminuir o tempo computacional das opera√ß√µes realizadas em seguida.</p>
<pre class="r"><code>set.seed(1)
pubg_tpp1 &lt;- readRDS(&quot;pubg_tpp1.rds&quot;) %&gt;% sample_n(200)%&gt;% 
  select(-one_of(c(&quot;match_id&quot;, &quot;player_name&quot;)))</code></pre>
<p>Veja a seguir de forma visual como as vari√°veis num√©ricas se correlacionam:</p>
<pre class="r"><code>pubg_tpp1 %&gt;% 
  rev %&gt;% 
  grafico_descritivo()</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-4-1.png" style="width:80.0%" /></p>
</center>
<div id="vari√°vel-resposta" class="section level3">
<h3>Vari√°vel resposta</h3>
<p>Vejamos o que acontece ao analisar o tempo de sobreviv√™ncia de cada jogador</p>
<iframe src="https://giphy.com/embed/3oKIP5KxPss1gjwpG0" width="100%" height="270" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
<p>A seguir, a distribui√ß√£o da vari√°vel resposta <code>player_survive_time</code> :</p>
<pre class="r"><code>plot_grid(pubg_tpp1 %&gt;% 
            ggplot(aes(x=player_survive_time))+
            geom_histogram(aes(y = ..density..), bins = 30, fill=&quot;white&quot;, color=&quot;black&quot;)+
            geom_density(alpha=.2, fill=&quot;white&quot;)+
            scale_x_continuous(labels = scales::comma, limits = c(0,40), breaks = seq(0,40,5))+
            labs(x=&quot;&quot;,y=&quot;&quot;, title = &quot;Tempo de sobreviv√™ncia dos jogadores selecionados&quot;)
          ,
          pubg_tpp1 %&gt;% 
            ggplot(aes(x=&quot; &quot;, y=player_survive_time))+
            geom_boxplot()+
            labs(x=&quot;&quot;)+
            coord_flip()
          ,
          ncol = 1, nrow = 2, align = &quot;v&quot;, rel_heights = c(3,1))</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-6-1.png" style="width:80.0%" /></p>
</center>
<p>Note que possue uma <a href="https://binged.it/2BAYX3s">assimetria positiva</a></p>
</div>
<div id="data-wrangling" class="section level3">
<h3>Data Wrangling</h3>
<p>Primeiramente, vejamos as vari√°veis se relacionam entre si e com a vari√°vel resposta com os coeficientes de correla√ß√£o de Pearson:</p>
<pre class="r"><code># Correlations
pubg_tpp1 %&gt;% 
  select_if(is.numeric) %&gt;% 
  cor() %&gt;% 
  corrplot::corrplot(method = &quot;number&quot;,type = &quot;upper&quot;,diag = F, order = &quot;hclust&quot;,number.cex = 0.7, title = &quot;Correlation correlated numerics&quot;, mar=c(0,0,1,0))</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-7-1.png" style="width:80.0%" /></p>
</center>
<p>√â poss√≠vel notar que apenas a vari√°vel <code>player_assists</code> n√£o correlaciona-se com a vari√°vel resposta nem com as demais vari√°veis e <code>player_dmg</code> e <code>player_kills</code> s√£o fortemente correlacionadas, isso indica que pode ser interessante remover uma delas ou juntar toda essa informa√ß√£o em uma √∫nica vari√°vel, veremos‚Ä¶</p>
<p>Al√©m disso nota-se que a dist√¢ncia percorrida a p√© √© fortemente correlacionada com a vari√°vel resposta enquanto que a dist√¢ncia de quem andou de carro n√£o √© t√£o correlacionada. Uma transforma√ß√£o na vari√°vel <code>player_dist_ride</code> para uma dummy <code>drive</code> indicando se o indiv√≠duo dirigiu ou n√£o pode representar melhor esta informa√ß√£o.</p>
<p>Vejamos algumas caracter√≠sticas peculiares:</p>
<pre class="r"><code>pubg_tpp1 %&gt;% 
  select(player_kills, player_dist_ride, player_assists) %&gt;% 
  map_dfr(~quantile(.x,  probs = seq(0,1,0.25)) %&gt;% round(2)) %&gt;% 
  t  %&gt;% tidy() %&gt;% 
  `colnames&lt;-`(c(&quot;vari√°vel&quot;,percent(seq(0,1,0.25)))) %&gt;% 
  kable2()</code></pre>
<p>Praticamente metade da amostra n√£o registrou abates nem possui marca√ß√£o de <code>player_dist_ride</code>. Como a vari√°vel <code>player_dmg</code> apresentou correla√ß√£o com a vari√°vel resposta <code>player_survive_time</code>, vamos fazer algumas transforma√ß√µes:</p>
<ol style="list-style-type: decimal">
<li>Criar uma vari√°vel dummy <code>drive</code> se jogador usou carro</li>
<li>Somar a <code>player_dist_ride</code> e <code>player_dist_walk</code> em uma √∫nica vari√°vel: <code>player_dist</code></li>
<li>Juntar <code>player_kills</code>, <code>player_dmg</code> e <code>player_assists</code> em uma √∫nica vari√°vel: <code>player_performance</code></li>
</ol>
<div id="player-performance" class="section level4">
<h4>Player performance</h4>
<p>Como criar a vari√°vel <code>player_performance</code>?</p>
<iframe src="https://giphy.com/embed/xT9IgnOQS8e8uKkflK" width="100%" height="270" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
<p>Tentei inventar uma metodologia e com certeza devem existir maneiras mais eficientes de se fazer isso, por√©m, deixa eu explicar o que eu pensei, considere a formula:</p>
<p><span class="math display">\[
Playerperformance = log(WPlayerDmg + WPlayerAssists + WPlayerKills)
\]</span></p>
<p>onde:</p>
<p><span class="math display">\[
WPlayerKills = log(PlayerKills+0.5)\\
WPlayerDmg = log(PlayerDmg)\\
WPlayerAssists = PlayerAssists
\]</span></p>
<p>Note que:</p>
<ul>
<li><span class="math inline">\(WPlayerAssists\)</span>: N√£o √© feita qualquer transforma√ß√£o;</li>
<li><span class="math inline">\(WPlayerDmg\)</span>: A distribui√ß√£o fica ‚Äúquase sim√©trica‚Äù ap√≥s a transforma√ß√£o log;</li>
<li><span class="math inline">\(WPlayerKills\)</span>: adiciona-se 0.5 para poder tirar o log pois podem existir zeros nessa vari√°vel e al√©m disso, quem n√£o marcou abate ser√° penalizado com <span class="math inline">\(-1\)</span> na soma final do score: <code>player_performance</code>.</li>
</ul>
<p>Veja a seguir de forma visual a distribui√ß√£o das vari√°veis que far√£o parte da vari√°vel <code>player_performance</code> na parte de cima e na parte inferior o que acontece ap√≥s sua soma, gerando a nova vari√°vel <code>player_performance</code> :</p>
<pre class="r"><code>performance &lt;- tibble(w_player_kills = log(pubg_tpp1$player_kills+0.5),
                      w_player_dmg = log(pubg_tpp1$player_dmg),
                      w_player_assists = pubg_tpp1$player_assists) %&gt;% 
  mutate(player_performance = log(w_player_dmg + w_player_assists + w_player_kills))

grid.arrange(
  performance %&gt;% 
    select(-player_performance) %&gt;% 
    tidyr::gather() %&gt;% 
    ggplot(aes(x=value))+
    geom_histogram(aes(y = ..density..), bins = 30, fill=&quot;white&quot;, color=&quot;black&quot;)+
    geom_density(alpha=.2, fill=&quot;white&quot;)+
    scale_x_continuous(labels = scales::comma, limits = c(-1.5,8), breaks = seq(-1,8,1))+
    labs(x=&quot;&quot;, y=&quot;&quot;)+
    theme(axis.title.x=element_blank(),
          axis.text.x=element_blank(),
          axis.ticks.x=element_blank())+
    facet_wrap(~key, scales = &quot;free&quot;)
  ,
  performance %&gt;% 
    select(player_performance) %&gt;% 
    tidyr::gather() %&gt;% 
    ggplot(aes(x=value))+
    geom_histogram(aes(y = ..density..), fill=&quot;white&quot;, color=&quot;black&quot;,bins = 15)+
    geom_density(alpha=.2, fill=&quot;white&quot;)+
    scale_x_continuous(limits = c(-1.,2.5), breaks = seq(-1,3,0.5))+
    labs(x=&quot;&quot;, y=&quot;&quot;, title = &quot;performance&quot;),
  ncol=1
)</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-9-1.png" style="width:80.0%" /></p>
</center>
</div>
<div id="transforma√ß√µes-na-base" class="section level4">
<h4>Transforma√ß√µes na base</h4>
<p>A seguir faremos as mudan√ßas diretamente no dataset que estamos trabalhando:</p>
<pre class="r"><code>pubg_tpp1 &lt;- 
  pubg_tpp1 %&gt;% 
  mutate(player_dist = log(player_dist_ride + player_dist_walk)) %&gt;%  
  mutate(player_assists_d = if_else(player_assists ==0, 0, 1)) %&gt;% 
  mutate(player_performance = performance$player_performance )%&gt;% 
  mutate(drive = ifelse(player_dist_ride==0, &quot;no&quot;, &quot;yes&quot;) %&gt;% as.factor()) %&gt;% 
  mutate(player_kills_d = ifelse(player_kills==0, &quot;no&quot;, &quot;yes&quot;) %&gt;% as.factor()) </code></pre>
<p>A manipula√ß√£o acima cria as seguintes vari√°veis:</p>
<ol style="list-style-type: decimal">
<li><code>player_dist</code> como o log da soma de <code>player_dist_ride</code> e <code>player_dist_walk</code></li>
<li><code>player_assists_d</code> como uma dummy: 1 se o jogador deu assist√™ncia; 0 c.c.</li>
<li><code>player_performaec</code> como a combina√ß√£o de <code>player_dmg</code>, <code>player_assists</code> e <code>player_kills</code></li>
<li><code>drive</code> como uma dummy: 1 se o jogador dirigiu; 0 c.c.</li>
<li><code>player_kills_d</code> como uma dummy: 1 se jogador matou algu√©m; 0 c.c.</li>
</ol>
<p>Vejamos como ocorre a distribui√ß√£o das vari√°veis num√©ricas ap√≥s as transforma√ß√µes:</p>
<pre class="r"><code>g1 &lt;- 
  pubg_tpp1 %&gt;% 
  # select_if(~ !length(table(.x))==2 &amp; is.numeric(.x)) %&gt;% colnames() %&gt;% 
  select(player_survive_time,player_performance,player_dist) %&gt;% colnames() %&gt;% 
  map2(c(&quot;Densidade&quot;, &quot;&quot;, &quot;&quot;),
       ~ plot_grid(
         pubg_tpp1 %&gt;% 
           ggplot(aes_string(x=.x)) + 
           geom_histogram(aes(y=..density..),colour=&quot;black&quot;, fill=&quot;white&quot;, bins = 15) +
           geom_density(alpha=.2, fill=&quot;lightgrey&quot;) +
           scale_x_continuous()+
           ggtitle(.x)+
           labs(x=&quot;&quot;, y=.y)+
           theme(axis.title.x=element_blank(),
                 axis.text.x=element_blank(),
                 axis.ticks.x=element_blank())
         ,
         pubg_tpp1 %&gt;% 
           ggplot(aes_string(, y=.x))+
           geom_boxplot(aes(x=&quot; &quot;))+
           labs(x=&quot;&quot;, y=&quot;&quot;)+
           coord_flip()+
           theme(axis.title.x=element_blank(),
                 axis.text.x=element_blank(),
                 axis.ticks.x=element_blank()),
         
         ncol = 1, nrow = 2, align = &quot;v&quot;, rel_heights = c(3,1)
       )
  )

dat &lt;- 
  pubg_tpp1 %&gt;% 
  select_if(~.x %&gt;% table %&gt;% length == 2) %&gt;% 
  mutate_at(2,~if_else(.x==0, &quot;no&quot;, &quot;yes&quot;)) %&gt;% 
  .[,-1]

g2 &lt;- map2(colnames(dat),
           c( &quot;Porcentagem&quot;, &quot;&quot;,&quot;&quot;),
           ~ dat[,.x] %&gt;% 
             tidyr::gather() %&gt;% 
             group_by(key, value) %&gt;% 
             summarise(n = n()) %&gt;% 
             mutate(prop = n/sum(n)) %&gt;% 
             ggplot(aes(x = key, y = prop,fill = value)) + 
             geom_bar(position = &quot;fill&quot;,stat = &quot;identity&quot;, alpha=0.7) +
             scale_y_continuous(labels = percent_format())+
             labs(x=&quot;&quot;, y = .y)+
             scale_fill_manual(values = c(&quot;grey&quot;, &quot;#FCC14B&quot;), name = &quot;Legenda:&quot;)
)

grid.arrange(g1[[1]], g1[[2]], g1[[3]],g2[[1]], g2[[2]], g2[[3]], ncol=3, heights=c(3/5, 2/5))</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-11-1.png" style="width:80.0%" /></p>
</center>
<!-- A distribui√ß√£o dos dados ordenada pela vari√°vel resposta `player_survive_time` : -->
<!-- ```{r} -->
<!-- # Sorted -->
<!-- pubg_tpp1 %>%  -->
<!--   select(player_survive_time, everything()) %>%  -->
<!--   mutate_if(~length(unique(.x))==2, as.factor) %>%  -->
<!--   tabplot::tableplot(sortCol = player_survive_time,decreasing = T) -->
<!-- ``` -->
<p>Apos a transforma√ß√£o a distribui√ß√£o e demais informa√ß√µes dos dados, vejamos novamente a distribui√ß√£o das vari√°veis da amostra com os gr√°ficos de dispers√£o, densidade e correla√ß√µes levando em conta se dirigiu ou n√£o:</p>
<pre class="r"><code>grafico_descritivo(x = pubg_tpp1,
                   colNames = c(&#39;player_survive_time&#39;, &quot;player_performance&quot;, &#39;player_dist&#39;,
                                &#39;player_assists_d&#39;,&quot;player_kills_d&quot;, &#39;drive&#39;),
                   color=&#39;drive&#39;,
                   colors = c(&quot;grey&quot;, &quot;#FCC14B&quot;))</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-13-1.png" style="width:80.0%" /></p>
</center>
<p>O fato do jogador ter dirigido ou n√£o exibiu padr√µes interessantes, pode ser que seja significante no ajuste do modelo final.</p>
</div>
</div>
</div>
<div id="an√°lise-de-sobrevivencia" class="section level1">
<h1>An√°lise de sobrevivencia</h1>
<p>O passo inicial de qualquer an√°lise estat√≠stica consiste em uma descri√ß√£o dos dados e o principal componente da an√°lise descritiva envolvendo dados de tempo de vida √© a fun√ß√£o de sobreviv√™ncia: <span class="math inline">\(S(t) = P(T&gt;t)\)</span>, que determina a probabilidade de um indiv√≠duo sobreviver por mais do que um determinado tempo <span class="math inline">\(t\)</span>, ou por no m√≠nimo um tempo igual a <span class="math inline">\(t\)</span>.</p>
<p>A descri√ß√£o dos dados j√° foi realizada, agora faremos a descri√ß√£o envolvendo a fun√ß√£o de sobreviv√™ncia.</p>
<iframe src="https://giphy.com/embed/xT0xeMrCEGPiU5uw0w" width="100%" height="266" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
<div id="kaplan-meier" class="section level2">
<h2>Kaplan-Meier</h2>
<p>Para isso existem algumas alternativas como o estimador de Kaplan-Meier, que utiliza os conceitos de independ√™ncia e de probabilidade condicional para deduzir a probabilidade de sobreviver at√© o tempo <span class="math inline">\(t\)</span>.</p>
<p>Veja a seguir s√£o ajustados os modelos univariados de Kaplan-Meier para cada uma das coivar√°veis da amostra:</p>
<pre class="r"><code>surv &lt;- Surv(pubg_tpp1$player_survive_time)
resultado_km &lt;-
  list(geral            = survfit(surv ~ 1 ,data = pubg_tpp1),
       player_assists_d = survfit(surv ~ player_assists_d ,data = pubg_tpp1),
       drive            = survfit(surv ~ drive,data = pubg_tpp1 ),
       player_kills_d   = survfit(surv ~ player_kills_d,data = pubg_tpp1))</code></pre>
<p>Veja os resultados da fun√ß√£o de sobreviv√™ncia sem levar em considera√ß√£o nenhuma das coivar√°veis:</p>
<pre class="r"><code>surv_summary(resultado_km[[1]], pubg_tpp1) %&gt;% .[1:5,-ncol(.)] %&gt;% cbind(variable = &quot;Geral&quot;) %&gt;% 
  select(variable, everything())%&gt;%
  kable2()</code></pre>
<p>A fun√ß√£o <code>surv_summary()</code> retorna um quadro de dados com as seguintes colunas:</p>
<ul>
<li>time: o tempo em que a curva tem um passo.</li>
<li>n.risk: o n√∫mero de sujeitos em risco em t.</li>
<li>n.evento: o n√∫mero de eventos que ocorrem no tempo t.</li>
<li>n.censor: n√∫mero de eventos censurados.</li>
<li>surv: estimativa da probabilidade de sobreviv√™ncia.</li>
<li>std.err: erro padr√£o de sobreviv√™ncia.</li>
<li>superior: extremidade superior do intervalo de confian√ßa</li>
<li>inferior: extremidade inferior do intervalo de confian√ßa</li>
<li>estratos: indica a estratifica√ß√£o da estimativa de curvas. Os n√≠veis de estratos (um fator) s√£o os r√≥tulos das curvas (se houver).</li>
</ul>
<div id="log-rank" class="section level3">
<h3>Log-rank</h3>
<p>Al√©m da an√°lise visual das estimativas √© importante comparar as curvas de sobreviv√™ncia com testes de hip√≥teses para obter-se signific√¢ncia estat√≠stica para nossas afirma√ß√µes.</p>
<p>O teste log rank √© um teste n√£o param√©trico, que n√£o faz suposi√ß√µes sobre as distribui√ß√µes de sobreviv√™ncia. Essencialmente, o teste log rank compara o n√∫mero observado de eventos em cada grupo com o que seria esperado se a hip√≥tese nula fosse verdadeira. Considere ent√£o <span class="math inline">\(H_0: S_1(t)=S_2(t)\)</span> para todo <span class="math inline">\(t\)</span> no per√≠odo de acompanhamento (ou seja, se as curvas de sobreviv√™ncia fossem id√™nticas). A estat√≠stica utilizada no teste √© um <span class="math inline">\(T\)</span> com distribui√ß√£o aproximadamente <span class="math inline">\(\chi^2\)</span> com 1 grau de liberdade.</p>
<p>O objeto criado abaixo guarda o valor p para o teste de log-rank de cada em cada um dos modelos:</p>
<pre class="r"><code>resultado_log_rank &lt;- 
  c(geral = &quot;&quot;,
    player_assists_d=round(1-pchisq(survdiff(surv~player_assists_d,data = pubg_tpp1)$chisq,1),5),
    drive=round(1-pchisq(survdiff(surv~drive,data=pubg_tpp1)$chisq,1),5),
    player_kills_d=round(1-pchisq(survdiff(surv~player_kills_d,data=pubg_tpp1)$chisq,1),5)
  )</code></pre>
<p>Os gr√°ficos gerados a partir dos modelos ajustados acima bem como o resultado dos testes de log-rank s√£o exibidos na imagem a seguir:</p>
<pre class="r"><code>survplot &lt;- map2(resultado_km,
                 case_when(resultado_log_rank == &#39;0&#39; ~ &quot;log-rank: \n p &lt; 0,00001&quot;,
                           resultado_log_rank == &quot;&quot; ~ &quot;log-rank n√£o se aplica&quot;,
                           resultado_log_rank != &#39;0&#39; | resultado_log_rank != &#39;&#39; ~ 
                             paste0(&quot;log-rank: \n p =&quot;,as.numeric(resultado_log_rank))),
                 ~ autoplot(.x)+
                   ggtitle(stringr::str_remove_all(names(.x$strata)[1],&quot;(=no|=yes)&quot;))+
                   annotate(&quot;label&quot;,y = 0.20, x = 5,
                            label = .y,
                            size = 4, colour = &quot;red&quot;,hjust=0.1)+ 
                   scale_fill_manual(values = c(&quot;grey&quot;, &quot;#FCC14B&quot;))+
                   scale_color_manual(values = c(&quot;grey&quot;, &quot;#FCC14B&quot;))+
                   theme(legend.position = c(0.85,0.7))+
                   scale_x_continuous(limits = c(0,30), breaks = seq(0,30,5))
                 
)
grid.arrange(survplot[[1]], survplot[[2]] ,survplot[[3]], survplot[[4]], ncol=2)</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-17-1.png" style="width:80.0%" /></p>
</center>
<p>O eixo horizontal (eixo x) representa o tempo em minutos, e o eixo vertical (eixo y) mostra a probabilidade de sobreviv√™ncia ou a propor√ß√£o de jogadores que sobrevivem. As linhas representam curvas de sobreviv√™ncia dos dois grupos.</p>
<p>Uma queda vertical nas curvas indica um evento. No tempo zero, a probabilidade de sobreviv√™ncia √© de 1,0 (ou 100% dos jogadores vivos).</p>
<p>Interpreta√ß√£o: Pelo gr√°fico, aparentemente n√£o existe diferen√ßa no tempo de sobreviv√™ncia com estratifica√ß√£o dos dados de acordo com quem deu assist√™ncia ou n√£o, j√° para o teste que compara igualdade de fun√ß√µes de sobreviv√™ncia das demais vari√°veis, existem evidencias estat√≠sticas para rejeitar a hip√≥tese de que n√£o h√° diferen√ßa na sobrevida entre os dois grupos</p>
</div>
</div>
<div id="fun√ß√£o-de-risco-hazard-ou-taxa-de-falha" class="section level2">
<h2>Fun√ß√£o de risco (hazard) ou taxa de falha</h2>
<p>Fun√ß√£o de risco (hazard) ou taxa de falha √© o risco ‚Äúinstant√¢neo‚Äù denotada por <span class="math inline">\(\lambda(t)\)</span> √© uma taxa, n√£o uma probabilidade e pode assumir qualquer valor real maior que zero.</p>
<p>No exemplo representa a taxa de incid√™ncia ou risco acumulado para um indiv√≠duo morrer at√© o momento <span class="math inline">\(t\)</span>, dado que sobreviveu at√© este momento. √â muito informativa quando comparada com a fun√ß√£o de sobreviv√™ncia pois diferentes <span class="math inline">\(S(t)\)</span> podem ter formas semelhantes, enquanto que respectivas <span class="math inline">\(\lambda(t)\)</span> podem diferir drasticamente.</p>
<pre class="r"><code>survplot &lt;-
  map(resultado_km  ,
      ~ ggsurvplot(.x, conf.int = TRUE, 
                   palette = c(&quot;grey&quot;, &quot;#FCC14B&quot;),
                   risk.table = F,break.time.by = 5,
                   fun = &quot;cumhaz&quot;,title = stringr::str_remove_all(names(.x$strata)[1],&quot;(=no|=yes)&quot;))
  )
arrange_ggsurvplots(survplot, print = TRUE,
                    ncol = 2, nrow = 2)</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-18-1.png" style="width:80.0%" /></p>
</center>
<p>O risco cumulativo <span class="math inline">\(H( t)\)</span> pode ser interpretado como a for√ßa cumulativa da mortalidade.
Em outras palavras, corresponde ao n√∫mero de eventos que seriam esperados para cada indiv√≠duo
pelo tempo t se o evento fosse um processo repetitivo.</p>
</div>
<div id="modelo-de-cox" class="section level2">
<h2>Modelo de cox</h2>
<p>√â caracterizado pela presen√ßa dos coeficientes <span class="math inline">\(\beta\)</span>s que medem os efeitos (semelhantes √† an√°lise de regress√£o log√≠stica m√∫ltipla e linear m√∫ltipla) das vari√°veis explicativas sobre a fun√ß√£o de risco. Em um modelo de regress√£o de riscos proporcionais de Cox, a medida do efeito √© a <em>taxa de risco</em>, que √© o risco de falha, dado que o participante sobreviveu at√© um tempo espec√≠fico.</p>
<p>Algumas das suposi√ß√µes para o correto uso do modelo de regress√£o de riscos proporcionais de Co incluem:</p>
<ul>
<li>independ√™ncia dos tempos de sobreviv√™ncia entre indiv√≠duos distintos na amostra,</li>
<li>rela√ß√£o multiplicativa entre os preditores e o risco,</li>
<li>uma taxa de risco constante ao longo do tempo.</li>
</ul>
<p>O modelo de riscos proporcionais de Cox √© chamado de modelo semi-param√©trico , porque n√£o h√° suposi√ß√µes sobre o formato da fun√ß√£o de risco de linha de base. No entanto, existem outras suposi√ß√µes, como observado acima.</p>
<p>√â poss√≠vel utilizar as estat√≠sticas de Wald, da raz√£o de verossimilhan√ßa e escore para fazer infer√™ncias sobre os par√¢metros do modelo</p>
<p>Veja a seguir a signific√¢ncia dos coeficiente estimado em modelos univariados para cada vari√°vel candidata ao modelo:</p>
<pre class="r"><code># Modelos univariados
covariates    &lt;- c(&quot;player_kills&quot;,&quot;player_dist_ride&quot;,&quot;player_performance&quot;,
                   &quot;player_dist_walk&quot;,&quot;player_dmg&quot;, &quot;player_dist&quot;, 
                   &quot;player_assists_d&quot;,&quot;drive&quot;, &quot;player_kills_d&quot;)
univ_formulas &lt;- map(covariates,~ as.formula(paste(&#39;Surv(player_survive_time) ~&#39;, .x)))
univ_models   &lt;- map( univ_formulas, ~coxph(.x, data = pubg_tpp1))

# estrair resultados 
map2_df(univ_models,
        covariates,
        function(x,y){ 
          x                = summary(x)
          p.value          = signif(x$wald[&quot;pvalue&quot;], digits=2)
          wald.test        = signif(x$wald[&quot;test&quot;], digits=2)
          beta             = signif(x$coef[1], digits=2);#coeficient beta
          HR               = signif(x$coef[2], digits=2);#exp(beta)
          HR.confint.lower = signif(x$conf.int[,&quot;lower .95&quot;], 2)
          HR.confint.upper = signif(x$conf.int[,&quot;upper .95&quot;],2)
          HR               = paste0(HR, &quot; (&quot;, HR.confint.lower, &quot;-&quot;, HR.confint.upper, &quot;)&quot;)
          res              = tibble(y,beta, HR, wald.test, p.value)
          colnames(res)    = c(&quot;covariates&quot;,&quot;beta&quot;, &quot;HR (95% CI for HR)&quot;, &quot;wald.test&quot;, &quot;p.value&quot;)
          res
        }) %&gt;% 
  kable2(linhas = 7)</code></pre>
<p>Modelo de Cox usando uma vari√°vel categ√≥rica retorna uma raz√£o de risco, que, acima de 1 indica uma covari√°vel que est√° positivamente associada √† probabilidade do evento e, portanto, negativamente associada ao tempo de sobrevida. O oposto vale para HR menor que um e HR = 1 indica que a covari√°vel n√£o tem efeito.</p>
<pre class="r"><code>final_model  &lt;- 
  coxph(Surv(player_survive_time) ~ player_performance+player_dist+drive,
        data = pubg_tpp1,x=T,method=&quot;breslow&quot;)

summary(final_model)</code></pre>
<pre><code>## Call:
## coxph(formula = Surv(player_survive_time) ~ player_performance + 
##     player_dist + drive, data = pubg_tpp1, x = T, method = &quot;breslow&quot;)
## 
##   n= 200, number of events= 200 
## 
##                       coef exp(coef) se(coef)       z Pr(&gt;|z|)    
## player_performance -0.7469    0.4738   0.1787  -4.179 2.92e-05 ***
## player_dist        -1.7599    0.1721   0.1150 -15.307  &lt; 2e-16 ***
## driveyes            0.8832    2.4186   0.2091   4.225 2.39e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
##                    exp(coef) exp(-coef) lower .95 upper .95
## player_performance    0.4738     2.1105    0.3338    0.6726
## player_dist           0.1721     5.8117    0.1374    0.2156
## driveyes              2.4186     0.4135    1.6055    3.6434
## 
## Concordance= 0.883  (se = 0.008 )
## Likelihood ratio test= 362.7  on 3 df,   p=&lt;2e-16
## Wald test            = 274  on 3 df,   p=&lt;2e-16
## Score (logrank) test = 407.8  on 3 df,   p=&lt;2e-16</code></pre>
<p>No modelo ajustado note-se que existe uma associa√ß√£o negativa entre <code>player_performance</code> e mortalidade e entre <code>player_dist</code> e mortalidade (ou seja, o risco de morte diminui para jogadores que percorrem maiores dist√¢ncias e possuem melhor performance).</p>
<p>As estimativas dos par√¢metros representam o aumento no log esperado do risco relativo para cada aumento de uma unidade no preditor, mantendo os outros preditores constantes.</p>
<p>Para interpretabilidade, calcularemos as taxas de risco exponenciando das estimativas dos par√¢metros. Para a <code>player_performance</code>, <span class="math inline">\(exp(-0.7469196)= 0.4738239\)</span>. Isso implica que diminui para <span class="math inline">\(47.38\)</span> do valor original do risco esperado em rela√ß√£o a um aumento de uma unidade na performance, mantendo as demais vari√°veis constantes. A interpreta√ß√£o de <code>player_dist</code> em escala logar√≠timica √© feita de maneira semelhante.`</p>
<p>J√° para os jogadores onde <code>drive</code> = 1 (que dirigiram durante a partida) existe uma rela√ß√£o positiva, como <span class="math inline">\(exp(0.8831835)= 2.4185871\)</span>. O risco esperado corresponde √† <span class="math inline">\(2.4185871\)</span> do valor original nos que dirigiram em compara√ß√£o aos que n√£o dirigiram, mantendo as demais vari√°veis constantes.</p>
<pre class="r"><code>map2_df(1:3,final_model$coefficients %&gt;% names(),~
          tibble(
            variable = .y,
            beta             = signif(summary(final_model)$coef[.x,1], digits=2), #coeficient beta
            HR               = signif(summary(final_model)$coef[.x,2], digits=2), #exp(beta)
            HR.confint.lower = signif(summary(final_model)$conf.int[.x,&quot;lower .95&quot;], 2),
            HR.confint.upper = signif(summary(final_model)$conf.int[.x,&quot;upper .95&quot;],2)) %&gt;% 
          mutate(HR= paste0(HR, &quot; (&quot;, HR.confint.lower, &quot;-&quot;, HR.confint.upper, &quot;)&quot;)
          )
) %&gt;% kable2()</code></pre>
<p>Em suma:</p>
<ul>
<li>HR = 1: sem efeito</li>
<li>HR &lt;1: Redu√ß√£o do risco</li>
<li>HR&gt; 1: aumento do risco</li>
</ul>
<iframe src="https://giphy.com/embed/2Us3iTghyffcfeI35h" width="100%" height="200" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
<div id="res√≠duos-de-martingal-e-deviance" class="section level3">
<h3>Res√≠duos de Martingal e Deviance</h3>
<p>Como foi visto, o modelo de regress√£o de riscos proporcionais de Cox faz diversas suposi√ß√µes que precisam ser conferidas ap√≥s o ajuste do modelo para chegar a qualidade de seus resultados pois um modelo mais ajustado pode trazer resultados enganosos e que n√£o fa√ßam sentido algum</p>
<iframe src="https://giphy.com/embed/l0CLSXnSgbYma8EOA" width="100%" height="269" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
<p>Gr√°ficos dos res√≠duos Martingal ou deviance contra os tempos fornecem
uma forma de verificar a adequa√ß√£o do modelo ajustado, bem como
ajudar na detec√ß√£o de observa√ß√µes at√≠picas.</p>
<p><strong>Deviance</strong></p>
<p>Esses res√≠duos, que s√£o uma tentativa de tornar os res√≠duos
Martingal mais sim√©tricos em torno do zero, facilitam, em geral,
a detec√ß√£o de pontos at√≠picos (outliers).
Se o modelo for apropriado, esses res√≠duos devem apresentar um
comportamento aleat√≥rio em torno de zero.</p>
<p><strong>Martingal</strong></p>
<p>Esses res√≠duos s√£o vistos como uma estimativa do numero de falhas em excesso
observada nos dados mas n√£o predito pelo modelo. Os mesmos s√£o usados, em geral,
para examinar a melhor forma funcional (linear, quadr√°tica, etc.)
para uma dada covariavel em um modelo de regress√£o assumido para os dados do estudo.</p>
<pre class="r"><code>res &lt;- 
  tibble(residuo_deviance = resid(final_model,type=&quot;deviance&quot;) ,
         residuo_martingal = resid(final_model,type=&quot;martingal&quot;),
         linear_predictors = final_model$linear.predictors)

# Graficos:
grid.arrange(
  ggplot(res, aes(x=linear_predictors, y=residuo_martingal))+ geom_point()+geom_hline(yintercept=0, color=&#39;coral&#39;)+ylab(&quot;Res√≠duos Martingual&quot;),
  ggplot(res, aes(x=linear_predictors, y=residuo_deviance))+ geom_point()+geom_hline(yintercept=0, color=&#39;coral&#39;)+ylab(&quot;Deviance&quot;),
  ncol=2
)</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-22-1.png" style="width:80.0%" /></p>
</center>
<p>Interpreta√ß√£o:</p>
<ul>
<li><strong>Martingal</strong>: Parecido com deviance mais acentuado;</li>
<li><strong>Deviance</strong>: Modelo n√£o eh tao ruim assim, se fosse um modelo linear talvez dever√≠amos tomar cuidado.</li>
</ul>
<div id="residuos-de-schoenfeld" class="section level4">
<h4>Residuos de Schoenfeld</h4>
<p>Em princ√≠pio, os res√≠duos de Schoenfeld s√£o independentes do tempo.
Um gr√°fico que mostra um padr√£o n√£o aleat√≥rio contra o tempo √©
evid√™ncia de viola√ß√£o da suposi√ß√£o de hip√≥tese.</p>
<p>Para testar a suposi√ß√£o de riscos proporcionais:</p>
<pre class="r"><code>final_model %&gt;% cox.zph %&gt;% ggcoxzph</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-23-1.png" style="width:80.0%" /></p>
</center>
<p>A partir da inspe√ß√£o gr√°fica, n√£o h√° padr√£o com o tempo.
A suposi√ß√£o de riscos proporcionais parece ser suportada
pelas covari√°veis</p>
</div>
</div>
</div>
<div id="considera√ß√µes-finais" class="section level2">
<h2>Considera√ß√µes finais</h2>
<iframe src="https://giphy.com/embed/ZacieLN2WI2AedWrz9" width="100%" height="216" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
<p>Como era de se esperar, o risco de ser abatido diminui para jogadores que possuem melhor performance e tamb√©m para os jogadores que percorrem maiores dist√¢ncias (o que mostra que ficar parado no jogo em uma zona pode n√£o ser a melhor ideia, j√° √© quanto mais se movimenta maior a quantidade de itens que podem ser coletados).</p>
<p>Interessante notar que a curva de <strong>sobreviv√™ncia</strong> para os jogadores que dirigiram apresenta resultado oposto ao <strong>risco</strong> esperado nos que dirigiram, isso ocorre pois esses dois modelos calculam medidas diferentes.</p>
</div>
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias</h1>
<ul>
<li>Carvalho,M.A., Andreozzi,V.L., Codec¬∏o,C.T., Campos,D.P., Barbosa,M.T.S., Shimakura,S.E., An√°lise de sobreviv√™ncia: Teoria e aplica√ß√µes em sa√∫de, Segunda Edi√ß√£o, Editora FIOCRUZ, Rio de Janeiro, 2011.</li>
<li>Colosimo,E.A., Giolo,S.R., An√°lise de sobreviv√™ncia aplicada, ABE-Projeto Fisher, S√£o Paulo, 2010</li>
<li>Lewis,E.E., Introduction to reliability engineering, John Wiley, New York, 1987</li>
<li><a href="http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Survival/BS704_Survival6.html" class="uri">http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Survival/BS704_Survival6.html</a></li>
<li><a href="http://www.sthda.com/english/wiki/cox-model-assumptions" class="uri">http://www.sthda.com/english/wiki/cox-model-assumptions</a></li>
</ul>
<p>Cuiriosidades / Leituras futuras:</p>
<ul>
<li>Evaluating Random Forests for Survival Analysis Using Prediction Error Curves: <a href="https://www.jstatsoft.org/article/view/v050i11" class="uri">https://www.jstatsoft.org/article/view/v050i11</a></li>
<li>randomForestSRC: <a href="https://cran.r-project.org/web/packages/randomForestSRC/index.html" class="uri">https://cran.r-project.org/web/packages/randomForestSRC/index.html</a></li>
<li>WTTE-RNN - Less hacky churn prediction: <a href="https://ragulpr.github.io/2016/12/22/WTTE-RNN-Hackless-churn-modeling/" class="uri">https://ragulpr.github.io/2016/12/22/WTTE-RNN-Hackless-churn-modeling/</a></li>
<li>Weibull Time To Event Recurrent Neural Network: <a href="https://github.com/ragulpr/wtte-rnn/" class="uri">https://github.com/ragulpr/wtte-rnn/</a></li>
<li>Neural Networks as Statistical Methods in Survival Analysis: <a href="https://www.stats.ox.ac.uk/pub/bdr/NNSM.pdf" class="uri">https://www.stats.ox.ac.uk/pub/bdr/NNSM.pdf</a></li>
<li>Continuous and Discrete Time Survival Analysis: Neural Network
Approaches: <a href="http://pcwww.liv.ac.uk/~afgt/eleuteri_lyon07.pdf" class="uri">http://pcwww.liv.ac.uk/~afgt/eleuteri_lyon07.pdf</a></li>
<li>Cox Proportional Hazards Model - h2O Documentation: <a href="http://s3.amazonaws.com/h2o-release/h2o/master/1579/docs-website/datascience/coxph.html" class="uri">http://s3.amazonaws.com/h2o-release/h2o/master/1579/docs-website/datascience/coxph.html</a></li>
<li>Introduction to H2OCoxPH: <a href="https://www.slideshare.net/0xdata/introduction-to-h2ocoxph" class="uri">https://www.slideshare.net/0xdata/introduction-to-h2ocoxph</a></li>
</ul>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle/">An√°lise de sobreviv√™ncia com dados do jogo PUBG dispon√≠veis no Kaggle</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">analise-de-sobrevivencia</category>
      <category domain="tag">data-mining</category>
      <category domain="tag">estatistica</category>
      <category domain="tag">gamificacao</category>
      <category domain="tag">gamification</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">modelagem-estatistica</category>
      <category domain="tag">r</category>
      <category domain="tag">survivor</category>
    </item>
    <item>
      <title>Um estudo sobre modelos de aprendizagem baseados em √°rvores com desafio do Kaggle</title>
      <link>https://gomesfellipe.github.io/post/2018-08-31-modelos-em-arvore/modelos-em-arvore/</link>
      <pubDate>Fri, 31 Aug 2018 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2018-08-31-modelos-em-arvore/modelos-em-arvore/</guid>
      <description>Um estudo aplicado de modelos de aprendizagem baseados em √°rvores utilizando a base de dados do Kaggle para prever o pre√ßo final de casas residenciais em Ames, Iowa, utilizando uma variedade de aspectos</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="kaggle" class="section level1">
<h1>Kaggle</h1>
<p>Segundo o <a href="https://en.wikipedia.org/wiki/Kaggle">Wikip√©dia</a>: ‚ÄúKaggle √© a maior comunidade mundial de cientistas de dados e machine learning.‚Äù Aprendo muito estudando as resolu√ß√µes de alguns competidores pois l√° √© poss√≠vel conferir tanto as metodologias utilizadas pelos competidores quando os c√≥digos e √© not√°vel o cuidado dos participantes para que seja poss√≠vel a reprodutibilidade dos resultados, o que pode impulsionar o aprendizado.</p>
<p>O Kaggle trabalha com a ideia de <a href="https://en.wikipedia.org/wiki/Gamification">gamifica√ß√£o</a>, que √© um assunto do qual j√° escrevi em um post sobre <a href="https://gomesfellipe.github.io/post/2018-02-17-cheatsheet-gamificacao-r/cheatsheet-gamificacao-r/">gamifica√ß√£o e porque aprender R √© t√£o divertido</a> e gosto deste conceito de se criar jogos para motivar e engajar as pessoas em atividades profissionais e a ideia de se estar em um jogo possibilita doses de motiva√ß√£o especialmente a quem gosta de competir.</p>
<p>A plataforma √© focada em competi√ß√µes que envolvem modelagem preditiva, que julgam apenas o seu desempenho preditivo, embora a inteligibilidade n√£o deixe de ser importante. Neste post farei tamb√©m a modelagem descritiva com modelos de aprendizagem baseados em √°rvores, na qual o principal objetivo ser√° obter informa√ß√µes sobre os dados para o ajuste dos modelos preditivos que iremos submeter √† competi√ß√£o do Kaggle <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/">House Prices: Advanced Regression Techniques</a>.</p>
<p>A diferen√ßa entre modelos preditivos e descritivos n√£o √© t√£o rigorosa assim pois algumas das t√©cnicas podem ser utilizadas para ambos e geralmente um modelo pode servir para ambos os prop√≥sitos (mesmo que de de forma insuficiente).</p>
<p>Al√©m dos modelos de machine learning baseados em √°rvores, tamb√©m ser√° ajustado um modelo de regress√£o linear multivariado para compararmos os resultados dos ajustes e submeter nossas previs√µes no site do <a href="https://kaggle.com">kaggle</a>.</p>
<p>Os pacotes que ser√£o utilizados ser√£o os seguintes:</p>
<pre class="r"><code>library(purrr)       # Programacao funciona
library(broom)       # Arrumar outputs
library(dplyr)       # Manipulacao de dados
library(magrittr)    # pipes
library(funModeling) # df_status()
library(plyr)        # revalue()
library(gridExtra)   # Juntar ggplots
library(reshape)     # funcao melt()
library(rpart)       # Arvore de Decisoes
library(rpart.plot)  # Plot da Arvore de Decisoes
library(data.table)  # aux na manipulacao do heatmap
library(readr)       # Leitura da base de dados
library(stringr)     # Manipulacao de strings
library(ggplot2)     # Graficos elegantes
library(caret)       # Machine Learning 
library(GGally)      # up ggplot
library(ggfortify)   # autoplot()</code></pre>
<div id="base-de-dados" class="section level2">
<h2>Base de dados</h2>
<p>A base de dados deste post vem de uma competi√ß√£o √≥tima para estudantes de ci√™ncia de dados de dados com alguma experi√™ncia com R ou Python e no√ß√µes b√°sicas de machine learning e estat√≠stica.</p>
<p>Pode ser √∫til para aqueles que desejam expandir seu conjunto de habilidades em uma tarefa de regress√£o, quando a vari√°vel <span class="math inline">\(y\)</span> que desejamos estimar √© do tipo num√©rico (cont√≠nuo ou discreto).</p>
<p>Trata-se do <a href="https://ww2.amstat.org/publications/jse/v19n3/decock.pdf">conjunto de dados Ames Housing</a> que foi compilado por Dean De Cock para uso em educa√ß√£o de ci√™ncia de dados.</p>
<pre class="r"><code>train &lt;- read_csv(&quot;train.csv&quot;)
test  &lt;- read_csv(&quot;test.csv&quot;)
full  &lt;- bind_rows(train, test)

id    &lt;- test$Id
full %&lt;&gt;% select(-Id)</code></pre>
<div id="descri√ß√£o-da-competi√ß√£o" class="section level3">
<h3>Descri√ß√£o da Competi√ß√£o</h3>
<p>Traduzido do site oficial do kaggle:</p>
<p>"Pe√ßa a um comprador que descreva a casa dos seus sonhos, e eles provavelmente n√£o come√ßar√£o com a altura do teto do por√£o ou a proximidade de uma ferrovia leste-oeste. Mas o conjunto de dados desta competi√ß√£o de playground prova que muito mais influencia as negocia√ß√µes de pre√ßo do que o n√∫mero de quartos ou uma cerca branca.</p>
<p>Com 79 vari√°veis explicativas descrevendo (quase) todos os aspectos de casas residenciais em Ames, Iowa, esta competi√ß√£o desafia voc√™ a prever o pre√ßo final de cada casa."</p>
<p>Portanto, primeiramente vamos entender o comportamento da vari√°vel resposta, depois buscar quais dessas 79 vari√°veis explicativas s√£o mais importantes para representar a varia√ß√£o do pre√ßo de venda das casas atrav√©s dos m√©todos baseados em √°rvores e por fim ajustar os modelos propostos e submeter nossas estimativas no site!</p>
</div>
</div>
</div>
<div id="an√°lise-explorat√≥ria-dos-dados" class="section level1">
<h1>An√°lise explorat√≥ria dos dados</h1>
<p>Antes de pensar em ajustar algum modelo √© extremamente necess√°rio entender como se comportam os dados, portanto, tanto a vari√°vel resposta quanto as vari√°veis explicativas ser√£o avaliadas.</p>
<div id="vari√°vel-resposta" class="section level2">
<h2>Vari√°vel resposta:</h2>
<p><code>SalePrice</code> - o pre√ßo de venda da propriedade em d√≥lares. Essa √© a vari√°vel de destino que estamos tentando prever.</p>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Note que a distribui√ß√£o dos dados referentes ao pre√ßo de venda se distribui de maneira assim√©trica e n√£o possuem evid√™ncias de normalidade dos dados. Apesar dos m√©todos baseados em √°rvore se tratarem de t√©cnicas n√£o param√©tricas essa transforma√ß√£o ser√° feita pois ao final deste post desejo comparar os resultados com um modelo de regress√£o linear m√∫ltipla.</p>
</div>
</div>
<div id="√°rvore-de-decis√£o" class="section level1">
<h1>√Årvore de decis√£o</h1>
<p>Uma t√©cnica muito popular que √© mais comumente usada para resolver tarefas de classifica√ß√£o de dados por√©m a √°rvore conhecida como <a href="https://tinyurl.com/ybhlsgom">CART (Classification and Regression Trees)(Breiman, 1986)</a> lida com todos os tipos de atributos (incluindo atributos num√©ricos que s√£o tratados a partir da cria√ß√£o de intervalos). Para seu ajuste √© poss√≠vel realizar podas e produzir √°rvores bin√°rias.</p>
<p>A constru√ß√£o da √°rvore √© realizada por meio do algoritmo que iterativamente analisa os atributos descritivos de um conjunto de dados previamente rotulado. Sua popularidade como apoio para a tomada de decis√£o se deve principalmente ao fato da f√°cil visualiza√ß√£o do conhecimento gerado e o f√°cil entendimento.</p>
<p>Outra caracter√≠stica legal da √°rvore de decis√µes √© que ela permite ajustar um modelo sem um pr√©-processamento detalhado, pois √© f√°cil de ajustar, aceita valores faltantes e √© de f√°cil interpreta√ß√£o, veja:</p>
<pre class="r"><code>library(rpart)

control &lt;- rpart.control(minsplit =10, # o n√∫mero m√≠nimo de observa√ß√µes em um n√≥
                         cp = 0.006    # parametro de complexidade q controla o tamanho da arvore
)
rpartFit &lt;- rpart(exp(SalePrice) ~ . , train, method = &quot;anova&quot;, control = control) 

rpart.plot::rpart.plot(rpartFit,cex = 0.6)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-5-1.png" width="1200" /></p>
<p>No topo, vemos o primeiro n√≥ com 100% das observa√ß√µes, que representa o total da base (100%). Em seguida, vemos que a primeira vari√°vel que determina o pre√ßo de venda das casas <code>SalePrice</code> √© a vari√°vel <code>OverallQual</code>. As casas que apresentaram <code>OverallQual</code> &lt; 7.5 ocorrem em maior propor√ß√£o do que as que tiveram <code>OverallQual</code>&gt;7.5. A interpreta√ß√£o pode continuar dessa forma recursivamente.</p>
<p>√â poss√≠vel notar que as vari√°veis <code>OverallQual</code>,<code>Neighborhood</code>,<code>1stFlrSF</code>,<code>2ndFlrSF</code>,<code>GrLivArea</code>, <code>BsmtFinSF1</code> foram as que melhor representaram os dados de acordo com os par√¢metros que determinamos para ajustar esta √°rvore, vejamos com mais detalhes se existe rela√ß√£o linear e intensidade e dire√ß√£o dessa rela√ß√£o com o <a href="https://pt.wikipedia.org/wiki/Coeficiente_de_correla%C3%A7%C3%A3o_de_Pearson">coeficiente de correla√ß√£o de Pearson</a> entre estas vari√°veis dois a dois e em rela√ß√£o √† vari√°vel resposta:</p>
<pre class="r"><code>devtools::source_url(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/correlations_for_ggpairs.R&quot;)

train %&gt;% 
  select(SalePrice,OverallQual,`1stFlrSF`,`2ndFlrSF`,GrLivArea,BsmtFinSF1) %&gt;% 
  ggpairs(lower = list(continuous = my_fn))+
  theme_bw()</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Com esta figura temos muitas informa√ß√µes, destaca-se que todas essas vari√°veis possuem algum tipo de rela√ß√£o linear com a vari√°vel resposta, a menor correla√ß√£o observada foi com o <code>BsmtFinSF1</code> e a vari√°vel que apresentou a maior correla√ß√£o foi a <code>OverallQual</code>. Aten√ß√£o para a correla√ß√£o entre <code>SalePrice</code> e <code>OverallQual</code>, pois <code>Overallqual</code> parece ser uma vari√°vel ordinal e uma outra medida de correla√ß√£o que melhor representaria esta rela√ß√£o √© o <a href="https://pt.wikipedia.org/wiki/Coeficiente_de_correla%C3%A7%C3%A3o_de_postos_de_Spearman">coeficiente de correla√ß√£o de Spearman</a>, veja:</p>
<pre class="r"><code>cor(full$SalePrice, full$OverallQual, method = &quot;spearman&quot;, use = &quot;complete.obs&quot;)</code></pre>
<pre><code>## [1] 0.8098286</code></pre>
<p>Um pouco diferente do resultado da correla√ß√£o de Pearson pois avalia rela√ß√µes lineares, j√° a correla√ß√£o de Spearman avalia rela√ß√µes mon√≥tonas, sejam elas lineares ou n√£o.</p>
<div id="an√°lise-explorat√≥ria-e-input-de-nas" class="section level2 tabset">
<h2>An√°lise explorat√≥ria e input de <code>NA</code>s</h2>
<p>Arrumar a base de dados √© uma tarefa longa e que geralmente consome grande parte no tempo em um projeto de ci√™ncia de dados. N√£o adianta usar o algor√≠timo mais poderoso de machine learning se a base de dados n√£o estiver arrumada de maneira que possibilite a an√°lise dos dados.</p>
<p>Para obter informa√ß√µes da amostra, confira no <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data">link do dataset da competi√ß√£o no Kaggle</a>. Na p√°gina √© poss√≠vel conferir <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/download/data_description.txt">a descri√ß√£o da amostra</a> e nela nota-se que alguns dos valores faltantes possuem significado, ent√£o √© necess√°rio rotul√°-los para que o R possa interpretar estes valores da maneira correta.</p>
<div id="status-da-amostra" class="section level3">
<h3>Status da amostra</h3>
<p>Conferindo o status da amostra com a fun√ß√£o <code>df_status()</code> do pacote <a href="https://cran.r-project.org/web/packages/funModeling/index.html"><code>funModeling</code></a>:</p>
<pre class="r"><code>full %&gt;% 
  df_status(print_results = F) %&gt;% 
  as_tibble() %&gt;%
  arrange(-p_na, -p_zeros)</code></pre>
<pre><code>## # A tibble: 80 x 9
##    variable     q_zeros p_zeros  q_na  p_na q_inf p_inf type      unique
##    &lt;chr&gt;          &lt;int&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;int&gt;
##  1 PoolQC             0       0  2909 99.7      0     0 character      3
##  2 MiscFeature        0       0  2814 96.4      0     0 character      4
##  3 Alley              0       0  2721 93.2      0     0 character      2
##  4 Fence              0       0  2348 80.4      0     0 character      4
##  5 SalePrice          0       0  1459 50.0      0     0 numeric      663
##  6 FireplaceQu        0       0  1420 48.6      0     0 character      5
##  7 LotFrontage        0       0   486 16.6      0     0 numeric      128
##  8 GarageYrBlt        0       0   159  5.45     0     0 numeric      103
##  9 GarageFinish       0       0   159  5.45     0     0 character      3
## 10 GarageQual         0       0   159  5.45     0     0 character      5
## # ‚Ä¶ with 70 more rows</code></pre>
<p>Note que as vari√°veis problem√°ticas foram ordenadas de forma decrescente (maior n√∫mero de dados faltantes e zeros) vamos tratar uma de cada vez partindo da vari√°vel mais cr√≠tica</p>
</div>
<div id="pool" class="section level3">
<h3>Pool</h3>
<ul>
<li><code>PoolQC</code> √© a vari√°vel que possui mais <code>NA</code> e a descri√ß√£o da base informa que:</li>
</ul>
<p><code>PoolQC</code>: qualidade da piscina</p>
<ul>
<li>Ex Excelente</li>
<li>Gd Good</li>
<li>TA M√©dia / T√≠pica</li>
<li>Fa Pequena</li>
<li>NA sem piscina</li>
</ul>
<p>√â poss√≠vel observar que se trata de uma vari√°vel ordinal, portanto vamos criar uma vari√°vel auxiliar (pois esta descri√ß√£o se repete em outras vari√°veis):</p>
<pre class="r"><code># Criando vari√°vel auxilar ordinal
Qualidade &lt;- c(&#39;None&#39; = 0, &#39;Po&#39; = 1, &#39;Fa&#39; = 2, &#39;TA&#39; = 3, &#39;Gd&#39; = 4, &#39;Ex&#39; = 5)

full %&lt;&gt;%
  mutate(PoolQC =  ifelse(PoolQC %&gt;% is.na, &quot;None&quot;, PoolQC) %&gt;% as.factor() ) %&gt;% 
  mutate(PoolQC = as.integer(revalue(PoolQC, Qualidade)))</code></pre>
<p>Al√©m disso, existe outra vari√°vel relacionada √† piscina, veja:</p>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Pool&quot;)]) %&gt;% 
  table </code></pre>
<pre><code>##         PoolQC
## PoolArea    1    2    3    4
##      0      0    0    0 2906
##      144    1    0    0    0
##      228    1    0    0    0
##      368    0    0    0    1
##      444    0    0    0    1
##      480    0    0    1    0
##      512    1    0    0    0
##      519    0    1    0    0
##      555    1    0    0    0
##      561    0    0    0    1
##      576    0    0    1    0
##      648    0    1    0    0
##      738    0    0    1    0
##      800    0    0    1    0</code></pre>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Pool&quot;)]) %&gt;%
  map(~sum(is.na(.x)))</code></pre>
<pre><code>## $PoolArea
## [1] 0
## 
## $PoolQC
## [1] 0</code></pre>
<pre class="r"><code># Arrumando inconsist√´ncias:
full %&lt;&gt;% 
  mutate(PoolQC = ifelse(PoolQC == 0 &amp; PoolArea !=0, 2, PoolQC))

# Arrumando inconsist√´ncias:
full %&lt;&gt;% 
  mutate(Pool = ifelse(PoolQC == 0 &amp; PoolArea ==0, &quot;no&quot;, &quot;yes&quot;))</code></pre>
</div>
<div id="misc" class="section level3">
<h3>Misc</h3>
<p>Se referem aos recursos diversos</p>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Misc&quot;)],
         SalePrice
  ) %&gt;%
  map(~sum(is.na(.x)))</code></pre>
<pre><code>## $MiscFeature
## [1] 2814
## 
## $MiscVal
## [1] 0
## 
## $SalePrice
## [1] 1459</code></pre>
<p><code>MiscFeature</code>: recurso diverso n√£o coberto em outras categorias</p>
<ul>
<li>Elevador elev</li>
<li>Gar2 2nd Garage (se n√£o for descrito na se√ß√£o de garagem)</li>
<li>Othr Outro</li>
<li>Galp√£o derramado (mais de 100 SF)</li>
<li>TenC Campo de t√©nis</li>
<li>NA Nenhum</li>
</ul>
<p>Desta vez n√£o se trata de uma vari√°vel ordinal, vejamos:</p>
<pre class="r"><code>full %&lt;&gt;%
  mutate(MiscFeature =  if_else(MiscFeature %&gt;% is.na, &quot;None&quot;, MiscFeature) %&gt;% as.factor) 

# Breve resumo:
g1 &lt;- 
  full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Misc&quot;)], SalePrice) %&gt;% 
  ggplot(aes(y=MiscVal,x= reorder(MiscFeature, -MiscVal,FUN = median) ,fill=MiscFeature))+
  geom_boxplot()+
  theme_bw() +
  scale_fill_viridis_d() +
  labs(x = &quot;Recurso Diverso&quot;)

g2 &lt;- 
  full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Misc&quot;)], SalePrice) %&gt;% 
  ggplot(aes(y=SalePrice,x= reorder(MiscFeature, -MiscVal,FUN = median) ,fill=MiscFeature))+
  geom_boxplot()+
  theme_bw() +
  scale_fill_viridis_d() +
  labs(x = &quot;Pre√ßo de Venda&quot;)

grid.arrange(g1, g2)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<pre class="r"><code>rm(g1,g2)</code></pre>
<p>Al√©m disso, <code>MiscVal</code>: Valor do recurso variado</p>
</div>
<div id="alley" class="section level3">
<h3>Alley</h3>
<p><code>Alley</code>: Tipo de acesso ao beco para a propriedade</p>
<ul>
<li>Grvl Cascalho</li>
<li>Pave pavimentado</li>
<li>NA Nenhum acesso de beco</li>
</ul>
<p>Basta realizar o input:</p>
<pre class="r"><code>full %&lt;&gt;% 
  mutate(Alley = Alley %&gt;% str_replace_na(&quot;None&quot;)) %&gt;% 
  mutate(Alley = as.factor(Alley))</code></pre>
<pre class="r"><code>full[!is.na(full$SalePrice),] %&gt;% 
  select(Alley, SalePrice) %&gt;% 
  ggplot(aes(y=SalePrice,x= reorder(Alley, -SalePrice,FUN = median) ,fill=Alley))+
  geom_boxplot()+
  theme_bw() +
  scale_fill_viridis_d() +
  labs(x = &quot;tipo de Acesso&quot;)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
</div>
<div id="fence" class="section level3">
<h3>Fence</h3>
<p><code>Fence</code>: qualidade da cerca</p>
<ul>
<li>GdPrv Boa privacidade</li>
<li>MnPrv minima privacidade</li>
<li>GdWo boa madeira</li>
<li>MnWw M√≠nima Madeira / Fio</li>
<li>NA Sem cerca</li>
</ul>
<p>Input ser√° da seguinte forma:</p>
<pre class="r"><code>full %&lt;&gt;% 
  mutate(Fence = Fence %&gt;% str_replace_na(&quot;None&quot;))</code></pre>
<pre class="r"><code>full[1:nrow(train),] %&gt;% 
  select(Fence, SalePrice) %&gt;% 
  ggplot(aes(y=SalePrice,x= reorder(Fence, -SalePrice, median) ,fill=Fence))+
  geom_boxplot()+
  theme_bw() +
  scale_fill_viridis_d() +
  labs(x = &quot;tipo de Acesso&quot;)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<pre class="r"><code>full %&lt;&gt;% mutate(Fence = as.factor(Fence))</code></pre>
<p>Aparentemente n√£o parece existir uma rela√ß√£o ordinal sobre o tipo de cerca quanto ao pre;o de venda da casa, portanto foi convertida para fator</p>
</div>
<div id="fireplace" class="section level3">
<h3>FirePlace</h3>
<p>Vari√°veis relacionadas com lareira. Segundo a descri√ß√£o, temos:</p>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Fireplace&quot;)], SalePrice)</code></pre>
<pre><code>## # A tibble: 2,919 x 3
##    Fireplaces FireplaceQu SalePrice
##         &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;
##  1          0 &lt;NA&gt;             12.2
##  2          1 TA               12.1
##  3          1 TA               12.3
##  4          1 Gd               11.8
##  5          1 TA               12.4
##  6          0 &lt;NA&gt;             11.9
##  7          1 Gd               12.6
##  8          2 TA               12.2
##  9          2 TA               11.8
## 10          2 TA               11.7
## # ‚Ä¶ with 2,909 more rows</code></pre>
<p><code>Fireplaces</code>: Numero de lareiras</p>
<p><code>FireplaceQu</code>: Qualidade da lareira</p>
<ul>
<li>Ex Excellente - Excepcional Lareira de Alvenaria</li>
<li>Gd Boa - Lareira de alvenaria no n√≠vel principal</li>
<li>TA M√©dia - lareira pr√©-fabricada na sala principal ou Lareira de alvenaria no por√£o</li>
<li>Fa Pequena - Lareira pr√©-fabricada no por√£o</li>
<li>Po Pobre - Fog√£o Ben Franklin</li>
<li>NA sem lareira</li>
</ul>
<p>Nota-se que se trata de uma vari√°vel ordinal de acordo com a qualidade, portanto:</p>
<pre class="r"><code>full %&lt;&gt;% 
  mutate(FireplaceQu =  if_else(FireplaceQu %&gt;% is.na, &quot;None&quot;, FireplaceQu) ) %&gt;% 
  mutate(FireplaceQu = as.integer(revalue(FireplaceQu, Qualidade)))</code></pre>
<p>Conferindo se existem inconsist√™ncias:</p>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Fireplace&quot;)]) %&gt;% 
  table </code></pre>
<pre><code>##           FireplaceQu
## Fireplaces    0    1    2    3    4    5
##          0 1420    0    0    0    0    0
##          1    0   46   63  495  627   37
##          2    0    0   10   92  112    5
##          3    0    0    1    4    5    1
##          4    0    0    0    1    0    0</code></pre>
</div>
<div id="lot" class="section level3">
<h3>Lot</h3>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Lot&quot;)], SalePrice) %&gt;% 
  map_dbl(~sum(is.na(.x)))</code></pre>
<pre><code>## LotFrontage     LotArea    LotShape   LotConfig   SalePrice 
##         486           0           0           0        1459</code></pre>
<p>Segundo a descri√ß√£o:</p>
<p><code>LotFrontage</code>: Ruas linearmente conectadas √† propriedade</p>
<p><code>LotArea</code> : Tamanho do lote em p√©s quadrados</p>
<p><code>LotShape</code>: forma geral da propriedade</p>
<ul>
<li>Regue Regular<br />
</li>
<li>IR1 ligeiramente irregular</li>
<li>IR2 moderadamente irregular</li>
<li>IR3 Irregular</li>
</ul>
<p><code>LotConfig</code>: configura√ß√£o de lote</p>
<ul>
<li>Inside Lote muito para dentro</li>
<li>Corner Canto de esquina</li>
<li>CulDSac Cul-de-sac</li>
<li>FR2 Frente em 2 lados da propriedade</li>
<li>FR3 Frente em 3 lados da propriedade</li>
</ul>
<p>Input para o <code>LotFrontage</code> ser√° feito considerando a configura√ß√£o do lote, veja:</p>
<pre class="r"><code>inputsLot &lt;- full %&gt;% 
  select(LotFrontage, LotConfig) %&gt;% 
  group_by(LotConfig) %&gt;%
  dplyr::summarise(Media = mean(LotFrontage, na.rm = T),
            Mediana = median(LotFrontage, na.rm = T))

full$LotFrontage[is.na(full$LotFrontage) &amp; full$LotConfig == inputsLot$LotConfig[1]] &lt;- inputsLot$Mediana[1] 
full$LotFrontage[is.na(full$LotFrontage) &amp; full$LotConfig == inputsLot$LotConfig[2]] &lt;- inputsLot$Mediana[2] 
full$LotFrontage[is.na(full$LotFrontage) &amp; full$LotConfig == inputsLot$LotConfig[3]] &lt;- inputsLot$Mediana[3] 
full$LotFrontage[is.na(full$LotFrontage) &amp; full$LotConfig == inputsLot$LotConfig[4]] &lt;- inputsLot$Mediana[4] 
full$LotFrontage[is.na(full$LotFrontage) &amp; full$LotConfig == inputsLot$LotConfig[5]] &lt;- inputsLot$Mediana[5] </code></pre>
<p>Arrumando vari√°veis nominais e ordinais:</p>
<pre class="r"><code>full %&lt;&gt;% 
  mutate(LotShape = as.integer(revalue(full$LotShape, c(&#39;IR3&#39;=0, &#39;IR2&#39;=1, &#39;IR1&#39;=2, &#39;Reg&#39;=3))))</code></pre>
</div>
<div id="garages" class="section level3">
<h3>Garages</h3>
<p>Vari√°veis relacionadas, segundo a descri√ß√£o, temos:</p>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Garage&quot;)], SalePrice) %&gt;% 
  map_dbl(~sum(is.na(.x)))</code></pre>
<pre><code>##   GarageType  GarageYrBlt GarageFinish   GarageCars   GarageArea   GarageQual 
##          157          159          159            1            1          159 
##   GarageCond    SalePrice 
##          159         1459</code></pre>
<p><code>GarageType</code>: localiza√ß√£o da garagem</p>
<ul>
<li>2Types Mais de um tipo de garagem</li>
<li>Attchd anexa a casa</li>
<li>Basement tipo porao</li>
<li>BuiltIn (garagem parte da casa - normalmente tem sala acima da garagem)</li>
<li>CarPort Porta do carro</li>
<li>Detchd nao anexa a casa</li>
<li>NA Sem Garagem</li>
</ul>
<p><code>GarageYrBlt</code>: garagem do ano foi constru√≠da</p>
<p><code>GarageFinish</code>: acabamento interior da garagem</p>
<ul>
<li>Fin Finished</li>
<li>RFn √Åspero Finalizado<br />
</li>
<li>Unf inacabado</li>
<li>NA Sem Garagem</li>
</ul>
<p><code>GarageCars</code>: Tamanho da garagem na capacidade do carro</p>
<p><code>GarageArea</code>: Tamanho da garagem em p√©s quadrados</p>
<p><code>GarageQual</code>: GarageQuality</p>
<ul>
<li>Ex Excelente</li>
<li>Gd Good</li>
<li>TA T√≠pico / M√©dio</li>
<li>FA Justo</li>
<li>Po Poor</li>
<li>NA Sem Garagem</li>
</ul>
<p><code>GarageCond</code>: condi√ß√£o de garagem</p>
<ul>
<li>Ex Excelente</li>
<li>Gd Good</li>
<li>TA T√≠pico / M√©dio</li>
<li>Fa Justo</li>
<li>Po Poor</li>
<li>NA Sem Garagem</li>
</ul>
<pre class="r"><code>full %&lt;&gt;% 
  mutate(GarageType   =  if_else(GarageType %&gt;% is.na, &quot;None&quot;, GarageType) ) %&gt;% 
  mutate(GarageYrBlt  = if_else(GarageYrBlt %&gt;% is.na,YearBuilt, GarageYrBlt) ) %&gt;% 
  mutate(GarageFinish =  if_else(GarageFinish %&gt;% is.na, &quot;None&quot;, GarageFinish) ) %&gt;% 
  mutate(GarageFinish = as.integer(revalue(GarageFinish, c(&#39;None&#39;=0, &#39;Unf&#39;=1, &#39;RFn&#39;=2, &#39;Fin&#39;=3)))) %&gt;% 
  mutate(GarageCars   = ifelse(GarageCars %&gt;% is.na, 0, GarageCars) ) %&gt;% 
  mutate(GarageArea   = ifelse(GarageArea %&gt;% is.na, 0, GarageArea)) %&gt;% 
  mutate(GarageQual   = if_else(GarageQual %&gt;% is.na, &quot;None&quot;, GarageQual)) %&gt;% 
  mutate(GarageQual   = as.integer(revalue(GarageQual, Qualidade))) %&gt;% 
  mutate(GarageCond   = if_else(GarageCond %&gt;% is.na, &quot;None&quot;, GarageCond)) %&gt;% 
  mutate(GarageCond   = as.integer(revalue(GarageCond, Qualidade))) 
  
table(full$GarageCond)</code></pre>
<pre><code>## 
##    0    1    2    3    4    5 
##  159   14   74 2654   15    3</code></pre>
</div>
<div id="bsmt" class="section level3">
<h3>Bsmt</h3>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Bsmt&quot;)], SalePrice) %&gt;% 
  map_dbl(~sum(is.na(.x)))</code></pre>
<pre><code>##     BsmtQual     BsmtCond BsmtExposure BsmtFinType1   BsmtFinSF1 BsmtFinType2 
##           81           82           82           79            1           80 
##   BsmtFinSF2    BsmtUnfSF  TotalBsmtSF BsmtFullBath BsmtHalfBath    SalePrice 
##            1            1            1            2            2         1459</code></pre>
<p><code>BsmtQual</code>: Avalia a altura do por√£o</p>
<ul>
<li>Ex Excelente (100+ polegadas)<br />
</li>
<li>Gd Bom (90-99 polegadas)</li>
<li>TA T√≠pica (80-89 polegadas)</li>
<li>Fa Justo (70-79 polegadas)</li>
<li>Po Pobre (&lt;70 polegadas</li>
<li>NA Sem Por√£o</li>
</ul>
<p><code>BsmtCond</code>: Avalia o estado geral do por√£o</p>
<ul>
<li>Ex Excelente</li>
<li>Gd Bom</li>
<li>TA T√≠pica - umidade ligeira permitida</li>
<li>Fa Razo√°vel - umidade ou alguma rachadura ou sedimenta√ß√£o</li>
<li>Po Insuficiente - Craqueamento severo, sedimenta√ß√£o ou umidade</li>
<li>NA Sem Por√£o</li>
</ul>
<p><code>BsmtExposure</code>: Refere-se a paralisa√ß√µes ou paredes no n√≠vel do jardim</p>
<ul>
<li>Gd Good Exposi√ß√£o</li>
<li>Av M√©dia Exposi√ß√£o (n√≠veis divididos ou foyers normalmente pontua√ß√£o m√©dia ou acima)<br />
</li>
<li>Mn Exposi√ß√£o M√≠nima</li>
<li>No N√£o Exposi√ß√£o</li>
<li>NA Sem por√£o</li>
</ul>
<p><code>BsmtFinType1</code>: Avalia√ß√£o da √°rea acabada do por√£o</p>
<ul>
<li>GLQ Bons Viver</li>
<li>ALQ M√©dia Living Quarters</li>
<li>BLQ Abaixo da m√©dia Living Quarters<br />
</li>
<li>Rec M√©dia Rec Room</li>
<li>LwQ Baixa Qualidade</li>
<li>Unf unfinshed</li>
<li>NA nenhum por√£o</li>
</ul>
<p><code>BsmtFinSF1</code>: pes quadrados do tipo 1 terminado</p>
<p><code>BsmtFinType2</code>: Avalia√ß√£o do por√£o √°rea terminado (se v√°rios tipos)</p>
<ul>
<li>GLQ Bons aposentos</li>
<li>ALQ Medianos</li>
<li>BLQ abaixo da media</li>
<li>Rec Aposentos m√©dia qualidade</li>
<li>LwQ Baixa Qualidade</li>
<li>Unf</li>
<li>N√£o Sem Por√£o</li>
</ul>
<p><code>BsmtFinSF2</code>: P√©s quadrados acabados do Tipo 2</p>
<p><code>BsmtUnfSF</code>: P√©s quadrados inacabados da √°rea do por√£o</p>
<p><code>TotalBsmtSF</code>: Total p√©s quadrados da √°rea do por√£o</p>
<p>Input das vari√°veis n√£o num√©ricas com <code>None</code> e convertendo para ordinal as vari√°veis com rela√ß√£o de ordem. Para os faltantes das vari√°veis num√©ricas foram imputados o valor 0 (zeros).</p>
<pre class="r"><code># Categ√≥ricos:
full[,names(full)[names(full) %&gt;% str_detect(&quot;Bsmt&quot;)]] &lt;- 
  full[,names(full)[names(full) %&gt;% str_detect(&quot;Bsmt&quot;)]] %&gt;%
  select(names(full)[names(full) %&gt;% str_detect(&quot;Bsmt&quot;)]) %&gt;%
  mutate_if( ~ !is.numeric(.x) , ~ ifelse(is.na(.x), &quot;None&quot;, .x)) %&gt;% 
  mutate(BsmtQual = as.integer(revalue(BsmtQual, Qualidade))) %&gt;% 
  mutate(BsmtCond = as.integer(revalue(BsmtCond, Qualidade))) %&gt;% 
  mutate(BsmtExposure = as.integer(revalue(BsmtExposure, c(&#39;None&#39;=0, &#39;No&#39;=1, &#39;Mn&#39;=2, &#39;Av&#39;=3, &#39;Gd&#39;=4)))) %&gt;% 
  mutate(BsmtFinType1 = as.integer(revalue(BsmtFinType1,c(&#39;None&#39;=0, &#39;Unf&#39;=1, &#39;LwQ&#39;=2, &#39;Rec&#39;=3, &#39;BLQ&#39;=4, &#39;ALQ&#39;=5, &#39;GLQ&#39;=6)))) 

# Num√©ricos:
full[,names(full)[names(full) %&gt;% str_detect(&quot;Bsmt&quot;)]] &lt;- 
  full[,names(full)[names(full) %&gt;% str_detect(&quot;Bsmt&quot;)]] %&gt;%
  select(names(full)[names(full) %&gt;% str_detect(&quot;Bsmt&quot;)]) %&gt;%
  mutate_if( ~ is.numeric(.x) , ~ ifelse(is.na(.x), 0, .x))</code></pre>
</div>
<div id="masvnr" class="section level3">
<h3>MasVnr</h3>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;MasVnr&quot;)], SalePrice) %&gt;% 
  map_dbl(~sum(is.na(.x)))</code></pre>
<pre><code>## MasVnrType MasVnrArea  SalePrice 
##         24         23       1459</code></pre>
<p><code>MasVnrType</code>: Alvenaria tipo de verniz</p>
<ul>
<li>BrkCmn Brick Common</li>
<li>BrkFace Face de tijolos</li>
<li>CBlock Bloco cinza</li>
<li>None Nenhum</li>
<li>Stone Pedra</li>
</ul>
<p><code>MasVnrArea</code>: √Årea de folheado de alvenaria em p√©s quadrados</p>
<pre class="r"><code>full %&lt;&gt;% 
  mutate(MasVnrType = if_else(is.na(MasVnrType), &quot;None&quot;, MasVnrType)) %&gt;% 
  mutate(MasVnrType = as.integer(revalue(MasVnrType, c(&#39;None&#39;=0, &#39;BrkCmn&#39;=0, &#39;BrkFace&#39;=1, &#39;Stone&#39;=2)))) %&gt;% 
  mutate(MasVnrArea = if_else(is.na(MasVnrArea), 0, 1))</code></pre>
</div>
<div id="vari√°veis-restantes-com-poucos-na" class="section level3">
<h3>Vari√°veis restantes com poucos <code>NA</code></h3>
<p>A estrat√©gia adotada para imputar estes dados ser√° tomada de maneira arbitr√°ria. Os valores faltantes ser√£o preenchidos com o valor comum mais frequente daquela vari√°vel. As vari√°veis que restam s√£o:</p>
<pre class="r"><code>full %&gt;% 
  df_status(print_results = F) %&gt;% 
  as_tibble() %&gt;%
  arrange(-p_na, -p_zeros)</code></pre>
<pre><code>## # A tibble: 81 x 9
##    variable    q_zeros p_zeros  q_na  p_na q_inf p_inf type      unique
##    &lt;chr&gt;         &lt;int&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;int&gt;
##  1 SalePrice         0     0    1459 50.0      0     0 numeric      663
##  2 MSZoning          0     0       4  0.14     0     0 character      5
##  3 Utilities         0     0       2  0.07     0     0 character      2
##  4 Functional        0     0       2  0.07     0     0 character      7
##  5 Exterior1st       0     0       1  0.03     0     0 character     15
##  6 Exterior2nd       0     0       1  0.03     0     0 character     16
##  7 Electrical        0     0       1  0.03     0     0 character      5
##  8 KitchenQual       0     0       1  0.03     0     0 character      4
##  9 SaleType          0     0       1  0.03     0     0 character      9
## 10 PoolArea       2906    99.6     0  0        0     0 numeric       14
## # ‚Ä¶ with 71 more rows</code></pre>
<p>Vejamos:</p>
<p><code>MSZoning</code>: Identifica a classifica√ß√£o geral de zoneamento da venda.</p>
<ul>
<li>Ser√° convertida para fator, vari√°vel nominal</li>
</ul>
<p><code>KitchenQual</code>: Qualidade da cozinha</p>
<ul>
<li>Ser√° convertida para ordinal</li>
</ul>
<p><code>Utilities</code>: Tipo de utilidade dispon√≠vel</p>
<ul>
<li>Ser√° removida</li>
</ul>
<p><code>Functional</code>: Funcionalidade dom√©stica</p>
<ul>
<li>Ser√° considerada como ordinal</li>
</ul>
<p><code>Exterior1st</code>: revestimento Exterior em casa</p>
<ul>
<li>Convertida para fator, vari√°vel nominal</li>
</ul>
<p><code>Electrical</code>: Sistema el√©trico</p>
<ul>
<li>Convertida para fator, vari√°vel nominal</li>
</ul>
<p><code>SaleType</code>: Tipo de venda</p>
<ul>
<li>Convertida para fator, vari√°vel nominal</li>
</ul>
<pre class="r"><code>full &lt;- full %&gt;% 
  mutate(MSZoning    = ifelse(is.na(MSZoning),
                            full$MSZoning %&gt;% table %&gt;% sort %&gt;% names %&gt;% last, MSZoning)) %&gt;% 
  mutate(MSZoning    = as.factor(MSZoning)) %&gt;% 
  mutate(KitchenQual = ifelse(is.na(KitchenQual),
                            full$KitchenQual %&gt;% 
                              table %&gt;% sort %&gt;% names %&gt;% last, KitchenQual)) %&gt;% 
  mutate(KitchenQual = as.integer(revalue(as.character(full$KitchenQual), Qualidade))) %&gt;% 
  select(-Utilities) %&gt;% 
  mutate(Exterior1st = ifelse(is.na(Exterior1st),
                            full$Exterior1st %&gt;% table %&gt;% sort %&gt;% names %&gt;% last, Exterior1st)) %&gt;% 
  mutate(Exterior1st = as.factor(Exterior1st)) %&gt;% 
  mutate(Exterior2nd = ifelse(is.na(Exterior2nd),
                            full$Exterior2nd %&gt;% table %&gt;% sort %&gt;% names %&gt;% last, Exterior2nd)) %&gt;% 
  mutate(Exterior2nd = as.factor(Exterior2nd)) %&gt;% 
  mutate(Electrical  = ifelse(is.na(Electrical),
                            full$Electrical %&gt;% table %&gt;% sort %&gt;% names %&gt;% last, Electrical)) %&gt;% 
  mutate(Electrical  = as.factor(Electrical)) %&gt;% 
  mutate(SaleType    = ifelse(is.na(SaleType ),
                            full$SaleType  %&gt;% table %&gt;% sort %&gt;% names %&gt;% last, SaleType )) %&gt;% 
  mutate(SaleType    = as.factor(SaleType )) 


full[is.na(full$Functional),&quot;Functional&quot;] &lt;- full$Functional %&gt;% table %&gt;% sort %&gt;% names %&gt;% last
full$Functional = as.integer(revalue(full$Functional, c(&#39;Sal&#39;=0, &#39;Sev&#39;=1, &#39;Maj2&#39;=2, &#39;Maj1&#39;=3, &#39;Mod&#39;=4, &#39;Min2&#39;=5, &#39;Min1&#39;=6, &#39;Typ&#39;=7)))
full[is.na(full$KitchenQual),&quot;KitchenQual&quot;] &lt;- full$KitchenQual %&gt;% table %&gt;% sort %&gt;% names %&gt;% last %&gt;% as.numeric()
full$KitchenQual = as.integer(revalue(as.character(full$KitchenQual), Qualidade))
# full[is.na(full$Electrical),&quot;Electrical&quot;] &lt;- 3

to_remove &lt;- full %&gt;% map(~table(.x) %&gt;% length()) %&gt;% .[.== 1] %&gt;% names()
full &lt;- full %&gt;% select(-one_of(to_remove))</code></pre>
<p>Status da base no momento:</p>
<pre class="r"><code>full %&gt;% 
  df_status(print_results = F) %&gt;% 
  as_tibble() %&gt;%
  arrange(-p_na,-p_zeros, type)</code></pre>
<pre><code>## # A tibble: 79 x 9
##    variable      q_zeros p_zeros  q_na  p_na q_inf p_inf type    unique
##    &lt;chr&gt;           &lt;int&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt;
##  1 SalePrice           0     0    1459  50.0     0     0 numeric    663
##  2 PoolArea         2906    99.6     0   0       0     0 numeric     14
##  3 3SsnPorch        2882    98.7     0   0       0     0 numeric     31
##  4 LowQualFinSF     2879    98.6     0   0       0     0 numeric     36
##  5 MiscVal          2816    96.5     0   0       0     0 numeric     38
##  6 BsmtHalfBath     2744    94       0   0       0     0 numeric      3
##  7 ScreenPorch      2663    91.2     0   0       0     0 numeric    121
##  8 BsmtFinSF2       2572    88.1     0   0       0     0 numeric    272
##  9 EnclosedPorch    2460    84.3     0   0       0     0 numeric    183
## 10 HalfBath         1834    62.8     0   0       0     0 numeric      3
## # ‚Ä¶ with 69 more rows</code></pre>
<p>Transformando o <code>character</code> para <code>factor</code>:</p>
<pre class="r"><code>full %&lt;&gt;% mutate_if(is.character, as.factor)</code></pre>
<p>Transformando novamente nossa base de treino e de teste:</p>
<pre class="r"><code>train &lt;- full[1:nrow(train),] %&gt;% as.data.frame() 
test  &lt;- full[(nrow(train)+1):nrow(full),] %&gt;% select(-SalePrice) %&gt;% as.data.frame()

# # Input Missing
# train_miss_model = preProcess(train, &quot;knnImpute&quot;)
# train = predict(train_miss_model, train)
# test = predict(train_miss_model, test)
# 
# train$SalePrice &lt;- y</code></pre>
</div>
</div>
</div>
<div id="machine-learning-com-algor√≠tmos-de-aprendizagem-baseados-em-√°rvores" class="section level1">
<h1>Machine Learning com algor√≠tmos de aprendizagem baseados em √°rvores</h1>
<p>Os m√©todos baseados em √°rvores fornecem modelos preditivos de alta precis√£o, estabilidade e facilidade de interpreta√ß√£o. Ao contr√°rio dos modelos lineares, eles s√£o capazes de lidar bem com rela√ß√µes n√£o-lineares al√©m de poderem ser adaptados para resolver tanto problemas de classifica√ß√£o quanto problemas de regress√£o.</p>
<p>Algoritmos como √°rvores de decis√£o, random forest e ‚Äúgradient boosting‚Äù est√£o sendo muito usados em todos os tipos de problemas de data science e √© not√°vel o uso desses algor√≠timos para resolver os desafios do <a href="https://www.kaggle.com/">Kaggle</a>. Para resolver este problema utilizaremos estes tr√™s algoritmos e ao final, pegando carona na sele√ß√£o de vari√°veis para os algoritmos de √°rvore, ser√° ajustado um modelo de regress√£o linear para compararmos e conferirmos a signific√¢ncia estat√≠stica de cada uma das vari√°veis.</p>
<div id="varimp-com-random-forest" class="section level2">
<h2>VarImp com Random Forest</h2>
<p>Um dos benef√≠cios da floresta aleat√≥ria √© o poder de lidar com grande conjunto de dados com maior dimensionalidade e identificar as vari√°veis a import√¢ncia das vari√°veis, que pode ser uma caracter√≠stica muito √∫til por√©m deve ser feita com cautela.</p>
<p>Veja uma reflex√£o (traduzida) da <a href="https://www.stat.berkeley.edu/~breiman/RandomForests/reg_philosophy.htm">nota de Leo Breiman (Universidade da Calif√≥rnia em Berkeley)</a></p>
<blockquote>
<p>‚ÄúUma nota filos√≥fica: RF √© um exemplo de uma ferramenta que √© √∫til para fazer an√°lises de dados cient√≠ficos; Mas os algoritmos mais inteligentes n√£o substituem a intelig√™ncia humana e o conhecimento dos dados do problema; Pegue a sa√≠da de florestas aleat√≥rias n√£o como verdade absoluta, mas como suposi√ß√µes geradas por um computador inteligente que podem ser √∫teis para levar a uma compreens√£o mais profunda do problema.‚Äù</p>
</blockquote>
<p>O ajuste da √°rvore ser√° feito com o pacote <code>caret</code> e o estudo de estimativas de erro foi definido como o <a href="https://en.wikipedia.org/wiki/Out-of-bag_error">Out of bag</a> que remove a necessidade de um conjunto de teste pois √© o erro m√©dio de previs√£o em cada amostra de treinamento <span class="math inline">\(x_i\)</span> , usando apenas as √°rvores que n√£o tinham <span class="math inline">\(x_i\)</span> em sua amostra de <a href="https://www.ime.usp.br/~chang/home/mae5704/aula-bootstrap.pdf">bootstrap</a>.</p>
<pre class="r"><code>set.seed(1)
control &lt;- trainControl(method = &quot;oob&quot;,verboseIter = F)

rfFit1 &lt;- train(SalePrice ~. ,
      data=train,
      method=&quot;rf&quot;,
      metric = &quot;Rsquared&quot;,
      trControl = control,
      preProcess = c(&quot;knnImpute&quot;)
      )

randomForest::varImpPlot(rfFit1$finalModel)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<pre class="r"><code>rfFit1$finalModel$importance %&gt;% 
  as.data.frame %&gt;%
  mutate(row = rownames(.)) %&gt;% 
  arrange(desc(IncNodePurity)) %&gt;% 
  as_tibble()</code></pre>
<pre><code>## # A tibble: 217 x 2
##    IncNodePurity row        
##            &lt;dbl&gt; &lt;chr&gt;      
##  1         77.9  OverallQual
##  2         35.0  GrLivArea  
##  3         14.8  YearBuilt  
##  4         11.5  KitchenQual
##  5          9.75 TotalBsmtSF
##  6          9.29 GarageCars 
##  7          6.74 `1stFlrSF` 
##  8          6.33 GarageArea 
##  9          5.02 ExterQualTA
## 10          4.04 BsmtFinSF1 
## # ‚Ä¶ with 207 more rows</code></pre>
<p>Ap√≥s inspecionar a import√¢ncia das vari√°veis vamos selecionar as seguintes vari√°veis:</p>
<pre class="r"><code>full %&lt;&gt;% 
  select(
    SalePrice  , Neighborhood, OverallQual , GrLivArea   , YearBuilt   ,  KitchenQual, 
    GarageCars ,  GarageArea , `1stFlrSF`  , ExterQual   , BsmtFinSF1  , FireplaceQu, 
    BsmtQual   , `2ndFlrSF`  , CentralAir  , GarageFinish, YearRemodAdd, FullBath, 
    GarageYrBlt, Fireplaces  , LotFrontage , BsmtUnfSF   , TotalBsmtSF , BsmtFinType1,
    OpenPorchSF, GarageType  , BsmtExposure, OverallCond , TotalBsmtSF , LotArea
  )</code></pre>
<p>Portanto, vamos definir novamente o conjunto de dados de treino e de teste:</p>
<pre class="r"><code>train &lt;- full[1:nrow(train),] %&gt;% as.data.frame()
test  &lt;- full[(nrow(train)+1):nrow(full),-1] %&gt;% as.data.frame()</code></pre>
</div>
<div id="vari√°veis-num√©ricas" class="section level2">
<h2>Vari√°veis num√©ricas</h2>
<p>Ap√≥s a sele√ß√£o dessas vari√°veis, vamos entender como elas est√£o correlacionadas dois a dois com o <a href="https://pt.wikipedia.org/wiki/Coeficiente_de_correla%C3%A7%C3%A3o_de_Pearson">coeficiente de correla√ß√£o de pearson</a>, exibindo a matrix em um <a href="https://en.wikipedia.org/wiki/Heat_map">Heatmap</a> (ou mapa de calor ), que √© uma representa√ß√£o gr√°fica de dados em que os valores individuais contidos em uma matriz representados como cores.</p>
<pre class="r"><code>cormat &lt;- 
  full %&gt;% 
  select(SalePrice, everything()) %&gt;% 
  select_if(is.numeric) %&gt;% 
  as.data.frame() %&gt;% 
  cor(use = &quot;na.or.complete&quot;) %&gt;% 
  melt

cormat %&gt;%   
  ggplot( aes(reorder(Var1,value), reorder(Var2,value), fill=value))+
  geom_tile(color=&quot;white&quot;)+
  scale_fill_gradient2(low=&quot;blue&quot;, high=&quot;red&quot;, mid=&quot;white&quot;, midpoint=0, limit=c(-1,1), space=&quot;Lab&quot;, name=&quot;Pearson\nCorrelation&quot;)+
  theme_bw()+
  theme(axis.text.x=element_text(angle=45, vjust=1, size=10, hjust=1))+
  coord_fixed()+
  labs(x=&quot;&quot;,y=&quot;&quot;)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-37-1.png" width="1152" /></p>
<p>√â poss√≠vel notar que existem vari√°veis explicativas correlacionadas o que indica que a presen√ßa de algumas vari√°veis pode possivelmente interferir no ajuste final do modelo linear multivariado.</p>
</div>
<div id="vari√°veis-categ√≥ricas" class="section level2">
<h2>Vari√°veis categ√≥ricas</h2>
<p>J√° a rela√ß√£o das var√°veis categ√≥ricas n√£o podem ser calculada com o coeficiente de correla√ß√£o calculado anteriormente, para avaliar como elas est√£o associadas ser√° calculado a medida de associa√ß√£o <a href="https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V">V de Cram√©r</a>. Novamente a matrix dos resultados ser√£o novamente apresentados em um <a href="https://en.wikipedia.org/wiki/Heat_map">Heatmap</a> (ou mapa de calor ) que foi inspirado <a href="http://analysingstuffs.xyz/2017/12/01/visualizing-the-correlations-between-categorical-variables-with-r-a-cramers-v-heatmap/">neste post</a> (neste post tamb√©m √© apresentada uma fun√ß√£o para o c√°lculo da matrix, adaptei de forma que se tornasse mais geral e disponibilizei no meu github <a href="https://github.com/gomesfellipe/functions/blob/master/interaction_all.R">neste link</a>).</p>
<pre class="r"><code># Carrega funcao que calcula o V de Cramer:
devtools::source_url(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/cv_test.R&quot;)
# Carrega a funcao que realiza as intera√ß√µes dos calculos dois a dois:
devtools::source_url(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/interaction_all.R&quot;)</code></pre>
<p>Veja:</p>
<pre class="r"><code>cvmat &lt;- 
train %&gt;%
  select_if(~!is.numeric(.x)) %&gt;% 
  as.data.table() %&gt;%
  interaction_all(cv_test) %&gt;% 
  as_tibble() 

cvmat %&gt;% 
  ggplot( aes(variable_x, variable_y, fill=v_cramer))+
  geom_tile(color=&quot;white&quot;)+
  scale_fill_gradient2(low=&quot;blue&quot;, high=&quot;red&quot;, mid=&quot;white&quot;, midpoint=0, limit=c(-1,1), space=&quot;Lab&quot;, name=&quot;Cramer&#39;s V&quot;)+
  theme_bw()+
  theme(axis.text.x=element_text(angle=45, vjust=1, size=10, hjust=1))+
  coord_fixed()+
  labs(x=&quot;&quot;,y=&quot;&quot;)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
</div>
</div>
<div id="ajustando-modelos" class="section level1">
<h1>Ajustando modelos</h1>
<div id="arvore-de-decisao" class="section level2">
<h2>Arvore de decisao</h2>
<p>O modelo de √°rvore de decis√£o j√° foi comentado e deixei algumas refer√™ncias ao final do post portanto vejamos a seguir o ajusto no R. Segundo a <a href="https://cran.r-project.org/web/packages/rpart/rpart.pdf">documenta√ß√£o</a>:</p>
<p><code>cp</code>: par√¢metro de complexidade. No nosso caso isso significa que o <a href="https://pt.wikipedia.org/wiki/R%C2%B2"><span class="math inline">\(R^2\)</span></a> total deve aumentar em cp em cada etapa. O principal papel desse par√¢metro √© economizar tempo de computa√ß√£o removendo as divis√µes que obviamente n√£o valem a pena. Essencialmente, informamos ao programa que qualquer divis√£o que n√£o melhore o ajuste por <code>cp</code> provavelmente ser√° eliminada por <a href="https://pt.wikipedia.org/wiki/Valida%C3%A7%C3%A3o_cruzada">valida√ß√£o cruzada</a>, e que, portanto, o programa n√£o precisa busc√°-la.</p>
<p>Para pesquisa de grade existem duas maneiras de ajustar um algoritmo no pacote <code>caret</code>: permitir que o sistema fa√ßa isso automaticamente ou especificar o <code>tuneGride</code> manualmente onde cada par√¢metro do algoritmo pode ser especificado como um vetor de valores poss√≠veis. Confira o ajuste manual em R:</p>
<pre class="r"><code>set.seed(1)

control &lt;- trainControl(method = &quot;cv&quot;, number = 5,verboseIter = F)

tunegrid &lt;- expand.grid(cp=seq(0.001, 0.01, 0.001))

rpartFit2 &lt;- 
  train(y=train$SalePrice, x=train[,-1],
        method=&quot;rpart&quot;,
        trControl=control,
        tuneGrid=tunegrid,
        metric = &quot;Rsquared&quot;
  )
rpartFit2</code></pre>
<pre><code>## CART 
## 
## 1460 samples
##   28 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 1169, 1169, 1168, 1168, 1166 
## Resampling results across tuning parameters:
## 
##   cp     RMSE       Rsquared   MAE      
##   0.001  0.1918932  0.7757730  0.1386651
##   0.002  0.1943654  0.7690391  0.1410967
##   0.003  0.2016485  0.7513005  0.1457213
##   0.004  0.2029596  0.7462748  0.1457752
##   0.005  0.2098812  0.7279462  0.1534384
##   0.006  0.2090073  0.7291130  0.1539830
##   0.007  0.2110066  0.7227211  0.1544402
##   0.008  0.2120734  0.7198280  0.1555415
##   0.009  0.2142488  0.7143975  0.1570535
##   0.010  0.2148236  0.7126454  0.1575360
## 
## Rsquared was used to select the optimal model using the largest value.
## The final value used for the model was cp = 0.001.</code></pre>
<p>Podemos conferir os resultados novamente de maneira visual:</p>
<pre class="r"><code>rpart.plot(rpartFit2$finalModel, cex = 0.5)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-41-1.png" width="1200" /></p>
<p>Gerando arquivo para submiss√£o no kaggle:</p>
<pre class="r"><code>id %&gt;% cbind(predict(rpartFit2, test) %&gt;% exp) %&gt;% 
  `colnames&lt;-`(c(&quot;Id&quot;, &quot;SalePrice&quot;)) %&gt;%
  write.csv(&quot;rpartFit2.csv&quot;,row.names = F)</code></pre>
</div>
<div id="bagging" class="section level2">
<h2>Bagging</h2>
<p><a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating">‚ÄúBagging‚Äù</a> √© usado quando desejamos reduzir a varia√ß√£o de uma √°rvore de decis√£o. Ela combina o resultado de v√°rios modelos onde todas as vari√°veis s√£o considerados para divis√£o um n√≥. Em R:</p>
<pre class="r"><code>set.seed(1)

control &lt;- trainControl(method = &quot;cv&quot;, number = 5,verboseIter = F)

treebagFit &lt;- train(y=train$SalePrice, 
                    x=train[,-1], 
                    method = &quot;treebag&quot;,
                    metric = &quot;Rsquared&quot;,
                    trControl=control
)
treebagFit</code></pre>
<pre><code>## Bagged CART 
## 
## 1460 samples
##   28 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 1169, 1169, 1168, 1168, 1166 
## Resampling results:
## 
##   RMSE       Rsquared   MAE      
##   0.1831872  0.7946059  0.1288626</code></pre>
<p>Note que o <span class="math inline">\(R^2\)</span> aumentou e o <a href="https://en.wikipedia.org/wiki/Root-mean-square_deviation"><span class="math inline">\(RMSE\)</span></a> diminuiu ap√≥s o uso desta t√©cnica.</p>
<p>Resultados para enviar para o Kaggle:</p>
<pre class="r"><code>id %&gt;% cbind(predict(treebagFit, test)%&gt;% exp) %&gt;% 
  `colnames&lt;-`(c(&quot;Id&quot;, &quot;SalePrice&quot;)) %&gt;%
  write.csv(&quot;treebagFit.csv&quot;,row.names = F)</code></pre>
</div>
<div id="random-forest" class="section level2">
<h2>Random Forest</h2>
<p>A principal diferen√ßa entre ‚Äúbagging‚Äù e o algoritmo Random Forest √© que em <code>randomForest</code>, apenas um subconjunto de caracter√≠sticas √© selecionado aleatoriamente em cada divis√£o em uma √°rvore de decis√£o enquanto que no bagging todos os recursos s√£o usados.</p>
<p>Para pesquisa de grade especificaremos um vetor com os poss√≠veis valores, <a href="https://cran.r-project.org/web/packages/randomForest/randomForest.pdf">pois o default adotado para o par√¢metro</a> <code>mtry</code> √© <code>mtry</code> = p/3 (N√∫mero de vari√°veis amostradas aleatoriamente como candidatos em cada divis√£o), onde p √© o n√∫mero de vari√°veis e pode ser que o modelo se ajuste melhor aos dados ao utilizar outro valor.</p>
<p>Veja:</p>
<pre class="r"><code>set.seed(1)

tunegrid &lt;- expand.grid(mtry = seq(4, ncol(train) * 0.8, 2))

control &lt;- trainControl(method = &quot;cv&quot;, number = 5,verboseIter = F)

rfFit &lt;- train(SalePrice ~. ,
               data=train,
               method=&quot;rf&quot;,
               metric = &quot;Rsquared&quot;,
               tuneGrid=tunegrid,
               trControl=control
)
rfFit</code></pre>
<pre><code>## Random Forest 
## 
## 1460 samples
##   28 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 1169, 1169, 1168, 1168, 1166 
## Resampling results across tuning parameters:
## 
##   mtry  RMSE       Rsquared   MAE       
##    4    0.1455656  0.8781772  0.09755474
##    6    0.1417368  0.8817193  0.09435674
##    8    0.1405084  0.8826370  0.09350712
##   10    0.1395367  0.8834153  0.09290816
##   12    0.1385338  0.8845102  0.09181049
##   14    0.1386865  0.8840165  0.09223527
##   16    0.1381776  0.8846283  0.09155563
##   18    0.1384532  0.8837305  0.09222536
##   20    0.1380863  0.8840803  0.09173754
##   22    0.1383788  0.8835938  0.09189772
## 
## Rsquared was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 16.</code></pre>
<p>Note que o <span class="math inline">\(R^2\)</span> aumentou e o <span class="math inline">\(RMSE\)</span> apresentou resultados ainda mais satisfat√≥rios.</p>
<p>Veja visualmente a import√¢ncia de ada vari√°vel:</p>
<pre class="r"><code>randomForest::varImpPlot(rfFit$finalModel)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
<p>Resultados para enviar para o Kaggle:</p>
<pre class="r"><code>id %&gt;% cbind(predict(rfFit, test) %&gt;% exp) %&gt;% 
  `colnames&lt;-`(c(&quot;Id&quot;, &quot;SalePrice&quot;)) %&gt;%
  write.csv(&quot;rfFit.csv&quot;,row.names = F) </code></pre>
</div>
<div id="gbm" class="section level2">
<h2>GBM</h2>
<p>Diferentemente do ‚Äúbagging‚Äù, o ‚Äúboosting‚Äù √© uma t√©cnica de ensemble (conjunto) na qual os preditores n√£o s√£o feitos independentemente, mas sequencialmente. Na imagem a seguir √© poss√≠vel ver uma representa√ß√£o visual dessa diferen√ßa:</p>
<p><img src="https://cdn-images-1.medium.com/max/1600/1*PaXJ8HCYE9r2MgiZ32TQ2A.png" /></p>
<p>A imagem foi obtida <a href="https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d">neste artigo: Gradient Boosting from scratch</a>, recomendo a leitura pois da uma boa intui√ß√£o de como o algoritmo funciona.</p>
<p>Para a pesquisa de grade vamos permitir que o sistema fa√ßa isso automaticamente configurando apenas o <code>tuneLength</code> para indicar o n√∫mero de valores diferentes para cada par√¢metro do algoritmo.</p>
<pre class="r"><code>set.seed(1)

control &lt;- trainControl(method = &quot;cv&quot;, number = 5,verboseIter = F)

gbmFit &lt;- train(SalePrice~.,data=train,
                method = &quot;gbm&quot;,
                trControl=control,
                tuneLength=5,
                metric = &quot;Rsquared&quot;,
                verbose = FALSE
)
gbmFit</code></pre>
<pre><code>## Stochastic Gradient Boosting 
## 
## 1460 samples
##   28 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 1169, 1169, 1168, 1168, 1166 
## Resampling results across tuning parameters:
## 
##   interaction.depth  n.trees  RMSE       Rsquared   MAE       
##   1                   50      0.1736970  0.8346902  0.12145158
##   1                  100      0.1474386  0.8663694  0.10371271
##   1                  150      0.1400060  0.8775141  0.09804851
##   1                  200      0.1381902  0.8803999  0.09607709
##   1                  250      0.1375854  0.8817130  0.09502881
##   2                   50      0.1511051  0.8640075  0.10557294
##   2                  100      0.1379357  0.8815852  0.09546142
##   2                  150      0.1360260  0.8846503  0.09326628
##   2                  200      0.1355702  0.8852090  0.09248558
##   2                  250      0.1362827  0.8841734  0.09254710
##   3                   50      0.1434808  0.8743589  0.09910961
##   3                  100      0.1363881  0.8838715  0.09355652
##   3                  150      0.1346606  0.8868808  0.09163759
##   3                  200      0.1339427  0.8880370  0.09062153
##   3                  250      0.1336666  0.8886732  0.08979366
##   4                   50      0.1376575  0.8824442  0.09516571
##   4                  100      0.1334392  0.8884173  0.09192150
##   4                  150      0.1330866  0.8890336  0.09156893
##   4                  200      0.1334706  0.8886198  0.09096598
##   4                  250      0.1335809  0.8884950  0.09101981
##   5                   50      0.1384852  0.8813449  0.09535954
##   5                  100      0.1350803  0.8863344  0.09231165
##   5                  150      0.1340246  0.8878172  0.09112111
##   5                  200      0.1342892  0.8874590  0.09088714
##   5                  250      0.1349331  0.8867525  0.09104875
## 
## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1
## 
## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10
## Rsquared was used to select the optimal model using the largest value.
## The final values used for the model were n.trees = 150, interaction.depth =
##  4, shrinkage = 0.1 and n.minobsinnode = 10.</code></pre>
<p>Note que este foi o modelo que apresentou os melhores resultados quanto s√≥ <span class="math inline">\(R^2\)</span> e ao <span class="math inline">\(RMSE\)</span> em compara√ß√£o com os outros modelos.</p>
<p>Submiss√£o para Kaggle:</p>
<pre class="r"><code>id %&gt;% cbind(predict(gbmFit, test) %&gt;% exp) %&gt;%
  `colnames&lt;-`(c(&quot;Id&quot;, &quot;SalePrice&quot;)) %&gt;%
  write.csv(&quot;gbmFit.csv&quot;, row.names = F)</code></pre>
</div>
<div id="regress√£o-linear" class="section level2">
<h2>Regress√£o Linear</h2>
<p>Por fim faremos o ajuste de um modelo de regress√£o linear multivariado utilizando o pacote caret.</p>
<p>Utilizaremos valida√ß√£o cruzada separando nossa amostra em 5 e utilizaremos o m√©todo <code>lmStepAIC</code> que realiza a sele√ß√£o do modelo escalonado pelo crit√©rio de informa√ß√£o de Akaike - <a href="https://en.wikipedia.org/wiki/Akaike_information_criterion">AIC</a>.</p>
<pre class="r"><code>set.seed(1)

control &lt;- trainControl(method = &quot;cv&quot;, number = 5,verboseIter = F)

lmFit &lt;- train(SalePrice~.,data=train,
               method = &quot;lmStepAIC&quot;,
               trControl=control,
               metric = &quot;Rsquared&quot;,trace=F
)
lmFit</code></pre>
<pre><code>## Linear Regression with Stepwise Selection 
## 
## 1460 samples
##   28 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 1169, 1169, 1168, 1168, 1166 
## Resampling results:
## 
##   RMSE      Rsquared   MAE       
##   0.147716  0.8632513  0.09574552</code></pre>
<p>Note que o ajuste do modelo se apresenta de maneira satisfat√≥ria com <span class="math inline">\(R^2\)</span> e <span class="math inline">\(RMSE\)</span> semelhantes aos modelos de <code>bagging</code> e <code>boosting</code> e al√©m disso, diferente dos modelos baseados em √°rvore, com este ajuste √© poss√≠vel notar a signific√¢ncia estat√≠stica de cada par√¢metro ajustado, o que possibilita tanto o uso tanto como modelo preditivo quanto como modelo descritivo. Veja:</p>
<pre class="r"><code>ggcoef(
  lmFit$finalModel,                      #O modelo a ser conferido
  vline_color = &quot;red&quot;,          #Reta em zero  
  errorbar_color = &quot;blue&quot;,      #Cor da barra de erros
  errorbar_height = .25,
  shape = 18,                   #Altera o formato dos pontos centrais
  size=2,                      #Altera o tamanho do ponto
  color=&quot;black&quot;,
  exclude_intercept = TRUE,                #Altera a cor do ponto
  mapping = aes(x = estimate, y = term, size = p.value))+
  scale_size_continuous(trans = &quot;reverse&quot;)+ #Essa linha faz com que inverta o tamanho
  theme_bw()</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-51-1.png" width="672" /></p>
<p>Note que o intercepto <span class="math inline">\(\beta_0\)</span> foi retirado da imagem pois √© muito superior aos demais coeficientes. Note tamb√©m que <span class="math inline">\(\beta_i\)</span> informa qu√£o sens√≠vel √© <span class="math inline">\(y\)</span>, no caso <code>log(SalePrice)</code> √†s varia√ß√µes de cara umas das <span class="math inline">\(x_{i,j}\)</span> vari√°veis explicativas. Mais concretamente, se <span class="math inline">\(x_{i,j}\)</span> aumenta em uma unidade, o valor de <span class="math inline">\(y\)</span> varia em <span class="math inline">\(\beta_1\)</span> unidades.</p>
<p>Uma r√°pida <a href="http://www.portalaction.com.br/analise-de-regressao/analise-dos-residuos">An√°lise dos Res√≠duos</a>:</p>
<pre class="r"><code>lmFit$finalModel %&gt;% 
  autoplot(which = 1:2) + 
  theme_bw()</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-52-1.png" width="1500" /></p>
<p>√â poss√≠vel notar que parece haver alguns outliers em ambas as figuras. Na primeira √© poss√≠vel notar uma nuvem de pontos aleat√≥rios em torno de zero por√©m na segunda figura nota-se que alguns valores n√£o est√£o de acordo com os quantils te√≥ricos de uma distribui√ß√£o normal, o que pode prejudicar nossa interpreta√ß√£o dos coeficientes do modelo. Vamos encerrar o modelo por aqui mesmo e ver como ele se sai na competi√ß√£o do Kaggle, preparando a submiss√£o:</p>
<pre class="r"><code>id %&gt;% cbind(predict(lmFit, test) %&gt;% exp ) %&gt;% 
  `colnames&lt;-`(c(&quot;Id&quot;, &quot;SalePrice&quot;)) %&gt;%
  write.csv(&quot;lmFit.csv&quot;,row.names = F)</code></pre>
<p>O score obtido com esta submiss√£o no Kaggle foi muito pr√≥ximo dos modelos baseados e √°rvore e o tempo computacional para este ajuste foi bem menor.</p>
</div>
<div id="comparando-ajustes" class="section level2">
<h2>Comparando ajustes</h2>
<p>Vejamos a seguir uma compara√ß√£o entre estes modelos com as fun√ß√µes fornecidas pelo pacote `caret:.</p>
<pre class="r"><code>resamps &lt;- resamples(list(rpart = rpartFit2,
                          treebag = treebagFit,
                          rf = rfFit,
                          gbm = gbmFit,
                          lm = lmFit 
                          )) 
bwplot(resamps)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-54-1.png" width="672" /></p>
<p>Com este gr√°fico √© poss√≠vel notar que o modelo de regress√£o linear m√∫ltipla apresentou resultados semelhantes aos de bagging e boosting.</p>
<p>√â importante frisar que a maneira como as vari√°veis foram selecionadas para o modelo de regress√£o linear m√∫ltipla atrav√©s da import√¢ncia das vari√°veis obtida com o modelo randomForest n√£o √© um padr√£o e existem diversos outros modos estat√≠sticos de se de determinar a signific√¢ncia e a rela√ß√£o das vari√°veis para o modelo.</p>
<p>Um poss√≠vel problema neste m√©todo √© que n√£o detecta a multicolinearidade, que ocorre quando as vari√°veis explicativas est√£o fortemente correlacionadas entre si e a an√°lise de regress√£o linear pode ficar confusa e desprovida de significado, pois h√° dificuldade em distinguir o efeito de uma ou outra vari√°vel explicativa sobre a vari√°vel resposta <span class="math inline">\(Y\)</span> devido √† vari√¢ncias muito elevadas ou sinais inconsistentes.</p>
<p>Essa proposta de aprender se divertindo e de maneira produtiva me deixa muito empolgado, espero que tenham se divertido como eu me diverti fazendo este post!</p>
</div>
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias:</h1>
<ul>
<li><a href="https://www.datacamp.com/courses/machine-learning-with-tree-based-models-in-r">DataCamp Course:Machine Learning with Tree-Based Models in R</a></li>
<li><a href="https://tinyurl.com/y796aa4t">Data Science <em>for</em> Business</a></li>
<li><a href="https://lethalbrains.com/learn-ml-algorithms-by-coding-decision-trees-439ac503c9a4">Learn ML Algorithms by coding: Decision Trees</a></li>
<li><a href="https://www.datacamp.com/community/tutorials/decision-trees-R">DataCamp Tutorials: Decision Trees in R</a></li>
<li><a href="https://topepo.github.io/caret/">The caret Package - Max Kuhn</a></li>
<li><a href="https://www.vooo.pro/insights/um-tutorial-completo-sobre-a-modelagem-baseada-em-tree-arvore-do-zero-em-r-python/">Um tutorial completo sobre modelagem baseada em √°rvores de decis√£o (c√≥digos R e Python)</a></li>
<li><a href="https://machinelearningmastery.com/tuning-machine-learning-models-using-the-caret-r-package/">Tuning Machine Learning Models Using the Caret R Package</a></li>
<li><a href="https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d">Gradient Boosting from scratch</a></li>
<li><a href="https://machinelearningmastery.com/tune-machine-learning-algorithms-in-r/">Tune Machine Learning Algorithms in R (random forest case study)</a></li>
<li><a href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_manual.htm">Random Forests - Leo Breiman and Adele Cutler</a></li>
<li><a href="https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf">An Introduction to Recursive Partitioning Using the RPART Routines - CRAN</a></li>
</ul>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2018-08-31-modelos-em-arvore/modelos-em-arvore/">Um estudo sobre modelos de aprendizagem baseados em √°rvores com desafio do Kaggle</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Analise Explorat√≥ria</category>
      <category>Aprendizado Supervisionado</category>
      <category>Data mining</category>
      <category>Estatistica</category>
      <category>Machine Learning</category>
      <category>Pr√°tica</category>
      <category>Probabilidade</category>
      <category>R</category>
      <category>modelo baseado em arvores</category>
      <category>kaggle</category>
      <category>Regress√£o</category>
      <category domain="tag">Data Mining</category>
      <category domain="tag">Estatistica</category>
      <category domain="tag">gomesfellipe</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">Correlacoes</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">modelagem estatistica</category>
      <category domain="tag">Pr√°tica</category>
      <category domain="tag">R</category>
      <category domain="tag">regression</category>
      <category domain="tag">caret</category>
      <category domain="tag">xgboost</category>
      <category domain="tag">random forest</category>
      <category domain="tag">decisiontree</category>
    </item>
    <item>
      <title>modelo bayesiano do zero</title>
      <link>https://gomesfellipe.github.io/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero/</link>
      <pubDate>Sat, 28 Jul 2018 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero/</guid>
      <description>Um pouco sobre as duas grandes escolas de infer√™ncia, contas e implementa√ß√£o de um modelo linear bayesiano na m√£o para dados simulados e para dados reais</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/plotly-binding/plotly.js"></script>
<script src="/rmarkdown-libs/typedarray/typedarray.min.js"></script>
<script src="/rmarkdown-libs/jquery/jquery.min.js"></script>
<link href="/rmarkdown-libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="/rmarkdown-libs/crosstalk/js/crosstalk.min.js"></script>
<link href="/rmarkdown-libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="/rmarkdown-libs/plotly-main/plotly-latest.min.js"></script>


<div id="modelagem-estat√≠stica-e-as-duas-grandes-escolas-de-infer√™ncia" class="section level1">
<h1>Modelagem estat√≠stica e as duas grandes escolas de infer√™ncia</h1>
<p>Atrav√©s da modelagem estat√≠stica √© poss√≠vel tomar decis√µes sobre diversos assuntos de interesse como por exemplo na an√°lise de risco de cr√©dito, previs√µes de quantidade de chuva em um dado local, estimativas de erros ou falhas de um novo produto ou servi√ßo al√©m de diversas √°reas como na Educa√ß√£o, Economia, nas Ci√™ncias Sociais, Sa√∫de etc.</p>
<p>Muitas vezes os par√¢metros das distribui√ß√µes em estudo podem ser desconhecidos e existe o desejo de se inferir sobre eles. Existem duas grandes escolas de infer√™ncia: a cl√°ssica e a bayesiana. A cl√°ssica trata esses par√¢metros como quantidades fixas e n√£o atribui distribui√ß√£o a eles, a estima√ß√£o desses par√¢metros √© dada atrav√©s da fun√ß√£o de verossimilhan√ßa, enquanto que na escola bayesiana atribui-se uma distribui√ß√£o, chamada de distribui√ß√£o a priori, ao conjunto de par√¢metros desconhecidos quantificando a sua cren√ßa sobre esse conjunto e a estima√ß√£o dos par√¢metros √© dada atrav√©s da distribui√ß√£o √† posteriori, que √© proporcional ao produto da fun√ß√£o de verossimilhan√ßa com a distribui√ß√£o a priori.</p>
<p>O interesse pela modelagem estat√≠stica atrav√©s da abordagem bayesiana surgiu a partir de um projeto de inicia√ß√£o cient√≠fica quando cursava o 6¬∫ per√≠odo do curso de Gradua√ß√£o em Estat√≠stica que tinha como objetivo o c√°lculo e apresenta√ß√£o de estat√≠sticas descritivas para ajudar uma pesquisadora. Ap√≥s obter os resultados da an√°lise explorat√≥ria e descritiva, notei, junto com meu orientador, que havia possibilidade de dar continuidade ao estudo a partir de uma abordagem estat√≠stica mais elaborada. Sendo assim, outro projeto de inicia√ß√£o cient√≠fica foi iniciado em seguida com a finalidade de me preparar para utilizar um modelo linear hier√°rquico bayesiano sob os dados disponibilizados pela pesquisadora em minha monografia.</p>
<p>Caso tenha interesse em conferir o projeto com o estudo sobre modelos hier√°rquicos bayesianos, disponibilizei os resultados e os c√≥digos em meu github <a href="https://github.com/gomesfellipe/projeto_modelos_hierarquicos_bayesianos">neste reposit√≥rio</a>. Neste post farei uma breve introdu√ß√£o sobre o ajuste de um modelo linear bayesiano simples e os resultados obtidos (utilizando uma distribui√ß√£o a priori n√£o informativa). Os resultados obtidos ser√£o comparados com os resultados obtidos com o ajuste de um modelo de regress√£o linear atrav√©s da abordagem cl√°ssica.</p>
<div id="distribui√ß√£o-a-priori" class="section level2">
<h2>Distribui√ß√£o a priori</h2>
<p>Para o estudo, optou-se pela utiliza√ß√£o de valores elevados para vari√¢ncia a priori (tamb√©m consideradas como ‚Äún√£o informativas‚Äù, fazendo uma analogia √† modelos cl√°ssicos) obtendo ajustes que atribuem maior import√¢ncia √† informa√ß√£o provinda da amostra.</p>
<p>Portanto com valores elevados para vari√¢ncia da distribui√ß√£o a priori (consideradas como ‚Äún√£o informativas‚Äù) foram obtida a distribui√ß√£o a posteriori de um par√¢metro <span class="math inline">\(\theta\)</span> que cont√©m toda a informa√ß√£o probabil√≠stica a respeito deste par√¢metro e quando a forma anal√≠tica dessa distribui√ß√£o √© conhecida o gr√°fico da <a href="https://pt.wikipedia.org/wiki/Fun%C3%A7%C3%A3o_densidade">fdp</a> pode ilustrar o comportamento probabil√≠stico do par√¢metro de interesse e auxiliar em alguma tomada de decis√£o, por√©m, quando a forma anal√≠tica n√£o √© conhecida ou √© muito custosa de ser obtida, pode-se recorrer a m√©todos de simula√ß√£o tais como os m√©todos MCMC.</p>
</div>
<div id="amostrador-de-gibbs---m√©todo-mcmc" class="section level2">
<h2>Amostrador de Gibbs - m√©todo MCMC</h2>
<p>Com os avan√ßos dos m√©todos de MCMC, surgiu o amostrador de Gibbs, proposto por <span class="citation">@GemanGeman</span> e tornou-se popular por <span class="citation">@GelfandSmith</span>, falo um pouco mais sobre o algoritmo no <a href="https://github.com/gomesfellipe/projeto_modelos_hierarquicos_bayesianos/blob/master/texto.pdf">texto do projeto</a>.</p>
<p>Como a converg√™ncia ocorre ap√≥s o aquecimento (ou burn-in), √© comum usar os valores de <span class="math inline">\(\theta^{(a)}\)</span>, <span class="math inline">\(\theta^{(a+t)}\)</span>, <span class="math inline">\(\theta^{(a+2t)}\)</span>,‚Ä¶ para compor a amostra de <span class="math inline">\(\theta\)</span>, sendo <span class="math inline">\(a-1\)</span> o n√∫mero de itera√ß√µes iniciais do aquecimento e <span class="math inline">\(t\)</span> o espa√ßamento utilizado para diminuir a autocorrela√ß√£o dos par√¢metros. Maiores detalhes podem ser vistos em <span class="citation">@Gamerman06</span>.</p>
</div>
</div>
<div id="ao-que-interessa" class="section level1">
<h1>Ao que interessa</h1>
<p>O objetivo deste post √© apresentar e comparar os resultados do ajuste de um modelo linear bayesiano simples utilizando uma distribui√ß√£o a priori n√£o informativa com o modelo de regress√£o linear simples para dados simulados e para dados reais.</p>
<p>Diversas fun√ß√µes foram criadas ao longo o estudo para conferir o comportamento das cadeias geradas e os resultados do ajuste do modelo, aproveitarei essas fun√ß√µes para este post importando do <a href="https://github.com/gomesfellipe/projeto_modelos_hierarquicos_bayesianos/blob/master/dependencies.R">reposit√≥rio no github</a> da seguinte maneira:</p>
<pre class="r"><code>path_to_dep &lt;- &quot;https://raw.githubusercontent.com/gomesfellipe/projeto_modelos_hierarquicos_bayesianos/master/dependencies.R&quot;
devtools::source_url(path_to_dep, encoding=&quot;UTF-8&quot;)</code></pre>
</div>
<div id="ajuste-do-modelo-para-dados-simulados" class="section level1">
<h1>Ajuste do modelo para dados simulados</h1>
<p>Suponha ent√£o um exemplo em que a popula√ß√£o de interesse tenha distribui√ß√£o normal com m√©dia <span class="math inline">\(\beta_0 + \beta_1 X\)</span>, sendo <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span> desconhecidos e vari√¢ncia <span class="math inline">\(\sigma^2\)</span> desconhecida. Seja <span class="math inline">\(\tau=\frac{1}{\sigma^2}\)</span> o par√¢metro chamado de precis√£o.</p>
<p>O par√¢metro <span class="math inline">\(\beta_0\)</span> √© conhecido como intercepto ou coeficiente linear e o <span class="math inline">\(\beta_1\)</span> como coeficiente angular. Al√©m disso, suponha que as unidades dessa popula√ß√£o sejam iid. Dessa forma, tem-se que as unidades dessa popula√ß√£o tem a seguinte distribui√ß√£o:</p>
<p><span class="math display">\[
Y_i \stackrel{iid}{\sim} N(\beta_0 + \beta_1 X_i,\frac{1}{\tau}), 
\]</span></p>
<p>onde <span class="math inline">\(i=1,...,N\)</span>.</p>
<p>Para o estudo do modelo primeiramente foi utilizado um conjunto de dados simulados utilizando uma amostra de tamanho <span class="math inline">\(N=1000\)</span> e com os seguintes par√¢metros ‚Äúdesconhecidos‚Äù dos quais desejamos estimar: <span class="math inline">\(\beta_0 = 1\)</span>, <span class="math inline">\(\beta_1 = 0,5\)</span>, <span class="math inline">\(\tau = 2\)</span>. A amostra ser√° simulada segundo a vari√°vel aleat√≥ria: <span class="math inline">\(X_i ~ N(0,1)\)</span> e em seguida os par√¢metros deste modelo, denotados por <span class="math inline">\(\theta = (\beta_0, \beta_1, \tau)\)</span> foram estimados usando o paradigma Bayesiano.</p>
<div id="gerando-a-amostra" class="section level2">
<h2>Gerando a amostra</h2>
<p>A amostra que foi simulada foi obtida da seguinte maneira:</p>
<pre class="r"><code># Amostra que sera utilizada:

set.seed(12)
n   &lt;- 1000                 # N=1000
b0  &lt;- 1                    # \beta_0 = 1
b1  &lt;- 0.5                  # \beta_1 = 0,5
tau &lt;- 2                    # \tau = 2 e 
x   &lt;- rnorm(n)             # X_i ~ N(0,1), logo:
y   &lt;- b0 + b1 * x + rnorm(n,0,sqrt(1/tau))</code></pre>
<p>Obtendo-se uma amostra de tamanho <span class="math inline">\(n\)</span>, pode-se inferir sob os par√¢metros desconhecidos <span class="math inline">\(\theta = (\beta_0, \beta_1, \tau)\)</span> atrav√©s da distribui√ß√£o a posteriori e para obter essa distribui√ß√£o faz-se necess√°rio calcular a fun√ß√£o de verossimilhan√ßa, que pode ser obtida da seguinte forma:</p>
<p><span class="math display">\[
p(y| \beta_0, \beta_1 , \tau) =\prod^n_{i=1} p(y_i | \beta_0, \beta_1, \tau )  
\]</span></p>
<p>portanto</p>
<p><span class="math display">\[
p(y| \beta_0, \beta_1 , \tau) = \prod_{i=1}^n \frac{ \sqrt{\tau} }{ \sqrt{2\pi} } exp { - \frac{\tau}{2} ( y_i - \beta_0 - \beta_1 x_i )^2 }
\]</span></p>
<p>onde <span class="math inline">\(y = (y_1, ..., y_n)\)</span> √© a amostra coletada. O valor p para o teste de Shapiro para conferir a suposi√ß√£o de normalidade da vari√°vel resposta foi de 0.6181791 enquanto que o valor p para conferir a normalidade da vari√°vel explicativa foi de 0.7413229.</p>
</div>
<div id="distribui√ß√£o-a-priori-1" class="section level2">
<h2>Distribui√ß√£o a priori</h2>
<p>Durante o estudo diversos valores os par√¢metros a priori foram selecionados para que fosse poss√≠vel avaliar a sensibilidade da qualidade da escolha da distribui√ß√£o priori, aqui ser√° apresentado os resultados obtidos com valores elevados para vari√¢ncia a priori (tamb√©m consideradas como ‚Äún√£o informativas‚Äù, fazendo uma analogia √† modelos cl√°ssicos) que ajusta o modelo atribuindo maior import√¢ncia √† informa√ß√£o provinda da amostra.</p>
<p>Considere a priori que os par√¢metros sejam independentes e que</p>
<p><span class="math display">\[
\beta_0 \sim N(m_0,\sigma_0^2),  \\
\beta_1 \sim N(m_1,\sigma_1^2) \mbox{ e }  \\
\tau    \sim G(a,b).
\]</span></p>
<p>Portanto, para a estima√ß√£o foram utilizados os seguintes hiperpar√¢metros : <span class="math inline">\(m_0 = m_1 = 0\)</span>, <span class="math inline">\(\sigma_0^2 = \sigma_1^2 = 100\)</span>, <span class="math inline">\(a=0,1\)</span> e <span class="math inline">\(b=0,1\)</span></p>
<p>No R:</p>
<pre class="r"><code>#Parametros para b0 ~ N(mu0, sig0)
mu0 &lt;-  0
sig0 &lt;-  1000

#Parametros para b1 ~ N(mu1, sig1)
mu1 &lt;-  0
sig1 &lt;-  1000

#Parametros para tau ~ G(a,b)
a &lt;-  0.1
b &lt;-  0.1</code></pre>
<p>Dessa forma, tem-se que a distribui√ß√£o conjunta a priori possui a seguinte forma:</p>
<p><span class="math display">\[
 p(\beta_0, \beta_1 , \tau) \propto exp\Big\{-\frac{1}{2\sigma_0^2}( \beta_0 - m_0)^2\Big\} exp\Big\{-\frac{1}{2\sigma_1^2}( \beta_1 - m_1)^2\Big\} \tau^{a-1}exp \{-b \tau\}.
\]</span></p>
</div>
<div id="distribui√ß√£o-a-posteriori" class="section level2">
<h2>Distribui√ß√£o a posteriori</h2>
<p>Combinando a fun√ß√£o de verossimilhan√ßa com a distribui√ß√£o a priori, obt√™m-se a distribui√ß√£o a posteriori que √© proporcional a:</p>
<p><span class="math display">\[
p(\beta_0, \beta_1 , \tau|y) \propto \tau^{\frac{n}{2}+a-1} exp \left\{ -\frac{\tau}{2} \sum^n_{i=1} (y_i - \beta_0 - \beta_1 x_i)^2 - b\tau  - \frac{1}{2\sigma_0^2}(\beta_0-m_0)^2  \right\} \times   exp\left\{- \frac{1}{2\sigma_1^2}(\beta_1-m_1)^2  \right\} . 
\]</span></p>
<p>Note que essa distribui√ß√£o √© multivariada e n√£o possui forma anal√≠tica conhecida. Sendo assim, recorre-se aos m√©todos de MCMC para se obter amostras dessa distribui√ß√£o. E ent√£o faz-se necess√°rio obter as DCCP de <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span> e <span class="math inline">\(\tau\)</span>.</p>
</div>
<div id="implementando-o-amostrador-de-gibbs" class="section level2">
<h2>Implementando o amostrador de Gibbs</h2>
<p>O tamanho da cadeia foi de 30000 simula√ß√µes e o <em>burn-in</em> (ou amostra de aquecimento) utilizado considerada ap√≥s o ajuste foi de 15000. no R:</p>
<pre class="r"><code>nsim           &lt;-  3*10000
burnin         &lt;-  nsim / 2 
cadeia.b0      &lt;-  rep(0,nsim)
cadeia.b1      &lt;-  rep(0,nsim)
cadeia.tau     &lt;-  rep(0,nsim)

# Chutes iniciais: 
cadeia.b0[1]    &lt;-  0
cadeia.b1[1]    &lt;-  0
cadeia.tau[1]   &lt;-  1</code></pre>
<div id="calculos-para-implementar-o-algoritimo-na-m√£o" class="section level3">
<h3>Calculos para implementar o algoritimo na m√£o</h3>
<p>Para a implementa√ß√£o do algoritmo, fez-se necess√°rio o c√°lculo das distribui√ß√µes condicionais completas a posteriori (DCCP), primeiramente veja os resultados obtidos para <span class="math inline">\(\tau\)</span>:</p>
<ul>
<li>DCCP de <span class="math inline">\(\tau\)</span>:</li>
</ul>
<p><span class="math display">\[
\tau|y_1, ...,y_n,\beta_0, \beta_1 \sim Gama ( \frac{n}{2}+a,b+\frac{1}{2} \sum^n_{i=1}(y_i-\beta_0-\beta_1 x_i)^2 ) 
\]</span></p>
<p>Em seguida, veja o resultado obtido para <span class="math inline">\(\beta_0\)</span>, o coeficiente linear da reta, isto √©, a altura em que a reta de regress√£o intercepta o eixo dos <span class="math inline">\(Y\)</span>‚Äôs:</p>
<ul>
<li>DCCP de <span class="math inline">\(\beta_0\)</span>:</li>
</ul>
<p><span class="math display">\[
\beta_0 | y_1,...,y_n , \tau,\beta_1 \sim N(\dfrac{(\tau\sum^n_{i=1}y_i - \tau\beta_1\sum^n_{i=1}x_i  +\frac{m_0}{\sigma_0^2})}{ \tau n + \frac{1}{\sigma_0^2}},  (n\tau +   \frac{1}{\sigma_0^2} )^{-1})
\]</span></p>
<p>Por fim, veja o resultado obtido para <span class="math inline">\(\beta_1\)</span>, √© o coeficiente angular da reta, ou seja, √© o a varia√ß√£o esperada na vari√°vel <span class="math inline">\(Y\)</span> quando a vari√°vel explicativa √© acrescida de 1 unidade:</p>
<ul>
<li>DCCP de <span class="math inline">\(\beta_1\)</span>:</li>
</ul>
<p><span class="math display">\[
\beta_1 | y_1,...,y_n , \tau,\beta_0 \sim N(\frac{\tau\sum^n_{i=1}x_i y_i  - \tau\beta_0\sum^n_{i=1}x_i + \frac{m_1}{\sigma_1^2}}{\tau \sum^n_{i=1}x_i^2 + \frac{1}{\sigma_1^2}}, ( \tau \sum^n_{i=1}x_i^2 + \frac{1}{\sigma_1^2} )^{-1})
\]</span></p>
<p>Agora que todas as distribui√ß√µes condicionais completas est√£o calculadas o algor√≠timo j√° pode ser implementado, no R foi feito da seguinte maneira: (note que as linhas que foram comentadas executariam uma barra de carregamento, com ilustrado em seguida)</p>
<pre class="r"><code># pb &lt;- txtProgressBar(min = 0, max = nsim, style = 3) # iniciando barra de processo
for (k in 2:nsim){
  
  #Cadeia tau
  cadeia.tau[k]   &lt;-  rgamma(1, (n/2) + a, b + (sum((y - cadeia.b0[k-1] - (cadeia.b1[k-1]*x))^2)/2))
  
  # Cadeia B0
  c0              &lt;-  (n*cadeia.tau[k]) + (1/sig0)
  m0              &lt;-  (cadeia.tau[k]*sum(y) - (cadeia.tau[k]*cadeia.b1[k-1]*sum(x)) + (mu0/sig0))/c0
  cadeia.b0[k]    &lt;-  rnorm(1, m0, 1/sqrt(c0))
  
  # Cadeia B1
  c1              &lt;-   (sum(x^2)*cadeia.tau[k]) + (1/sig1)
  m1              &lt;-   ((cadeia.tau[k]*sum(x*y)) - (cadeia.tau[k]*cadeia.b0[k]*sum(x)) + (mu1/sig1))/c1
  cadeia.b1[k]    &lt;-   rnorm(1, m1, 1/sqrt(c1))
  
  # setTxtProgressBar(pb, k)
  
}# ;close(pb) #Encerrando barra de processo</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/loading.png" /></p>
</div>
<div id="resultados-da-cadeia" class="section level3">
<h3>Resultados da cadeia</h3>
<p>A seguir definiremos a vari√°vel <code>inds</code> que indica os valores ap√≥s a amostra de aquecimento (ou <em>burn-in</em>), a vari√°vel <code>real</code> que cont√©m os valores reais utilizados para gerar a amostra para conferir se o modelo foi capaz de recuper√°-los, os nomes dos par√¢metros e os resultados das cadeias foram agregados em uma matriz:</p>
<pre class="r"><code># Juntando resultados:
inds    &lt;- seq(burnin, nsim) # Definindo os indices
real    &lt;- c(b0, b1, tau)
name    &lt;- c(expression(beta[0]), expression(beta[1]), expression(tau))
results &lt;- cbind(cadeia.b0, cadeia.b1, cadeia.tau) %&gt;% as.data.frame() %&gt;% .[inds, ] %T&gt;% head</code></pre>
<div id="histograma-e-densidade" class="section level4">
<h4>Histograma e densidade</h4>
<p>A figura abaixo apresenta os histogramas junto com as densidades de tr√™s cadeias obtidas ao se inicializar o amostrador em pontos diferentes de todos os par√¢metros contidos em <span class="math inline">\(\theta\)</span> e uma linha vermelha indicar√° o valor do real par√¢metro utilizado para estimar a cadeia.</p>
<pre class="r"><code>g1 &lt;- hist_den(results[,1],name = name[1], p = real[1])
g2 &lt;- hist_den(results[,2],name = name[2], p = real[2])
g3 &lt;- hist_den(results[,3],name = name[3], p = real[3])
grid.arrange(g1,g2,g3,ncol=1)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<div id="cadeia" class="section level4">
<h4>Cadeia</h4>
<p>A figura abaixo apresenta os tra√ßos das cadeias dos par√¢metros amostrados exibindo o intervalo de credibilidade com a linha pontilhada em azul e o valor verdadeiro do par√¢metro em vermelho. Note que h√° ind√≠cios de converg√™ncia.</p>
<pre class="r"><code># Cadeia
cadeia(results, name, real)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>√© poss√≠vel notar que todos os intervalos de credibilidade cont√™m o par√¢metro populacional real utilizado para gerar a amostra.</p>
</div>
<div id="autocorrela√ß√£o" class="section level4">
<h4>Autocorrela√ß√£o</h4>
<p>A figura abaixo apresenta os gr√°ficos de autocorrela√ß√£o, que indicam se houve a influ√™ncia dos ‚Äúvalores vizinhos‚Äù dos par√¢metros amostrados. Note que parece haver independ√™ncia entre as intera√ß√µes.</p>
<pre class="r"><code># ACF
FAC(results)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>√© poss√≠vel notar que nenhuma das cadeias apresentaram estimativas autocorrelacionada</p>
</div>
<div id="estimativas" class="section level4">
<h4>Estimativas</h4>
<p>Agora que j√° foi verificado que a cadeia se comportou de maneira satisfat√≥ria, veja os resultados obtidos sobre as estimativas dos par√¢metros atrav√©s do algoritmo. apresenta os resumos a posteriori dos par√¢metros amostrados.</p>
<pre class="r"><code>coef &lt;- coeficientes(results, real = real) %&gt;% as.data.frame()

tabela_coeficientes(coef)</code></pre>
<div id="htmlwidget-1" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"visdat":{"11b9832b6bfa0":["function () ","plotlyVisDat"]},"cur_data":"11b9832b6bfa0","attrs":{"11b9832b6bfa0":{"columnorder":[1,2,3,4,5],"columnwidth":[80,80,80,80,80],"header":{"values":["M√©dia","Desv. Pad.","IC inf","IC sup","Real"],"line":{"color":"#506784"},"fill":{"color":"#1F8FFFB4"},"align":["center","center","center","center","center"],"font":{"color":"white","size":15},"height":40},"cells":{"values":[[1.0244,0.4933,1.9001],[0.023,0.0241,0.085],[0.9792,0.4464,1.7371],[1.0697,0.5409,2.0695],[1,0.5,2]],"line":{"color":"#506784"},"fill":{"color":["#1F8FFF58","white","white","white","#1F8FFF58"]},"align":["center","center","center","center","center"],"font":{"color":["white","#506784","#506784","#506784","white"],"size":12},"height":30},"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"table"}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"hovermode":"closest","showlegend":false},"source":"A","config":{"showSendToCloud":false},"data":[{"columnorder":[1,2,3,4,5],"columnwidth":[80,80,80,80,80],"header":{"values":["M√©dia","Desv. Pad.","IC inf","IC sup","Real"],"line":{"color":"#506784"},"fill":{"color":"#1F8FFFB4"},"align":["center","center","center","center","center"],"font":{"color":"white","size":15},"height":40},"cells":{"values":[[1.0244,0.4933,1.9001],[0.023,0.0241,0.085],[0.9792,0.4464,1.7371],[1.0697,0.5409,2.0695],[1,0.5,2]],"line":{"color":"#506784"},"fill":{"color":["#1F8FFF58","white","white","white","#1F8FFF58"]},"align":["center","center","center","center","center"],"font":{"color":["white","#506784","#506784","#506784","white"],"size":12},"height":30},"type":"table","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>Como se trata de uma amostra simulada √© poss√≠vel comparar as estimativas com os valores reais que geraram a amostra e os valores est√£o muito pr√≥ximos da m√©dia (todos eles est√£o inclu√≠dos no intervalo de credibilidade).</p>
</div>
</div>
<div id="comparando-com-o-modelo-linear-cl√°ssico" class="section level3">
<h3>Comparando com o modelo linear cl√°ssico</h3>
<p>Agora que os resultados sob o paradigma bayesiano j√° foram conferidos ser√° ajustado um modelo de regress√£o linear simples pelo m√©todo dos m√≠nimos quadrados atrav√©s da fun√ß√£o <code>lm()</code> sob o paradigma cl√°ssico para comparar com os resultados de um modelo de regress√£o linear simples sob o paradigma bayesiano utilizando os resultados calculados.</p>
<pre class="r"><code># Reta do modelo classico
plot(x, y)
modelo.classico &lt;- lm(y ~ 1 + x)
a.classico      &lt;- modelo.classico$coefficients[1]
b.classico      &lt;- modelo.classico$coefficients[2]
abline(a        &lt;- a.classico, b = b.classico, col = &quot;blue&quot;)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>O modelo estimado para estes dados sob o paradigma da infer√™ncia cl√°ssica foi o seguinte: <span class="math inline">\(\hat{y} = 1.0245 x + 0,4933\)</span>, o que mostra que as estimativas de <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span> foram muito parecidas com as estimativas sob o paradigma da infer√™ncia bayesiana.</p>
<pre class="r"><code># Reta do modelo bayesiano
plot(x, y)
a.bayes  &lt;-  mean(results[, 1])
b.bayes  &lt;-  mean(results[, 2])
abline(a = a.bayes, b = b.bayes, col = &quot;red&quot;)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>A figura apresenta o gr√°fico de dispers√£o entre as vari√°veis da amostra simulada e as retas dos ajustes de ambos os modelos:</p>
<pre class="r"><code>library(stringr)
library(ggplot2)
library(ggExtra)

# Texto da imagem
text.classico &lt;- str_c(&quot;Modelo Classico: &quot;,&quot;y = &quot;,round(a.classico,4),&quot; x + &quot;,round(b.classico,4))
text.bayes    &lt;- str_c(&quot;Modelo Bayesiano: &quot;,&quot;y = &quot;,round(a.bayes,4),&quot; x + &quot;,round(b.bayes,4))

# Gerando o e ambos:
cbind(y, x) %&gt;%
  as.data.frame %&gt;%
    ggplot(aes(y = y, x = x)) +
    geom_point() +
    geom_smooth(method = &quot;lm&quot;, se = F, col = &quot;red&quot;) +
    theme_classic() +
    geom_abline(slope = b.bayes,
    intercept = a.bayes,
    col = &quot;blue&quot;) +
    labs(title = &quot;&quot;,
    x = &quot;Covari√°vel&quot;,
    y = &quot;Reposta&quot;)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>Agora que os resultados no algoritmo j√° foram conferidos e avaliados de maneira satisfat√≥ria utilizando os dados simulados, √© a vez de fazer o ajuste para dados reais.</p>
</div>
</div>
</div>
<div id="ajuste-do-modelo-para-dados-reais" class="section level1">
<h1>Ajuste do modelo para dados reais</h1>
<p>O conjunto de dados que ser√° utilizado como exemplo foi disponibilizado por <span class="citation">@Ezekiel_cars</span> e hoje faz parte do conjunto de banco de dados nativos do R (a base de dados pode ser obtida ao escrever <code>cars</code> no console). Os dados informam a velocidade dos carros e as dist√¢ncias tomadas para parar, esses dados foram registrados na d√©cada de 1920 e s√£o de grande utilidade did√°tica at√© os dias de hoje.</p>
<p>Considere que deseja-se modelar a velocidade dos carros de acordo com as dist√¢ncias tomadas para parar, portanto a vari√°vel resposta ser√° a velocidade e a vari√°vel explicativa do modelo ser√° a dist√¢ncia tomada para parar.</p>
<div id="amostra-utilizada" class="section level2">
<h2>Amostra utilizada</h2>
<pre class="r"><code>y    &lt;-  cars$speed
x    &lt;-  cars$dist
n    &lt;-  nrow(cars)</code></pre>
<p>o valor p para o teste de Shapiro para conferir a suposi√ß√£o de normalidade da vari√°vel resposta foi de 0.4576319 enquanto que o valor p para conferir a normalidade da vari√°vel explicativa foi de 0.0390997</p>
</div>
<div id="distribui√ß√£o-a-priori-2" class="section level2">
<h2>Distribui√ß√£o a priori</h2>
<p>Ser√£o utilizados os mesmos valores que foram propostos na simula√ß√£o como hiperparametros e chutes iniciais para a cadeia, o c√≥digo usado foi exatamente o mesmo.</p>
</div>
<div id="resultados-da-cadeia-1" class="section level2">
<h2>Resultados da cadeia</h2>
<p>Definiremos novamente a vari√°vel <code>inds</code> que indica os valores ap√≥s a amostra de aquecimento (ou <em>burn-in</em>), desta vez n√£o haver√° a vari√°vel <code>real</code> pois n√£o conhecemos os valores reais utilizados para gerar a amostra para conferir se o modelo foi capaz de recuper√°-los. Desta vez utilizaremos a vari√°vel <code>classico</code>, que guarda os valores obtidos com o ajuste do modelo linear pela abordagem cl√°ssica.</p>
<pre class="r"><code># Juntando resultados:
inds     &lt;- seq(burnin, nsim) # Definindo os indices
results  &lt;- cbind(cadeia.b0, cadeia.b1, cadeia.tau) %&gt;% as.data.frame() %&gt;% .[inds, ]
classico &lt;- c(coefficients(lm(cars)), 1 / var(lm(cars)$residuals))
name     &lt;- c(expression(beta[0]), expression(beta[1]), expression(tau))</code></pre>
<div id="histograma-e-densidade-1" class="section level4">
<h4>Histograma e densidade</h4>
<p>A figura abaixo exibe os histogramas com as densidades de tr√™s cadeias obtidas ao se iniciar o amostrador em pontos diferentes de todos os par√¢metros <span class="math inline">\(\theta\)</span> mas dessa vez sem a linha vermelha que indicava o valor do par√¢metro real pois agora ele √© desconhecido.</p>
<pre class="r"><code>g1 &lt;- hist_den(results[, 1], name = name[1])
g2 &lt;- hist_den(results[, 2], name = name[2])
g3 &lt;- hist_den(results[, 3], name = name[3])
grid.arrange(g1, g2, g3, ncol = 1)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>Nota-se que ambas as cadeias convergiram uma mesma distribui√ß√£o e que as √∫ltimas tr√™s cadeias apresentaram valores pr√≥ximos.</p>
</div>
<div id="cadeias" class="section level4">
<h4>Cadeias</h4>
<p>A figura abaixo apresenta os tra√ßos das cadeias dos par√¢metros amostrados. Note que h√° ind√≠cios de converg√™ncia.</p>
<pre class="r"><code>cadeia(results,name)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
</div>
<div id="autocorrela√ß√£o-1" class="section level4">
<h4>Autocorrela√ß√£o</h4>
<p>A Figura abaixo apresenta os gr√°ficos de autocorrela√ß√£o dos par√¢metros amostrados.</p>
<pre class="r"><code>FAC(results)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>√â poss√≠vel notar que apenas nas primeiras defasagens das cadeias das estimativas para os par√¢metros <span class="math inline">\(\beta_0\)</span> e <span class="math inline">\(\beta_1\)</span> se apresentaram de forma autocorrelacionada e que a partir dessa defasagem o gr√°fico de autocorrela√ß√£o se apresentou de forma desej√°vel.</p>
</div>
<div id="estimativas-1" class="section level4">
<h4>Estimativas</h4>
<p>Como todas as caracter√≠sticas da cadeia gerada foram avaliadas de maneira satisfat√≥ria agora ser√° poss√≠vel conferir o ajuste dos par√¢metros de maneira mais segura pois j√° foi constatada a converg√™ncia da cadeia</p>
</div>
<div id="comparando-com-o-modelo-linear-cl√°ssico-1" class="section level4">
<h4>Comparando com o modelo linear cl√°ssico</h4>
<p>Agora que os resultados sob o paradigma bayesiano j√° foram conferidos novamente ser√° ajustado um modelo de regress√£o linear simples pelo m√©todo dos m√≠nimos quadrados sob o paradigma cl√°ssico para comparar com os resultados do um modelo de regress√£o linear simples sob o paradigma bayesiano utilizando os resultados calculados na se√ß√£o.</p>
<pre class="r"><code># Reta do modelo classico 
plot(x, y)
modelo.classico &lt;- lm(y ~ 1 + x)
a.classico      &lt;- modelo.classico$coefficients[1]
b.classico      &lt;- modelo.classico$coefficients[2]
abline(a        &lt;- a.classico, b = b.classico, col = &quot;blue&quot;)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<pre class="r"><code># Reta do modelo bayesiano
plot(x, y)
a.bayes &lt;- mean(results[, 1])
b.bayes &lt;- mean(results[, 2])
abline(a = a.bayes, b = b.bayes, col = &quot;red&quot;)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>A Tabela abaixo apresenta o resumo a posteriori dos par√¢metros estimados da cadeia e note que esta tabela n√£o conta com a coluna dos valores reais como no exemplo anterior e sim as estimativas sob o paradigma cl√°ssico.</p>
<pre class="r"><code>coef &lt;- 
  coeficientes(results,real = classico) %&gt;% as.data.frame()

tabela_coeficientes(coef)</code></pre>
<div id="htmlwidget-2" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-2">{"x":{"visdat":{"11b981cae4251":["function () ","plotlyVisDat"]},"cur_data":"11b981cae4251","attrs":{"11b981cae4251":{"columnorder":[1,2,3,4,5],"columnwidth":[80,80,80,80,80],"header":{"values":["M√©dia","Desv. Pad.","IC inf","IC sup","Real"],"line":{"color":"#506784"},"fill":{"color":"#1F8FFFB4"},"align":["center","center","center","center","center"],"font":{"color":"white","size":15},"height":40},"cells":{"values":[[8.2374,0.1663,0.1083],[0.8481,0.017,0.0214],[6.5848,0.1326,0.0699],[9.9239,0.1997,0.1542],[8.2839,0.1656,0.1025]],"line":{"color":"#506784"},"fill":{"color":["#1F8FFF58","white","white","white","#1F8FFF58"]},"align":["center","center","center","center","center"],"font":{"color":["white","#506784","#506784","#506784","white"],"size":12},"height":30},"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"table"}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"hovermode":"closest","showlegend":false},"source":"A","config":{"showSendToCloud":false},"data":[{"columnorder":[1,2,3,4,5],"columnwidth":[80,80,80,80,80],"header":{"values":["M√©dia","Desv. Pad.","IC inf","IC sup","Real"],"line":{"color":"#506784"},"fill":{"color":"#1F8FFFB4"},"align":["center","center","center","center","center"],"font":{"color":"white","size":15},"height":40},"cells":{"values":[[8.2374,0.1663,0.1083],[0.8481,0.017,0.0214],[6.5848,0.1326,0.0699],[9.9239,0.1997,0.1542],[8.2839,0.1656,0.1025]],"line":{"color":"#506784"},"fill":{"color":["#1F8FFF58","white","white","white","#1F8FFF58"]},"align":["center","center","center","center","center"],"font":{"color":["white","#506784","#506784","#506784","white"],"size":12},"height":30},"type":"table","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>O modelo estimado sob este paradigma pode ser escrito da seguinte maneira: <span class="math inline">\(\hat{y} = 8,2839 x + 0,1656\)</span>, ou seja, os valores de <span class="math inline">\(\beta_0\)</span> e de <span class="math inline">\(\beta_1\)</span> novamente foram muito pr√≥ximos dos par√¢metros obtidos ao estimar sob o paradigma cl√°ssico.</p>
</div>
<div id="comparando-de-forma-visual" class="section level4">
<h4>Comparando de forma visual</h4>
<p>A Figura ilustra o gr√°fico de dispers√£o dos dados citados acima, com a inten√ß√£o de exibir quanto uma vari√°vel √© afetada por outra, onde no eixo vertical representa a velocidade do carro e no eixo horizontal a dist√¢ncia tomada para parar.</p>
<p>Al√©m do comportamento das vari√°veis, neste gr√°fico √© exibido tamb√©m os resultados obtidos do ajuste ao se utilizar o m√©todo de m√≠nimos quadrados (representada pela linha em vermelho) para estimar os par√¢metros e o ajuste do modelo ao se utilizar o m√©todo apresentado acima em (representada pela linha azul).</p>
<pre class="r"><code># Texto da imagem
text.classico &lt;- str_c(&quot;Modelo Classico: &quot;,&quot;y = &quot;,round(a.classico,4),&quot; x + &quot;,round(b.classico,4))
text.bayes    &lt;- str_c(&quot;Modelo Bayesiano: &quot;,&quot;y = &quot;,round(a.bayes,4),&quot; x + &quot;,round(b.bayes,4))

#Gerando o scatter.plot
cbind(y, x) %&gt;%
  as.data.frame %&gt;%
  ggplot(aes(y = y, x = x)) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, se = F, col = &quot;red&quot;) +
  theme_classic() +
  geom_abline(slope = b.bayes,
              intercept = a.bayes,
              col = &quot;blue&quot;) +
  labs(title = &quot;Rela√ß√£o entre a Dist√¢ncia e a Velocidade com \nreta do modelo linear cl√°ssico vs bayesiano&quot;,
       x = &quot;Dist√¢ncia&quot;,
       y = &quot;Velocidade&quot;)</code></pre>
<p><img src="/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>√â poss√≠vel notar que os coeficientes calculados foram muito parecidos, mesmo apresentando pequenas diferen√ßas decimais no valor dos coeficientes ainda √© poss√≠vel notar que as retas est√£o basicamente sobrepostas, ou seja, os valores estimados em ambas as abordagens foram praticamente os mesmos.</p>
<p>Apesar dos valores dos ajustes terem apresentado basicamente os mesmo resultados, a maneira de se conferir a qualidade do ajuste √© diferente em ambas as abordagens. Enquanto sob o paradigma cl√°ssico o ajuste do modelo pode ser checado ao avaliar os pre-supostos quanto √† distribui√ß√£o dos res√≠duos, como recomenda <span class="citation">@GaussClarice</span>, ao utilizar um m√©todo de MCMC faz-se necess√°rio conferir tamb√©m outros aspectos como por exemplo se houve converg√™ncia da cadeias al√©m do comportamento das autocorrela√ß√µes, vide <span class="citation">@migon</span>.</p>
</div>
</div>
</div>
<div id="conclus√£o" class="section level1">
<h1>Conclus√£o</h1>
<p>O uso do algor√≠tmo para simular os dados da implementa√ß√£o do modelo hier√°rquico bayesiano envolveu diversas etapas. Inicialmente foi necess√°ria a revis√£o de literatura para a compreens√£o dos m√©todos que seriam utilizados na implementa√ß√£o do algoritmo, bem como em seu desenvolvimento. Essa pesquisa funcionou de maneira muito did√°tica, de forma que a cada semana a abordagem pudesse envolver maior grau de complexidade.</p>
<p>Durante o estudo, diversos valores de par√¢metros a priori foram selecionados para que fosse poss√≠vel avaliar a sensibilidade da qualidade da escolha da distribui√ß√£o a priori. Observou-se que valores elevados para vari√¢ncia a priori (tamb√©m consideradas como ‚Äún√£o informativas‚Äù - fazendo uma analogia √† modelos cl√°ssicos) obtiveram melhores ajustes atribuindo maior import√¢ncia √† informa√ß√£o provinda da amostra.</p>
<p>O estudo com dados simulados facilitou o entendimento do algoritmo pois foi poss√≠vel notar com facilidade a inadequabilidade das escolhas das prioris, que resultavam em estimativas muito distante do par√¢metro populacional que gerou a amostra.</p>
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias</h1>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2018-07-28-modelo-bayesiano-do-zero/modelo-bayesiano-do-zero/">modelo bayesiano do zero</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Aprendizado N√£o Supervisionado</category>
      <category>Bayes</category>
      <category>Infer√™ncia Bayesiana</category>
      <category>Estatistica</category>
      <category>Modelagem Estatistica</category>
      <category>Probabilidade</category>
      <category>R</category>
      <category>Simula√ß√£o</category>
      <category>Teoria</category>
      <category domain="tag">bayes</category>
      <category domain="tag">Estatistica</category>
      <category domain="tag">gomesfellipe</category>
      <category domain="tag">jags</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">modelagem estatistica</category>
      <category domain="tag">modelos generalizados</category>
      <category domain="tag">modelos lineares</category>
      <category domain="tag">probabilidade</category>
      <category domain="tag">R</category>
      <category domain="tag">regression</category>
      <category domain="tag">Teoria</category>
    </item>
    <item>
      <title>Brasil x Argentina, tidytext e Machine Learning</title>
      <link>https://gomesfellipe.github.io/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml/</link>
      <pubDate>Sun, 24 Jun 2018 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml/</guid>
      <description>Aplicando t√©cnincas de Text Mining como pacote tidy text para explorar a rivalidade entre Brasil e Argentina! Veja tamb√©m como a an√°lise de sentimentos pode ser divertida al√©m de poss√≠veis aplica√ß√µes de machine learning</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="brasil-vs-argentina-e-text-mining" class="section level1">
<h1>Brasil vs Argentina e Text Mining</h1>
<p>A copa do mundo esta ai novamente e como n√£o poderia ser diferente, com ela surgem novos <a href="http://cio.com.br/noticias/2015/10/27/tome-nota-2-5-quintilhoes-de-bytes-sao-criados-todos-os-dias/">quintilh√µes de bytes todos os dias</a>, saber analisar esses dados √© um grande desafio pois a maioria dessa informa√ß√£o se encontra de forma n√£o estruturada e al√©m do desafio de captar esses dados ainda existem mais desafios que podem ser ainda maiores, como o de process√°-los e obter respostas deles.</p>
<p>Dada a rivalidade hist√≥rica entre Brasil e Argentina achei que seria interessante avaliar como anda o comportamento das pessoas do Brasil nas m√≠dias sociais em rela√ß√£o a esses dois pa√≠ses. Para o post n√£o ficar muito longo, escolhi que iria recolher informa√ß√µes apenas do Twitter devido a praticidade, foram coletados os √∫ltimos 4.000 tweets com o termo ‚Äúbrasil‚Äù e os √∫ltimos ‚Äú4.000‚Äù tweets com o termo ‚Äúargentina‚Äù no Twitter atrav√©s da sua API com o pacote os <code>twitteR</code> e <code>ROAuth</code>. O c√≥digo pode ser conferido <a href="https://github.com/gomesfellipe/functions/blob/master/getting_twitter_data.R">neste link</a>.</p>
<p>An√°lise de textos sempre foi um tema que me interessou muito, no final do ano de 2017 quando era estagi√°rio me pediram para ajudar em uma pesquisa que envolvia a an√°lise de palavras criando algumas nuvens de palavras. Pesquisando sobre t√©cnicas de textmining descobri tantas abordagens diferentes que resolvi juntar tudo que tinha encontrado em uma √∫nica fun√ß√£o (que ser√° apresentada a seguir) para a confec√ß√£o dessas nuvens, utilizarei esta fun√ß√£o para ter uma primeira impress√£o dos dados.</p>
<p>Al√©m disso, como seria um problema a tarefa de criar as nuvens de palavras s√≥ poderia ser realizada por algu√©m com conhecimento em R, na √©poca estava come√ßando meus estudo sobre shiny e como treinamento desenvolvi um app que esta hospedado no link: <a href="https://gomesfellipe.shinyapps.io/appwordcloud/" class="uri">https://gomesfellipe.shinyapps.io/appwordcloud/</a> e o c√≥digo esta aberto e dispon√≠vel para quem se interessar no meu github <a href="https://github.com/gomesfellipe/appwordcloud/blob/master/appwordcloud.Rmd">neste link</a></p>
<p>Por√©m, ap√≥s ler e estudar o livro <a href="https://www.tidytextmining.com/">Text Mining with R - A Tidy Approach</a> por <span class="citation"><a href="#ref-tidytext" role="doc-biblioref">Silge; Robinson</a> (<a href="#ref-tidytext" role="doc-biblioref">2018</a>)</span> hoje em dia eu olho para tr√°s e vejo que poderia ter feito tanto a fun√ß√£o quanto o aplicativo de maneira muito mais eficiente portanto esse post tr√°s alguns dos meus estudos sobre esse livro maravilhoso e tamb√©m algum estudo sobre Machine Learning com o pacote <a href="https://cran.r-project.org/web/packages/caret"><code>caret</code></a></p>
<div id="importando-a-dados" class="section level2">
<h2>Importando a dados</h2>
<p>Como j√° foi dito, a base de dados foi obtida atrav√©s da API do twitter e o c√≥digo pode ser obtido <a href="https://github.com/gomesfellipe/functions/blob/master/getting_twitter_data.R">neste link</a>.</p>
<pre class="r"><code>library(dplyr)
library(kableExtra)
library(magrittr)

base &lt;- read.csv(&quot;original_books.csv&quot;) %&gt;% as_tibble()</code></pre>
</div>
<div id="nuvem-de-palavras-r√°pida-com-fun√ß√£o-customizada" class="section level2">
<h2>Nuvem de palavras r√°pida com fun√ß√£o customizada</h2>
<p>Para uma primeira impress√£o dos dados, vejamos o que retorna uma nuvem de palavras criada com a fun√ß√£o <a href="https://github.com/gomesfellipe/functions/blob/master/wordcloud_sentiment.R"><code>wordcloud_sentiment()</code></a> que desenvolvi antes de conhecer a ‚ÄúA Tidy Approach‚Äù para Text Mining:</p>
<pre class="r"><code>devtools::source_url(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/wordcloud_sentiment.R&quot;)

# Obtendo nuvem e salvando tabela num objeto com nome teste:
df &lt;- wordcloud_sentiment(base$text,
                      type = &quot;text&quot;,
                      sentiment = F,
                      excludeWords = c(&quot;nao&quot;,letters,LETTERS),
                      ngrams = 2,
                      tf_idf = F,
                      max = 100,
                      freq = 10,
                      horizontal = 0.9,
                      textStemming = F,
                      print=T)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-2-1.png" width="1056" /></p>
<p>N√£o poderia esquecer, al√©m da nuvem, a fun√ß√£o tamb√©m retorna um dataframe com a frequ√™ncia das palavras:</p>
<pre class="r"><code>df %&gt;% as_tibble()</code></pre>
<pre><code>## # A tibble: 29,064 x 2
##    words          freq  
##    &lt;chr&gt;          &lt;chr&gt; 
##  1 =              &quot;2795&quot;
##  2 brasil copa    &quot;2061&quot;
##  3 copa mundo     &quot;1959&quot;
##  4 hat trick      &quot;1327&quot;
##  5 = hoje         &quot;1248&quot;
##  6 hoje brasil    &quot;1215&quot;
##  7 mundo          &quot; 852&quot;
##  8 isl ndia       &quot; 820&quot;
##  9 pra copa       &quot; 813&quot;
## 10 estreia brasil &quot; 782&quot;
## # ‚Ä¶ with 29,054 more rows</code></pre>
<p>E outra fun√ß√£o interessante √© a de criar uma nuvem a partir de um webscraping muito (muito mesmo) introdut√≥rio, para isso foi pegar todo o texto da p√°gina sobre a copa do mundo no Wikip√©dia, veja:</p>
<pre class="r"><code># Obtendo nuvem e salvando tabela num objeto com nome teste:
df_html &lt;- wordcloud_sentiment(&quot;https://pt.wikipedia.org/wiki/Copa_do_Mundo_FIFA&quot;,
                      type = &quot;url&quot;,
                      sentiment = F,
                      excludeWords = c(&quot;nao&quot;,letters,LETTERS),
                      ngrams = 2,
                      tf_idf = F,
                      max = 100,
                      freq = 6,
                      horizontal = 0.9,
                      textStemming = F,
                      print=T)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Essa fun√ß√£o √© bem ‚Äúprematura,‚Äù existem infinitas maneiras de melhorar ela e n√£o alterei ela ainda por falta de tempo.</p>
</div>
<div id="a-tidy-approach" class="section level2">
<h2>A Tidy Approach</h2>
<p>O formato tidy, em que cada linha corresponde a uma observa√ß√£o e cada coluna √† uma vari√°vel, veja:</p>
<center>
<img src="http://garrettgman.github.io/images/tidy-1.png" style="width:70.0%" />
</center>
<p>Agora a tarefa ser√° simplificada com a abordagem tidy, al√©m das fun√ß√µes do livro <a href="https://www.tidytextmining.com/">Text Mining with R</a> utilizarei a fun√ß√£o <a href="https://github.com/gomesfellipe/functions/blob/master/clean_tweets.R"><code>clean_tweets</code></a> que adaptei inspirado nesse post dessa pagina: <a href="https://sites.google.com/site/miningtwitter/home">Quick guide to mining twitter with R</a> quando estudava sobre textmining.</p>
<div id="arrumando-e-transformando-a-base-de-dados" class="section level3">
<h3>Arrumando e transformando a base de dados</h3>
<p>Utilizando as fun√ß√µes do pacote <code>tidytext</code> em conjunto com os pacotes <code>stringr</code> e <code>abjutils</code>, ser√° poss√≠vel limpar e arrumar a base de dados.</p>
<p>Al√©m disso ser√£o removidas as stop words de nossa base, com a fun√ß√£o <code>stopwords::stopwords("pt")</code> podemos obter as stopwords da nossa l√≠ngua</p>
<pre class="r"><code>library(stringr)
library(tidytext)
library(abjutils)

devtools::source_url(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/clean_tweets.R&quot;)

original_books = base %&gt;% 
  mutate(text = clean_tweets(text) %&gt;% enc2native() %&gt;% rm_accent())

#Removendo stopwords:
excludewords=c(&quot;[:alpha:]&quot;,&quot;[:alnum:]&quot;,&quot;[:digit:]&quot;,&quot;[:xdigit:]&quot;,&quot;[:space:]&quot;,&quot;[:word:]&quot;,
               LETTERS,letters,1:10,
               &quot;hat&quot;,&quot;trick&quot;,&quot;bc&quot;,&quot;de&quot;,&quot;tem&quot;,&quot;twitte&quot;,&quot;fez&quot;,
               &#39;pra&#39;,&quot;vai&quot;,&quot;ta&quot;,&quot;so&quot;,&quot;ja&quot;,&quot;rt&quot;)

stop_words = data_frame(word = c(stopwords::stopwords(&quot;pt&quot;), excludewords))

tidy_books &lt;- original_books %&gt;%
  unnest_tokens(word, text) %&gt;% 
  anti_join(stop_words)</code></pre>
<p>Portando a base de dados ap√≥s a limpeza e a remo√ß√£o das stop words:</p>
<pre class="r"><code>#Palavras mais faladas:
tidy_books %&gt;% count(word, sort = TRUE) </code></pre>
<pre><code>## # A tibble: 3,900 x 2
##    word          n
##    &lt;chr&gt;     &lt;int&gt;
##  1 copa       6993
##  2 brasil     4164
##  3 argentina  3487
##  4 mundo      2030
##  5 hoje       1825
##  6 letras     1562
##  7 messi      1493
##  8 estreia    1107
##  9 est         866
## 10 isl         828
## # ‚Ä¶ with 3,890 more rows</code></pre>
<pre class="r"><code>#Apos a limpeza, caso precise voltar as frases:
original_books = tidy_books%&gt;%
  group_by(book,line)%&gt;%
  summarise(text=paste(word,collapse = &quot; &quot;))</code></pre>
<div id="palavras-mais-frequentes" class="section level4">
<h4>Palavras mais frequentes</h4>
<p>Vejamos as palavras mais faladas nessa pesquisa:</p>
<pre class="r"><code>library(ggplot2)

tidy_books %&gt;%
  count(word, sort = TRUE) %&gt;%
  filter(n &gt; 400) %&gt;%
  mutate(word = reorder(word, n)) %&gt;%
  
  ggplot(aes(word, n, fill = I(&quot;yellow&quot;), colour = I(&quot;green&quot;))) +
  geom_col(position=&quot;dodge&quot;) +
  xlab(NULL) +
  labs(title = &quot;Frequencia total das palavras pesquisadas&quot;)+
  coord_flip()+ theme(
  panel.background = element_rect(fill = &quot;#74acdf&quot;,
                                colour = &quot;lightblue&quot;,
                                size = 0.5, linetype = &quot;solid&quot;),
  panel.grid.major = element_line(size = 0.5, linetype = &#39;solid&#39;,
                                colour = &quot;white&quot;), 
  panel.grid.minor = element_line(size = 0.25, linetype = &#39;solid&#39;,
                                colour = &quot;white&quot;)
  )</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<div id="palavras-mais-frequentes-para-cada-termo" class="section level4">
<h4>Palavras mais frequentes para cada termo</h4>
<p>Vejamos as nuvens de palavras mais frequentes de acordo com cada um dos termos pesquisados:</p>
<pre class="r"><code>#Criando nuvem de palavra:
library(wordcloud)

par(mfrow=c(1,2))
tidy_books %&gt;%
  filter(book==&quot;br&quot;)%&gt;%
  count(word) %&gt;%
  with(wordcloud(word, n, max.words = 100,random.order = F,min.freq = 15,random.color = F,colors = c(&quot;#009b3a&quot;, &quot;#fedf00&quot;,&quot;#002776&quot;),scale = c(2,1),rot.per = 0.05))

tidy_books %&gt;%
  filter(book==&quot;arg&quot;)%&gt;%
  count(word) %&gt;%
  with(wordcloud(word, n, max.words = 100,min.freq = 15,random.order = F,random.color = F,colors = c(&quot;#75ade0&quot;, &quot;#ffffff&quot;,&quot;#f6b506&quot;),scale = c(2,1),rot.per = 0.05))</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-8-1.png" width="1056" /></p>
<pre class="r"><code>par(mfrow=c(1,1))</code></pre>
</div>
</div>
<div id="an√°lise-de-sentimentos" class="section level3">
<h3>An√°lise de sentimentos</h3>
<p>A an√°lise de sentimentos utilizando a abordagem tidy foi poss√≠vel gra√ßas ao pacote <a href="https://cran.r-project.org/package=lexiconPT"><code>lexiconPT</code></a>, que esta dispon√≠vel no CRAN e que conheci ao ler o <a href="https://sillasgonzaga.github.io/2017-09-23-sensacionalista-pt01/">post: ‚ÄúO Sensacionalista e Text Mining: An√°lise de sentimento usando o lexiconPT‚Äù</a> do blog <a href="https://sillasgonzaga.github.io/">Paix√£o por dados</a> que gosto tanto de acompanhar.</p>
<pre class="r"><code># Analise de sentimentos:
library(lexiconPT)

sentiment = data.frame(word = sentiLex_lem_PT02$term ,
                       polarity = sentiLex_lem_PT02$polarity) %&gt;% 
  mutate(sentiment = if_else(polarity&gt;0,&quot;positive&quot;,if_else(polarity&lt;0,&quot;negative&quot;,&quot;neutro&quot;)),
         word = as.character(word)) %&gt;% 
  as_tibble()


library(tidyr)

book_sentiment &lt;- tidy_books %&gt;%
  inner_join(sentiment) %&gt;%
  count(book,word, index = line , sentiment) %&gt;%
  spread(sentiment, n, fill = 0) %&gt;%
  mutate(sentiment = positive - negative) %T&gt;%
  print</code></pre>
<pre><code>## # A tibble: 2,953 x 7
##    book  word      index negative neutro positive sentiment
##    &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
##  1 arg   abandonar   857        1      0        0        -1
##  2 arg   absurdo     849        1      0        0        -1
##  3 arg   absurdo    1863        1      0        0        -1
##  4 arg   afogado    2275        1      0        0        -1
##  5 arg   afogado    3659        1      0        0        -1
##  6 arg   alegria    1134        0      0        1         1
##  7 arg   almo        186        0      0        1         1
##  8 arg   almo       2828        0      0        1         1
##  9 arg   almo       3433        0      0        1         1
## 10 arg   almo       3569        0      0        1         1
## # ‚Ä¶ with 2,943 more rows</code></pre>
<p>Cada palavra possui um valor associado a sua polaridade , vejamos como ficou distribu√≠do o n√∫mero de palavras de cada sentimento de acordo com cada termo escolhido para a pesquisa:</p>
<pre class="r"><code>book_sentiment%&gt;%
  count(sentiment,book)%&gt;%
  arrange(book) %&gt;%
  
  ggplot(aes(x = factor(sentiment),y = n,fill=book))+
  geom_bar(stat=&quot;identity&quot;,position=&quot;dodge&quot;)+
  facet_wrap(~book) +
  theme_bw()+ 
    scale_fill_manual(values=c(&quot;#75ade0&quot;, &quot;#009b3a&quot;))</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<div id="comparando-sentimentos-dos-termos-de-pesquisa" class="section level4">
<h4>Comparando sentimentos dos termos de pesquisa</h4>
<p>Para termos associados a palavra ‚ÄúBrasil‚Äù no twitter:</p>
<pre class="r"><code># Nuvem de compara√ß√£o:
library(reshape2)

tidy_books %&gt;%
  filter(book==&quot;br&quot;)%&gt;%
  inner_join(sentiment) %&gt;%
  count(word, sentiment, sort = TRUE) %&gt;%
  acast(word ~ sentiment, value.var = &quot;n&quot;, fill = 0) %&gt;%
  comparison.cloud(colors = c(&quot;red&quot;, &quot;gray80&quot;,&quot;green&quot;),
                   max.words = 200)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Para termos associados a palavra ‚ÄúArgentina‚Äù no twitter:</p>
<pre class="r"><code>tidy_books %&gt;%
  filter(book==&quot;arg&quot;)%&gt;%
  inner_join(sentiment) %&gt;%
  count(word, sentiment, sort = TRUE) %&gt;%
  acast(word ~ sentiment, value.var = &quot;n&quot;, fill = 0) %&gt;%
  comparison.cloud(colors = c(&quot;red&quot;, &quot;gray80&quot;,&quot;green&quot;),
                   max.words = 200)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
<div id="propor√ß√£o-de-palavras-positivas-e-negativas-por-texto" class="section level4">
<h4>Propor√ß√£o de palavras positivas e negativas por texto</h4>
<pre class="r"><code># Propor√ß√£o de palavras negativas:
bingnegative &lt;- sentiment %&gt;% 
  filter(sentiment == &quot;negative&quot;)

bingpositive &lt;- sentiment %&gt;% 
  filter(sentiment == &quot;positive&quot;)

wordcounts &lt;- tidy_books %&gt;%
  group_by(book, line) %&gt;%
  summarize(words = n())</code></pre>
<div id="para-negativas" class="section level5">
<h5>Para negativas;</h5>
<pre class="r"><code>tidy_books %&gt;%
  semi_join(bingnegative) %&gt;%
  group_by(book, line) %&gt;%
  summarize(negativewords = n()) %&gt;%
  left_join(wordcounts, by = c(&quot;book&quot;, &quot;line&quot;)) %&gt;%
  mutate(ratio = negativewords/words) %&gt;%
  top_n(5) %&gt;%
  ungroup() %&gt;% arrange(desc(ratio)) %&gt;% filter(book==&quot;br&quot;)</code></pre>
<pre><code>## # A tibble: 5 x 5
##   book   line negativewords words ratio
##   &lt;chr&gt; &lt;int&gt;         &lt;int&gt; &lt;int&gt; &lt;dbl&gt;
## 1 br     2003             1     3 0.333
## 2 br     2775             1     3 0.333
## 3 br     2580             2     7 0.286
## 4 br      126             1     4 0.25 
## 5 br     2335             1     4 0.25</code></pre>
<p>A frase mais negativa do brasil e da argentina::</p>
<pre class="r"><code>base %&gt;%
  filter(book==&quot;br&quot;,line==2580) %&gt;% mutate(text = as.character(text))%&gt;% select(text) %&gt;% c() </code></pre>
<pre><code>## $text
## [1] &quot;um medo? \x97 de nois criar expectativa e o Brasil perder a copa https://t.co/0chcNWHh0m&quot;</code></pre>
<pre class="r"><code>base %&gt;%
  filter(book==&quot;arg&quot;,line==572) %&gt;% mutate(text = as.character(text))%&gt;% select(text) %&gt;% c()  </code></pre>
<pre><code>## $text
## [1] &quot;RT @DavidmeMelo: @SantiiSanchez16 @Flamengo Perder a copa para o time mais sujo e mais corrupto da argentina \xe9 assim mesmo https://t.co/zIC\x85&quot;</code></pre>
</div>
<div id="para-positivas" class="section level5">
<h5>Para positivas:</h5>
<pre class="r"><code>tidy_books %&gt;%
  semi_join(bingpositive) %&gt;%
  group_by(book, line) %&gt;%
  summarize(positivewords = n()) %&gt;%
  left_join(wordcounts, by = c(&quot;book&quot;, &quot;line&quot;)) %&gt;%
  mutate(ratio = positivewords/words) %&gt;%
  top_n(5) %&gt;%
  ungroup() %&gt;% arrange(desc(ratio))</code></pre>
<pre><code>## # A tibble: 22 x 5
##    book   line positivewords words ratio
##    &lt;chr&gt; &lt;int&gt;         &lt;int&gt; &lt;int&gt; &lt;dbl&gt;
##  1 arg    2120             3     9 0.333
##  2 br     2374             1     3 0.333
##  3 arg    3272             2     7 0.286
##  4 arg    2301             1     4 0.25 
##  5 br      126             1     4 0.25 
##  6 br      553             2     8 0.25 
##  7 br     1499             2     8 0.25 
##  8 br     2054             2     8 0.25 
##  9 br     2591             1     4 0.25 
## 10 arg    2130             1     5 0.2  
## # ‚Ä¶ with 12 more rows</code></pre>
<p>A frase mais positiva do brasil e da argentina:</p>
<pre class="r"><code>base %&gt;%
  filter(book==&quot;br&quot;,line==2374) %&gt;% mutate(text = as.character(text))%&gt;% select(text) %&gt;% c() </code></pre>
<pre><code>## $text
## [1] &quot;Tirei Brasil, \xe9 uma honra https://t.co/OgNCot4Wu0&quot;</code></pre>
<pre class="r"><code>base %&gt;%
  filter(book==&quot;arg&quot;,line==2120) %&gt;% mutate(text = as.character(text))%&gt;% select(text) %&gt;% c()  </code></pre>
<pre><code>## $text
## [1] &quot;@_LeoFerreiraH Quero que a Argentina passe para possivelmente enfrentar o Brasil, ganhar da Argentina j\xe1 \xe9 bom, na\x85 https://t.co/bxHJUeGVpc&quot;</code></pre>
</div>
</div>
</div>
</div>
<div id="tf-idf" class="section level2">
<h2>TF-IDF</h2>
<p>Segundo <span class="citation"><a href="#ref-tidytext" role="doc-biblioref">Silge; Robinson</a> (<a href="#ref-tidytext" role="doc-biblioref">2018</a>)</span> no livro <a href="https://www.tidytextmining.com/tfidf.html">tidytextminig</a>:</p>
<blockquote>
<p>The statistic tf-idf is intended to measure how important a word is to a document in a collection (or corpus) of documents, for example, to one novel in a collection of novels or to one website in a collection of websites.</p>
</blockquote>
<p>Traduzido pelo Google tradutor:</p>
<blockquote>
<p>A estat√≠stica tf-idf destina-se a medir a import√¢ncia de uma palavra para um documento em uma cole√ß√£o (ou corpus) de documentos, por exemplo, para um romance em uma cole√ß√£o de romances ou para um site em uma cole√ß√£o de sites.</p>
</blockquote>
<p>Matematicamente:</p>
<p><span class="math display">\[
idf(\text{term}) = \ln{\left(\frac{n_{\text{documents}}}{n_{\text{documents containing term}}}\right)}
\]</span></p>
<p>E que com o pacote <code>tidytext</code> podemos obter usando o comando <code>bind_tf_idf()</code>, veja:</p>
<pre class="r"><code># Obtendo numero de palavras
book_words &lt;- original_books %&gt;%
  unnest_tokens(word, text) %&gt;%
  count(book, word, sort = TRUE) %&gt;%
  ungroup()%&gt;%
  anti_join(stop_words)

total_words &lt;- book_words %&gt;% 
  group_by(book) %&gt;% 
  summarize(total = sum(n))

book_words &lt;- left_join(book_words, total_words)

# tf-idf:
book_words &lt;- book_words %&gt;%
  bind_tf_idf(word, book, n)

book_words %&gt;%
  arrange(desc(tf_idf))</code></pre>
<pre><code>## # A tibble: 4,773 x 7
##    book  word              n total      tf   idf  tf_idf
##    &lt;chr&gt; &lt;chr&gt;         &lt;int&gt; &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;
##  1 br    letras         1562 30429 0.0513  0.693 0.0356 
##  2 br    ansioso         688 30429 0.0226  0.693 0.0157 
##  3 arg   classificou     666 40781 0.0163  0.693 0.0113 
##  4 arg   segundo         654 40781 0.0160  0.693 0.0111 
##  5 arg   especialistas   649 40781 0.0159  0.693 0.0110 
##  6 arg   nalti           649 40781 0.0159  0.693 0.0110 
##  7 arg   repito          649 40781 0.0159  0.693 0.0110 
##  8 br    icon            248 30429 0.00815 0.693 0.00565
##  9 arg   ncio            287 40781 0.00704 0.693 0.00488
## 10 arg   penalti         284 40781 0.00696 0.693 0.00483
## # ‚Ä¶ with 4,763 more rows</code></pre>
<p>O que nos tr√°s algo como: ‚Äútermos mais relevantes.‚Äù</p>
<p>Visualmente:</p>
<pre class="r"><code>book_words %&gt;%
  arrange(desc(tf_idf)) %&gt;%
  mutate(word = factor(word, levels = rev(unique(word)))) %&gt;% 
  group_by(book) %&gt;% 
  top_n(15) %&gt;% 
  ungroup %&gt;%
  
  ggplot(aes(word, tf_idf, fill = book)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = &quot;tf-idf&quot;) +
  facet_wrap(~book, ncol = 2, scales = &quot;free&quot;) +
  coord_flip()+
  theme_bw()+ 
    scale_fill_manual(values=c(&quot;#75ade0&quot;, &quot;#009b3a&quot;))</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
</div>
<div id="bi-grams" class="section level2">
<h2>bi grams</h2>
<p>OS bi grams s√£o sequencias de palavras, a seguir ser√° procurada as sequencias de duas palavras, o que nos permite estudar um pouco melhor o contexto do seu uso.</p>
<pre class="r"><code># Bi grams
book_bigrams &lt;- original_books %&gt;%
  unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2)

book_bigrams %&gt;%
  count(bigram, sort = TRUE)</code></pre>
<pre><code>## # A tibble: 15,106 x 3
## # Groups:   book [2]
##    book  bigram                    n
##    &lt;chr&gt; &lt;chr&gt;                 &lt;int&gt;
##  1 br    brasil copa            2039
##  2 br    copa mundo             1459
##  3 br    hoje brasil            1215
##  4 arg   argentina copa         1122
##  5 arg   isl ndia                818
##  6 br    estreia brasil          764
##  7 br    ansioso estreia         684
##  8 br    est ansioso             680
##  9 arg   classificou argentina   660
## 10 arg   copa segundo            649
## # ‚Ä¶ with 15,096 more rows</code></pre>
<p>Separando as coluna de bi grams:</p>
<pre class="r"><code>bigrams_separated &lt;- book_bigrams %&gt;%
  separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;)

bigrams_filtered &lt;- bigrams_separated %&gt;%
  filter(!word1 %in% stop_words$word) %&gt;%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts &lt;- bigrams_filtered %&gt;% 
  count(word1, word2, sort = TRUE)

bigram_counts</code></pre>
<pre><code>## # A tibble: 15,106 x 4
## # Groups:   book [2]
##    book  word1       word2         n
##    &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;     &lt;int&gt;
##  1 br    brasil      copa       2039
##  2 br    copa        mundo      1459
##  3 br    hoje        brasil     1215
##  4 arg   argentina   copa       1122
##  5 arg   isl         ndia        818
##  6 br    estreia     brasil      764
##  7 br    ansioso     estreia     684
##  8 br    est         ansioso     680
##  9 arg   classificou argentina   660
## 10 arg   copa        segundo     649
## # ‚Ä¶ with 15,096 more rows</code></pre>
<p>Caso seja preciso juntar novamente:</p>
<pre class="r"><code>bigrams_united &lt;- bigrams_filtered %&gt;%
  unite(bigram, word1, word2, sep = &quot; &quot;)

bigrams_united</code></pre>
<pre><code>## # A tibble: 71,208 x 2
## # Groups:   book [2]
##    book  bigram             
##    &lt;chr&gt; &lt;chr&gt;              
##  1 arg   isl ndia           
##  2 arg   ndia pouco         
##  3 arg   pouco mil          
##  4 arg   mil habitantes     
##  5 arg   habitantes montaram
##  6 arg   montaram sele      
##  7 arg   sele est           
##  8 arg   est copa           
##  9 arg   copa fizeram       
## 10 arg   fizeram gol        
## # ‚Ä¶ with 71,198 more rows</code></pre>
<div id="analisando-bi-grams-com-tf-idf" class="section level3">
<h3>Analisando bi grams com tf-idf</h3>
<p>Tamb√©m √© poss√≠vel aplicar a transforma√ß√£o <code>tf-idf</code> em bigrams, veja:</p>
<pre class="r"><code>#bi grams com tf idf
bigram_tf_idf &lt;- bigrams_united %&gt;%
  count(book, bigram) %&gt;%
  bind_tf_idf(bigram, book, n) %&gt;%
  arrange(desc(tf_idf))

bigram_tf_idf</code></pre>
<pre><code>## # A tibble: 15,106 x 6
## # Groups:   book [2]
##    book  bigram                    n     tf   idf  tf_idf
##    &lt;chr&gt; &lt;chr&gt;                 &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;
##  1 br    hoje brasil            1215 0.0399 0.693 0.0277 
##  2 br    ansioso estreia         684 0.0225 0.693 0.0156 
##  3 br    est ansioso             680 0.0223 0.693 0.0155 
##  4 br    letras letras           620 0.0204 0.693 0.0141 
##  5 arg   classificou argentina   660 0.0162 0.693 0.0112 
##  6 arg   copa segundo            649 0.0159 0.693 0.0110 
##  7 arg   messi repito            649 0.0159 0.693 0.0110 
##  8 arg   repito classificou      649 0.0159 0.693 0.0110 
##  9 arg   segundo especialistas   649 0.0159 0.693 0.0110 
## 10 br    brasil letras           313 0.0103 0.693 0.00713
## # ‚Ä¶ with 15,096 more rows</code></pre>
</div>
<div id="analisando-contexto-de-palavras-negativas" class="section level3">
<h3>Analisando contexto de palavras negativas:</h3>
<p>Uma das abordagens interessantes ao estudar as bi-grams √© a de avaliar o contexto das palavras negativas, veja:</p>
<pre class="r"><code>bigrams_separated %&gt;%
  filter(word1 == &quot;nao&quot;) %&gt;%
  count(word1, word2, sort = TRUE)</code></pre>
<pre><code>## # A tibble: 35 x 4
## # Groups:   book [2]
##    book  word1 word2         n
##    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;
##  1 br    nao   copa         10
##  2 arg   nao   abrir         3
##  3 arg   nao   convoca       3
##  4 arg   nao   ruim          3
##  5 br    nao   acredito      2
##  6 arg   nao   achei         1
##  7 arg   nao   acordem       1
##  8 arg   nao   argentina     1
##  9 arg   nao   assisti       1
## 10 arg   nao   compara       1
## # ‚Ä¶ with 25 more rows</code></pre>
<pre class="r"><code>not_words &lt;- bigrams_separated %&gt;%
  filter(word1 == &quot;nao&quot;) %&gt;%
  inner_join(sentiment, by = c(word2 = &quot;word&quot;)) %&gt;%
  count(word2, sentiment, sort = TRUE) %&gt;%
  ungroup()

not_words</code></pre>
<pre><code>## # A tibble: 3 x 4
##   book  word2    sentiment     n
##   &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;int&gt;
## 1 arg   ruim     negative      3
## 2 arg   vencer   positive      1
## 3 br    amistoso positive      1</code></pre>
<p>A palavra n√£o antes de uma palavra ‚Äúpositiva,‚Äù como por exemplo ‚Äún√£o gosto‚Äù pode ser anulada ao somar-se suas polaridades (‚Äún√£o‚Äù = - 1, ‚Äúgosto‚Äù = +1 e ‚Äún√£o gosto‚Äù = -1 + 1) o leva a necessidade de ser tomar um cuidado especial com essas palavras em uma an√°lise de texto mais detalhada, veja de forma visual:</p>
<pre class="r"><code>not_words %&gt;%
  mutate(sentiment=ifelse(sentiment==&quot;positive&quot;,1,ifelse(sentiment==&quot;negative&quot;,-1,0)))%&gt;%
  mutate(contribution = n * sentiment) %&gt;%
  arrange(desc(abs(contribution))) %&gt;%
  head(20) %&gt;%
  mutate(word2 = reorder(word2, contribution)) %&gt;%
  
  ggplot(aes(word2, n * sentiment, fill = n * sentiment &gt; 0)) +
  geom_col() +
  xlab(&quot;Words preceded by \&quot;not\&quot;&quot;) +
  ylab(&quot;Sentiment score * number of occurrences&quot;) +
  coord_flip()+
  theme_bw()</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="machine-learning" class="section level1">
<h1>Machine Learning</h1>
<p>Estava pesquisando sobre algor√≠timos recomendados para a an√°lise de texto quando encontrei um artigo da data camp chamado: <a href="https://www.datacamp.com/community/tutorials/R-nlp-machine-learning"><em>Lyric Analysis with NLP &amp; Machine Learning with R</em></a>, do qual a autora exp√µe a seguinte tabela:</p>
<center>
<img src="http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1517331396/MLImage_cygwsb.jpg" style="width:60.0%" />
</center>
<p>Portanto resolvi fazer uma brincadeira e ajustar 4 dos modelos propostos para a tarefa supervisionada de classifica√ß√£o: K-NN, Tress (tentarei o ajuste do algor√≠timo Random Forest), Logistic Regression (Modelo estat√≠stico) e Naive-Bayes (por meio do c√°lculo de probabilidades condicionais) para ver se conseguia recuperar a classifica√ß√£o de quais os termos de pesquisa que eu utilizei para obter esses dados</p>
<p>Al√©m de t√©cnicas apresentadas no livro do pacote <code>caret</code>, por <span class="citation"><a href="#ref-caret" role="doc-biblioref">Kuhn</a> (<a href="#ref-caret" role="doc-biblioref">2018</a>)</span>, muito do que apliquei aqui foi baseado no livro ‚ÄúIntrodu√ß√£o a minera√ß√£o de dados‚Äù por <span class="citation"><a href="#ref-miner" role="doc-biblioref">Silva; Peres; Boscarioli</a> (<a href="#ref-miner" role="doc-biblioref">2016</a>)</span>, que foi bastante √∫til na minha introdu√ß√£o sobre o tema Machine Learning.</p>
<p>Vou utilizar uma fun√ß√£o chamada <code>plot_pred_type_distribution()</code>,apresentada neste post de titulo: <a href="https://www.r-bloggers.com/illustrated-guide-to-roc-and-auc/">Illustrated Guide to ROC and AUC</a> e fiz uma pequena altera√ß√£o para que ela funcionasse para o dataset deste post . A fun√ß√£o adaptada pode ser encontrada <a href="https://github.com/gomesfellipe/functions/blob/master/plot_pred_type_distribution.R">neste link</a> no meu github e a fun√ß√£o original <a href="https://github.com/joyofdata/joyofdata-articles/blob/master/roc-auc/plot_pred_type_distribution.R">neste link do github do autor</a>.</p>
<div id="pacote-caret" class="section level2">
<h2>Pacote caret</h2>
<p>Basicamente o ajuste de todos os modelos envolveram o uso do pacote <code>caret</code> e muitos dos passos aqui foram baseados nas instru√ß√µes fornecidas no <a href="https://topepo.github.io/caret/index.html">livro do pacote</a>. O pacote facilita bastante o ajuste dos par√¢metros no ajuste de modelos.</p>
</div>
<div id="transformar-e-arrumar" class="section level2">
<h2>Transformar e arrumar</h2>
<p>Uma <a href="https://www.kaggle.com/kailex/tidy-xgboost-glmnet-text2vec-lsa">solu√ß√£o do kaggle</a> para o desafio <a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge">Toxic Comment Classification Challenge</a> me chamou aten√ß√£o, do qual o participante da competi√ß√£o criou colunas que sinalizassem os caracteres especiais de cada frase, utilizarei esta t√©cnica para o ajuste e novamente utilizarei o pacote de l√©xicos do apresentado no <a href="https://sillasgonzaga.github.io/2017-09-23-sensacionalista-pt01/">post do blog Paix√£o por dados</a></p>
<p>Veja a base transformada e arrumada:</p>
<pre class="r"><code># Ref: https://cfss.uchicago.edu/text_classification.html 
# https://www.r-bloggers.com/illustrated-guide-to-roc-and-auc/
devtools::source_url(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/plot_pred_type_distribution.R&quot;)

base &lt;- base %&gt;% 
  mutate(length = str_length(text),
         ncap = str_count(text, &quot;[A-Z]&quot;),
         ncap_len = ncap / length,
         nexcl = str_count(text, fixed(&quot;!&quot;)),
         nquest = str_count(text, fixed(&quot;?&quot;)),
         npunct = str_count(text, &quot;[[:punct:]]&quot;),
         nword = str_count(text, &quot;\\w+&quot;),
         nsymb = str_count(text, &quot;&amp;|@|#|\\$|%|\\*|\\^&quot;),
         nsmile = str_count(text, &quot;((?::|;|=)(?:-)?(?:\\)|D|P))&quot;),
         text = clean_tweets(text) %&gt;% enc2native() %&gt;% rm_accent())%&gt;%
  unnest_tokens(word, text) %&gt;% 
  anti_join(stop_words)%&gt;%
  group_by(book,line,length, ncap, ncap_len, nexcl, nquest, npunct, nword, nsymb, nsmile)%&gt;%
  summarise(text=paste(word,collapse = &quot; &quot;)) %&gt;% 
  select(text,everything())%T&gt;% 
  print()</code></pre>
<pre><code>## # A tibble: 7,995 x 12
## # Groups:   book, line, length, ncap, ncap_len, nexcl, nquest, npunct, nword,
## #   nsymb [7,995]
##    text  book   line length  ncap ncap_len nexcl nquest npunct nword nsymb
##    &lt;chr&gt; &lt;chr&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt;    &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;
##  1 isl ‚Ä¶ arg       1     NA     7  NA          0      0      6    24     1
##  2 pau ‚Ä¶ arg       2    108     6   0.0556     0      0      2    20     1
##  3 mess‚Ä¶ arg       3     NA    10  NA          0      0      3    24     1
##  4 minu‚Ä¶ arg       4     NA     2  NA          0      0      2    24     1
##  5 requ‚Ä¶ arg       5    129    23   0.178      0      0     15    21     1
##  6 bras‚Ä¶ arg       6     NA    11  NA          0      0     12    20     1
##  7 dupl‚Ä¶ arg       7    123    84   0.683      0      0      8    21     1
##  8 mess‚Ä¶ arg       8     NA    10  NA          0      0      3    24     1
##  9 mess‚Ä¶ arg       9     NA    10  NA          0      0      3    24     1
## 10 mess‚Ä¶ arg      10     NA    10  NA          0      0      3    24     1
## # ‚Ä¶ with 7,985 more rows, and 1 more variable: nsmile &lt;int&gt;</code></pre>
<p>Ap√≥s arrumar e transformar as informa√ß√µes que ser√£o utilizadas na classifica√ß√£o, ser√° criado um corpus sem a abordagem tidy para obter a matriz de documentos e termos, e depois utilizar a coluna de classifica√ß√£o, veja:</p>
<pre class="r"><code>library(tm)       #Pacote de para text mining
corpus &lt;- Corpus(VectorSource(base$text))

#Criando a matrix de termos:
book_dtm = DocumentTermMatrix(corpus, control = list(minWordLength=2,minDocFreq=3)) %&gt;% 
  weightTfIdf(normalize = T) %&gt;%    # Transforma√ß√£o tf-idf com pacote tm
  removeSparseTerms( sparse = .95)  # obtendo matriz esparsa com pacote tm

#Transformando em matrix, permitindo a manipulacao:
matrix = as.matrix(book_dtm)
dim(matrix)</code></pre>
<pre><code>## [1] 7995   18</code></pre>
<p>Pronto, agora j√° podemos juntar tudo em um data frame e separa em treino e teste para a classifica√ß√£o dos textos obtidos do twitter:</p>
<pre class="r"><code>#Criando a base de dados:
full=data.frame(cbind(
  base[,&quot;book&quot;],
  matrix,
  base[,-c(1:3)]
  )) %&gt;% na.omit()</code></pre>
</div>
<div id="treino-e-teste" class="section level2">
<h2>Treino e teste</h2>
<p>Ser√° utilizado tanto o m√©todo de hold-out e de cross-validation</p>
<pre class="r"><code>set.seed(825)
particao = sample(1:2,nrow(full), replace = T,prob = c(0.7,0.3))

train = full[particao==1,] 
test = full[particao==2,] 

library(caret)</code></pre>
</div>
<div id="ajustando-modelos" class="section level2">
<h2>Ajustando modelos</h2>
<div id="knn" class="section level3">
<h3>KNN</h3>
<p>√â uma t√©cnica de aprendizado baseado em inst√¢ncia, isto quer dizer que a classifica√ß√£o de uma observa√ß√£o com a classe desconhecida √© realizada a partir da compara√ß√£o com outras observa√ß√µes cada vez que uma observa√ß√£o √© apresentado ao modelo e tamb√©m √© conhecido como ‚Äúlazy evaluation,‚Äù j√° que um modelo n√£o √© induzido previamente.</p>
<p>Diversas medidas de dist√¢ncia podem ser utilizadas, utilizarei aqui a euclideana e al√©m disso a escolha do par√¢metro <span class="math inline">\(k\)</span> (de k vizinhos mais pr√≥ximos) deve ser feita com cuidado pois um <span class="math inline">\(k\)</span> pequeno pode expor o algor√≠timo a uma alta sensibilidade a um ru√≠do.</p>
<p>Utilizarei aqui o pacote <code>caret</code> como ferramenta para o ajuste deste modelo pois ela permite que eu configure que seja feita a valida√ß√£o cruzada em conjunto com a padroniza√ß√£o, pois esses complementos beneficiam no ajuste de modelos que calculam dist√¢ncias.</p>
<pre class="r"><code># knn -------
set.seed(825)
antes = Sys.time()
book_knn &lt;- train(book ~.,
                  data=train,
                 method = &quot;knn&quot;,
                 trControl = trainControl(method = &quot;cv&quot;,number = 10), # validacao cruzada
                 preProc = c(&quot;center&quot;, &quot;scale&quot;))                      
time_knn &lt;- Sys.time() - antes 
Sys.time() - antes</code></pre>
<pre><code>## Time difference of 2.465522 secs</code></pre>
<pre class="r"><code>plot(book_knn)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/knn-1.png" width="672" /></p>
<pre class="r"><code>previsao  = predict(book_knn, test)
confusionMatrix(previsao, factor(test$book))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction arg  br
##        arg 105   8
##        br    5 371
##                                          
##                Accuracy : 0.9734         
##                  95% CI : (0.955, 0.9858)
##     No Information Rate : 0.7751         
##     P-Value [Acc &gt; NIR] : &lt;2e-16         
##                                          
##                   Kappa : 0.9245         
##                                          
##  Mcnemar&#39;s Test P-Value : 0.5791         
##                                          
##             Sensitivity : 0.9545         
##             Specificity : 0.9789         
##          Pos Pred Value : 0.9292         
##          Neg Pred Value : 0.9867         
##              Prevalence : 0.2249         
##          Detection Rate : 0.2147         
##    Detection Prevalence : 0.2311         
##       Balanced Accuracy : 0.9667         
##                                          
##        &#39;Positive&#39; Class : arg            
## </code></pre>
<pre class="r"><code>df = cbind(fit = if_else(previsao==&quot;br&quot;,1,0), class = if_else(test$book==&quot;br&quot;,1,0)) %&gt;% as.data.frame()
plot_pred_type_distribution(df,0.5)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/knn-2.png" width="672" /></p>
<p>Como podemos ver, segundo a valida√ß√£o cruzada realizada com o pacote <code>caret</code>, o n√∫mero 5 de vizinhos mais pr√≥ximos foi o que apresentou o melhor resultado. Al√©m disso o modelo apresentou uma acur√°cia de 97,18% e isto parece bom dado que a sensibilidade (taxa de verdadeiros positivos) e a especificidade (taxa de verdadeiros negativos) foram altas tamb√©m, o que foi refor√ßado com o gr√°fico ilustrado da matriz de confus√£o.</p>
<p>O tempo computacional para o ajuste do modelo foi de:2.46385908126831 segundos</p>
</div>
<div id="random-forest" class="section level3">
<h3>Random Forest</h3>
<p>O modelo de Random Forest tem se tornado muito popular devido ao seu bom desempenho e pela sua alta capacidade de se adaptar aos dados. O modelo funciona atrav√©s da combina√ß√£o de v√°rias √°rvores de decis√µes e no seu ajuste alguns par√¢metros precisam ser levados em conta.</p>
<p>O par√¢metro que sera levado em conta para o ajuste ser√° apenas o <code>ntree</code>, que representa o n√∫mero de √°rvores ajustadas. Este par√¢metro deve ser escolhido com cuidado pois pode ser t√£o grande quanto voc√™ quiser e continua aumentando a precis√£o at√© certo ponto por√©m pode ser mais limitado pelo tempo computacional dispon√≠vel.</p>
<pre class="r"><code>set.seed(824)
# Random Forest
antes = Sys.time()
book_rf &lt;- train(book ~.,
                  data=train,
                     method = &quot;rf&quot;,trace=F,
                     ntree = 200,
                     trControl = trainControl(method = &quot;cv&quot;,number = 10))
time_rf &lt;- Sys.time() - antes 
Sys.time() - antes</code></pre>
<pre><code>## Time difference of 8.994044 secs</code></pre>
<pre class="r"><code>library(randomForest)
varImpPlot(book_rf$finalModel)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/rf-1.png" width="672" /></p>
<pre class="r"><code>previsao  = predict(book_rf, test)
confusionMatrix(previsao, factor(test$book))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction arg  br
##        arg 110   0
##        br    0 379
##                                      
##                Accuracy : 1          
##                  95% CI : (0.9925, 1)
##     No Information Rate : 0.7751     
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16  
##                                      
##                   Kappa : 1          
##                                      
##  Mcnemar&#39;s Test P-Value : NA         
##                                      
##             Sensitivity : 1.0000     
##             Specificity : 1.0000     
##          Pos Pred Value : 1.0000     
##          Neg Pred Value : 1.0000     
##              Prevalence : 0.2249     
##          Detection Rate : 0.2249     
##    Detection Prevalence : 0.2249     
##       Balanced Accuracy : 1.0000     
##                                      
##        &#39;Positive&#39; Class : arg        
## </code></pre>
<pre class="r"><code># https://www.r-bloggers.com/illustrated-guide-to-roc-and-auc/
df = cbind(fit = if_else(previsao==&quot;br&quot;,1,0), class = if_else(test$book==&quot;br&quot;,1,0)) %&gt;% as.data.frame()
plot_pred_type_distribution(df,0.5)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/rf-2.png" width="672" /></p>
<p>Segundo o gr√°fico de import√¢ncia, parece que as palavras ‚Äúbrasil,‚Äù ‚Äúargentina,‚Äù ‚Äúcopa‚Äù e ‚Äúmessi‚Äù foram as que apresentaram maior impacto do preditor (lembrando que essa medida n√£o √© um efeito espec√≠fico), o que mostra que a presen√ßa das palavras que estamos utilizando para classificar tiveram um impacto na classifica√ß√£o bastante superior aos demais.</p>
<p>Quanto a acur√°cia, o random forest apresentou valor um pouco maior do que o do algor√≠timo K-NN e al√©m disso apresentou altos valores para a sensibilidade (taxa de verdadeiros positivos) e a especificidade (taxa de verdadeiros negativos), o que foi refor√ßado com o gr√°fico ilustrado da matriz de confus√£o, por√©m o tempo computacional utilizado para ajustar este modelo foi muito maior, o que leva a questionar se esse pequeno aumento na taxa de acerto vale a pena aumentando tanto no tempo de processamento (outra alternativa seria diminuir o tamanho do n√∫mero de √°rvores para ver se melhoraria na qualidade do ajuste).</p>
<p>O tempo computacional para o ajuste do modelo foi de: 8.99299788475037 segundos</p>
</div>
<div id="naive-bayes" class="section level3">
<h3>Naive Bayes</h3>
<p>Este √© um algor√≠timo que trata-se de um classificador estat√≠stico baseado no <strong>Teorema de Bayes</strong> e recebe o nome de ing√™nuo (<em>naive</em>) porque pressup√µe que o valor de um atributo que exerce algum efeito sobre a distribui√ß√£o da vari√°vel resposta √© independente do efeito que outros atributos.</p>
<p>O c√°lculo para a classifica√ß√£o √© feito por meio do c√°lculo de probabilidades condicionais, ou seja, probabilidade de uma observa√ß√£o pertencer a cada classe dado os exemplares existentes no conjunto de dados usado para o treinamento.</p>
<pre class="r"><code># Naive Bayes ----
set.seed(825)
antes = Sys.time()
book_nb &lt;- train(book ~.,
                  data=train,
                 method= &quot;nb&quot;,
                 laplace =1,       
                 trControl = trainControl(method = &quot;cv&quot;,number = 10))
time_nb &lt;- Sys.time() - antes 
Sys.time() - antes</code></pre>
<pre><code>## Time difference of 7.141471 secs</code></pre>
<pre class="r"><code>previsao  = predict(book_nb, test)
confusionMatrix(previsao, factor(test$book))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction arg  br
##        arg 108   6
##        br    2 373
##                                          
##                Accuracy : 0.9836         
##                  95% CI : (0.968, 0.9929)
##     No Information Rate : 0.7751         
##     P-Value [Acc &gt; NIR] : &lt;2e-16         
##                                          
##                   Kappa : 0.9537         
##                                          
##  Mcnemar&#39;s Test P-Value : 0.2888         
##                                          
##             Sensitivity : 0.9818         
##             Specificity : 0.9842         
##          Pos Pred Value : 0.9474         
##          Neg Pred Value : 0.9947         
##              Prevalence : 0.2249         
##          Detection Rate : 0.2209         
##    Detection Prevalence : 0.2331         
##       Balanced Accuracy : 0.9830         
##                                          
##        &#39;Positive&#39; Class : arg            
## </code></pre>
<pre class="r"><code># https://www.r-bloggers.com/illustrated-guide-to-roc-and-auc/
df = cbind(fit = if_else(previsao==&quot;br&quot;,1,0), class = if_else(test$book==&quot;br&quot;,1,0)) %&gt;% as.data.frame()
plot_pred_type_distribution(df,0.5)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/nb-1.png" width="672" /></p>
<p>Apesar a aparente acur√°cia alta, o valor calculado para a especificidade (verdadeiros negativos) foi elevado o que aponta que o ajuste do modelo n√£o se apresentou de forma eficiente</p>
<p>O tempo computacional foi de 7.1403751373291 segundos</p>
</div>
<div id="glm---logit" class="section level3">
<h3>GLM - Logit</h3>
<p>Este √© um modelo estat√≠stico que j√° abordei aqui no blog no post sobre <a href="https://gomesfellipe.github.io/post/2018-05-26-smarteademachinelearning/smarteademachinelearning/">AED de forma r√°pida e um pouco de machine learning</a> e seguindo a recomenda√ß√£o do artigo da datacamp vejamos quais resultados obtemos com o ajuste deste modelo:</p>
<pre class="r"><code># Modelo log√≠stico ----
set.seed(825)
antes = Sys.time()
book_glm &lt;- train(book ~.,
                  data=train,
                  method = &quot;glm&quot;,                                         # modelo generalizado
                  family = binomial(link = &#39;logit&#39;),                      # Familia Binomial ligacao logit
                  trControl = trainControl(method = &quot;cv&quot;, number = 10))   # validacao cruzada
time_glm &lt;- Sys.time() - antes 
Sys.time() - antes</code></pre>
<pre><code>## Time difference of 1.378149 secs</code></pre>
<pre class="r"><code>library(ggfortify)

autoplot(book_glm$finalModel, which = 1:6, data = train,
         colour = &#39;book&#39;, label.size = 3,
         ncol = 3) + theme_classic()</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/glm-1.png" width="672" /></p>
<pre class="r"><code>previsao  = predict(book_glm, test)
confusionMatrix(previsao, factor(test$book))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction arg  br
##        arg 109   0
##        br    1 379
##                                           
##                Accuracy : 0.998           
##                  95% CI : (0.9887, 0.9999)
##     No Information Rate : 0.7751          
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.9941          
##                                           
##  Mcnemar&#39;s Test P-Value : 1               
##                                           
##             Sensitivity : 0.9909          
##             Specificity : 1.0000          
##          Pos Pred Value : 1.0000          
##          Neg Pred Value : 0.9974          
##              Prevalence : 0.2249          
##          Detection Rate : 0.2229          
##    Detection Prevalence : 0.2229          
##       Balanced Accuracy : 0.9955          
##                                           
##        &#39;Positive&#39; Class : arg             
## </code></pre>
<pre class="r"><code>df = cbind(fit = if_else(previsao==&quot;br&quot;,1,0), class = if_else(test$book==&quot;br&quot;,1,0)) %&gt;% as.data.frame()
plot_pred_type_distribution(df,0.5)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/glm-2.png" width="672" /></p>
</div>
</div>
</div>
<div id="comparando-modelos" class="section level1">
<h1>Comparando modelos</h1>
<p>Agora que temos 4 modelos ajustados e cada um apresentando resultados diferentes, vejamos qual deles seria o mais interessante para caso fosse necess√°rio recuperar a classifica√ß√£o dos termos pesquisados atrav√©s da API, veja a seguir um resumo das medidas obtidas:</p>
<pre class="r"><code># &quot;Dados esses modelos, podemos fazer declara√ß√µes estat√≠sticas sobre suas diferen√ßas de desempenho? Para fazer isso, primeiro coletamos os resultados de reamostragem usando resamples.&quot; - caret
resamps &lt;- resamples(list(knn = book_knn,
                          rf = book_rf,
                          nb = book_nb,
                          glm = book_glm)) 
summary(resamps)</code></pre>
<pre><code>## 
## Call:
## summary.resamples(object = resamps)
## 
## Models: knn, rf, nb, glm 
## Number of resamples: 10 
## 
## Accuracy 
##          Min.   1st Qu.    Median      Mean   3rd Qu. Max. NA&#39;s
## knn 0.9553571 0.9821824 0.9823009 0.9831305 0.9889381    1    0
## rf  0.9823009 1.0000000 1.0000000 0.9973451 1.0000000    1    0
## nb  0.9107143 0.9623894 0.9823009 0.9768726 1.0000000    1    0
## glm 0.9910714 0.9911504 1.0000000 0.9964523 1.0000000    1    0
## 
## Kappa 
##          Min.   1st Qu.    Median      Mean   3rd Qu. Max. NA&#39;s
## knn 0.8730734 0.9500663 0.9512773 0.9530901 0.9691458    1    0
## rf  0.9513351 1.0000000 1.0000000 0.9926689 1.0000000    1    0
## nb  0.7791798 0.8998204 0.9525409 0.9398109 1.0000000    1    0
## glm 0.9752868 0.9753544 1.0000000 0.9901350 1.0000000    1    0</code></pre>
<p>Como podemos ver, o modelo que apresentou a menor acur√°cia e o menor coeficiente kappa foi o Naive Bayes enquanto que o que apresentou as maiores medidas de qualidade do ajuste foi o modelo ajustado com o algor√≠timo Random Forest e tanto o modelo ajustado pelo algor√≠timo knn quanto o modelo linear generalizado com fun√ß√£o de liga√ß√£o ‚Äúlogit‚Äù tamb√©m apresentaram acur√°cia e coeficiente kappa pr√≥ximos do apresentado no ajuste do Random Forest.</p>
<p>Portanto, apesar dos ajustes, caso dois modelos n√£o apresentem diferen√ßa estatisticamente significante e o tempo computacional gasto para o ajuste de ambos for muito diferente pode ser que ser que tenhamos um modelo candidato para:</p>
<pre class="r"><code>c( knn= time_knn,rf = time_rf,nb = time_nb,glm = time_glm)</code></pre>
<pre><code>## Time differences in secs
##      knn       rf       nb      glm 
## 2.463859 8.992998 7.140375 1.377073</code></pre>
<p>O modelo linear generalizado foi o que apresentou o menor tempo computacional e foi o que apresentou o terceiro maior registro para os as medidas de qualidade do ajuste dos modelos, portanto esse modelo ser√° avaliado com mais cuidado em seguida para saber se ele ser√° o modelo selecionado</p>
<p><strong>Obs.:</strong> Sou suspeito para falar mas dentre esses modelos eu teria prefer√™ncia por este modelo de qualquer maneira por n√£o se tratar de uma ‚Äúcaixa preta,‚Äù da qual todos os efeitos de cada par√¢metro ajustado podem ser interpretado, al√©m de obter medidas como raz√µes de chance que ajudam bastante na compreens√£o dos dados.</p>
<p>Comparando de forma visual:</p>
<pre class="r"><code>splom(resamps)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<p>Assim fica mais claro o como o ajuste dos modelos Random Forest, K-NN e GLM se destacaram quando avaliados em rela√ß√£o a acur√°cia apresentada.</p>
<p>Vejamos a seguir como foi a distribui√ß√£o dessas medidas de acordo com cada modelo atrav√©s de boxplots:</p>
<pre class="r"><code>bwplot(resamps)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>Note que al√©m de apresentar os ajustes com menor acur√°cia (e elevada taxa de falsos negativos) o algor√≠timo Naive Bayes foi o que apresentou a maior varia√ß√£o interquartil das medidas de qualidade do ajuste do modelo.</p>
<p>Para finalizar a an√°lise visual vamos obter as diferen√ßas entre os modelos com a fun√ß√£o <code>diff()</code> e em seguida conferir de maneira visual o comportamento dessas informa√ß√µes:</p>
<pre class="r"><code>difValues &lt;- diff(resamps)

# plot:
bwplot(difValues)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>Observe que tanto o modelo log√≠stico quando o ajuste com o algor√≠timo K-NN apresentaram valores muito pr√≥ximos dos valores do ajuste do Random Forest e como j√° vimos o Random Forest foi o modelo que levou maior tempo computacional para ser ajustado, portanto vamos conferir a seguir se existe diferen√ßa estatisticamente significante entre os valores obtidos atrav√©s de cada um dos ajustes e decidir qual dos modelos se apresentou de maneira mais adequada para nosso caso:</p>
<pre class="r"><code>resamps$values %&gt;% 
  select_if(is.numeric) %&gt;% 
  purrr::map(function(x) shapiro.test(x))</code></pre>
<pre><code>## $`knn~Accuracy`
## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.87602, p-value = 0.1174
## 
## 
## $`knn~Kappa`
## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.87418, p-value = 0.1118
## 
## 
## $`rf~Accuracy`
## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.53165, p-value = 8.564e-06
## 
## 
## $`rf~Kappa`
## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.53234, p-value = 8.727e-06
## 
## 
## $`nb~Accuracy`
## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.80077, p-value = 0.01482
## 
## 
## $`nb~Kappa`
## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.81793, p-value = 0.02392
## 
## 
## $`glm~Accuracy`
## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.6429, p-value = 0.0001803
## 
## 
## $`glm~Kappa`
## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.64123, p-value = 0.0001722</code></pre>
<p>Como a hip√≥tese de normalidade n√£o foi rejeitada para nenhuma das amostras de acur√°cias registradas, vejamos se existe diferen√ßa estatisticamente significante entre as m√©dias dessas medidas de qualidade para cada modelo:</p>
<pre class="r"><code>t.test(resamps$values$`rf~Accuracy`,resamps$values$`knn~Accuracy`, paired = T)  </code></pre>
<pre><code>## 
##  Paired t-test
## 
## data:  resamps$values$`rf~Accuracy` and resamps$values$`knn~Accuracy`
## t = 3.9961, df = 9, p-value = 0.003129
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.0061678 0.0222614
## sample estimates:
## mean of the differences 
##               0.0142146</code></pre>
<p>Rejeita a hip√≥tese de que as m√©dias das acur√°cias calculadas para o ajuste do algor√≠timo Random Forest e K-NN foram iguais</p>
<pre class="r"><code>t.test(resamps$values$`rf~Accuracy`,resamps$values$`glm~Accuracy`, paired = T)  </code></pre>
<pre><code>## 
##  Paired t-test
## 
## data:  resamps$values$`rf~Accuracy` and resamps$values$`glm~Accuracy`
## t = 0.43326, df = 9, p-value = 0.675
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.003768926  0.005554640
## sample estimates:
## mean of the differences 
##            0.0008928571</code></pre>
<p>Novamente, rejeita-se a hip√≥tese de que as m√©dias das acur√°cias calculadas para o ajuste do algor√≠timo Random Forest e do modelo de log√≠stico foram iguais</p>
<pre class="r"><code>t.test(resamps$values$`knn~Accuracy`,resamps$values$`glm~Accuracy`, paired = T)</code></pre>
<pre><code>## 
##  Paired t-test
## 
## data:  resamps$values$`knn~Accuracy` and resamps$values$`glm~Accuracy`
## t = -4.0077, df = 9, p-value = 0.003074
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.020841197 -0.005802292
## sample estimates:
## mean of the differences 
##             -0.01332174</code></pre>
<p>J√° para a compara√ß√£o entre as m√©dias das acur√°cias calculadas para o algor√≠timo K-NN e para o modelo log√≠stico n√£o houve evid√™ncias estat√≠sticas para se rejeitas a hip√≥tese de que ambas as m√©dias s√£o iguais, o que nos sugere o modelo log√≠stico como o segundo melhor candidato como modelo de classifica√ß√£o para este problema com estes dados.</p>
<p>Ent√£o a escolha ficar√° a crit√©rio do que √© mais importante. Caso o tempo computacional fosse uma medida que tivesse mais import√¢ncia do que a pequena superioridade de acur√°cia apresentada pelo algor√≠timo Random Forest, escolheria o modelo log√≠stico, por√©m como neste caso os 7.61592507362366 segundos a mais para ajustar o modelo n√£o fazem diferen√ßa para mim, fico com o modelo Random Forest.</p>
<p>Este post tr√°s alguns dos conceitos que venho estudado e existem muitos t√≥picos apresentados aqui que podem (e devem) ser estudados com mais profundidade, espero que tenha gostado!</p>
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias</h1>
<p>obs.: links mensionados no corpo do texto</p>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-caret" class="csl-entry">
Kuhn, Max. 2018. <em>The Caret Package</em>. <a href="https://topepo.github.io/caret/index.html">https://topepo.github.io/caret/index.html</a>.
</div>
<div id="ref-tidytext" class="csl-entry">
Silge; Robinson, Julia; David. 2018. <em>Text Mining with R</em>. <em>A Tidy Approach</em>. <a href="https://www.tidytextmining.com/">https://www.tidytextmining.com/</a>.
</div>
<div id="ref-miner" class="csl-entry">
Silva; Peres; Boscarioli, Leandro Augusto; Sarajane Marques; Clodis. 2016. <em>Introdu√ß√£o √† Minera√ß√£o de Dados</em>. <em>Com Aplica√ß√µes Em R</em>. Vol. 3. Elsevier Editora Ltda.
</div>
</div>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml/">Brasil x Argentina, tidytext e Machine Learning</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Analise Explorat√≥ria</category>
      <category>Aprendizado N√£o Supervisionado</category>
      <category>Data mining</category>
      <category>Estatistica</category>
      <category>Machine Learning</category>
      <category>Modelagem Estatistica</category>
      <category>Pr√°tica</category>
      <category>R</category>
      <category>Text Mining</category>
      <category>An√°lise de Sentimentos</category>
      <category domain="tag">Data Mining</category>
      <category domain="tag">Estatistica</category>
      <category domain="tag">gomesfellipe</category>
      <category domain="tag">twitter</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">Pr√°tica</category>
      <category domain="tag">R</category>
      <category domain="tag">text mining</category>
    </item>
    <item>
      <title>AED de forma r√°pida e um pouco de Machine Learning</title>
      <link>https://gomesfellipe.github.io/post/2018-05-26-smarteademachinelearning/smarteademachinelearning/</link>
      <pubDate>Sat, 26 May 2018 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2018-05-26-smarteademachinelearning/smarteademachinelearning/</guid>
      <description>Veja como √© poss√≠vel realizar a AED de forma muito r√°pida com o pacote SmartEAD, al√©m de uma breve aplica√ß√£o de t√©cnicas de machine learning e estat√≠stica para ilustrar alguns poss√≠veis cen√°rios da analise da dados</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/pagedtable/css/pagedtable.css" rel="stylesheet" />
<script src="/rmarkdown-libs/pagedtable/js/pagedtable.js"></script>


<style>
  .col2 {
    columns: 2 200px;         /* number of columns and width in pixels*/
    -webkit-columns: 2 200px; /* chrome, safari */
    -moz-columns: 2 200px;    /* firefox */
  }
  .col3 {
    columns: 3 100px;
    -webkit-columns: 3 100px;
    -moz-columns: 3 100px;
  }
</style>
<!-- Resumo: Neste post mostro como √© poss√≠vel realizar a AED de forma muito r√°pida com o pacote SmartEAD, e aplico algumas t√©cnicas de machine learning e estat√≠stica para ilustrar alguns poss√≠veis cen√°rios-->
<div id="a-an√°lise-explorat√≥ria-dos-dados" class="section level1">
<h1>A an√°lise explorat√≥ria dos dados</h1>
<div class="col2">
<p>A an√°lise explorat√≥ria dos dados (AED) foi um termo que ganhou bastante popularidade quando Tukey publicou o livro Exploratory Data Analysis em 1977 que tratava uma ‚Äúbusca por conhecimento antes da an√°lise de dados de fato‚Äù. Ocorre quando busca-se obter informa√ß√µes ocultas sobre os dados, tais como: varia√ß√£o, anomalias, distribui√ß√£o, tend√™ncias, padr√µes e rela√ß√µes</p>
<p>Ao iniciar uma an√°lise de dados, come√ßamos pela AED para a partir dai decidir como buscar qual solu√ß√£o para o problema. √â importante frisar que a AED e a constru√ß√£o de gr√°ficos <strong>n√£o</strong> s√£o a mesma coisa, mesmo a AED sendo altamente baseada em produ√ß√£o de gr√°ficos como de dispers√£o, histogramas, boxplots etc.</p>
<p>Por vezes a AED no R pode envolver a produ√ß√£o de longos scripts utilizando fun√ß√µes como as do pacote <code>ggplot2</code> e mesmo sabendo que desejamos sempre criar o gr√°fico de maneira mais informativa e atraente poss√≠vel, as vezes precisamos ter uma no√ß√£o geral dos dados de forma r√°pida, n√£o necessariamente t√£o detalhada e customizada de cara.</p>
<p>A vezes queremos apenas ter uma primeira impress√£o dos dados e em seguida pensar em quais os gr√°ficos mais se adequariam para a entrega dos resultados que mesmo as fun√ß√µes base do R dependendo do caso tamb√©m envolvem a confec√ß√£o de longos scripts.</p>
<p>Existem pacotes que auxiliam na hora de se fazer uma r√°pida an√°lise explorat√≥ria, como o <a href="https://github.com/ropenscilabs/skimr">skimr</a> e o <a href="https://github.com/boxuancui/DataExplorer">DataExplorer</a>. Por√©m estava pesquisando de existiam mais op√ß√µes para uma r√°pida abordagem de AED e me deparei com esta <a href="https://cran.r-project.org/web/packages/SmartEDA/vignettes/Report_r1.html">vinheta</a>, por Dayanand, Kiran, Ravi.</p>
<p>Essa vinheta apresenta o pacote <a href="https://cran.r-project.org/web/packages/SmartEDA"><code>SmartEAD</code></a> que tr√°s uma s√©rie de fun√ß√µes que auxiliam na AED de forma bem pr√°tica. O pacote est√° dispon√≠vel no CRAN.</p>
<p>Para testar o pacote foi utilizada uma base de dados do artigo <a href="http://people.stern.nyu.edu/wgreene/Lugano2013/Fair-ExtramaritalAffairs.pdf">A Theory of Extramarital Affairs</a>, publicado pela <a href="http://www.jstor.org/publisher/ucpress">The University of Chicago Press</a>.</p>
<p>Gostei tanto da proposta do pacote que resolvi preparar este post que conta com a explana√ß√£o de alguns t√≥picos apresentados pelo autor, algumas explica√ß√µes da teoria estat√≠stica apresentada na an√°lise descritiva e explorat√≥ria dos dados e al√©m da aplica√ß√£o de algumas t√©cnicas estat√≠sticas e de machine learning para o entendimento da base de dados.</p>
</div>
<p></br></p>
</div>
<div id="smarteda" class="section level1">
<h1>SmartEDA</h1>
<p>Como ele pode ajud√°-lo a criar uma an√°lise de dados explorat√≥ria? O <code>SmartEDA</code> inclui v√°rias fun√ß√µes personalizadas para executar uma an√°lise explorat√≥ria inicial em qualquer dado de entrada. A sa√≠da gerada pode ser obtida em formato resumido e gr√°fico e os resultados tamb√©m podem ser exportados como relat√≥rios.</p>
<p>O pacote SmartEDA ajuda a construir uma boa base de compreens√£o de dados, algumas de suas funcionalidades s√£o:</p>
<ul>
<li>O pacote SmartEDA far√° com que voc√™ seja capaz de aplicar diferentes tipos de EDA sem ter que lembre-se dos diferentes nomes dos pacotes R e escrever longos scripts R com esfor√ßo manual para preparar o relat√≥rio da EDA, permitindo o entendimento dos dados de maneira mais r√°pida</li>
<li>N√£o h√° necessidade de categorizar as vari√°veis em caractere, num√©rico, fator etc. As fun√ß√µes do SmartEDA categorizam automaticamente todos os recursos no tipo de dados correto (caractere, num√©rico, fator etc.) com base nos dados de entrada.</li>
</ul>
<p>O pacote SmartEDA ajuda a obter a an√°lise completa dos dados explorat√≥rios apenas executando a fun√ß√£o em vez de escrever um longo c√≥digo r.</p>
<div id="carregando-o-pacote" class="section level2">
<h2>Carregando o pacote:</h2>
<pre class="r"><code># install.packages(&quot;SmartEDA&quot;)
library(&quot;SmartEDA&quot;)</code></pre>
<p>outros pactes que ser√£o utilizados no post (incluindo um script com algumas fun√ß√µes, que estar√° dispon√≠vel no meu github <a href="https://github.com/gomesfellipe/gomesfellipe.github.io/blob/master/post/2018-05-26-smarteademachinelearning/functions.R">neste link</a>).</p>
<pre class="r"><code>library(knitr)        # Para tabelas interativas
library(DT)           # Para tabelas interativas
library(dplyr)        # Para manipulacao de dados
library(plotly)       # Para gerar uma tabela
library(psych)        # para an√°lise fatorial
source(&quot;functions.R&quot;) # script com funcoes customizadas</code></pre>
<div id="base-de-dados-utilizada" class="section level3">
<h3>Base de dados utilizada:</h3>
<div class="col2">
<p>Estava √† procura de uma base de dados para testar as funcionalidades do pacote <code>SmartEAD</code> quando um colega de trabalho me mostrou um artigo chamado <a href="http://people.stern.nyu.edu/wgreene/Lugano2013/Fair-ExtramaritalAffairs.pdf">A Theory of Extramarital Affairs</a>, publicado pela <a href="http://www.jstor.org/publisher/ucpress">The University of Chicago Press</a>. Neste artigo √© desenvolvido um <a href="https://en.wikipedia.org/wiki/Tobit_model">modelo pelo estimador de Tobit</a> que explica a aloca√ß√£o de um tempo do indiv√≠duo entre o trabalho e dois tipos de atividades de lazer: tempo passou com o c√¥njuge e tempo gasto com o amante.</p>
<p>N√£o conhecia o modelo proposto e em uma r√°pida pesquisa no Google notei que alguns dos dados utilizados nesse artigo est√£o dispon√≠veis no pacote <a href="ftp://cran.r-project.org/pub/R/web/packages/AER">AER</a> de Econometria Aplicada com R, que cont√©m fun√ß√µes, conjuntos de dados, exemplos, demonstra√ß√µes e vinhetas para o livro <a href="http://jrsyzx.njau.edu.cn/__local/C/94/F1/35C7CC5EDA214D4AAE7FE2BA0FD_0D3DFF32_3CDD40.pdf?e=.pdf">Applied Econometrics with R</a> e como esses dados j√° foram tratados e est√£o ‚Äúprontos para an√°lise‚Äù, resolvi usar essa amostra pela conveni√™ncia.</p>
<p>Portanto farei aqui uma an√°lise explorat√≥ria e ao final de cada caso (<em>sem vari√°vel reposta</em>, <em>com vari√°vel resposta num√©rica</em> e <em>com vari√°vel resposta bin√°ria</em>), para ter uma breve intui√ß√£o de como se comportam os dados irei primeiro utilizar um <em>algor√≠timo de machine learning n√£o supervisionado</em> para o agrupamento das observa√ß√µes (sem considerar q j√° conhecemos a vari√°vel resposta), depois ajustar um* modelo de regress√£o linear simples* considerando a vari√°vel resposta como num√©rica e por fim o ajuste de um <em>algor√≠timo de machine learning supervisonado de classifica√ß√£o</em> ap√≥s discretizar a vari√°vel resposta.</p>
<p>A base de dados pode ser conferida a seguir:</p>
</div>
<pre class="r"><code>library(AER)
data(Affairs)
Affairs %&gt;% rmarkdown::paged_table()</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["affairs"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["gender"],"name":[2],"type":["fct"],"align":["left"]},{"label":["age"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["yearsmarried"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["children"],"name":[5],"type":["fct"],"align":["left"]},{"label":["religiousness"],"name":[6],"type":["int"],"align":["right"]},{"label":["education"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["occupation"],"name":[8],"type":["int"],"align":["right"]},{"label":["rating"],"name":[9],"type":["int"],"align":["right"]}],"data":[{"1":"0","2":"male","3":"37.0","4":"10.000","5":"no","6":"3","7":"18","8":"7","9":"4","_rn_":"4"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"no","6":"4","7":"14","8":"6","9":"4","_rn_":"5"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"1","7":"12","8":"1","9":"4","_rn_":"11"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"5","7":"18","8":"6","9":"5","_rn_":"16"},{"1":"0","2":"male","3":"22.0","4":"0.750","5":"no","6":"2","7":"17","8":"6","9":"3","_rn_":"23"},{"1":"0","2":"female","3":"32.0","4":"1.500","5":"no","6":"2","7":"17","8":"5","9":"5","_rn_":"29"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"2","7":"12","8":"1","9":"3","_rn_":"44"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"2","7":"14","8":"4","9":"4","_rn_":"45"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"4","7":"16","8":"1","9":"2","_rn_":"47"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"no","6":"4","7":"14","8":"4","9":"5","_rn_":"49"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"2","7":"20","8":"7","9":"2","_rn_":"50"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"4","7":"18","8":"6","9":"4","_rn_":"55"},{"1":"0","2":"male","3":"47.0","4":"15.000","5":"yes","6":"5","7":"17","8":"6","9":"4","_rn_":"64"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"17","8":"5","9":"4","_rn_":"80"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"no","6":"4","7":"14","8":"5","9":"4","_rn_":"86"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"1","7":"17","8":"5","9":"5","_rn_":"93"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"2","7":"18","8":"4","9":"3","_rn_":"108"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"3","7":"16","8":"5","9":"4","_rn_":"114"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"16","8":"5","9":"5","_rn_":"115"},{"1":"0","2":"female","3":"27.0","4":"10.000","5":"yes","6":"2","7":"14","8":"1","9":"5","_rn_":"116"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"16","8":"5","9":"5","_rn_":"123"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"16","8":"5","9":"5","_rn_":"127"},{"1":"0","2":"female","3":"27.0","4":"10.000","5":"yes","6":"4","7":"16","8":"5","9":"4","_rn_":"129"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"3","7":"14","8":"1","9":"5","_rn_":"134"},{"1":"0","2":"male","3":"37.0","4":"4.000","5":"yes","6":"2","7":"20","8":"6","9":"4","_rn_":"137"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"18","8":"5","9":"5","_rn_":"139"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"no","6":"4","7":"16","8":"1","9":"5","_rn_":"147"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"5","7":"20","8":"6","9":"4","_rn_":"151"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"3","7":"16","8":"5","9":"5","_rn_":"153"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"3","7":"17","8":"5","9":"4","_rn_":"155"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"3","_rn_":"162"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"3","7":"16","8":"5","9":"5","_rn_":"163"},{"1":"0","2":"male","3":"27.0","4":"0.417","5":"no","6":"4","7":"17","8":"6","9":"4","_rn_":"165"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"5","7":"14","8":"5","9":"4","_rn_":"168"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"yes","6":"1","7":"18","8":"6","9":"4","_rn_":"170"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"4","7":"16","8":"5","9":"3","_rn_":"172"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"3","7":"12","8":"1","9":"4","_rn_":"184"},{"1":"0","2":"female","3":"22.0","4":"4.000","5":"no","6":"4","7":"17","8":"5","9":"5","_rn_":"187"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"yes","6":"1","7":"14","8":"3","9":"5","_rn_":"192"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"3","7":"16","8":"1","9":"5","_rn_":"194"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"yes","6":"5","7":"20","8":"6","9":"5","_rn_":"210"},{"1":"0","2":"male","3":"52.0","4":"15.000","5":"yes","6":"5","7":"18","8":"6","9":"3","_rn_":"217"},{"1":"0","2":"female","3":"22.0","4":"0.417","5":"no","6":"5","7":"14","8":"1","9":"4","_rn_":"220"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"2","7":"18","8":"6","9":"1","_rn_":"224"},{"1":"0","2":"female","3":"32.0","4":"7.000","5":"yes","6":"5","7":"17","8":"5","9":"3","_rn_":"227"},{"1":"0","2":"male","3":"22.0","4":"4.000","5":"no","6":"3","7":"16","8":"5","9":"5","_rn_":"228"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"18","8":"6","9":"5","_rn_":"239"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"2","7":"18","8":"5","9":"4","_rn_":"241"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"yes","6":"4","7":"16","8":"3","9":"5","_rn_":"245"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"2","7":"20","8":"6","9":"4","_rn_":"249"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"5","7":"14","8":"3","9":"5","_rn_":"262"},{"1":"0","2":"male","3":"32.0","4":"7.000","5":"yes","6":"2","7":"20","8":"6","9":"4","_rn_":"265"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"5","7":"20","8":"6","9":"5","_rn_":"267"},{"1":"0","2":"male","3":"27.0","4":"10.000","5":"yes","6":"4","7":"20","8":"6","9":"4","_rn_":"269"},{"1":"0","2":"male","3":"22.0","4":"4.000","5":"no","6":"1","7":"18","8":"5","9":"5","_rn_":"271"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"14","8":"3","9":"1","_rn_":"277"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"yes","6":"5","7":"16","8":"4","9":"4","_rn_":"290"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"17","8":"1","9":"5","_rn_":"292"},{"1":"0","2":"female","3":"27.0","4":"0.750","5":"no","6":"4","7":"17","8":"5","9":"4","_rn_":"293"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"yes","6":"4","7":"20","8":"6","9":"4","_rn_":"295"},{"1":"0","2":"female","3":"47.0","4":"15.000","5":"yes","6":"5","7":"14","8":"7","9":"2","_rn_":"299"},{"1":"0","2":"male","3":"37.0","4":"10.000","5":"yes","6":"3","7":"20","8":"6","9":"4","_rn_":"320"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"2","7":"16","8":"5","9":"5","_rn_":"321"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"no","6":"2","7":"18","8":"4","9":"5","_rn_":"324"},{"1":"0","2":"male","3":"32.0","4":"7.000","5":"no","6":"4","7":"20","8":"6","9":"4","_rn_":"334"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"2","7":"17","8":"3","9":"5","_rn_":"351"},{"1":"0","2":"male","3":"37.0","4":"10.000","5":"yes","6":"4","7":"20","8":"6","9":"4","_rn_":"355"},{"1":"0","2":"female","3":"47.0","4":"15.000","5":"yes","6":"3","7":"17","8":"6","9":"5","_rn_":"361"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"5","7":"16","8":"5","9":"5","_rn_":"362"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"no","6":"2","7":"16","8":"6","9":"4","_rn_":"366"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"no","6":"3","7":"17","8":"5","9":"5","_rn_":"370"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"5","7":"14","8":"4","9":"5","_rn_":"374"},{"1":"0","2":"female","3":"22.0","4":"0.125","5":"no","6":"2","7":"12","8":"5","9":"5","_rn_":"378"},{"1":"0","2":"male","3":"47.0","4":"15.000","5":"yes","6":"4","7":"14","8":"4","9":"3","_rn_":"381"},{"1":"0","2":"male","3":"32.0","4":"15.000","5":"yes","6":"1","7":"14","8":"5","9":"5","_rn_":"382"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"yes","6":"4","7":"16","8":"5","9":"5","_rn_":"383"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"yes","6":"3","7":"16","8":"5","9":"5","_rn_":"384"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"3","7":"17","8":"6","9":"5","_rn_":"400"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"3","7":"16","8":"5","9":"5","_rn_":"403"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"2","7":"14","8":"7","9":"2","_rn_":"409"},{"1":"0","2":"male","3":"17.5","4":"1.500","5":"yes","6":"3","7":"18","8":"6","9":"5","_rn_":"412"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"5","_rn_":"413"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"2","7":"16","8":"3","9":"4","_rn_":"416"},{"1":"0","2":"male","3":"42.0","4":"4.000","5":"no","6":"4","7":"17","8":"3","9":"3","_rn_":"418"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"yes","6":"4","7":"12","8":"1","9":"5","_rn_":"422"},{"1":"0","2":"female","3":"22.0","4":"0.417","5":"no","6":"1","7":"17","8":"6","9":"4","_rn_":"435"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"4","7":"17","8":"5","9":"5","_rn_":"439"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"no","6":"3","7":"18","8":"5","9":"2","_rn_":"445"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"yes","6":"3","7":"14","8":"1","9":"5","_rn_":"447"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"3","7":"14","8":"1","9":"4","_rn_":"448"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"4","7":"14","8":"3","9":"4","_rn_":"449"},{"1":"0","2":"male","3":"37.0","4":"10.000","5":"yes","6":"2","7":"14","8":"5","9":"3","_rn_":"478"},{"1":"0","2":"male","3":"37.0","4":"10.000","5":"yes","6":"4","7":"16","8":"5","9":"4","_rn_":"482"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"5","7":"20","8":"5","9":"3","_rn_":"486"},{"1":"0","2":"male","3":"27.0","4":"0.417","5":"no","6":"1","7":"16","8":"3","9":"4","_rn_":"489"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"5","7":"14","8":"1","9":"5","_rn_":"490"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"3","7":"16","8":"6","9":"1","_rn_":"491"},{"1":"0","2":"male","3":"37.0","4":"10.000","5":"yes","6":"1","7":"16","8":"6","9":"4","_rn_":"492"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"3","7":"17","8":"5","9":"5","_rn_":"503"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"5","_rn_":"508"},{"1":"0","2":"female","3":"27.0","4":"10.000","5":"yes","6":"5","7":"14","8":"1","9":"5","_rn_":"509"},{"1":"0","2":"male","3":"37.0","4":"10.000","5":"yes","6":"2","7":"18","8":"6","9":"4","_rn_":"512"},{"1":"0","2":"female","3":"22.0","4":"0.125","5":"no","6":"4","7":"12","8":"4","9":"5","_rn_":"515"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"5","7":"20","8":"6","9":"5","_rn_":"517"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"18","8":"6","9":"4","_rn_":"532"},{"1":"0","2":"male","3":"22.0","4":"4.000","5":"yes","6":"4","7":"14","8":"6","9":"4","_rn_":"533"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"yes","6":"4","7":"18","8":"5","9":"4","_rn_":"535"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"4","7":"20","8":"5","9":"4","_rn_":"537"},{"1":"0","2":"male","3":"32.0","4":"15.000","5":"yes","6":"3","7":"14","8":"6","9":"3","_rn_":"538"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"14","8":"5","9":"4","_rn_":"543"},{"1":"0","2":"female","3":"32.0","4":"7.000","5":"yes","6":"4","7":"17","8":"1","9":"5","_rn_":"547"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"17","8":"6","9":"5","_rn_":"550"},{"1":"0","2":"female","3":"32.0","4":"1.500","5":"no","6":"5","7":"18","8":"5","9":"5","_rn_":"558"},{"1":"0","2":"male","3":"42.0","4":"10.000","5":"yes","6":"5","7":"20","8":"7","9":"4","_rn_":"571"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"no","6":"3","7":"16","8":"5","9":"4","_rn_":"578"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"no","6":"4","7":"20","8":"6","9":"5","_rn_":"583"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"14","8":"3","9":"2","_rn_":"586"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"no","6":"5","7":"18","8":"6","9":"4","_rn_":"594"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"4","7":"16","8":"1","9":"5","_rn_":"597"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"12","8":"2","9":"4","_rn_":"602"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"2","7":"16","8":"2","9":"5","_rn_":"603"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"5","7":"18","8":"5","9":"4","_rn_":"604"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"4","7":"17","8":"5","9":"3","_rn_":"612"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"2","7":"16","8":"1","9":"2","_rn_":"613"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"3","7":"16","8":"5","9":"5","_rn_":"621"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"5","7":"20","8":"6","9":"5","_rn_":"627"},{"1":"0","2":"female","3":"22.0","4":"0.125","5":"no","6":"2","7":"14","8":"4","9":"5","_rn_":"630"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"no","6":"4","7":"16","8":"5","9":"5","_rn_":"631"},{"1":"0","2":"male","3":"32.0","4":"1.500","5":"no","6":"2","7":"18","8":"6","9":"5","_rn_":"632"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"no","6":"2","7":"17","8":"6","9":"5","_rn_":"639"},{"1":"0","2":"female","3":"27.0","4":"10.000","5":"yes","6":"4","7":"16","8":"1","9":"3","_rn_":"645"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"4","7":"18","8":"6","9":"5","_rn_":"647"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"no","6":"2","7":"16","8":"6","9":"5","_rn_":"648"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"no","6":"2","7":"18","8":"6","9":"3","_rn_":"651"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"3","7":"14","8":"5","9":"3","_rn_":"655"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"3","7":"18","8":"5","9":"4","_rn_":"667"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"2","7":"18","8":"6","9":"5","_rn_":"670"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"2","7":"16","8":"1","9":"4","_rn_":"671"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"4","7":"20","8":"5","9":"5","_rn_":"673"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"no","6":"1","7":"20","8":"5","9":"4","_rn_":"701"},{"1":"0","2":"female","3":"27.0","4":"10.000","5":"yes","6":"2","7":"12","8":"1","9":"4","_rn_":"705"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"5","7":"18","8":"6","9":"4","_rn_":"706"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"yes","6":"5","7":"12","8":"5","9":"3","_rn_":"709"},{"1":"0","2":"male","3":"52.0","4":"15.000","5":"yes","6":"2","7":"18","8":"5","9":"4","_rn_":"717"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"no","6":"3","7":"20","8":"6","9":"3","_rn_":"719"},{"1":"0","2":"male","3":"37.0","4":"4.000","5":"yes","6":"1","7":"18","8":"5","9":"4","_rn_":"723"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"4","7":"14","8":"5","9":"4","_rn_":"724"},{"1":"0","2":"female","3":"52.0","4":"15.000","5":"yes","6":"5","7":"12","8":"1","9":"3","_rn_":"726"},{"1":"0","2":"female","3":"57.0","4":"15.000","5":"yes","6":"4","7":"16","8":"6","9":"4","_rn_":"734"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"yes","6":"1","7":"16","8":"5","9":"4","_rn_":"735"},{"1":"0","2":"male","3":"37.0","4":"7.000","5":"yes","6":"4","7":"20","8":"6","9":"3","_rn_":"736"},{"1":"0","2":"male","3":"22.0","4":"0.750","5":"no","6":"2","7":"14","8":"4","9":"3","_rn_":"737"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"yes","6":"2","7":"18","8":"5","9":"3","_rn_":"739"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"3","_rn_":"743"},{"1":"0","2":"male","3":"22.0","4":"0.750","5":"yes","6":"2","7":"14","8":"4","9":"3","_rn_":"745"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"3","_rn_":"747"},{"1":"0","2":"female","3":"52.0","4":"15.000","5":"yes","6":"5","7":"17","8":"1","9":"1","_rn_":"751"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"14","8":"1","9":"2","_rn_":"752"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"yes","6":"4","7":"14","8":"5","9":"3","_rn_":"754"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"yes","6":"2","7":"16","8":"5","9":"5","_rn_":"760"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"2","7":"18","8":"6","9":"5","_rn_":"763"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"2","7":"18","8":"5","9":"5","_rn_":"774"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"5","7":"18","8":"6","9":"5","_rn_":"776"},{"1":"0","2":"female","3":"47.0","4":"15.000","5":"yes","6":"5","7":"12","8":"5","9":"4","_rn_":"779"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"3","7":"17","8":"1","9":"4","_rn_":"784"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"yes","6":"4","7":"17","8":"1","9":"2","_rn_":"788"},{"1":"0","2":"female","3":"57.0","4":"15.000","5":"yes","6":"2","7":"18","8":"5","9":"2","_rn_":"794"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"4","7":"14","8":"5","9":"4","_rn_":"795"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"3","7":"14","8":"3","9":"4","_rn_":"798"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"4","7":"9","8":"2","9":"2","_rn_":"800"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"5","_rn_":"803"},{"1":"0","2":"female","3":"22.0","4":"0.125","5":"no","6":"4","7":"14","8":"4","9":"5","_rn_":"807"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"4","7":"14","8":"1","9":"5","_rn_":"812"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"3","7":"18","8":"5","9":"4","_rn_":"820"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"no","6":"2","7":"18","8":"6","9":"5","_rn_":"823"},{"1":"0","2":"male","3":"32.0","4":"0.125","5":"yes","6":"2","7":"18","8":"5","9":"2","_rn_":"830"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"no","6":"3","7":"16","8":"5","9":"4","_rn_":"843"},{"1":"0","2":"female","3":"27.0","4":"10.000","5":"yes","6":"2","7":"16","8":"1","9":"4","_rn_":"848"},{"1":"0","2":"female","3":"32.0","4":"7.000","5":"yes","6":"4","7":"16","8":"1","9":"3","_rn_":"851"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"14","8":"5","9":"4","_rn_":"854"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"5","7":"17","8":"6","9":"2","_rn_":"856"},{"1":"0","2":"male","3":"32.0","4":"1.500","5":"yes","6":"4","7":"14","8":"6","9":"5","_rn_":"857"},{"1":"0","2":"female","3":"32.0","4":"4.000","5":"yes","6":"3","7":"17","8":"5","9":"3","_rn_":"859"},{"1":"0","2":"female","3":"37.0","4":"7.000","5":"no","6":"4","7":"18","8":"5","9":"5","_rn_":"863"},{"1":"0","2":"female","3":"22.0","4":"0.417","5":"yes","6":"3","7":"14","8":"3","9":"5","_rn_":"865"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"14","8":"1","9":"5","_rn_":"867"},{"1":"0","2":"male","3":"27.0","4":"0.750","5":"no","6":"3","7":"16","8":"5","9":"5","_rn_":"870"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"2","7":"20","8":"5","9":"5","_rn_":"873"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"yes","6":"4","7":"16","8":"4","9":"5","_rn_":"875"},{"1":"0","2":"male","3":"32.0","4":"15.000","5":"yes","6":"1","7":"14","8":"5","9":"5","_rn_":"876"},{"1":"0","2":"male","3":"22.0","4":"0.750","5":"no","6":"3","7":"17","8":"4","9":"5","_rn_":"877"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"17","8":"1","9":"4","_rn_":"880"},{"1":"0","2":"male","3":"27.0","4":"0.417","5":"yes","6":"4","7":"20","8":"5","9":"4","_rn_":"903"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"20","8":"5","9":"4","_rn_":"904"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"2","7":"14","8":"1","9":"3","_rn_":"905"},{"1":"0","2":"male","3":"22.0","4":"4.000","5":"yes","6":"1","7":"18","8":"5","9":"4","_rn_":"908"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"17","8":"5","9":"3","_rn_":"909"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"14","8":"4","9":"5","_rn_":"910"},{"1":"0","2":"male","3":"52.0","4":"15.000","5":"yes","6":"4","7":"14","8":"6","9":"2","_rn_":"912"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"4","7":"17","8":"5","9":"5","_rn_":"914"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"yes","6":"5","7":"14","8":"3","9":"5","_rn_":"915"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"yes","6":"2","7":"14","8":"3","9":"5","_rn_":"916"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"3","7":"16","8":"6","9":"5","_rn_":"920"},{"1":"0","2":"male","3":"27.0","4":"0.750","5":"no","6":"2","7":"18","8":"3","9":"3","_rn_":"921"},{"1":"0","2":"female","3":"22.0","4":"7.000","5":"yes","6":"2","7":"14","8":"5","9":"2","_rn_":"925"},{"1":"0","2":"female","3":"27.0","4":"0.750","5":"no","6":"2","7":"17","8":"5","9":"3","_rn_":"926"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"12","8":"1","9":"2","_rn_":"929"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"1","7":"14","8":"1","9":"5","_rn_":"931"},{"1":"0","2":"female","3":"37.0","4":"10.000","5":"no","6":"2","7":"12","8":"4","9":"4","_rn_":"945"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"18","8":"5","9":"3","_rn_":"947"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"3","7":"12","8":"3","9":"3","_rn_":"949"},{"1":"0","2":"male","3":"22.0","4":"4.000","5":"no","6":"2","7":"18","8":"5","9":"5","_rn_":"950"},{"1":"0","2":"male","3":"52.0","4":"7.000","5":"yes","6":"2","7":"20","8":"6","9":"2","_rn_":"961"},{"1":"0","2":"male","3":"27.0","4":"0.750","5":"no","6":"2","7":"17","8":"5","9":"5","_rn_":"965"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"no","6":"2","7":"17","8":"4","9":"5","_rn_":"966"},{"1":"0","2":"male","3":"42.0","4":"1.500","5":"no","6":"5","7":"20","8":"6","9":"5","_rn_":"967"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"no","6":"4","7":"17","8":"6","9":"5","_rn_":"987"},{"1":"0","2":"male","3":"22.0","4":"4.000","5":"no","6":"4","7":"17","8":"5","9":"3","_rn_":"990"},{"1":"0","2":"female","3":"22.0","4":"4.000","5":"yes","6":"1","7":"14","8":"5","9":"4","_rn_":"992"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"5","7":"20","8":"4","9":"5","_rn_":"995"},{"1":"0","2":"female","3":"37.0","4":"10.000","5":"yes","6":"3","7":"16","8":"6","9":"3","_rn_":"1009"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"4","7":"17","8":"6","9":"5","_rn_":"1021"},{"1":"0","2":"female","3":"47.0","4":"15.000","5":"yes","6":"4","7":"17","8":"5","9":"5","_rn_":"1026"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"no","6":"4","7":"16","8":"5","9":"4","_rn_":"1027"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"3","7":"12","8":"1","9":"4","_rn_":"1030"},{"1":"0","2":"female","3":"22.0","4":"7.000","5":"yes","6":"1","7":"14","8":"3","9":"5","_rn_":"1031"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"4","7":"17","8":"5","9":"4","_rn_":"1034"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"yes","6":"2","7":"16","8":"2","9":"4","_rn_":"1037"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"14","8":"5","9":"5","_rn_":"1038"},{"1":"0","2":"male","3":"42.0","4":"4.000","5":"yes","6":"3","7":"14","8":"4","9":"5","_rn_":"1039"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"5","7":"14","8":"5","9":"4","_rn_":"1045"},{"1":"0","2":"female","3":"32.0","4":"7.000","5":"yes","6":"4","7":"17","8":"5","9":"5","_rn_":"1046"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"4","7":"18","8":"6","9":"5","_rn_":"1054"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"no","6":"4","7":"18","8":"6","9":"4","_rn_":"1059"},{"1":"0","2":"male","3":"22.0","4":"0.750","5":"no","6":"4","7":"18","8":"6","9":"5","_rn_":"1063"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"4","7":"14","8":"5","9":"3","_rn_":"1068"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"5","7":"18","8":"1","9":"5","_rn_":"1070"},{"1":"0","2":"female","3":"52.0","4":"15.000","5":"yes","6":"5","7":"9","8":"5","9":"5","_rn_":"1072"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"yes","6":"3","7":"14","8":"5","9":"5","_rn_":"1073"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"16","8":"4","9":"4","_rn_":"1077"},{"1":"0","2":"male","3":"32.0","4":"7.000","5":"yes","6":"2","7":"20","8":"5","9":"4","_rn_":"1081"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"3","7":"18","8":"1","9":"4","_rn_":"1083"},{"1":"0","2":"male","3":"32.0","4":"15.000","5":"yes","6":"1","7":"16","8":"5","9":"5","_rn_":"1084"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"3","7":"18","8":"5","9":"5","_rn_":"1086"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"4","7":"12","8":"3","9":"4","_rn_":"1087"},{"1":"0","2":"male","3":"22.0","4":"0.750","5":"yes","6":"3","7":"14","8":"2","9":"4","_rn_":"1089"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"3","7":"16","8":"5","9":"3","_rn_":"1096"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"4","7":"14","8":"3","9":"5","_rn_":"1102"},{"1":"0","2":"female","3":"52.0","4":"15.000","5":"yes","6":"3","7":"16","8":"5","9":"4","_rn_":"1103"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"5","7":"20","8":"6","9":"4","_rn_":"1107"},{"1":"0","2":"female","3":"47.0","4":"15.000","5":"yes","6":"4","7":"12","8":"2","9":"3","_rn_":"1109"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"2","7":"20","8":"6","9":"4","_rn_":"1115"},{"1":"0","2":"male","3":"32.0","4":"7.000","5":"yes","6":"4","7":"17","8":"5","9":"5","_rn_":"1119"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"17","8":"1","9":"4","_rn_":"1124"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"no","6":"1","7":"18","8":"6","9":"5","_rn_":"1126"},{"1":"0","2":"female","3":"22.0","4":"4.000","5":"yes","6":"3","7":"9","8":"1","9":"4","_rn_":"1128"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"14","8":"1","9":"5","_rn_":"1129"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"2","7":"20","8":"6","9":"4","_rn_":"1130"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"4","7":"9","8":"2","9":"4","_rn_":"1133"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"2","7":"18","8":"1","9":"5","_rn_":"1140"},{"1":"0","2":"female","3":"22.0","4":"4.000","5":"yes","6":"3","7":"14","8":"1","9":"5","_rn_":"1143"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"14","8":"5","9":"3","_rn_":"1146"},{"1":"0","2":"male","3":"32.0","4":"7.000","5":"yes","6":"1","7":"18","8":"6","9":"4","_rn_":"1153"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"14","8":"5","9":"5","_rn_":"1156"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"yes","6":"3","7":"12","8":"1","9":"3","_rn_":"1157"},{"1":"0","2":"male","3":"52.0","4":"15.000","5":"yes","6":"2","7":"14","8":"5","9":"5","_rn_":"1158"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"2","7":"14","8":"1","9":"1","_rn_":"1160"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"2","7":"14","8":"5","9":"5","_rn_":"1161"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"4","7":"20","8":"4","9":"5","_rn_":"1166"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"3","7":"18","8":"4","9":"5","_rn_":"1177"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"5","_rn_":"1178"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"no","6":"3","7":"18","8":"5","9":"5","_rn_":"1180"},{"1":"0","2":"female","3":"22.0","4":"0.125","5":"no","6":"2","7":"16","8":"6","9":"3","_rn_":"1187"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"yes","6":"2","7":"20","8":"6","9":"3","_rn_":"1191"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"no","6":"4","7":"18","8":"5","9":"4","_rn_":"1195"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"2","7":"12","8":"5","9":"1","_rn_":"1207"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"yes","6":"5","7":"18","8":"6","9":"3","_rn_":"1208"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"2","7":"17","8":"5","9":"5","_rn_":"1209"},{"1":"0","2":"male","3":"47.0","4":"15.000","5":"no","6":"4","7":"20","8":"6","9":"4","_rn_":"1211"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"no","6":"1","7":"18","8":"5","9":"5","_rn_":"1215"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"4","_rn_":"1221"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"4","7":"18","8":"1","9":"4","_rn_":"1226"},{"1":"0","2":"female","3":"32.0","4":"7.000","5":"yes","6":"4","7":"17","8":"5","9":"4","_rn_":"1229"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"3","7":"14","8":"1","9":"3","_rn_":"1231"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"3","7":"16","8":"1","9":"4","_rn_":"1234"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"no","6":"3","7":"16","8":"4","9":"2","_rn_":"1235"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"no","6":"3","7":"16","8":"3","9":"5","_rn_":"1242"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"3","7":"16","8":"4","9":"2","_rn_":"1245"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"3","7":"12","8":"1","9":"2","_rn_":"1260"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"2","7":"18","8":"5","9":"4","_rn_":"1266"},{"1":"0","2":"female","3":"37.0","4":"7.000","5":"yes","6":"3","7":"14","8":"4","9":"4","_rn_":"1271"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"no","6":"2","7":"16","8":"5","9":"5","_rn_":"1273"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"5","7":"20","8":"5","9":"4","_rn_":"1276"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"4","7":"16","8":"5","9":"3","_rn_":"1280"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"4","7":"16","8":"1","9":"5","_rn_":"1282"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"no","6":"2","7":"17","8":"5","9":"3","_rn_":"1285"},{"1":"0","2":"female","3":"22.0","4":"0.417","5":"no","6":"4","7":"14","8":"5","9":"5","_rn_":"1295"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"no","6":"2","7":"18","8":"5","9":"5","_rn_":"1298"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"18","8":"5","9":"3","_rn_":"1299"},{"1":"0","2":"male","3":"37.0","4":"10.000","5":"yes","6":"5","7":"20","8":"7","9":"4","_rn_":"1304"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"2","7":"14","8":"4","9":"2","_rn_":"1305"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"yes","6":"2","7":"16","8":"5","9":"5","_rn_":"1311"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"yes","6":"2","7":"16","8":"6","9":"4","_rn_":"1314"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"no","6":"3","7":"18","8":"4","9":"5","_rn_":"1319"},{"1":"0","2":"female","3":"22.0","4":"4.000","5":"yes","6":"4","7":"14","8":"3","9":"4","_rn_":"1322"},{"1":"0","2":"female","3":"17.5","4":"0.750","5":"no","6":"2","7":"18","8":"5","9":"4","_rn_":"1324"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"yes","6":"4","7":"20","8":"4","9":"5","_rn_":"1327"},{"1":"0","2":"female","3":"32.0","4":"0.750","5":"no","6":"5","7":"14","8":"3","9":"3","_rn_":"1328"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"17","8":"5","9":"3","_rn_":"1330"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"no","6":"3","7":"14","8":"4","9":"5","_rn_":"1332"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"no","6":"2","7":"17","8":"3","9":"2","_rn_":"1333"},{"1":"0","2":"female","3":"22.0","4":"7.000","5":"yes","6":"4","7":"14","8":"1","9":"5","_rn_":"1336"},{"1":"0","2":"male","3":"47.0","4":"15.000","5":"yes","6":"5","7":"14","8":"6","9":"5","_rn_":"1341"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"1","7":"16","8":"4","9":"4","_rn_":"1344"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"5","7":"14","8":"1","9":"3","_rn_":"1352"},{"1":"0","2":"male","3":"42.0","4":"4.000","5":"yes","6":"4","7":"18","8":"5","9":"5","_rn_":"1358"},{"1":"0","2":"female","3":"32.0","4":"4.000","5":"yes","6":"2","7":"14","8":"1","9":"5","_rn_":"1359"},{"1":"0","2":"male","3":"52.0","4":"15.000","5":"yes","6":"2","7":"14","8":"7","9":"4","_rn_":"1361"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"16","8":"1","9":"4","_rn_":"1364"},{"1":"0","2":"male","3":"52.0","4":"15.000","5":"yes","6":"4","7":"12","8":"2","9":"4","_rn_":"1368"},{"1":"0","2":"female","3":"22.0","4":"0.417","5":"no","6":"3","7":"17","8":"1","9":"5","_rn_":"1384"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"16","8":"5","9":"5","_rn_":"1390"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"4","7":"20","8":"6","9":"4","_rn_":"1393"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"4","7":"14","8":"1","9":"5","_rn_":"1394"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"no","6":"2","7":"16","8":"3","9":"5","_rn_":"1402"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"no","6":"1","7":"20","8":"6","9":"5","_rn_":"1407"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"3","7":"20","8":"6","9":"4","_rn_":"1408"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"no","6":"2","7":"16","8":"6","9":"5","_rn_":"1412"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"5","7":"14","8":"5","9":"5","_rn_":"1413"},{"1":"0","2":"male","3":"37.0","4":"1.500","5":"yes","6":"4","7":"18","8":"5","9":"3","_rn_":"1416"},{"1":"0","2":"male","3":"32.0","4":"1.500","5":"no","6":"2","7":"18","8":"4","9":"4","_rn_":"1417"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"4","7":"14","8":"1","9":"4","_rn_":"1418"},{"1":"0","2":"female","3":"47.0","4":"15.000","5":"yes","6":"4","7":"18","8":"5","9":"4","_rn_":"1419"},{"1":"0","2":"female","3":"27.0","4":"10.000","5":"yes","6":"5","7":"12","8":"1","9":"5","_rn_":"1420"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"3","7":"16","8":"4","9":"5","_rn_":"1423"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"12","8":"4","9":"2","_rn_":"1424"},{"1":"0","2":"female","3":"27.0","4":"0.750","5":"no","6":"4","7":"16","8":"5","9":"5","_rn_":"1432"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"16","8":"1","9":"5","_rn_":"1433"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"3","7":"16","8":"1","9":"5","_rn_":"1437"},{"1":"0","2":"female","3":"27.0","4":"10.000","5":"yes","6":"2","7":"16","8":"1","9":"5","_rn_":"1438"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"no","6":"2","7":"20","8":"6","9":"5","_rn_":"1439"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"2","7":"14","8":"1","9":"3","_rn_":"1446"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"yes","6":"2","7":"17","8":"4","9":"4","_rn_":"1450"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"yes","6":"2","7":"14","8":"1","9":"5","_rn_":"1451"},{"1":"0","2":"male","3":"22.0","4":"4.000","5":"yes","6":"4","7":"14","8":"2","9":"4","_rn_":"1452"},{"1":"0","2":"male","3":"42.0","4":"0.125","5":"no","6":"4","7":"17","8":"6","9":"4","_rn_":"1453"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"yes","6":"4","7":"18","8":"6","9":"5","_rn_":"1456"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"yes","6":"3","7":"16","8":"6","9":"3","_rn_":"1464"},{"1":"0","2":"female","3":"52.0","4":"15.000","5":"yes","6":"4","7":"14","8":"1","9":"3","_rn_":"1469"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"no","6":"5","7":"20","8":"5","9":"2","_rn_":"1473"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"no","6":"2","7":"16","8":"5","9":"5","_rn_":"1481"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"no","6":"3","7":"17","8":"5","9":"5","_rn_":"1482"},{"1":"0","2":"male","3":"22.0","4":"0.125","5":"no","6":"5","7":"16","8":"4","9":"4","_rn_":"1496"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"4","7":"16","8":"1","9":"5","_rn_":"1497"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"4","7":"12","8":"1","9":"5","_rn_":"1504"},{"1":"0","2":"female","3":"47.0","4":"15.000","5":"yes","6":"2","7":"14","8":"5","9":"5","_rn_":"1513"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"3","7":"14","8":"5","9":"3","_rn_":"1515"},{"1":"0","2":"male","3":"42.0","4":"7.000","5":"yes","6":"2","7":"16","8":"5","9":"5","_rn_":"1534"},{"1":"0","2":"male","3":"22.0","4":"0.750","5":"no","6":"4","7":"16","8":"6","9":"4","_rn_":"1535"},{"1":"0","2":"male","3":"27.0","4":"0.125","5":"no","6":"3","7":"20","8":"6","9":"5","_rn_":"1536"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"yes","6":"3","7":"20","8":"6","9":"5","_rn_":"1540"},{"1":"0","2":"female","3":"22.0","4":"0.417","5":"no","6":"5","7":"14","8":"4","9":"5","_rn_":"1551"},{"1":"0","2":"female","3":"47.0","4":"15.000","5":"yes","6":"5","7":"14","8":"1","9":"4","_rn_":"1555"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"3","7":"14","8":"1","9":"5","_rn_":"1557"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"4","7":"17","8":"5","9":"5","_rn_":"1566"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"3","7":"20","8":"6","9":"5","_rn_":"1567"},{"1":"0","2":"female","3":"32.0","4":"7.000","5":"yes","6":"4","7":"17","8":"1","9":"5","_rn_":"1576"},{"1":"0","2":"female","3":"37.0","4":"10.000","5":"yes","6":"4","7":"16","8":"1","9":"5","_rn_":"1584"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"1","7":"18","8":"1","9":"4","_rn_":"1585"},{"1":"0","2":"female","3":"22.0","4":"4.000","5":"no","6":"3","7":"14","8":"1","9":"4","_rn_":"1590"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"14","8":"3","9":"2","_rn_":"1594"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"5","7":"18","8":"5","9":"2","_rn_":"1595"},{"1":"0","2":"male","3":"32.0","4":"7.000","5":"yes","6":"2","7":"18","8":"5","9":"5","_rn_":"1603"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"no","6":"4","7":"17","8":"1","9":"3","_rn_":"1608"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"no","6":"4","7":"14","8":"5","9":"5","_rn_":"1609"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"yes","6":"4","7":"14","8":"5","9":"4","_rn_":"1615"},{"1":"0","2":"female","3":"32.0","4":"7.000","5":"yes","6":"3","7":"16","8":"1","9":"5","_rn_":"1616"},{"1":"0","2":"female","3":"47.0","4":"15.000","5":"yes","6":"3","7":"16","8":"5","9":"4","_rn_":"1617"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"3","7":"16","8":"1","9":"5","_rn_":"1620"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"yes","6":"2","7":"14","8":"5","9":"5","_rn_":"1621"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"1","7":"16","8":"5","9":"5","_rn_":"1637"},{"1":"0","2":"male","3":"52.0","4":"15.000","5":"yes","6":"4","7":"16","8":"5","9":"5","_rn_":"1638"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"yes","6":"4","7":"20","8":"6","9":"5","_rn_":"1650"},{"1":"0","2":"male","3":"47.0","4":"15.000","5":"yes","6":"4","7":"16","8":"6","9":"4","_rn_":"1654"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"2","7":"14","8":"1","9":"2","_rn_":"1665"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"4","7":"14","8":"4","9":"5","_rn_":"1670"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"2","7":"16","8":"5","9":"4","_rn_":"1671"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"2","7":"16","8":"5","9":"4","_rn_":"1675"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"16","8":"5","9":"5","_rn_":"1688"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"3","7":"18","8":"6","9":"4","_rn_":"1691"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"5","7":"14","8":"4","9":"5","_rn_":"1695"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"4","7":"16","8":"4","9":"4","_rn_":"1698"},{"1":"0","2":"female","3":"57.0","4":"15.000","5":"yes","6":"3","7":"18","8":"5","9":"2","_rn_":"1704"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"3","7":"18","8":"6","9":"2","_rn_":"1705"},{"1":"0","2":"female","3":"32.0","4":"7.000","5":"yes","6":"2","7":"14","8":"1","9":"2","_rn_":"1711"},{"1":"0","2":"male","3":"22.0","4":"4.000","5":"no","6":"5","7":"12","8":"4","9":"5","_rn_":"1719"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"1","7":"16","8":"6","9":"5","_rn_":"1723"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"1","7":"14","8":"4","9":"5","_rn_":"1726"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"4","7":"12","8":"1","9":"5","_rn_":"1749"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"no","6":"2","7":"18","8":"5","9":"3","_rn_":"1752"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"5","7":"17","8":"2","9":"5","_rn_":"1754"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"4","7":"12","8":"1","9":"5","_rn_":"1758"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"5","7":"18","8":"5","9":"4","_rn_":"1761"},{"1":"0","2":"male","3":"32.0","4":"1.500","5":"no","6":"2","7":"20","8":"7","9":"3","_rn_":"1773"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"no","6":"4","7":"9","8":"3","9":"1","_rn_":"1775"},{"1":"0","2":"male","3":"37.0","4":"7.000","5":"no","6":"4","7":"18","8":"5","9":"5","_rn_":"1786"},{"1":"0","2":"male","3":"52.0","4":"15.000","5":"yes","6":"2","7":"17","8":"5","9":"4","_rn_":"1793"},{"1":"0","2":"male","3":"47.0","4":"15.000","5":"yes","6":"4","7":"17","8":"6","9":"5","_rn_":"1799"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"no","6":"2","7":"17","8":"5","9":"4","_rn_":"1803"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"14","8":"5","9":"5","_rn_":"1806"},{"1":"0","2":"female","3":"22.0","4":"4.000","5":"no","6":"2","7":"14","8":"3","9":"3","_rn_":"1807"},{"1":"0","2":"male","3":"37.0","4":"7.000","5":"yes","6":"2","7":"20","8":"6","9":"5","_rn_":"1808"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"no","6":"4","7":"12","8":"4","9":"3","_rn_":"1814"},{"1":"0","2":"male","3":"42.0","4":"10.000","5":"yes","6":"4","7":"18","8":"6","9":"4","_rn_":"1815"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"3","7":"14","8":"1","9":"5","_rn_":"1818"},{"1":"0","2":"female","3":"22.0","4":"4.000","5":"yes","6":"2","7":"14","8":"1","9":"3","_rn_":"1827"},{"1":"0","2":"female","3":"57.0","4":"15.000","5":"no","6":"4","7":"20","8":"6","9":"5","_rn_":"1834"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"14","8":"4","9":"3","_rn_":"1835"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"3","7":"18","8":"5","9":"5","_rn_":"1843"},{"1":"0","2":"female","3":"17.5","4":"10.000","5":"no","6":"4","7":"14","8":"4","9":"5","_rn_":"1846"},{"1":"0","2":"male","3":"22.0","4":"4.000","5":"yes","6":"4","7":"16","8":"5","9":"5","_rn_":"1850"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"2","7":"16","8":"1","9":"4","_rn_":"1851"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"2","7":"14","8":"5","9":"1","_rn_":"1854"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"5","7":"14","8":"1","9":"4","_rn_":"1859"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"yes","6":"2","7":"20","8":"5","9":"4","_rn_":"1861"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"4","7":"14","8":"5","9":"5","_rn_":"1866"},{"1":"0","2":"male","3":"22.0","4":"0.125","5":"no","6":"1","7":"16","8":"3","9":"5","_rn_":"1873"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"14","8":"1","9":"4","_rn_":"1875"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"5","7":"16","8":"5","9":"3","_rn_":"1885"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"yes","6":"4","7":"18","8":"5","9":"4","_rn_":"1892"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"2","7":"14","8":"3","9":"4","_rn_":"1895"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"3","7":"17","8":"5","9":"5","_rn_":"1896"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"4","7":"17","8":"4","9":"4","_rn_":"1897"},{"1":"0","2":"female","3":"52.0","4":"15.000","5":"yes","6":"5","7":"14","8":"1","9":"5","_rn_":"1899"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"2","7":"12","8":"1","9":"2","_rn_":"1904"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"3","7":"12","8":"1","9":"4","_rn_":"1905"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"2","7":"14","8":"1","9":"4","_rn_":"1908"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"4","7":"14","8":"5","9":"4","_rn_":"1916"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"yes","6":"4","7":"14","8":"3","9":"3","_rn_":"1918"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"yes","6":"2","7":"20","8":"6","9":"2","_rn_":"1920"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"3","7":"12","8":"3","9":"3","_rn_":"1930"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"3","7":"16","8":"3","9":"5","_rn_":"1940"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"3","7":"14","8":"1","9":"4","_rn_":"1947"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"14","8":"4","9":"5","_rn_":"1949"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"4","7":"14","8":"1","9":"4","_rn_":"1951"},{"1":"0","2":"female","3":"22.0","4":"4.000","5":"no","6":"4","7":"14","8":"5","9":"5","_rn_":"1952"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"16","8":"4","9":"5","_rn_":"1960"},{"1":"0","2":"male","3":"47.0","4":"15.000","5":"no","6":"4","7":"14","8":"5","9":"4","_rn_":"9001"},{"1":"0","2":"male","3":"37.0","4":"10.000","5":"yes","6":"2","7":"18","8":"6","9":"2","_rn_":"9012"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"3","7":"17","8":"5","9":"4","_rn_":"9023"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"2","7":"16","8":"1","9":"4","_rn_":"9029"},{"1":"3","2":"male","3":"27.0","4":"1.500","5":"no","6":"3","7":"18","8":"4","9":"4","_rn_":"6"},{"1":"3","2":"female","3":"27.0","4":"4.000","5":"yes","6":"3","7":"17","8":"1","9":"5","_rn_":"12"},{"1":"7","2":"male","3":"37.0","4":"15.000","5":"yes","6":"5","7":"18","8":"6","9":"2","_rn_":"43"},{"1":"12","2":"female","3":"32.0","4":"10.000","5":"yes","6":"3","7":"17","8":"5","9":"2","_rn_":"53"},{"1":"1","2":"male","3":"22.0","4":"0.125","5":"no","6":"4","7":"16","8":"5","9":"5","_rn_":"67"},{"1":"1","2":"female","3":"22.0","4":"1.500","5":"yes","6":"2","7":"14","8":"1","9":"5","_rn_":"79"},{"1":"12","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"14","8":"5","9":"2","_rn_":"122"},{"1":"7","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"14","8":"3","9":"4","_rn_":"126"},{"1":"2","2":"male","3":"37.0","4":"15.000","5":"yes","6":"2","7":"18","8":"6","9":"4","_rn_":"133"},{"1":"3","2":"female","3":"32.0","4":"15.000","5":"yes","6":"4","7":"12","8":"3","9":"2","_rn_":"138"},{"1":"1","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"14","8":"4","9":"2","_rn_":"154"},{"1":"7","2":"female","3":"42.0","4":"15.000","5":"yes","6":"3","7":"17","8":"1","9":"4","_rn_":"159"},{"1":"12","2":"female","3":"42.0","4":"15.000","5":"yes","6":"5","7":"9","8":"4","9":"1","_rn_":"174"},{"1":"12","2":"male","3":"37.0","4":"10.000","5":"yes","6":"2","7":"20","8":"6","9":"2","_rn_":"176"},{"1":"12","2":"female","3":"32.0","4":"15.000","5":"yes","6":"3","7":"14","8":"1","9":"2","_rn_":"181"},{"1":"3","2":"male","3":"27.0","4":"4.000","5":"no","6":"1","7":"18","8":"6","9":"5","_rn_":"182"},{"1":"7","2":"male","3":"37.0","4":"10.000","5":"yes","6":"2","7":"18","8":"7","9":"3","_rn_":"186"},{"1":"7","2":"female","3":"27.0","4":"4.000","5":"no","6":"3","7":"17","8":"5","9":"5","_rn_":"189"},{"1":"1","2":"male","3":"42.0","4":"15.000","5":"yes","6":"4","7":"16","8":"5","9":"5","_rn_":"204"},{"1":"1","2":"female","3":"47.0","4":"15.000","5":"yes","6":"5","7":"14","8":"4","9":"5","_rn_":"215"},{"1":"7","2":"female","3":"27.0","4":"4.000","5":"yes","6":"3","7":"18","8":"5","9":"4","_rn_":"232"},{"1":"1","2":"female","3":"27.0","4":"7.000","5":"yes","6":"5","7":"14","8":"1","9":"4","_rn_":"233"},{"1":"12","2":"male","3":"27.0","4":"1.500","5":"yes","6":"3","7":"17","8":"5","9":"4","_rn_":"252"},{"1":"12","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"14","8":"6","9":"2","_rn_":"253"},{"1":"3","2":"female","3":"42.0","4":"15.000","5":"yes","6":"4","7":"16","8":"5","9":"4","_rn_":"274"},{"1":"7","2":"female","3":"27.0","4":"10.000","5":"yes","6":"4","7":"12","8":"7","9":"3","_rn_":"275"},{"1":"1","2":"male","3":"27.0","4":"1.500","5":"no","6":"2","7":"18","8":"5","9":"2","_rn_":"287"},{"1":"1","2":"male","3":"32.0","4":"4.000","5":"no","6":"4","7":"20","8":"6","9":"4","_rn_":"288"},{"1":"1","2":"female","3":"27.0","4":"7.000","5":"yes","6":"3","7":"14","8":"1","9":"3","_rn_":"325"},{"1":"3","2":"female","3":"32.0","4":"10.000","5":"yes","6":"4","7":"14","8":"1","9":"4","_rn_":"328"},{"1":"3","2":"male","3":"27.0","4":"4.000","5":"yes","6":"2","7":"18","8":"7","9":"2","_rn_":"344"},{"1":"1","2":"female","3":"17.5","4":"0.750","5":"no","6":"5","7":"14","8":"4","9":"5","_rn_":"353"},{"1":"1","2":"female","3":"32.0","4":"10.000","5":"yes","6":"4","7":"18","8":"1","9":"5","_rn_":"354"},{"1":"7","2":"female","3":"32.0","4":"7.000","5":"yes","6":"2","7":"17","8":"6","9":"4","_rn_":"367"},{"1":"7","2":"male","3":"37.0","4":"15.000","5":"yes","6":"2","7":"20","8":"6","9":"4","_rn_":"369"},{"1":"7","2":"female","3":"37.0","4":"10.000","5":"no","6":"1","7":"20","8":"5","9":"3","_rn_":"390"},{"1":"12","2":"female","3":"32.0","4":"10.000","5":"yes","6":"2","7":"16","8":"5","9":"5","_rn_":"392"},{"1":"7","2":"male","3":"52.0","4":"15.000","5":"yes","6":"2","7":"20","8":"6","9":"4","_rn_":"423"},{"1":"7","2":"female","3":"42.0","4":"15.000","5":"yes","6":"1","7":"12","8":"1","9":"3","_rn_":"432"},{"1":"1","2":"male","3":"52.0","4":"15.000","5":"yes","6":"2","7":"20","8":"6","9":"3","_rn_":"436"},{"1":"2","2":"male","3":"37.0","4":"15.000","5":"yes","6":"3","7":"18","8":"6","9":"5","_rn_":"483"},{"1":"12","2":"female","3":"22.0","4":"4.000","5":"no","6":"3","7":"12","8":"3","9":"4","_rn_":"513"},{"1":"12","2":"male","3":"27.0","4":"7.000","5":"yes","6":"1","7":"18","8":"6","9":"2","_rn_":"516"},{"1":"1","2":"male","3":"27.0","4":"4.000","5":"yes","6":"3","7":"18","8":"5","9":"5","_rn_":"518"},{"1":"12","2":"male","3":"47.0","4":"15.000","5":"yes","6":"4","7":"17","8":"6","9":"5","_rn_":"520"},{"1":"12","2":"female","3":"42.0","4":"15.000","5":"yes","6":"4","7":"12","8":"1","9":"1","_rn_":"526"},{"1":"7","2":"male","3":"27.0","4":"4.000","5":"no","6":"3","7":"14","8":"3","9":"4","_rn_":"528"},{"1":"7","2":"female","3":"32.0","4":"7.000","5":"yes","6":"4","7":"18","8":"4","9":"5","_rn_":"553"},{"1":"1","2":"male","3":"32.0","4":"0.417","5":"yes","6":"3","7":"12","8":"3","9":"4","_rn_":"576"},{"1":"3","2":"male","3":"47.0","4":"15.000","5":"yes","6":"5","7":"16","8":"5","9":"4","_rn_":"611"},{"1":"12","2":"male","3":"37.0","4":"15.000","5":"yes","6":"2","7":"20","8":"5","9":"4","_rn_":"625"},{"1":"7","2":"male","3":"22.0","4":"4.000","5":"yes","6":"2","7":"17","8":"6","9":"4","_rn_":"635"},{"1":"1","2":"male","3":"27.0","4":"4.000","5":"no","6":"2","7":"14","8":"4","9":"5","_rn_":"646"},{"1":"7","2":"female","3":"52.0","4":"15.000","5":"yes","6":"5","7":"16","8":"1","9":"3","_rn_":"657"},{"1":"1","2":"male","3":"27.0","4":"4.000","5":"no","6":"3","7":"14","8":"3","9":"3","_rn_":"659"},{"1":"1","2":"female","3":"27.0","4":"10.000","5":"yes","6":"4","7":"16","8":"1","9":"4","_rn_":"666"},{"1":"1","2":"male","3":"32.0","4":"7.000","5":"yes","6":"3","7":"14","8":"7","9":"4","_rn_":"679"},{"1":"7","2":"male","3":"32.0","4":"7.000","5":"yes","6":"2","7":"18","8":"4","9":"1","_rn_":"729"},{"1":"3","2":"male","3":"22.0","4":"1.500","5":"no","6":"1","7":"14","8":"3","9":"2","_rn_":"755"},{"1":"7","2":"male","3":"22.0","4":"4.000","5":"yes","6":"3","7":"18","8":"6","9":"4","_rn_":"758"},{"1":"7","2":"male","3":"42.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"4","_rn_":"770"},{"1":"2","2":"female","3":"57.0","4":"15.000","5":"yes","6":"1","7":"18","8":"5","9":"4","_rn_":"786"},{"1":"7","2":"female","3":"32.0","4":"4.000","5":"yes","6":"3","7":"18","8":"5","9":"2","_rn_":"797"},{"1":"1","2":"male","3":"27.0","4":"4.000","5":"yes","6":"1","7":"16","8":"4","9":"4","_rn_":"811"},{"1":"7","2":"male","3":"32.0","4":"7.000","5":"yes","6":"4","7":"16","8":"1","9":"4","_rn_":"834"},{"1":"2","2":"male","3":"57.0","4":"15.000","5":"yes","6":"1","7":"17","8":"4","9":"4","_rn_":"858"},{"1":"7","2":"female","3":"42.0","4":"15.000","5":"yes","6":"4","7":"14","8":"5","9":"2","_rn_":"885"},{"1":"7","2":"male","3":"37.0","4":"10.000","5":"yes","6":"1","7":"18","8":"5","9":"3","_rn_":"893"},{"1":"3","2":"male","3":"42.0","4":"15.000","5":"yes","6":"3","7":"17","8":"6","9":"1","_rn_":"927"},{"1":"1","2":"female","3":"52.0","4":"15.000","5":"yes","6":"3","7":"14","8":"4","9":"4","_rn_":"928"},{"1":"2","2":"female","3":"27.0","4":"7.000","5":"yes","6":"3","7":"17","8":"5","9":"3","_rn_":"933"},{"1":"12","2":"male","3":"32.0","4":"7.000","5":"yes","6":"2","7":"12","8":"4","9":"2","_rn_":"951"},{"1":"1","2":"male","3":"22.0","4":"4.000","5":"no","6":"4","7":"14","8":"2","9":"5","_rn_":"968"},{"1":"3","2":"male","3":"27.0","4":"7.000","5":"yes","6":"3","7":"18","8":"6","9":"4","_rn_":"972"},{"1":"12","2":"female","3":"37.0","4":"15.000","5":"yes","6":"1","7":"18","8":"5","9":"5","_rn_":"975"},{"1":"7","2":"female","3":"32.0","4":"15.000","5":"yes","6":"3","7":"17","8":"1","9":"3","_rn_":"977"},{"1":"7","2":"female","3":"27.0","4":"7.000","5":"no","6":"2","7":"17","8":"5","9":"5","_rn_":"981"},{"1":"1","2":"female","3":"32.0","4":"7.000","5":"yes","6":"3","7":"17","8":"5","9":"3","_rn_":"986"},{"1":"1","2":"male","3":"32.0","4":"1.500","5":"yes","6":"2","7":"14","8":"2","9":"4","_rn_":"1002"},{"1":"12","2":"female","3":"42.0","4":"15.000","5":"yes","6":"4","7":"14","8":"1","9":"2","_rn_":"1007"},{"1":"7","2":"male","3":"32.0","4":"10.000","5":"yes","6":"3","7":"14","8":"5","9":"4","_rn_":"1011"},{"1":"7","2":"male","3":"37.0","4":"4.000","5":"yes","6":"1","7":"20","8":"6","9":"3","_rn_":"1035"},{"1":"1","2":"female","3":"27.0","4":"4.000","5":"yes","6":"2","7":"16","8":"5","9":"3","_rn_":"1050"},{"1":"12","2":"female","3":"42.0","4":"15.000","5":"yes","6":"3","7":"14","8":"4","9":"3","_rn_":"1056"},{"1":"1","2":"male","3":"27.0","4":"10.000","5":"yes","6":"5","7":"20","8":"6","9":"5","_rn_":"1057"},{"1":"12","2":"male","3":"37.0","4":"10.000","5":"yes","6":"2","7":"20","8":"6","9":"2","_rn_":"1075"},{"1":"12","2":"female","3":"27.0","4":"7.000","5":"yes","6":"1","7":"14","8":"3","9":"3","_rn_":"1080"},{"1":"3","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"12","8":"1","9":"2","_rn_":"1125"},{"1":"3","2":"male","3":"32.0","4":"10.000","5":"yes","6":"2","7":"14","8":"4","9":"4","_rn_":"1131"},{"1":"12","2":"female","3":"17.5","4":"0.750","5":"yes","6":"2","7":"12","8":"1","9":"3","_rn_":"1138"},{"1":"12","2":"female","3":"32.0","4":"15.000","5":"yes","6":"3","7":"18","8":"5","9":"4","_rn_":"1150"},{"1":"2","2":"female","3":"22.0","4":"7.000","5":"no","6":"4","7":"14","8":"4","9":"3","_rn_":"1163"},{"1":"1","2":"male","3":"32.0","4":"7.000","5":"yes","6":"4","7":"20","8":"6","9":"5","_rn_":"1169"},{"1":"7","2":"male","3":"27.0","4":"4.000","5":"yes","6":"2","7":"18","8":"6","9":"2","_rn_":"1198"},{"1":"1","2":"female","3":"22.0","4":"1.500","5":"yes","6":"5","7":"14","8":"5","9":"3","_rn_":"1204"},{"1":"12","2":"female","3":"32.0","4":"15.000","5":"no","6":"3","7":"17","8":"5","9":"1","_rn_":"1218"},{"1":"12","2":"female","3":"42.0","4":"15.000","5":"yes","6":"2","7":"12","8":"1","9":"2","_rn_":"1230"},{"1":"7","2":"male","3":"42.0","4":"15.000","5":"yes","6":"3","7":"20","8":"5","9":"4","_rn_":"1236"},{"1":"12","2":"male","3":"32.0","4":"10.000","5":"no","6":"2","7":"18","8":"4","9":"2","_rn_":"1247"},{"1":"12","2":"female","3":"32.0","4":"15.000","5":"yes","6":"3","7":"9","8":"1","9":"1","_rn_":"1259"},{"1":"7","2":"male","3":"57.0","4":"15.000","5":"yes","6":"5","7":"20","8":"4","9":"5","_rn_":"1294"},{"1":"12","2":"male","3":"47.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"4","_rn_":"1353"},{"1":"2","2":"female","3":"42.0","4":"15.000","5":"yes","6":"2","7":"17","8":"6","9":"3","_rn_":"1370"},{"1":"12","2":"male","3":"37.0","4":"15.000","5":"yes","6":"3","7":"17","8":"6","9":"3","_rn_":"1427"},{"1":"12","2":"male","3":"37.0","4":"15.000","5":"yes","6":"5","7":"17","8":"5","9":"2","_rn_":"1445"},{"1":"7","2":"male","3":"27.0","4":"10.000","5":"yes","6":"2","7":"20","8":"6","9":"4","_rn_":"1460"},{"1":"2","2":"male","3":"37.0","4":"15.000","5":"yes","6":"2","7":"16","8":"5","9":"4","_rn_":"1480"},{"1":"12","2":"female","3":"32.0","4":"15.000","5":"yes","6":"1","7":"14","8":"5","9":"2","_rn_":"1505"},{"1":"7","2":"male","3":"32.0","4":"10.000","5":"yes","6":"3","7":"17","8":"6","9":"3","_rn_":"1543"},{"1":"2","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"18","8":"5","9":"1","_rn_":"1548"},{"1":"7","2":"female","3":"27.0","4":"1.500","5":"no","6":"2","7":"17","8":"5","9":"5","_rn_":"1550"},{"1":"3","2":"female","3":"47.0","4":"15.000","5":"yes","6":"2","7":"17","8":"5","9":"2","_rn_":"1561"},{"1":"12","2":"male","3":"37.0","4":"15.000","5":"yes","6":"2","7":"17","8":"5","9":"4","_rn_":"1564"},{"1":"12","2":"female","3":"27.0","4":"4.000","5":"no","6":"2","7":"14","8":"5","9":"5","_rn_":"1573"},{"1":"2","2":"female","3":"27.0","4":"10.000","5":"yes","6":"4","7":"14","8":"1","9":"5","_rn_":"1575"},{"1":"1","2":"female","3":"22.0","4":"4.000","5":"yes","6":"3","7":"16","8":"1","9":"3","_rn_":"1599"},{"1":"12","2":"male","3":"52.0","4":"7.000","5":"no","6":"4","7":"16","8":"5","9":"5","_rn_":"1622"},{"1":"2","2":"female","3":"27.0","4":"4.000","5":"yes","6":"1","7":"16","8":"3","9":"5","_rn_":"1629"},{"1":"7","2":"female","3":"37.0","4":"15.000","5":"yes","6":"2","7":"17","8":"6","9":"4","_rn_":"1664"},{"1":"2","2":"female","3":"27.0","4":"4.000","5":"no","6":"1","7":"17","8":"3","9":"1","_rn_":"1669"},{"1":"12","2":"female","3":"17.5","4":"0.750","5":"yes","6":"2","7":"12","8":"3","9":"5","_rn_":"1674"},{"1":"7","2":"female","3":"32.0","4":"15.000","5":"yes","6":"5","7":"18","8":"5","9":"4","_rn_":"1682"},{"1":"7","2":"female","3":"22.0","4":"4.000","5":"no","6":"1","7":"16","8":"3","9":"5","_rn_":"1685"},{"1":"2","2":"male","3":"32.0","4":"4.000","5":"yes","6":"4","7":"18","8":"6","9":"4","_rn_":"1697"},{"1":"1","2":"female","3":"22.0","4":"1.500","5":"yes","6":"3","7":"18","8":"5","9":"2","_rn_":"1716"},{"1":"3","2":"female","3":"42.0","4":"15.000","5":"yes","6":"2","7":"17","8":"5","9":"4","_rn_":"1730"},{"1":"1","2":"male","3":"32.0","4":"7.000","5":"yes","6":"4","7":"16","8":"4","9":"4","_rn_":"1731"},{"1":"12","2":"male","3":"37.0","4":"15.000","5":"no","6":"3","7":"14","8":"6","9":"2","_rn_":"1732"},{"1":"1","2":"male","3":"42.0","4":"15.000","5":"yes","6":"3","7":"16","8":"6","9":"3","_rn_":"1743"},{"1":"1","2":"male","3":"27.0","4":"4.000","5":"yes","6":"1","7":"18","8":"5","9":"4","_rn_":"1751"},{"1":"2","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"20","8":"7","9":"3","_rn_":"1757"},{"1":"7","2":"male","3":"37.0","4":"15.000","5":"yes","6":"3","7":"20","8":"6","9":"4","_rn_":"1763"},{"1":"3","2":"male","3":"22.0","4":"1.500","5":"no","6":"2","7":"12","8":"3","9":"3","_rn_":"1766"},{"1":"3","2":"male","3":"32.0","4":"4.000","5":"yes","6":"3","7":"20","8":"6","9":"2","_rn_":"1772"},{"1":"2","2":"male","3":"32.0","4":"15.000","5":"yes","6":"5","7":"20","8":"6","9":"5","_rn_":"1776"},{"1":"12","2":"female","3":"52.0","4":"15.000","5":"yes","6":"1","7":"18","8":"5","9":"5","_rn_":"1782"},{"1":"12","2":"male","3":"47.0","4":"15.000","5":"no","6":"1","7":"18","8":"6","9":"5","_rn_":"1784"},{"1":"3","2":"female","3":"32.0","4":"15.000","5":"yes","6":"4","7":"16","8":"4","9":"4","_rn_":"1791"},{"1":"7","2":"female","3":"32.0","4":"15.000","5":"yes","6":"3","7":"14","8":"3","9":"2","_rn_":"1831"},{"1":"7","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"16","8":"1","9":"2","_rn_":"1840"},{"1":"12","2":"male","3":"42.0","4":"15.000","5":"yes","6":"3","7":"18","8":"6","9":"2","_rn_":"1844"},{"1":"7","2":"female","3":"42.0","4":"15.000","5":"yes","6":"2","7":"14","8":"3","9":"2","_rn_":"1856"},{"1":"12","2":"male","3":"27.0","4":"7.000","5":"yes","6":"2","7":"17","8":"5","9":"4","_rn_":"1876"},{"1":"3","2":"male","3":"32.0","4":"10.000","5":"yes","6":"4","7":"14","8":"4","9":"3","_rn_":"1929"},{"1":"7","2":"male","3":"47.0","4":"15.000","5":"yes","6":"3","7":"16","8":"4","9":"2","_rn_":"1935"},{"1":"1","2":"male","3":"22.0","4":"1.500","5":"yes","6":"1","7":"12","8":"2","9":"5","_rn_":"1938"},{"1":"7","2":"female","3":"32.0","4":"10.000","5":"yes","6":"2","7":"18","8":"5","9":"4","_rn_":"1941"},{"1":"2","2":"male","3":"32.0","4":"10.000","5":"yes","6":"2","7":"17","8":"6","9":"5","_rn_":"1954"},{"1":"2","2":"male","3":"22.0","4":"7.000","5":"yes","6":"3","7":"18","8":"6","9":"2","_rn_":"1959"},{"1":"1","2":"female","3":"32.0","4":"15.000","5":"yes","6":"3","7":"14","8":"1","9":"5","_rn_":"9010"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Neste post, a an√°lise de dados ser√° feita considerando a vari√°vel <code>affairs</code> (Quantas vezes envolvido em caso extraconjugal no √∫ltimo ano (aparentemente em 1977)) e a base de dados conta com as vari√°veis g√™nero, idade, anos de casado, se tem crian√ßas, religiosidade, educa√ß√£o, ocupa√ß√£o e como avalia o casamento.</p>
<p>Informa√ß√µes detalhadas podem ser conferidas na tabela a seguir, retirada do artigo apresentado:</p>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/tab.png" /></p>
<p>Obs.: Essa tabela foi feita com o pacote <a href="https://plot.ly/r/"><code>plotly</code></a>, o c√≥digo pode ser conferido <a href="https://gist.github.com/gomesfellipe/4d1d17ca97ac6dadfabad6baef3c5539">aqui</a>.</p>
</div>
</div>
</div>
<div id="vis√£o-geral-dos-dados" class="section level1">
<h1>Vis√£o geral dos dados</h1>
<p>Entendendo as dimens√µes do conjunto de dados, nomes de vari√°veis, resumo geral, vari√°veis ausentes e tipos de dados de cada vari√°vel com a fun√ß√£o <code>ExpData()</code>, se o argumento Type = 1, visualiza√ß√£o dos dados (os nomes das colunas s√£o ‚ÄúDescri√ß√µes‚Äù, ‚ÄúObs.‚Äù), j√° se Type = 2, estrutura dos dados (os nomes das colunas s√£o ‚ÄúS.no‚Äù, ‚ÄúVarName‚Äù, ‚ÄúVarClass‚Äù, ‚ÄúVarType‚Äù)
:</p>
<pre class="r"><code># Visao geral dos dados - Type = 1
ExpData(data=Affairs, type=1) # O tipo 1 √© uma vis√£o geral dos dados</code></pre>
<pre><code>##                                           Descriptions    Value
## 1                                   Sample size (nrow)      601
## 2                              No. of variables (ncol)        9
## 3                    No. of numeric/interger variables        7
## 4                              No. of factor variables        2
## 5                                No. of text variables        0
## 6                             No. of logical variables        0
## 7                          No. of identifier variables        0
## 8                                No. of date variables        0
## 9             No. of zero variance variables (uniform)        0
## 10               %. of variables having complete cases 100% (9)
## 11   %. of variables having &gt;0% and &lt;50% missing cases   0% (0)
## 12 %. of variables having &gt;=50% and &lt;90% missing cases   0% (0)
## 13          %. of variables having &gt;=90% missing cases   0% (0)</code></pre>
<p>Conferindo o nome das vari√°veis e os tipos de cada uma:</p>
<pre class="r"><code># Estrutura dos dados - Type = 2
ExpData(data=Affairs, type=2) # O tipo 2 √© a estrutura dos dados</code></pre>
<pre><code>##   Index Variable_Name Variable_Type Per_of_Missing No_of_distinct_values
## 1     1       affairs       numeric              0                     6
## 2     2        gender        factor              0                     2
## 3     3           age       numeric              0                     9
## 4     4  yearsmarried       numeric              0                     8
## 5     5      children        factor              0                     2
## 6     6 religiousness       integer              0                     5
## 7     7     education       numeric              0                     7
## 8     8    occupation       integer              0                     7
## 9     9        rating       integer              0                     5</code></pre>
<p>Esta fun√ß√£o fornece vis√£o geral e estrutura dos quadros de dados.</p>
</div>
<div id="an√°lise-explorat√≥ria-dos-dados" class="section level1">
<h1>An√°lise explorat√≥ria dos dados</h1>
<p>As fun√ß√µes a seguir apresentam a sa√≠da EDA para 3 casos diferentes de an√°lise explorat√≥ria dos dados, s√£o elas:</p>
<ul>
<li><p>A vari√°vel de destino n√£o est√° definida</p></li>
<li><p>A vari√°vel alvo √© cont√≠nua</p></li>
<li><p>A vari√°vel de destino √© categ√≥rica</p></li>
</ul>
<p>Para fins ilustrativos, ser√° feita inicialmente uma an√°lise considerando que n√£o existe vari√°vel resposta, em seguida ser√° considerada a vari√°vel <code>affairs</code> como vari√°vel resposta e por fim, ser√° feita uma transforma√ß√£o nesta vari√°vel resposta num√©rica de forma que ela seja discretizada da seguinte maneira:</p>
<p><span class="math display">\[
1 \text{ se j√° houve caso extraconjugal} \\
0 \text{ se n√£o houve caso extraconjugal}
\]</span></p>
</div>
<div id="relat√≥rio-em-uma-linha" class="section level1">
<h1>Relat√≥rio em uma linha</h1>
<p>Caso o interesse seja apenas ter uma no√ß√£o geral dos dados de forma extremamente r√°pida, basta rodar a linha de c√≥digo abaixo:</p>
<pre><code>ExpReport(Affairs,op_file = &quot;teste.html&quot;)</code></pre>
<p>Antes de come√ßar a explanar cada um dos casos, achei que seria legal frisar que al√©m de tudo que ser√° apresentado, existe a op√ß√£o de se obter um relat√≥rio extenso sobre a an√°lise explorat√≥ria dos dados em apenas uma linha!</p>
<div id="exemplo-para-o-caso-1-a-vari√°vel-de-destino-n√£o-est√°-definida" class="section level2">
<h2>Exemplo para o caso 1: a vari√°vel de destino n√£o est√° definida</h2>
<p>Para ilustrar o primeiro caso, onde a vari√°vel destino n√£o √© definida, vamos supor que n√£o existe uma vari√°vel alvo na nossa base de dados e estamos interessados em simplesmente obter uma vis√£o geral enquanto pensamos em quais t√©cnicas estat√≠sticas ser√£o utilizadas para avaliar nosso dataset.</p>
<div id="resumo-das-vari√°veis-num√©ricas" class="section level3">
<h3>Resumo das vari√°veis num√©ricas</h3>
<p>Resumo de de todas as vari√°veis num√©ricas:</p>
<pre class="r"><code>ExpNumStat (Affairs, 
            by = &quot;A&quot;,       # Agrupar por A (estat√≠sticas resumidas por Todos), G (estat√≠sticas resumidas por grupo), GA (estat√≠sticas resumidas por grupo e Geral)
            gp = NULL,      # vari√°vel de destino, se houver, padr√£o NULL
            MesofShape = 2, # Medidas de formas (assimetria e curtose).
            Outlier = TRUE, # Calcular o limite inferior, o limite superior e o n√∫mero de outliers
            round = 2)      # Arredondar</code></pre>
<pre><code>##   Vname Group
## 1     1   All</code></pre>
<p>Podemos ver que n√£o existem vari√°veis negativas e a √∫nica vari√°vel que apresentou ‚Äúzero‚Äù foi a vari√°vel resposta. Nenhum registro como <code>Inf</code> ou como <code>NA</code> e al√©m das medidas descritivas tamb√©m podemos notar as medidas de <code>skweness</code> e <code>kurtosis</code>. Alguns coment√°rios sobre essas medidas:</p>
<p>Medidas de forma para dar uma avalia√ß√£o detalhada dos dados. Explica a quantidade e a dire√ß√£o do desvio.</p>
<ul>
<li><strong>Kurotsis</strong> explica o qu√£o alto e afiado √© o pico central (Achatamento).</li>
<li><strong>Skewness</strong> n√£o tem unidades: mas um n√∫mero, como um escore z (medida da assimetria)</li>
</ul>
<p>Onde:</p>
<p><a href="https://pt.wikipedia.org/wiki/Curtose"><strong>Kurtose</strong></a>:</p>
<p>A curtose √© uma medida de forma que caracteriza o achatamento da curva da fun√ß√£o de distribui√ß√£o de probabilidade, Assim:</p>
<ul>
<li>Se o valor da curtose for = 0 (ou 3, pela segunda defini√ß√£o), ent√£o tem o mesmo achatamento que a distribui√ß√£o normal. Chama-se a estas fun√ß√µes de mesoc√∫rticas</li>
<li>Se o valor √© &gt; 0 (ou &gt; 3), ent√£o a distribui√ß√£o em quest√£o √© mais alta (afunilada) e concentrada que a distribui√ß√£o normal. Diz-se que esta fun√ß√£o probabilidade √© leptoc√∫rtica, ou que a distribui√ß√£o tem caudas pesadas (o significado √© que √© relativamente f√°cil obter valores que n√£o se aproximam da m√©dia a v√°rios m√∫ltiplos do desvio padr√£o)</li>
<li>Se o valor √© &lt; 0 (ou &lt; 3), ent√£o a fun√ß√£o de distribui√ß√£o √© mais ‚Äúachatada‚Äù que a distribui√ß√£o normal. Chama-se-lhe platic√∫rtica</li>
</ul>
<p><a href="https://pt.wikipedia.org/wiki/Obliquidade"><strong>Skewness</strong></a>:</p>
<p>O Skewness mede a assimetria das caudas da distribui√ß√£o. Distribui√ß√µes assim√©tricas que tem uma cauda mais ‚Äúpesada‚Äù que a outra apresentam obliquidade. Distribui√ß√µes sim√©tricas tem obliquidade zero. Assim:</p>
<ul>
<li>Se v&gt;0, ent√£o a distribui√ß√£o tem uma cauda direita (valores acima da m√©dia) mais pesada</li>
<li>Se v&lt;0, ent√£o a distribui√ß√£o tem uma cauda esquerda (valores abaixo da m√©dia) mais pesada</li>
<li>Se v=0, ent√£o a distribui√ß√£o √© aproximadamente sim√©trica (na terceira pot√™ncia do desvio em rela√ß√£o √† m√©dia).</li>
</ul>
<div id="distribui√ß√µes-de-vari√°veis-num√©ricas" class="section level4">
<h4>Distribui√ß√µes de vari√°veis num√©ricas</h4>
<p>Representa√ß√£o gr√°fica de todos os recursos num√©ricos com <strong>gr√°fico de densidade</strong> (uni variada):</p>
<pre class="r"><code># Nota: Vari√°vel exclu√≠da (se o valor √∫nico da vari√°vel for menor ou igual a 10 [im = 10])

ExpNumViz(Affairs,
          Page=c(2,2), # padr√£o de sa√≠da. 
          sample=NULL) # sele√ß√£o aleat√≥ria de plots</code></pre>
<pre><code>## $`0`</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-9-1.png" width="672" /><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-9-2.png" width="672" /></p>
<p>Exibidos os gr√°ficos com as densidades das vari√°veis num√©ricas. Como podemos ver a maioria da amostra n√£o registrou caso extraconjugal, a maioria tem de 12 ou mais anos de casado. A m√©dia amostral da idade dos indiv√≠duos √© de aproximadamente 32 anos apresentando leve assimetria com cauda a direita. As demais vari√°veis podem ser conferidas visualmente.</p>
</div>
</div>
<div id="resumo-de-vari√°veis-categ√≥ricas" class="section level3">
<h3>Resumo de vari√°veis categ√≥ricas</h3>
<p>Essa fun√ß√£o selecionar√° automaticamente vari√°veis categ√≥ricas e gerar√° frequ√™ncia ou tabelas cruzadas com base nas entradas do usu√°rio. A sa√≠da inclui contagens, porcentagens, total de linhas e total de colunas.</p>
<p>Frequ√™ncia para todas as vari√°veis independentes categ√≥ricas:</p>
<pre class="r"><code>ExpCTable(Affairs,
          Target=NULL)</code></pre>
<pre><code>##         Variable  Valid Frequency Percent CumPercent
## 1         gender female       315   52.41      52.41
## 2         gender   male       286   47.59     100.00
## 3         gender  TOTAL       601      NA         NA
## 4       children     no       171   28.45      28.45
## 5       children    yes       430   71.55     100.00
## 6       children  TOTAL       601      NA         NA
## 7        affairs      0       451   75.04      75.04
## 8        affairs      1        34    5.66      80.70
## 9        affairs     12        38    6.32      87.02
## 10       affairs      2        17    2.83      89.85
## 11       affairs      3        19    3.16      93.01
## 12       affairs      7        42    6.99     100.00
## 13       affairs  TOTAL       601      NA         NA
## 14           age   17.5         6    1.00       1.00
## 15           age     22       117   19.47      20.47
## 16           age     27       153   25.46      45.93
## 17           age     32       115   19.13      65.06
## 18           age     37        88   14.64      79.70
## 19           age     42        56    9.32      89.02
## 20           age     47        23    3.83      92.85
## 21           age     52        21    3.49      96.34
## 22           age     57        22    3.66     100.00
## 23           age  TOTAL       601      NA         NA
## 24  yearsmarried  0.125        11    1.83       1.83
## 25  yearsmarried  0.417        10    1.66       3.49
## 26  yearsmarried   0.75        31    5.16       8.65
## 27  yearsmarried    1.5        88   14.64      23.29
## 28  yearsmarried     10        70   11.65      34.94
## 29  yearsmarried     15       204   33.94      68.88
## 30  yearsmarried      4       105   17.47      86.35
## 31  yearsmarried      7        82   13.64      99.99
## 32  yearsmarried  TOTAL       601      NA         NA
## 33 religiousness      1        48    7.99       7.99
## 34 religiousness      2       164   27.29      35.28
## 35 religiousness      3       129   21.46      56.74
## 36 religiousness      4       190   31.61      88.35
## 37 religiousness      5        70   11.65     100.00
## 38 religiousness  TOTAL       601      NA         NA
## 39     education     12        44    7.32       7.32
## 40     education     14       154   25.62      32.94
## 41     education     16       115   19.13      52.07
## 42     education     17        89   14.81      66.88
## 43     education     18       112   18.64      85.52
## 44     education     20        80   13.31      98.83
## 45     education      9         7    1.16      99.99
## 46     education  TOTAL       601      NA         NA
## 47    occupation      1       113   18.80      18.80
## 48    occupation      2        13    2.16      20.96
## 49    occupation      3        47    7.82      28.78
## 50    occupation      4        68   11.31      40.09
## 51    occupation      5       204   33.94      74.03
## 52    occupation      6       143   23.79      97.82
## 53    occupation      7        13    2.16      99.98
## 54    occupation  TOTAL       601      NA         NA
## 55        rating      1        16    2.66       2.66
## 56        rating      2        66   10.98      13.64
## 57        rating      3        93   15.47      29.11
## 58        rating      4       194   32.28      61.39
## 59        rating      5       232   38.60      99.99
## 60        rating  TOTAL       601      NA         NA</code></pre>
<p>Obs.: <code>NA</code> significa <code>Not Applicable</code></p>
</div>
<div id="distribui√ß√µes-de-vari√°veis-categ√≥ricas" class="section level3">
<h3>Distribui√ß√µes de vari√°veis categ√≥ricas</h3>
<p>Essa fun√ß√£o varre automaticamente cada vari√°vel e cria um gr√°fico de barras para vari√°veis categ√≥ricas.</p>
<p>Gr√°ficos de barra para todas as vari√°veis categ√≥ricas</p>
<pre class="r"><code>ExpCatViz(Affairs,
          fname=NULL, # Nome do arquivo de saida, default √© pdf
          clim=10,# categorias m√°ximas a incluir nos gr√°ficos de barras.
          margin=2,# √≠ndice, 1 para propor√ß√µes baseadas em linha e 2 para propor√ß√µes baseadas em colunas
          Page = c(2,1), # padrao de saida
          sample=4) # sele√ß√£o aleat√≥ria de plot</code></pre>
<pre><code>## $`0`</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-11-1.png" width="672" /><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
</div>
<div id="machine-lerning-usando-algor√≠timo-n√£o-supervisionado-de-agrupamento" class="section level3">
<h3>Machine Lerning usando algor√≠timo n√£o supervisionado de agrupamento</h3>
<p>Apenas para efeitos ilustrativos, como estamos supondo que n√£o temos a vari√°vel resposta vou remover a coluna <code>affairs</code> do data set e considerarei apenas as vari√°veis num√©ricas para fazer uma an√°lise multivariada com o algor√≠timo de machine learning <a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/kmeans.html"><code>kmeans</code></a>.</p>
<p>A fun√ß√£o <a href="https://github.com/gomesfellipe/functions/blob/master/plot_kmeans.R"><code>plot_kmeans()</code></a> pode ser encontrada em <a href="github.com/gomesfellipe">meu github</a> no <a href="https://github.com/gomesfellipe/functions">reposit√≥rio aberto de fun√ß√µes</a>.</p>
<p>Vejamos os resultados:</p>
<pre class="r"><code>plot_kmeans(Affairs[,-c(1)] %&gt;% select_if(is.numeric) , 2)</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Como podemos observar, foram detectados dois grupos no conjunto de dados. O ideal agora seria fazer uma AED desses clusters identificados e avaliar qual o comportamento dos grupos formados mas como essa vari√°vel foi omitida e a seguir discutiremos a avalia√ß√£o da base diante de da vari√°vel resposta, deixo essas an√°lises aos curiosos de plant√£o.</p>
<p>Mais informa√ß√µes sobre an√°lise multivariava podem ser encontrada no meu post sobre <a href="https://gomesfellipe.github.io/post/2018-01-01-analise-multivariada-em-r/an%C3%A1lise-multivariada-em-r/">An√°lise Multivariada com r</a> e tamb√©m em um <a href="https://www.kaggle.com/gomes555/an-lise-multivariada-pca-e-kmeans">kernel que escrevi para a plataforma kaggle</a>.</p>
<p>Al√©m disso disponibilizo uma aplica√ß√£o Shiny que criei a algum tempo para PCA (An√°lise de componentes Principais) e tarefa de machine learning com agrupamento <a href="https://gomesfellipe.shinyapps.io/appPCAkmeans/">nenste link</a>.</p>
</div>
</div>
<div id="exemplo-para-o-caso-2-a-vari√°vel-de-destino-√©-cont√≠nua" class="section level2">
<h2>Exemplo para o caso 2: A vari√°vel de destino √© cont√≠nua</h2>
<p>Agora vamos considerar que estamos diante de um desfecho onde a vari√°vel alvo √© cont√≠nua, para isso ser√° considerada a vari√°vel <code>affairs</code> como vari√°vel alvo.</p>
<div id="resumo-da-vari√°vel-dependente-cont√≠nua" class="section level3">
<h3>Resumo da vari√°vel dependente cont√≠nua</h3>
<p>Descri√ß√£o da vari√°vel affairs:</p>
<pre class="r"><code>summary(Affairs[,&quot;affairs&quot;])</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   0.000   0.000   0.000   1.456   0.000  12.000</code></pre>
</div>
<div id="resumo-das-vari√°veis-num√©ricas-1" class="section level3">
<h3>Resumo das vari√°veis num√©ricas</h3>
<p>Estat√≠sticas de resumo quando a vari√°vel dependente √© cont√≠nua Pre√ßo.</p>
<pre class="r"><code>ExpNumStat(Affairs,
           by=&quot;A&quot;, # Agrupar por A (estat√≠sticas resumidas por Todos), G (estat√≠sticas resumidas por grupo), GA (estat√≠sticas resumidas por grupo e Geral)
           Qnt=seq(0,1,0.1), # padr√£o NULL. Quantis especificados [c (0,25,0,75) encontrar√£o os percentis 25 e 75]
           MesofShape=1, # Medidas de formas (assimetria e curtose)
           Outlier=TRUE, # Calcular limite superior , inferior e numero de outliers
           round=2) # Arredondamento</code></pre>
<pre><code>##   Vname Group
## 1     1   All</code></pre>
<pre class="r"><code>#Se a vari√°vel de destino for cont√≠nua, as estat√≠sticas de resumo adicionar√£o a coluna de correla√ß√£o (Correla√ß√£o entre a vari√°vel de destino e todas as vari√°veis independentes)</code></pre>
<div id="distribui√ß√µes-de-vari√°veis-num√©ricas-1" class="section level4">
<h4>Distribui√ß√µes de vari√°veis num√©ricas</h4>
<p>Representa√ß√£o gr√°fica de todas as vari√°veis num√©ricas com gr√°ficos de dispers√£o (bivariada)</p>
<p>Gr√°fico de dispers√£o entre todas as vari√°veis num√©ricas e a vari√°vel de destino affairs. Esta trama ajuda a examinar qu√£o bem uma vari√°vel alvo est√° correlacionada com vari√°veis dependentes.</p>
<p>Vari√°vel dependente √© affairs (cont√≠nuo).</p>
<pre class="r"><code>ExpNumViz(Affairs,
            target=&quot;affairs&quot;, # Variavel alvo
            nlim=4, # a vari√°vel num√©rica com valor exclusivo √© maior que 4
            Page=c(2,2), # formato de saida
            sample=NULL) # selecionado aleatoriamente 8 gr√°ficos de dispers√£o</code></pre>
<pre><code>## $`0`</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-15-1.png" width="672" /><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-15-2.png" width="672" /></p>
</div>
</div>
<div id="resumo-de-vari√°veis-categ√≥ricas-1" class="section level3">
<h3>Resumo de vari√°veis categ√≥ricas</h3>
<p>Resumo de vari√°veis categ√≥ricas de acordo com a frequ√™ncia para todas as vari√°veis independentes categ√≥ricas por Affairs</p>
<pre class="r"><code>##bin=4, descretized 4 categories based on quantiles
ExpCTable(Affairs, Target=&quot;affairs&quot;)</code></pre>
<pre><code>##         VARIABLE CATEGORY affairs:(-0.012,4] affairs:(4,8] affairs:(8,12] TOTAL
## 1         gender   female                273            22             20   315
## 2         gender     male                248            20             18   286
## 3         gender    TOTAL                521            42             38   601
## 4       children       no                157             7              7   171
## 5       children      yes                364            35             31   430
## 6       children    TOTAL                521            42             38   601
## 7        affairs        0                451             0              0   451
## 8        affairs        1                 34             0              0    34
## 9        affairs       12                  0             0             38    38
## 10       affairs        2                 17             0              0    17
## 11       affairs        3                 19             0              0    19
## 12       affairs        7                  0            42              0    42
## 13       affairs    TOTAL                521            42             38   601
## 14           age     17.5                  4             0              2     6
## 15           age       22                112             4              1   117
## 16           age       27                138             9              6   153
## 17           age       32                 95            11              9   115
## 18           age       37                 71             8              9    88
## 19           age       42                 44             6              6    56
## 20           age       47                 19             1              3    23
## 21           age       52                 17             2              2    21
## 22           age       57                 21             1              0    22
## 23           age    TOTAL                521            42             38   601
## 24  yearsmarried    0.125                 11             0              0    11
## 25  yearsmarried    0.417                 10             0              0    10
## 26  yearsmarried     0.75                 29             0              2    31
## 27  yearsmarried      1.5                 85             2              1    88
## 28  yearsmarried       10                 57             8              5    70
## 29  yearsmarried       15                165            17             22   204
## 30  yearsmarried        4                 94             9              2   105
## 31  yearsmarried        7                 70             6              6    82
## 32  yearsmarried    TOTAL                521            42             38   601
## 33 religiousness        1                 37             5              6    48
## 34 religiousness        2                138            14             12   164
## 35 religiousness        3                105            13             11   129
## 36 religiousness        4                177             6              7   190
## 37 religiousness        5                 64             4              2    70
## 38 religiousness    TOTAL                521            42             38   601
## 39     education       12                 36             2              6    44
## 40     education       14                139             6              9   154
## 41     education       16                108             5              2   115
## 42     education       17                 72             9              8    89
## 43     education       18                 94            11              7   112
## 44     education       20                 67             9              4    80
## 45     education        9                  5             0              2     7
## 46     education    TOTAL                521            42             38   601
## 47    occupation        1                101             6              6   113
## 48    occupation        2                 13             0              0    13
## 49    occupation        3                 39             5              3    47
## 50    occupation        4                 60             4              4    68
## 51    occupation        5                177            12             15   204
## 52    occupation        6                120            13             10   143
## 53    occupation        7                 11             2              0    13
## 54    occupation    TOTAL                521            42             38   601
## 55        rating        1                 11             1              4    16
## 56        rating        2                 43             8             15    66
## 57        rating        3                 80             9              4    93
## 58        rating        4                169            18              7   194
## 59        rating        5                218             6              8   232
## 60        rating    TOTAL                521            42             38   601</code></pre>
<div id="distribui√ß√µes-de-vari√°veis-categ√≥ricas-1" class="section level4">
<h4>Distribui√ß√µes de vari√°veis categ√≥ricas</h4>
<p>Essa fun√ß√£o varre automaticamente cada vari√°vel e cria um gr√°fico de barras para vari√°veis categ√≥ricas.</p>
<p>Gr√°ficos de barra para todas as vari√°veis categ√≥ricas</p>
<pre class="r"><code>ExpCatViz(Affairs,
          target=&quot;affairs&quot;, # Variavel target
          fname=NULL, # Nome do arquivo de saida, default √© pdf
          clim=10,# categorias m√°ximas a incluir nos gr√°ficos de barras.
          margin=2,# √≠ndice, 1 para propor√ß√µes baseadas em linha e 2 para propor√ß√µes baseadas em colunas
          Page = c(2,1), # padrao de saida
          sample=4) # sele√ß√£o aleat√≥ria de plot</code></pre>
<pre><code>## $`0`</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-17-1.png" width="672" /><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-17-2.png" width="672" /></p>
</div>
</div>
<div id="avaliando-a-correla√ß√£o-entre-as-vari√°veis" class="section level3">
<h3>Avaliando a correla√ß√£o entre as vari√°veis</h3>
<pre class="r"><code>library(ggplot2)
library(dplyr)
library(GGally)
data(&quot;Affairs&quot;)
#Correla√ßoes cruzadas
Affairs%&gt;%
  select(age:rating,affairs)%&gt;%
ggpairs(lower = list(continuous = my_fn,combo=wrap(&quot;facethist&quot;, binwidth=1), 
                                       continuous=wrap(my_bin, binwidth=0.25)),aes(fill=affairs))+theme_bw()</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<pre class="r"><code>ggcorr(Affairs,label = T,nbreaks = 5,label_round = 4)</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
</div>
<div id="modelo-de-regress√£o-linear-usando-stepwiseaic" class="section level3">
<h3>Modelo de regress√£o linear usando stepwiseAIC</h3>
<p>Por fim, vamos ajustar um modelo de regress√£o linear para entender quais s√£o as vari√°veis significativas para explicar a varia√ß√£o da vari√°vel resposta e qual o efeito de cada uma dessas vari√°veis explicativas no nosso desfecho.</p>
<p>Com o R base √© poss√≠vel ajustar um modelo de regress√£o linear simples utilizando a fun√ß√£o <code>lm()</code> e em seguida usar a fun√ß√£o <code>step()</code> para utilizar t√©cnicas como <a href="https://en.wikipedia.org/wiki/Stepwise_regression">stepwise</a>, por√©m como quero utilizar tamb√©m a t√©cnica de <a href="https://pt.wikipedia.org/wiki/Valida%C3%A7%C3%A3o_cruzada">valida√ß√£o cruzada</a>. Para isso vou utilizar o pacote <a href="https://cran.r-project.org/web/packages/caret/caret.pdf"><code>caret</code></a>, muito famoso por facilitar o ajuste de modelos de machine learning (ou mesmo modelos estat√≠sticos tradicionais).</p>
<p>Al√©m disso estou usando as transforma√ß√µes <a href="https://www.rdocumentation.org/packages/caret/versions/6.0-79/topics/preProcess"><code>center()</code></a>, que subtrai a m√©dia dos dados e <a href="https://www.rdocumentation.org/packages/caret/versions/6.0-79/topics/preProcess"><code>scale()</code></a> divide pelo desvio padr√£o.</p>
<pre class="r"><code>data(&quot;Affairs&quot;)
library(caret)
set.seed(123)
index &lt;- sample(1:2,nrow(Affairs),replace=T,prob=c(0.8,0.2))
train = Affairs[index==1,] %&gt;%as.data.frame()
test = Affairs[index==2,] %&gt;%as.data.frame()

# Setando os par√¢metros para o controle do ajuste do modelo:
fitControl &lt;- trainControl(method = &quot;repeatedcv&quot;,         # 10fold cross validation
                     number = 10, repeats=5                         # do 5 repititi√ß√µes of cv
                     )

# Regress√£o Linear com Stepwise
set.seed(825)
lmFit &lt;- train(affairs ~ ., data = train,
                method = &quot;lmStepAIC&quot;, 
                trControl = fitControl,
                preProc = c(&quot;center&quot;, &quot;scale&quot;),trace=F)
summary(lmFit)</code></pre>
<pre><code>## 
## Call:
## lm(formula = .outcome ~ age + yearsmarried + religiousness + 
##     occupation + rating, data = dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.1452 -1.7819 -0.7601  0.2719 11.3518 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     1.4146     0.1401  10.096  &lt; 2e-16 ***
## age            -0.6890     0.2291  -3.007 0.002779 ** 
## yearsmarried    1.1058     0.2302   4.804 2.09e-06 ***
## religiousness  -0.5121     0.1455  -3.519 0.000475 ***
## occupation      0.3858     0.1445   2.669 0.007858 ** 
## rating         -0.7830     0.1470  -5.326 1.55e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.07 on 474 degrees of freedom
## Multiple R-squared:  0.144,  Adjusted R-squared:  0.135 
## F-statistic: 15.95 on 5 and 474 DF,  p-value: 1.542e-14</code></pre>
<p>Como podemos ver as vari√°veis Idade, Anos de casado, religiosidade, ocupa√ß√£o e como avaliam o pr√≥prio relacionamento se apresentaram significantes</p>
<p>Como o <span class="math inline">\(R^2=0,144\)</span>, conclui-se que <span class="math inline">\(14,4%\)</span> da varia√ß√£o da quantidade de vezes que foi envolvida em caso extraconjugal no √∫ltimo ano √© explicada pelo modelo ajustado.</p>
<p>Observando a coluna das estimativas, podemos notar o quanto varia a quantidade de vezes que foi envolvido em caso extraconjugal ao aumentar em 1 unidade cada uma das vari√°veis explicativas.</p>
<p>Al√©m disso o valor p obtido atrav√©s da estat√≠stica F foi menor do que <span class="math inline">\(\alpha = 0.05\)</span>, o que implica que pelo menos uma das vari√°veis explicativas tem rela√ß√£o significativa com a vari√°vel resposta.</p>
<p>Selecionando apenas as vari√°veis selecionadas com o ajuste do modelo:</p>
<pre class="r"><code>train=as.data.frame(train[,c(1,3,4,6,8,9)])
test=as.data.frame(test[,c(1,3,4,6,8,9)])</code></pre>
<div id="diagn√≥stico-do-modelo" class="section level4">
<h4>Diagn√≥stico do modelo</h4>
<p>Existem varias formas e t√©cnicas de se avaliar o ajuste de um modelo e como o foco deste post √© apresentar as utilidades do pacote <code>SmartEAD</code> irei fazer uma avalia√ß√£o muito breve sobre os res√≠duos, apresento mais algumas maneiras no post sobre <a href="https://gomesfellipe.github.io/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos/">pacotes do R para avaliar o ajuste de modelos</a>.</p>
<div id="avaliando-residuos" class="section level5">
<h5>Avaliando residuos</h5>
<pre class="r"><code>library(GGally)
# calculate all residuals prior to display
residuals &lt;- lapply(train[2:ncol(train)], function(x) {
  summary(lm(affairs ~ x, data = train))$residuals
})

# add a &#39;fake&#39; column
train$Residual &lt;- seq_len(nrow(train))

# calculate a consistent y range for all residuals
y_range &lt;- range(unlist(residuals))

# plot the data
ggduo(
  train,
  2:6, c(1,7),
  types = list(continuous = lm_or_resid)
)+ theme_bw()</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<pre class="r"><code>train=train%&gt;%
  select(-Residual)</code></pre>
<p>Neste gr√°fico √© poss√≠vel observar como se comportam os ajustes de modelos lineares de cada vari√°vel explicativa em rela√ß√£o √† vari√°vel resposta e al√©m disso na segunda linha √© poss√≠vel notar o comportamento dos res√≠duos no modelo.</p>
<p>Uma das suposi√ß√µes do ajuste de um modelo linear normal √© de que <span class="math inline">\(\epsilon \sim N(0,\sigma^2)\)</span> e visualmente parece que essa condi√ß√£o n√£o deve ser atendida, pois esperar√≠amos algo como uma ‚Äúnuvem‚Äù aleat√≥ria de pontos em torno de zero.</p>
</div>
<div id="residuos-e-medidas-de-influencia" class="section level5">
<h5>Residuos e medidas de influencia</h5>
<p>Al√©m da suposi√ß√£o da normalidade dos res√≠duos, existem ainda mais detalhes do comportamento desses erros, uma breve apresenta√ß√£o no gr√°fico a seguir:</p>
<pre class="r"><code>library(ggfortify)

autoplot(lmFit$finalModel, which = 1:6, data = train,
         colour = &#39;affairs&#39;, label.size = 3,
         ncol = 3)+theme_classic()</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>Pelo que parece no gr√°fico com t√≠tulo ‚ÄúNormal Q-Q‚Äù, as vari√°veis associadas √† vari√°vel resposta com valores acima de 6 se comportam de forma inesperadas quando comparadas com os quantis te√≥ricos.</p>
</div>
</div>
</div>
</div>
<div id="exemplo-para-o-caso-3-a-vari√°vel-de-destino-√©-categ√≥rica" class="section level2">
<h2>Exemplo para o caso 3: a vari√°vel de destino √© categ√≥rica</h2>
<p>Para finalizar a avalia√ß√£o da base de dados, a Vari√°vel alvo ser√° discretizado de tal forma:</p>
<p><span class="math display">\[
1 = \text{se affairs} &gt; 0\\
0 = c.c.
\]</span></p>
<p>Essa transforma√ß√£o ser√° utilizada apenas com fins ilustrativos do algor√≠timo de √°rvore de decis√µes, que est√° ficando muito comum na ci√™ncia de dados como uma tarefa supervisionada de machine learning.</p>
<pre class="r"><code>Affairs = Affairs %&gt;% 
  mutate(daffairs = ifelse(Affairs$affairs!=0,1,0)) %&gt;% 
  mutate(daffairs = as.factor(daffairs))%&gt;% 
  select(-affairs)
levels(Affairs$daffairs) = c(&quot;N√£o&quot;, &quot;Sim&quot;)</code></pre>
<div id="resumo-das-vari√°veis-num√©ricas-2" class="section level3">
<h3>Resumo das vari√°veis num√©ricas</h3>
<p>Resumo de todas as vari√°veis num√©ricas</p>
<pre class="r"><code>ExpNumStat(Affairs,
           by=&quot;A&quot;, # Agrupar por A (estat√≠sticas resumidas por Todos), G (estat√≠sticas resumidas por grupo), GA (estat√≠sticas resumidas por grupo e Geral)
           gp=&quot;daffairs&quot;, # Variavel alvo
           Qnt=seq(0,1,0.1), # padr√£o NULL. Quantis especificados [c (0,25,0,75) encontrar√£o os percentis 25 e 75]
           MesofShape=1, # Medidas de formas (assimetria e curtose)
           Outlier=TRUE, # Calcular limite superior , inferior e numero de outliers
           round=2) # Arredondamento</code></pre>
<pre><code>##   Vname Group
## 1     1   All</code></pre>
<div id="distribui√ß√µes-de-vari√°veis-num√©ricas-2" class="section level4">
<h4>Distribui√ß√µes de vari√°veis num√©ricas</h4>
<p>Box plots para todas as vari√°veis num√©ricas vs vari√°vel dependente categ√≥rica - Compara√ß√£o bivariada apenas com categorias</p>
<p>Boxplot para todos os atributos num√©ricos por cada categoria de affair</p>
<pre class="r"><code>ExpNumViz(Affairs, target=&quot;daffairs&quot;) # amostra de variaveis para o resumo</code></pre>
<pre><code>## [[1]]</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<pre><code>## 
## [[2]]</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-26-2.png" width="672" /></p>
<pre><code>## 
## [[3]]</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-26-3.png" width="672" /></p>
<pre><code>## 
## [[4]]</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-26-4.png" width="672" /></p>
<pre><code>## 
## [[5]]</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-26-5.png" width="672" /></p>
<pre><code>## 
## [[6]]</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-26-6.png" width="672" /></p>
</div>
</div>
<div id="resumo-das-vari√°veis-categ√≥ricas" class="section level3">
<h3>Resumo das vari√°veis categ√≥ricas</h3>
<p>Tabula√ß√£o cruzada com vari√°vel de destino com tabelas customizadas entre todas as vari√°veis independentes categ√≥ricas e a vari√°vel de destino <code>daffairs</code>:</p>
<pre class="r"><code>ExpCTable(Affairs,
          Target=&quot;daffairs&quot;, # variavel alvo
          margin=1, # 1 para proporcoes por linha, 2 para colunas
          clim=10, # maximo de categorias consideradas por frequencia/ custom table
          round=2, # arredondar
          per=F) # valores percentuais. Tabela padr√£o dar√° contagens.</code></pre>
<pre><code>##         VARIABLE CATEGORY daffairs:N√£o daffairs:Sim TOTAL
## 1         gender   female          243           72   315
## 2         gender     male          208           78   286
## 3         gender    TOTAL          451          150   601
## 4       children       no          144           27   171
## 5       children      yes          307          123   430
## 6       children    TOTAL          451          150   601
## 7            age     17.5            3            3     6
## 8            age       22          101           16   117
## 9            age       27          117           36   153
## 10           age       32           77           38   115
## 11           age       37           65           23    88
## 12           age       42           38           18    56
## 13           age       47           16            7    23
## 14           age       52           15            6    21
## 15           age       57           19            3    22
## 16           age    TOTAL          451          150   601
## 17  yearsmarried    0.125           10            1    11
## 18  yearsmarried    0.417            9            1    10
## 19  yearsmarried     0.75           28            3    31
## 20  yearsmarried      1.5           76           12    88
## 21  yearsmarried       10           49           21    70
## 22  yearsmarried       15          142           62   204
## 23  yearsmarried        4           78           27   105
## 24  yearsmarried        7           59           23    82
## 25  yearsmarried    TOTAL          451          150   601
## 26 religiousness        1           28           20    48
## 27 religiousness        2          123           41   164
## 28 religiousness        3           86           43   129
## 29 religiousness        4          157           33   190
## 30 religiousness        5           57           13    70
## 31 religiousness    TOTAL          451          150   601
## 32     education       12           31           13    44
## 33     education       14          119           35   154
## 34     education       16           95           20   115
## 35     education       17           62           27    89
## 36     education       18           79           33   112
## 37     education       20           60           20    80
## 38     education        9            5            2     7
## 39     education    TOTAL          451          150   601
## 40    occupation        1           90           23   113
## 41    occupation        2           10            3    13
## 42    occupation        3           32           15    47
## 43    occupation        4           47           21    68
## 44    occupation        5          160           44   204
## 45    occupation        6          104           39   143
## 46    occupation        7            8            5    13
## 47    occupation    TOTAL          451          150   601
## 48        rating        1            8            8    16
## 49        rating        2           33           33    66
## 50        rating        3           66           27    93
## 51        rating        4          146           48   194
## 52        rating        5          198           34   232
## 53        rating    TOTAL          451          150   601</code></pre>
<div id="distribui√ß√µes-de-vari√°veis-categ√≥ricas-2" class="section level4">
<h4>Distribui√ß√µes de vari√°veis categ√≥ricas</h4>
<p>Gr√°fico de barras empilhadas com barras verticais ou horizontais para todas as vari√°veis categ√≥ricas</p>
<pre class="r"><code>ExpCatViz(Affairs,
          target=&quot;daffairs&quot;,
          fname=NULL, # Nome do arquivo de saida, default √© pdf
          clim=10,# categorias m√°ximas a incluir nos gr√°ficos de barras.
          margin=2,# √≠ndice, 1 para propor√ß√µes baseadas em linha e 2 para propor√ß√µes baseadas em colunas
          Page = c(2,1), # padrao de saida
          sample=4) # sele√ß√£o aleat√≥ria de plot</code></pre>
<pre><code>## $`0`</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-28-1.png" width="672" /><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-28-2.png" width="672" /></p>
</div>
</div>
<div id="valor-da-informa√ß√£o" class="section level3">
<h3>Valor da informa√ß√£o</h3>
<p><code>IV</code> √© o peso da evid√™ncia e valores da informa√ß√£o, <span class="math inline">\(ln(odss) \times(pct0 - pct1)\)</span> onde <span class="math inline">\(pct1 =\frac{\text{&quot;boas observa√ß√µes&quot;}}{\text{&quot;total boas observa√ß√µes&quot;}}\)</span>; <span class="math inline">\(pct0 = \frac{&quot;\text{observa√ß√µes ruins&quot;} }{ \text{&quot;total de observa√ß√µes ruins&quot;}}\)</span> e $odds =  $</p>
<pre class="r"><code>ExpCatStat(Affairs %&gt;% mutate(daffairs = if_else(daffairs==&quot;N√£o&quot;, 0, 1)) ,
           Target=&quot;daffairs&quot;,
           result = &quot;IV&quot;) %&gt;% 
  select(-one_of(&quot;Target&quot;,&quot;Ref_1&quot;,&quot;Ref_0&quot;))</code></pre>
<pre><code>##           Variable  Class Out_1 Out_0 TOTAL Per_1 Per_0 Odds   WOE   IV
## 1         gender.1 female    72   243   315  0.48  0.54 0.79 -0.12 0.01
## 2         gender.2   male    78   208   286  0.52  0.46 1.27  0.12 0.01
## 3       children.1     no    27   144   171  0.18  0.32 0.47 -0.58 0.08
## 4       children.2    yes   123   307   430  0.82  0.68 2.14  0.19 0.03
## 5            age.1   17.5     3     3     6  0.02  0.01 3.05  0.69 0.01
## 6            age.2     22    16   101   117  0.11  0.22 0.41 -0.69 0.08
## 7            age.3     27    36   117   153  0.24  0.26 0.90 -0.08 0.00
## 8            age.4     32    38    77   115  0.25  0.17 1.65  0.39 0.03
## 9            age.5     37    23    65    88  0.15  0.14 1.08  0.07 0.00
## 10           age.6     42    18    38    56  0.12  0.08 1.48  0.41 0.02
## 11           age.7     47     7    16    23  0.05  0.04 1.33  0.22 0.00
## 12           age.8     52     6    15    21  0.04  0.03 1.21  0.29 0.00
## 13           age.9     57     3    19    22  0.02  0.04 0.46 -0.69 0.01
## 14  yearsmarried.1  0.125     1    10    11  0.01  0.02 0.30 -0.69 0.01
## 15  yearsmarried.2  0.417     1     9    10  0.01  0.02 0.33 -0.69 0.01
## 16  yearsmarried.3   0.75     3    28    31  0.02  0.06 0.31 -1.11 0.04
## 17  yearsmarried.4    1.5    12    76    88  0.08  0.17 0.43 -0.76 0.07
## 18  yearsmarried.5     10    21    49    70  0.14  0.11 1.34  0.24 0.01
## 19  yearsmarried.6     15    62   142   204  0.41  0.31 1.53  0.28 0.03
## 20  yearsmarried.7      4    27    78   105  0.18  0.17 1.05  0.06 0.00
## 21  yearsmarried.8      7    23    59    82  0.15  0.13 1.20  0.14 0.00
## 22 religiousness.1      1    20    28    48  0.13  0.06 2.32  0.77 0.05
## 23 religiousness.2      2    41   123   164  0.27  0.27 1.00  0.00 0.00
## 24 religiousness.3      3    43    86   129  0.29  0.19 1.71  0.43 0.04
## 25 religiousness.4      4    33   157   190  0.22  0.35 0.53 -0.46 0.06
## 26 religiousness.5      5    13    57    70  0.09  0.13 0.66 -0.37 0.01
## 27     education.1     12    13    31    44  0.09  0.07 1.29  0.25 0.00
## 28     education.2     14    35   119   154  0.23  0.26 0.85 -0.13 0.00
## 29     education.3     16    20    95   115  0.13  0.21 0.58 -0.48 0.04
## 30     education.4     17    27    62    89  0.18  0.14 1.38  0.25 0.01
## 31     education.5     18    33    79   112  0.22  0.18 1.33  0.20 0.01
## 32     education.6     20    20    60    80  0.13  0.13 1.00  0.00 0.00
## 33     education.7      9     2     5     7  0.01  0.01 1.21  0.00 0.00
## 34    occupation.1      1    23    90   113  0.15  0.20 0.73 -0.29 0.01
## 35    occupation.2      2     3    10    13  0.02  0.02 0.90  0.00 0.00
## 36    occupation.3      3    15    32    47  0.10  0.07 1.45  0.36 0.01
## 37    occupation.4      4    21    47    68  0.14  0.10 1.40  0.34 0.01
## 38    occupation.5      5    44   160   204  0.29  0.35 0.75 -0.19 0.01
## 39    occupation.6      6    39   104   143  0.26  0.23 1.17  0.12 0.00
## 40    occupation.7      7     5     8    13  0.03  0.02 1.91  0.41 0.00
## 41        rating.1      1     8     8    16  0.05  0.02 3.12  0.92 0.03
## 42        rating.2      2    33    33    66  0.22  0.07 3.57  1.14 0.17
## 43        rating.3      3    27    66    93  0.18  0.15 1.28  0.18 0.01
## 44        rating.4      4    48   146   194  0.32  0.32 0.98  0.00 0.00
## 45        rating.5      5    34   198   232  0.23  0.44 0.37 -0.65 0.14</code></pre>
</div>
<div id="testes-estat√≠sticos" class="section level3">
<h3>Testes estat√≠sticos</h3>
<p>Al√©m de toda a informa√ß√£o visual e das estat√≠sticas descritivas, ainda contamos com alguma fun√ß√£o que fornece estat√≠sticas resumidas para todas as colunas de caracteres ou categ√≥ricas no data frame</p>
<pre class="r"><code>ExpCatStat(Affairs %&gt;% mutate(daffairs = if_else(daffairs==&quot;N√£o&quot;, 0, 1)),
           Target=&quot;daffairs&quot;, # variavel alvo
           result = &quot;Stat&quot;) # resumo de estatisticas</code></pre>
<pre><code>##        Variable   Target Unique Chi-squared p-value df IV Value Cramers V
## 1        gender daffairs      2       1.334   0.248  1     0.02      0.05
## 2      children daffairs      2      10.055   0.002  1     0.11      0.13
## 3           age daffairs      9      17.771   0.023  8     0.15      0.17
## 4  yearsmarried daffairs      8      17.177   0.016  7     0.17      0.17
## 5 religiousness daffairs      5      19.354   0.001  4     0.16      0.18
## 6     education daffairs      7       7.057   0.316  6     0.06      0.11
## 7    occupation daffairs      7       6.718   0.348  6     0.04      0.11
## 8        rating daffairs      5      41.433   0.000  4     0.35      0.26
##   Degree of Association    Predictive Power
## 1             Very Weak      Not Predictive
## 2                  Weak Somewhat Predictive
## 3                  Weak Somewhat Predictive
## 4                  Weak Somewhat Predictive
## 5                  Weak Somewhat Predictive
## 6                  Weak      Not Predictive
## 7                  Weak      Not Predictive
## 8              Moderate   Highly Predictive</code></pre>
<p>Os crit√©rios usados para classifica√ß√£o de poder preditivo vari√°vel categ√≥rico s√£o</p>
<ul>
<li><p>Se o valor da informa√ß√£o for &lt;0,03, ent√£o, poder de previs√£o = ‚ÄúN√£o Preditivo‚Äù</p></li>
<li><p>Se o valor da informa√ß√£o √© de 0,3 a 0,1, ent√£o o poder preditivo = ‚Äúum pouco preditivo‚Äù</p></li>
<li><p>Se o valor da informa√ß√£o for de 0,1 a 0,3, ent√£o, poder preditivo = ‚ÄúMedium Predictive‚Äù</p></li>
<li><p>Se o valor da informa√ß√£o for&gt; 0.3, ent√£o, poder preditivo = ‚ÄúAltamente Preditivo‚Äù</p></li>
</ul>
<p>Nota para a vari√°vel <code>rating</code> que segundo essas regras, demonstrou alto poder preditivo.</p>
</div>
<div id="machine-learning-com-random-forest" class="section level3">
<h3>Machine Learning com Random Forest</h3>
<p>O algor√≠timo supervisionado de machine learning conhecido como <a href="https://www.stat.berkeley.edu/~breiman/RandomForests/">Random Forest</a> √© uma grande caixa preta. Apresenta resultados muito robustos pois combina o resultado de v√°rias √°rvores de decis√µes e pode ser facilmente aplicada com o pacote <code>caret</code>.</p>
<p><a href="https://topepo.github.io/caret/variable-importance.html">No livro do pacote caret</a> o algor√≠timo √© apresentado da seguinte maneira: ‚Äúsegundo o pacote do R: Para cada √°rvore, a precis√£o da previs√£o na parte fora do saco dos dados √© registrada. Ent√£o, o mesmo √© feito ap√≥s a permuta√ß√£o de cada vari√°vel preditora. A diferen√ßa entre as duas precis√µes √© calculada pela m√©dia de todas as √°rvores e normalizada pelo erro padr√£o. Para a regress√£o, o MSE √© calculado nos dados fora da bolsa para cada √°rvore e, em seguida, o mesmo √© computado ap√≥s a permuta√ß√£o de uma vari√°vel. As diferen√ßas s√£o calculadas e normalizadas pelo erro padr√£o. Se o erro padr√£o √© igual a 0 para uma vari√°vel, a divis√£o n√£o √© feita.‚Äù</p>
<p>N√£o entrarei em muitos detalhes sobre o algor√≠timo pois esta parte √© apenas um demonstrativo dos diferentes cen√°rios de an√°lise explorat√≥ria dos dados. Ser√£o comentadas apenas algumas m√©tricas utilizadas.</p>
<p>Ajuste com o algor√≠timo Random Forest:</p>
<pre class="r"><code>library(caret)
set.seed(1)
index &lt;- sample(1:2,nrow(Affairs),replace=T,prob=c(0.8,0.2))
train = Affairs[index==1,] %&gt;%as.data.frame()
test = Affairs[index==2,] %&gt;%as.data.frame()


# Setando os par√¢metros para o controle do ajuste do modelo:
fitControl &lt;- trainControl(method = &quot;repeatedcv&quot;,         # 10fold cross validation
                     number = 10
                     )

# Random Forest
set.seed(825)
antes = Sys.time()
rfFit &lt;- train(daffairs ~ ., data = train,
                method = &quot;rf&quot;, 
                trControl = fitControl,
                trace=F,
                preProc = c(&quot;center&quot;, &quot;scale&quot;))

antes - Sys.time() # Para saber quanto tempo durou o ajuste</code></pre>
<pre><code>## Time difference of -13.38876 secs</code></pre>
<p>Resultados do ajuste:</p>
<pre class="r"><code>rfFit</code></pre>
<pre><code>## Random Forest 
## 
## 484 samples
##   8 predictor
##   2 classes: &#39;N√£o&#39;, &#39;Sim&#39; 
## 
## Pre-processing: centered (8), scaled (8) 
## Resampling: Cross-Validated (10 fold, repeated 1 times) 
## Summary of sample sizes: 436, 436, 435, 435, 435, 436, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##   2     0.7539966  0.1369868
##   5     0.7292942  0.1727691
##   8     0.7231718  0.1613695
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 2.</code></pre>
<p><strong>Accurary e Kappa</strong></p>
<p>Essas s√£o as m√©tricas padr√£o usadas para avaliar algoritmos em conjuntos de dados de classifica√ß√£o bin√°ria.</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Accuracy_and_precision"><strong>Accuray</strong></a>: √© a porcentagem de classificar corretamente as inst√¢ncias fora de todas as inst√¢ncias. √â mais √∫til em uma classifica√ß√£o bin√°ria do que problemas de classifica√ß√£o de v√°rias classes, porque pode ser menos claro exatamente como a precis√£o √© dividida entre essas classes (por exemplo, voc√™ precisa ir mais fundo com uma matriz de confus√£o).</li>
<li><a href="https://en.wikipedia.org/wiki/Cohen%27s_kappa"><strong>Kappa ou Kappa de Cohen</strong></a> √© como a precis√£o da classifica√ß√£o, exceto que √© normalizado na linha de base da chance aleat√≥ria em seu conjunto de dados. √â uma medida mais √∫til para usar em problemas que t√™m um desequil√≠brio nas classes (por exemplo, divis√£o de 70 a 30 para as classes 0 e 1 e voc√™ pode atingir 70% de precis√£o prevendo que todas as inst√¢ncias s√£o para a classe 0).</li>
</ul>
<p>A seguir a ‚ÄúVariable Importance‚Äù de cada vari√°vel:</p>
<pre class="r"><code>rfImp = varImp(rfFit);rfImp</code></pre>
<pre><code>## rf variable importance
## 
##               Overall
## rating         100.00
## age             94.66
## religiousness   85.77
## education       78.99
## yearsmarried    67.41
## occupation      62.48
## gendermale       6.94
## childrenyes      0.00</code></pre>
<pre class="r"><code>plot(rfImp)</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>A fun√ß√£o dimensiona automaticamente as pontua√ß√µes de import√¢ncia entre 0 e 100, os escores de import√¢ncia da vari√°vel em Random Forest s√£o medidas agregadas. Eles apenas quantificam o impacto do preditor, n√£o o efeito espec√≠fico, para isso utilizamos o ajuste um modelo param√©trico onde conseguimos estimar termos estruturais.</p>
<p>√â claro que existem muitos adentos a serem feitos tanto na forma como os dados foram apresentados no ajuste do modelo linear e no Random Forest, mas como a finalidade do post continua sendo apresentar o pacote SmartEAD, encerrarei a avalia√ß√£o por aqui.</p>
<p>Caso algu√©m queira entender com mais detalhes a avalia√ß√£o de modelos de machine learning, talvez <a href="https://topepo.github.io/caret/measuring-performance.html">o livro do pacote caret</a> seja uma alternativa interessante para ter uma no√ß√£o geral.</p>
<blockquote>
<p><em>Todos os modelos est√£o errados, alguns s√£o √∫teis - George Box</em></p>
</blockquote>
<p>N√£o conseguimos nenhum modelo √∫til que quantificasse as incertezas nas modelagens deste post mas conseguimos executar praticamente todas as fun√ß√µes do pacote <code>SmartEAD</code> e foi muito √∫til para conhecer a base em poucas linhas, obrigado Dayanand Ubrangala, Kiran R. e Ravi Prasad Kondapalli!</p>
</div>
</div>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2018-05-26-smarteademachinelearning/smarteademachinelearning/">AED de forma r√°pida e um pouco de Machine Learning</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Analise Explorat√≥ria</category>
      <category>Data mining</category>
      <category>Estatistica</category>
      <category>Modelagem Estatistica</category>
      <category>Pr√°tica</category>
      <category>R</category>
      <category>Reports</category>
      <category>Machine Learning</category>
      <category>Analise Mutivariada</category>
      <category>Aprendizado Supervisionado</category>
      <category>Aprendizado N√£o Supervisionado</category>
      <category domain="tag">analise multivariada</category>
      <category domain="tag">Correlacoes</category>
      <category domain="tag">Data Mining</category>
      <category domain="tag">Estatistica</category>
      <category domain="tag">gomesfellipe</category>
      <category domain="tag">kmeans</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">pca</category>
      <category domain="tag">R</category>
      <category domain="tag">RStudio</category>
    </item>
    <item>
      <title>O paradoxo dos anivers√°rios com simula√ß√£o e probabilidade</title>
      <link>https://gomesfellipe.github.io/post/2018-01-20-simulacao-e-probabilidade/simulacao-e-probabilidade/</link>
      <pubDate>Sat, 20 Jan 2018 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2018-01-20-simulacao-e-probabilidade/simulacao-e-probabilidade/</guid>
      <description>Quanto voc√™ acha que √© a probabiliddade num grupo de 23 pessoas escolhidas aleatoriamente que duas delas far√£o anivers√°rio no mesmo dia? Acreditaria se eu te dissesse que essa chance √© maior do que 50%? A probabilidade √© contra intuitiva e neste post vamos demonstrar de forma analitica e atraves de simula√ß√£o esse e outros resultados al√©m de dissertar um pouco sobre a hist√≥ria e conceitos importantes de probabilidade</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="curiosidades-sobre-a-teoria-das-probabilidades" class="section level1">
<h1>Curiosidades sobre a teoria das probabilidades</h1>
<p>O uso de c√°lculo de probabilidades para avaliar incertezas j√° √© utilizado a centenas de anos. Foram tantas √°reas que se encontraram aplica√ß√µes (como na medicina, jogos de azar, previs√£o do tempo‚Ä¶) que hoje n√£o restam d√∫vidas de que os dados s√£o onipresentes, ainda mais em plena era da informa√ß√£o.</p>
<p>Os conceitos de chances e de incertezas s√£o t√£o antigos quando a pr√≥pria civiliza√ß√£o. Pessoas sempre tiveram que lidar com incertezas sobre o clima, suprimento de alimentos, suprimentos de √°gua, risco de vida e tantas outras amea√ßas ao ser humano que o esfor√ßo para reduzir essas incertezas e seus efeitos passou a ser muito importante.</p>
<p>A ideia do jogo tem uma longa hist√≥ria,j√° no egito antigo em 2000 a.c foram encontrados em tumbas (<a href="https://pt.wikipedia.org/wiki/Jogo_de_azar#Hist%C3%B3ria">dados c√∫bicos com marca√ß√µes praticamente id√™nticas √†s de dados modernos (wikipedia)</a>).</p>
<p>Segundo <span class="citation"><a href="#ref-DeGroot" role="doc-biblioref">DeGroot</a> (<a href="#ref-DeGroot" role="doc-biblioref">n.d.</a>)</span>, a teoria da probabilidade foi desenvolvida de forma constante desde o s√©culo XVII e tem sido amplamente aplicada em diversos campos de estudo. Hoje, a teoria da probabilidade √© uma ferramenta importante na maioria das √°reas de engenharia, ci√™ncia e gest√£o.</p>
<p>Muitos pesquisadores est√£o ativamente envolvidos na descoberta e no estabelecimento de novas aplica√ß√µes de probabilidade em campos de qu√≠mica, meteorologia, fotografia de sat√©lites, marketing, previs√£o de terremoto, comportamento humano, design de sistemas inform√°ticos, finan√ßas, gen√©tica e lei.</p>
<div id="conceitos-e-interpreta√ß√µes-para-probabilidades" class="section level2">
<h2>Conceitos e interpreta√ß√µes para probabilidades</h2>
<p>Al√©m das muitas aplica√ß√µes formais da teoria da probabilidade, o conceito de probabilidade entra em nossa vida cotidiana e conversa.</p>
<p>Muitas vezes ouvimos e usamos express√µes como ‚Äú<em>Provavelmente vai chover a amanh√£ √† noite</em>,‚Äù ‚Äú<em>√â muito prov√°vel que o onibus atrase</em>,‚Äù ou ‚Äú<em>As chances s√£o altas de n√£o poder se juntar a n√≥s para almo√ßar esta tarde</em>.‚Äù Cada uma dessas express√µes √© baseada no conceito da probabilidade de que algum evento espec√≠fico ocorrer√°.</p>
<p>Existem tr√™s abordagens atualmente, as duas primeiras s√£o:</p>
<div id="cl√°ssica" class="section level4">
<h4>Cl√°ssica</h4>
<ul>
<li><p>Se refere √† subconjuntos unit√°rios equiprov√°veis</p></li>
<li><p><span class="math inline">\(P(A)=\dfrac{\text{N√∫mero de elementos de }A}{\text{N√∫mero de elementos de }\Omega}\)</span></p></li>
</ul>
</div>
<div id="frequentista-ou-estat√≠stica" class="section level4">
<h4>Frequentista ou Estat√≠stica</h4>
<ul>
<li><p>Considera o limite de frequ√™ncias relativas como o valor de probabilidade</p></li>
<li><p><span class="math inline">\(P(A)=lim_{n \rightarrow \infty} \frac{n_A}{n}\)</span></p></li>
</ul>
<p>onde <span class="math inline">\(n_A\)</span> √© o n¬∫ de ocorr√™ncias de <span class="math inline">\(A\)</span> em <span class="math inline">\(n\)</span> repeti√ß√µes independentes do experimento</p>
</div>
<div id="defini√ß√£o-de-probabilidade" class="section level4">
<h4>Defini√ß√£o de probabilidade</h4>
<p>Segundo <span class="citation"><a href="#ref-Magalhaes" role="doc-biblioref">Magalh√£es</a> (<a href="#ref-Magalhaes" role="doc-biblioref">n.d.</a>)</span>, as defini√ß√µes acima possuem o apelo da intui√ß√£o e permanecem sendo usadas para resolver in√∫meros problemas, entretanto elas n√£o s√£o suficientes para uma formula√ß√£o matem√°tica rigorosa da probabilidade.</p>
<p>Aproximadamente em 1930 A. N. Kolmogorov apresentou um conjunto de axiomas matem√°ticos para definir probabilidade, permitindo incluir as defini√ß√µes anteriores como casos particulares.</p>
<p>Por√©m, como o verdadeiro significado da probabilidade ainda √© um assunto altamente pol√™mico e est√° envolvido em muitas discuss√µes filos√≥ficas atuais sobre as bases da estat√≠stica e quando se trata de probabilidades, n√£o adianta utilizar apenas a intui√ß√£o pois nosso c√©rebro vai da bug!</p>
<p>A probabilidade √© extremamente contra intuitiva e seu estudo deve sempre envolver uma vasta gama de exerc√≠cios para treinar nosso racioc√≠nio anal√≠tico. Existem diversos problemas pr√°ticos que j√° ilustraram isso e um √≥timo exemplo que todo mundo que j√° fez um curso b√°sico de probabilidade j√° conhece, o <a href="https://pt.wikipedia.org/wiki/Paradoxo_do_anivers%C3%A1rio">Parad√≥xo do anivers√°rio</a></p>
</div>
</div>
</div>
<div id="o-paradoxo-do-anivers√°rio-ou-problema-dos-anivers√°rios---feller68" class="section level1">
<h1>O paradoxo do anivers√°rio (ou problema dos anivers√°rios - Feller[68])</h1>
<p>Exemplo retirado do livro do <span class="citation"><a href="#ref-Feller" role="doc-biblioref">Feller</a> (<a href="#ref-Feller" role="doc-biblioref">n.d.</a>)</span>, questiona:</p>
<p>‚ÄúNum grupo de <span class="math inline">\(n\)</span> pessoas, qual √© a probabilidade de pelo menos duas delas fazerem anivers√°rio no mesmo dia?‚Äù</p>
<p>Esse problema surpreende todo mundo porque dependendo do valor de <span class="math inline">\(n\)</span> pessoas, a probabilidade √© bastante alta! Segundo veremos a probabilidade de isso ocorrer em uma turma de 23 pessoas ou mais escolhidas <strong>aleatoriamente</strong> √© maior que <strong>50%</strong>!</p>
<p>Qual aluno de qualquer turma de probabilidade que nunca foi desafiado numa aposta pelo professor que tinha dois alunos com mesma data de anivers√°rio na sala de aula e se deu conta que perderia em poucos minutos?</p>
<p>Vamos resolver esse problema tanto pela abordagem cl√°ssica quanto pela abordagem frequentista, para utilizar a segunda abordagem dados de muitas turmas de variados tamanhos ser√£o simulados utilizando o <strong>R</strong> e podemos comparar os resultados e buscar alguma evid√™ncia de que os dados se distribuem de forma semelhante!</p>
<p><strong>Obs</strong>: Simular dados permitem imitar o funcionamento de, praticamente, qualquer tipo de opera√ß√£o ou processo (sistemas) do mundo real!</p>
</div>
<div id="probabilidade" class="section level1">
<h1>Probabilidade</h1>
<p>Considerando o ano com 365 dias, podemos assumir que <span class="math inline">\(n&lt;365\)</span> primeiramente devemos definir o espa√ßo amostral <span class="math inline">\(\Omega\)</span> que ser√° o conjunto de todas as sequ√™ncias formadas com as datas dos anivers√°rios (associamos cada data a um dos 365 dias do ano), defini-se:</p>
<p><em>experimento</em>: observar o anivers√°rio de n pessoas</p>
<p><span class="math display">\[
\Omega = \{ (1,1,...,1),(1,2,53,...,201),(24,27,109,...,200),... \}
\]</span></p>
<p>portanto, sua cardinalidade ser√°:</p>
<p><span class="math display">\[
\#\Omega = 365^n
\]</span></p>
<p>Definindo o evento:</p>
<p><span class="math display">\[
A = \text{pelo meno 2 alunos fazendo anivers√°rio no mesmo dia em uma turma de tamanho }n
\]</span>
Observa-se que √© um evento complicado de se calcular. Uma pr√°tica muito comum na teoria das probabilidades nestes casos √© estudar o complementar do evento de interesse, veja:</p>
<p><span class="math display">\[
A^c = \text{nenhum dos alunos fazenndo anivers√°rio no mesmo dia em uma turma de tamanho }n
\]</span></p>
<p>Agora basta fazer a conta:</p>
<p><span class="math display">\[
P(A^c)=\frac{\#A^c}{\#\Omega}=\frac{365 \times 364 \times ... \times (365-n+1)}{365^n}=\frac{365!}{365^n (365-n)!}
\]</span></p>
<p>segundo propriedades , se o evento √© o complementar de todos n serem diferentes consequentemente o seguinte resultado √© verdadeiro:</p>
<p><span class="math display">\[
p(A)=1- \frac{365!}{365^n (365-n)!}
\]</span></p>
<p>Agora que j√° sabemos a probabilidade de pelo menos duas pessoas fazerem anivers√°rio no mesmo dia em uma turma de <span class="math inline">\(n\)</span> alunos, vejamos o comportamento deste ajuste e uma tabela com poss√≠veis valores de <span class="math inline">\(n\)</span>:</p>
<p>Em R:</p>
<p>Utilizando expans√£o em s√©rie de Taylor (<a href="https://pt.wikipedia.org/wiki/Paradoxo_do_anivers%C3%A1rio#Aproxima%C3%A7%C3%B5es">mais informa√ß√µes</a>):</p>
<pre class="r"><code>birthday=function(x){
  a=1-exp(-(x^2)/(2*365))
  return(a)
}
birthday(23)</code></pre>
<pre><code>## [1] 0.5155095</code></pre>
<table class="table table-condensed">
<thead>
<tr>
<th style="text-align:right;">
n
</th>
<th style="text-align:right;">
P
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FFF5EB; width: 20.00%">5</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FFF5EB; width: 20.00%">0.0336668</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FEE6CE; width: 30.00%">15</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FEE6CE; width: 39.17%">0.2652457</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FDD0A2; width: 40.00%">25</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FDD0A2; width: 64.84%">0.5752117</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FDAE6B; width: 50.00%">35</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FDAE6B; width: 84.54%">0.8132683</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FD8D3C; width: 60.00%">45</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FD8D3C; width: 94.84%">0.9375864</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #F16913; width: 70.00%">55</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #F16913; width: 98.69%">0.9841381</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #D94801; width: 80.00%">65</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #D94801; width: 99.75%">0.9969349</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #A63603; width: 90.00%">75</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #A63603; width: 99.97%">0.9995496</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #7F2704; width: 100.00%">85</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #7F2704; width: 100.00%">0.9999497</span>
</td>
</tr>
</tbody>
</table>
<p>Em Python (fun√ß√£o retirada do <a href="https://pt.wikipedia.org/wiki/Paradoxo_do_anivers%C3%A1rio#Implementa%C3%A7%C3%A3o_em_Python">wikp√©dia</a> para comparar os resultados):</p>
<pre class="python"><code>def birthday(x):
    p = (1.0/365)**x
    for i in range((366-x),366):
        p *= i
    return 1-p
    
print(&quot;%1.7f&quot; %(birthday(23))) #Arredondando para o mesmo numero de casas decimais default do R</code></pre>
<pre><code>## 0.5072972</code></pre>
<p>Tanto a aproxima√ß√£o do R quanto a do Python obtiveram resultados semelhantes</p>
<p>Vejamos como √© o comportamento da curva te√≥rica e as estima√ß√µes:</p>
<p><img src="/post/2018-01-20-simulacao-e-probabilidade/simulacao-e-probabilidade_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Note que segundo a distribui√ß√£o te√≥rica, confirmamos que a probabilidade do evento ocorrer em uma turma de 23 pessoas ou mais escolhidas <strong>aleatoriamente</strong> √© maior que <strong>50%</strong>!</p>
</div>
<div id="simula√ß√£o" class="section level1">
<h1>Simula√ß√£o</h1>
<p>Segundo o <a href="https://pt.wikipedia.org/wiki/Simula%C3%A7%C3%A3o">wikip√©dia</a>, a simula√ß√£o ‚Äúconsiste em empregar formaliza√ß√µes em computadores, como express√µes matem√°ticas ou especifica√ß√µes mais ou menos formalizadas, com o prop√≥sito de imitar um processo ou opera√ß√£o do mundo real‚Äù</p>
<p>Nossa simula√ß√£o ir√° consistir em imitar o comportamento de um processo do mundo real utilizando o seguinte c√≥digo para simular o experimento de <em>observar o anivers√°rio de <span class="math inline">\(n\)</span> pessoas</em> milhares de vezes:</p>
<pre class="r"><code>N&lt;- 5000                                    #Numero de simulacoes do experimento

prob=0
for(n in 2:100){                            #Para n variand de 2 at√© 50
  cont_a=0                                  #Inicia o contador
  M=matrix(NA, N, n)                        #Delara uma matriz varia com as dimensoes desejadas  
  for(i in 1:N){                            #indice i que percorre todas as N linhas simuladas
    M[i,] = sample(1:365, n, replace = T)   #Sorteio de uma amosra de tamanho n de numeros de 1 a 365 
    linha=M[i,]                             #objeto linha recebe a linha simulada
    tab=table(linha)                        #objeto tab guarda a tabela de frequencias dessa amostra
    if(length(tab)&lt;n){                      #se o tamanho da tabela de frequencias for menor que o tamanho da turma
      cont_a=cont_a+1                       #contador recebe 1 pois duas pessoas fizeram aniversario no mesmo dia
    } 
  }
  prob[n]=cont_a/N                          #a probabilidade ser√° a proporcao de pessoas que fazem aniversario no mesmo dia observadas em N amostra simuladas
}

prob[23]</code></pre>
<pre><code>## [1] 0.5088</code></pre>
<p>Notamos que o resultado observado √© muito pr√≥ximo d resultado calculado de acordo com a probabilidade teoria para a chance de se se encontrar pelo menos 2 pessoas que fazem anivers√°rio em uma turma de 23 anos (<em>novamente ultrapassou os 50%!!!</em>)</p>
<p>Para efeito de compara√ß√£o visual com a resolu√ß√£o anterior:</p>
<table class="table table-condensed">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
n
</th>
<th style="text-align:right;">
P
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
5
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FFF5EB; width: 20.00%">5</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FFF5EB; width: 20.00%">0.0236</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
15
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FEE6CE; width: 30.00%">15</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FEE6CE; width: 38.30%">0.2470</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
25
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FDD0A2; width: 40.00%">25</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FDD0A2; width: 65.21%">0.5754</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
35
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FDAE6B; width: 50.00%">35</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FDAE6B; width: 85.02%">0.8172</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
45
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FD8D3C; width: 60.00%">45</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #FD8D3C; width: 95.00%">0.9390</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
55
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #F16913; width: 70.00%">55</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #F16913; width: 98.87%">0.9862</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
65
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #D94801; width: 80.00%">65</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #D94801; width: 99.85%">0.9982</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
75
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #A63603; width: 90.00%">75</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #A63603; width: 99.97%">0.9996</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
85
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #7F2704; width: 100.00%">85</span>
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: #7F2704; width: 100.00%">1.0000</span>
</td>
</tr>
</tbody>
</table>
<p><img src="/post/2018-01-20-simulacao-e-probabilidade/simulacao-e-probabilidade_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<div id="comparando" class="section level1">
<h1>Comparando</h1>
<p>Por fim, vejamos de forma visual se o comportamento dos resultados simulados est√£o de acordo com o resultado te√≥rico calculado:</p>
<p><img src="/post/2018-01-20-simulacao-e-probabilidade/simulacao-e-probabilidade_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Como podemos ver o comportamento dos dados simulados foi muito similar ao da curva te√≥rica calculada.</p>
</div>
<div id="modelagem-e-simula√ß√£o-em-probabilidade" class="section level1">
<h1>Modelagem e simula√ß√£o em probabilidade</h1>
<p>Existe uma vasta gama de aplica√ß√µes de simula√ß√µes como em projetos de an√°lises de sistemas de manufatura, avalia√ß√£o de requisitos n√£o funcionais de hardware e software, avalia√ß√£o de novas armas e t√°ticas militares, reposi√ß√£o de estoque, projeto de sistemas de transporte, avalia√ß√µes de servi√ßos, aplica√ß√µes estat√≠sticas de cadeias MCMC‚Ä¶</p>
<p>Um simulador permite testar v√°rias alternativas a um custo <strong>geralmente</strong> mais baixo do que no mundo real, possibilitando o melhor entendimento sobre o problema!</p>
</div>
<div id="refer√™ncias" class="section level1 unnumbered">
<h1>Refer√™ncias</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-DeGroot" class="csl-entry">
DeGroot, Morris H. n.d. <em>Probability and Statistics</em>. Vol. 4.
</div>
<div id="ref-Feller" class="csl-entry">
Feller, William. n.d. <em>An Introduction to Probability Theory and Its Applications</em>. Vol. 3.
</div>
<div id="ref-Magalhaes" class="csl-entry">
Magalh√£es, Mascos N. n.d. <em>Probabilidade e Vari√°veis Aleat√≥riasa</em>. Vol. 1.
</div>
</div>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2018-01-20-simulacao-e-probabilidade/simulacao-e-probabilidade/">O paradoxo dos anivers√°rios com simula√ß√£o e probabilidade</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Estatistica</category>
      <category>Modelagem Estatistica</category>
      <category>Analise Explorat√≥ria</category>
      <category>Teoria</category>
      <category>Simula√ß√£o</category>
      <category>Probabilidade</category>
      <category domain="tag">gomesfellipe</category>
      <category domain="tag">Estatistica</category>
      <category domain="tag">Teoria</category>
      <category domain="tag">analise multivariada</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">modelagem estatistica</category>
      <category domain="tag">simulacao</category>
      <category domain="tag">probabilidade</category>
    </item>
    <item>
      <title>Pacotes do R para avaliar o ajuste de modelos</title>
      <link>https://gomesfellipe.github.io/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos/</link>
      <pubDate>Sun, 24 Dec 2017 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos/</guid>
      <description>Alguns pacotes √∫teis para avaliar o ajuste do modelo de forma r√°pida, precisa e elegante</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="fun√ß√µes-do-r-para-avaliar-o-ajuste-de-modelos" class="section level1">
<h1>Fun√ß√µes do R para avaliar o ajuste de modelos</h1>
<p>Traduzindo:</p>
<p>‚Äú<em>Essencialmente, todos os modelos est√£o errados, mas alguns s√£o √∫teis</em>‚Äù - George E. P. Box</p>
<p>Se voc√™ estuda estat√≠stica provavelmente j√° deve saber quem √© este simp√°tico senhor. Box teve grande contribui√ß√£o para a estat√≠stica. Foi aluno do Ronald Aylmer Fisher e ainda se casou com a filha dele!</p>
<p>Lendo um <a href="http://jaguar.fcav.unesp.br/RME/fasciculos/v27/v27_n4/A10_Millor.pdf">artigo sobre a vida de Fisher</a> um par√°grafo me chamou aten√ß√£o com uma fala de sua filha, que dizia o seguinte:</p>
<p>‚ÄúJoan Fisher Box, filha de Fisher, em seu livro sobre a vida do pai, se referindo √† p√©ssima classifica√ß√£o dele em franc√™s, escreveu: ‚Äú‚Ä¶ ele nunca teve muita paci√™ncia com irrelev√¢ncias.‚Äù (Box, 1978)"</p>
<p>Fico imaginando o tamanho da contribui√ß√£o desdes cr√¢nios para a comunidade se tivessem acesso a tantos mecanismos que temos hoje em dia e o que eles achariam relevantes..</p>
<p>Para o bom ajuste de um modelo, certamente; a infer√™ncia, as an√°lises de desvios, os crit√©rios de sele√ß√£o de um modelo, conferir comportamento dos res√≠duos e avalia√ß√£o das estat√≠sticas de diagn√≥sticos s√£o muito relevantes.</p>
<p>No <a href="https://cran.r-project.org/">CRAN</a> j√° contamos com muitos pacotes dispon√≠veis para nos auxiliar nessas avalia√ß√µes, portanto vou mostrar aqui alguns pacotes com fun√ß√µes que j√° me ajudaram muito em avalia√ß√µes de modelos indo al√©m das fun√ß√µes nativas do R e do pacote <code>ggplot2</code> (Um excelente pacote para apresenta√ß√µes elegantes e pr√°ticas de resultados visuais).</p>
</div>
<div id="ggally" class="section level1">
<h1>GGally</h1>
<p>Este pacote √© sensacional, existem fun√ß√µes muito relevantes nele para melhorar a nossa experi√™ncia com ajuste de modelos, as fun√ß√µes apresentadas aqui s√£o baseadas na <a href="http://ggobi.github.io/ggally/#ggally">p√°gina de documenta√ß√£o GGally</a>, l√° voc√™ pode conferir a documenta√ß√£o completa.</p>
<p>Primeiramente vamos carregar o pacote:</p>
<pre class="r"><code>library(GGally)</code></pre>
<p>Carregado o pacote, vejamos as principais fun√ß√µes que podem nos auxiliar.</p>
<div id="ggallyggcoef" class="section level2">
<h2><code>GGally::ggcoef</code></h2>
<p>O objetivo da fun√ß√£o <code>GGally::ggcoef</code> √© tra√ßar rapidamente os coeficientes de um modelo.</p>
<p>Para um modelo linear:</p>
<pre class="r"><code>reg &lt;- lm(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width, data = iris)
ggcoef(reg)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Para um modelo log√≠stico podemos utilizar o argumento <code>exponentiate = TRUE</code> e al√©m disso, somos capazes de fazer diversas altera√ß√µes no gr√°fico utilizando o <code>ggcoef()</code> veja alguns exemplo de argumentos que podem ser usados para personalizar como barras de erro e a linha vertical s√£o plotadas:</p>
<pre class="r"><code>#Ajustando o modelo:
d &lt;- as.data.frame(Titanic)
log.reg &lt;- glm(Survived ~ Sex + Age + Class, family = binomial, data = d, weights = d$Freq)

#Elaborando o gr√°fico
ggcoef(
  log.reg,                      #O modelo a ser conferido
  exponentiate = TRUE,          #Para avaliar o modelo log√≠stico
  vline_color = &quot;red&quot;,          #Reta em zero  
  #vline_linetype =  &quot;solid&quot;,   #Altera a linha de refer√™ncia
  errorbar_color = &quot;blue&quot;,      #Cor da barra de erros
  errorbar_height = .25,
  shape = 18,                   #Altera o formato dos pontos centrais
  #size=3,                      #Altera o tamanho do ponto
  color=&quot;black&quot;,                #Altera a cor do ponto
  mapping = aes(x = estimate, y = term, size = p.value))+
  scale_size_continuous(trans = &quot;reverse&quot;) #Essa linha faz com que inverta o tamanho                 </code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="ggallyggduo" class="section level2">
<h2><code>GGally::ggduo</code></h2>
<p>O objetivo desta fun√ß√£o √© exibir dois dados agrupados em uma matriz de plotagem. Isso √© √∫til para an√°lise de correla√ß√£o can√¥nica, an√°lise de s√©ries temporais m√∫ltiplas e an√°lise de regress√£o.</p>
<p>Os dados do exemplo apresentados aqui podem ser encontrados neste <a href="http://www.stats.idre.ucla.edu/r/dae/canonical-correlation-analysis">link</a></p>
<pre class="r"><code>data(psychademic)
head(psychademic)</code></pre>
<pre><code>##   locus_of_control self_concept motivation read write math science    sex
## 1            -0.84        -0.24          4 54.8  64.5 44.5    52.6 female
## 2            -0.38        -0.47          3 62.7  43.7 44.7    52.6 female
## 3             0.89         0.59          3 60.6  56.7 70.5    58.0   male
## 4             0.71         0.28          3 62.7  56.7 54.7    58.0   male
## 5            -0.64         0.03          4 41.6  46.3 38.4    36.3 female
## 6             1.11         0.90          2 62.7  64.5 61.4    58.0 female</code></pre>
<pre class="r"><code>psych_variables &lt;- attr(psychademic, &quot;psychology&quot;)
academic_variables &lt;- attr(psychademic, &quot;academic&quot;)</code></pre>
<pre class="r"><code>ggduo(
  psychademic, psych_variables, academic_variables,
  types = list(continuous = &quot;smooth_lm&quot;),
  title = &quot;Correla√ß√£o entre as vari√°veis psicol√≥gicas e academicas&quot;,
  xlab = &quot;Psicol√≥gicos&quot;,
  ylab = &quot;Academicas&quot;
)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Uma vez que o <code>ggduo</code> n√£o tem uma se√ß√£o superior para exibir os valores de correla√ß√£o, podemos usar uma fun√ß√£o personalizada para adicionar a informa√ß√£o nas parcelas cont√≠nuas.</p>
<p>Criando uma fun√ß√£o personalizada para informar a correla√ß√£o entre as observa√ß√µes:</p>
<pre class="r"><code>lm_with_cor &lt;- function(data, mapping, ..., method = &quot;pearson&quot;) {
  x &lt;- eval(mapping$x, data)
  y &lt;- eval(mapping$y, data)
  cor &lt;- cor(x, y, method = method)
  ggally_smooth_lm(data, mapping, ...) +
    ggplot2::geom_label(
      data = data.frame(
        x = min(x, na.rm = TRUE),
        y = max(y, na.rm = TRUE),
        lab = round(cor, digits = 3)
      ),
      mapping = ggplot2::aes(x = x, y = y, label = lab),
      hjust = 0, vjust = 1,
      size = 5, fontface = &quot;bold&quot;,
      inherit.aes = FALSE # do not inherit anything from the ...
    )
}</code></pre>
<p>Portanto:</p>
<pre class="r"><code>ggduo(
  psychademic, psych_variables, academic_variables,
  types = list(continuous = &quot;smooth_lm&quot;),
  title = &quot;Correla√ß√£o entre vari√°veis acad√™mica e psicol√≥gica&quot;,
  xlab = &quot;Psicol√≥gica&quot;,
  ylab = &quot;Academica&quot;
)+
  theme(legend.position = &quot;bottom&quot;)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Para avaliar res√≠duos da uma regress√£o ajustada para cada uma das vari√°veis explanat√≥rias vs.¬†as vari√°veis explanat√≥rias:</p>
<pre class="r"><code>dados &lt;- datasets::swiss

# Criando uma coluna &quot;fake&quot;:
dados$Residual &lt;- seq_len(nrow(dados))

# Calculando todos os res√≠duos que ser√£o exibidos:
colunas=2:6  #Informe as colunas que contem as variaveis explanatorias
residuals &lt;- lapply(dados[colunas], function(x) {
  summary(lm(Fertility ~ x, data = dados))$residuals
})
# Calculando um intervalo constante para todos os res√≠duos
y_range &lt;- range(unlist(residuals))

# Fun√ß√£o modificada para mostrar os res√≠duos:

lm_or_resid &lt;- function(data, mapping, ..., line_color = &quot;red&quot;, line_size = 1) {
  if (as.character(mapping$y) != &quot;Residual&quot;) {
    return(ggally_smooth_lm(data, mapping, ...))
  }

  # Criando os res√≠duos para apresentar:
  resid_data &lt;- data.frame(
    x = data[[as.character(mapping$x)]],
    y = residuals[[as.character(mapping$x)]]
  )

  ggplot(data = data, mapping = mapping) +
    geom_hline(yintercept = 0, color = line_color, size = line_size) +
    ylim(y_range) +
    geom_point(data = resid_data, mapping = aes(x = x, y = y), ...)

}

# Plote os dados:
ggduo(
  dados,
  2:6, c(1,7),
  types = list(continuous = lm_or_resid)
)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
</div>
<div id="ggallyggnostic" class="section level2">
<h2><code>GGally::ggnostic</code></h2>
<p>O <code>ggnostic</code> √© um wrapper de exibi√ß√£o para <code>ggduo</code> que exibe diagn√≥sticos de modelo completo para cada vari√°vel explicativa dada.</p>
<p>Por padr√£o, o ggduo exibe os valores residuais, o sigma do modelo de ‚Äúleave-one-out‚Äù, os pontos de alavanca e a dist√¢ncia de Cook em rela√ß√£o a cada vari√°vel explicativa.</p>
<p>As linhas da matriz de plotagem podem ser expandidas para incluir valores ajustados, erro padr√£o dos valores ajustados, res√≠duos padronizados e qualquer uma das vari√°veis de resposta.</p>
<p>Se o modelo for um modelo linear, os asteriscos (*) s√£o adicionados de acordo com a signific√¢ncia anova de cada vari√°vel explicativa.</p>
<p>A maioria das parcelas diagn√≥sticas cont√™m linhas de refer√™ncia para ajudar a determinar se o modelo est√° adequadamente instalado</p>
<p>Olhando para os conjuntos de dados do conjunto de dados <code>state.x77</code> ajustaremos um modelo de regress√£o m√∫ltipla para a expectativa de vida.</p>
<pre class="r"><code>#Dados que ser√£o utilizados no exemplos:
state &lt;- as.data.frame(state.x77)
#Arrumando o nome das variaveis:
colnames(state)[c(4, 6)] &lt;- c(&quot;Life.Exp&quot;, &quot;HS.Grad&quot;)
# Ajustando o modelo completo:
model &lt;- lm(Life.Exp ~ ., data = state)
# Executando o stepwise para encontrar o melhor ajuste
model &lt;- step(model, trace = FALSE)</code></pre>
<p>Executando o diagn√≥stico deste modelo com a fun√ß√£o <code>ggnostic()</code>:</p>
<pre class="r"><code># look at model diagnostics
ggnostic(model)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Para acessar as vari√°veis influentes do modelo podemos utilizar a fun√ß√£o <code>influence.measures()</code>, veja:</p>
<pre class="r"><code>summary(influence.measures(model))</code></pre>
<pre><code>## Potentially influential observations of
##   lm(formula = Life.Exp ~ Population + Murder + HS.Grad + Frost,      data = state) :
## 
##            dfb.1_ dfb.Pplt dfb.Mrdr dfb.HS.G dfb.Frst dffit   cov.r   cook.d
## Alaska      0.41   0.18    -0.40    -0.35    -0.16    -0.50    1.36_*  0.05 
## California  0.04  -0.09     0.00    -0.04     0.03    -0.12    1.81_*  0.00 
## Hawaii     -0.03  -0.57    -0.28     0.66    -1.24_*   1.43_*  0.74    0.36 
## Nevada      0.40   0.14    -0.42    -0.29    -0.28    -0.52    1.46_*  0.05 
## New York    0.01  -0.06     0.00     0.00    -0.01    -0.07    1.44_*  0.00 
##            hat    
## Alaska      0.25  
## California  0.38_*
## Hawaii      0.24  
## Nevada      0.29  
## New York    0.23</code></pre>
<p>Esta fun√ß√£o retorna as seguintes estat√≠sticas:</p>
<table>
<colgroup>
<col width="23%" />
<col width="25%" />
<col width="17%" />
<col width="17%" />
<col width="17%" />
</colgroup>
<thead>
<tr class="header">
<th>DFBeta</th>
<th>DFFit</th>
<th>CovRatio</th>
<th>D.Cook</th>
<th>h</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Altera√ß√£o no vetor estimado <span class="math inline">\(\hat \beta\)</span> ao se retirar o i-√©simo ponto da an√°lise</td>
<td>Altera√ß√£o provocada no valor ajustado pela retirada da observa√ß√£o <span class="math inline">\(i\)</span></td>
<td>Expressa o rela√ß√£o de covariancia</td>
<td>Medida de afastamento das estimativas ao retirar <span class="math inline">\(i\)</span> e tamb√©m considera o res√≠duo estudentizado internamente</td>
<td>Elementos da diagonal da matriz H</td>
</tr>
</tbody>
</table>
<p>Vejamos ent√£o um exemplo de matriz de matriz de diagn√≥stico completo.</p>
<p>As seguintes linhas de c√≥digo exibir√£o uma matriz de diagn√≥stico para o mesmo modelo:</p>
<pre class="r"><code>#Ajustando um modelo de exemplo:
flea_model &lt;- step(lm(head ~ ., data = flea), trace = FALSE)</code></pre>
<p>Todas as colunas poss√≠veis e usando <code>ggally_smooth()</code> para exibir os pontos ajustados e as vari√°veis de resposta temos:</p>
<pre class="r"><code># default output
ggnostic(flea_model,
 #        mapping = ggplot2::aes(color = species),  #Para colorir segundo um fator
         columnsY = c(&quot;head&quot;, &quot;.fitted&quot;, &quot;.se.fit&quot;, &quot;.resid&quot;, &quot;.std.resid&quot;, &quot;.hat&quot;, &quot;.sigma&quot;, &quot;.cooksd&quot;),
        continuous = list(default = ggally_smooth, .fitted = ggally_smooth)
)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
<div id="ggallyggpairs" class="section level2">
<h2><code>GGally::ggpairs</code></h2>
<p>O <code>ggpairs</code> √© uma forma especial de uma ggmatrix que produz uma compara√ß√£o pairwise de dados multivariados. Por padr√£o, o ggpairs fornece duas compara√ß√µes diferentes de cada par de colunas e exibe a densidade ou a contagem da vari√°vel respectiva ao longo da diagonal. Com diferentes configura√ß√µes de par√¢metros, a diagonal pode ser substitu√≠da pelos valores do eixo e r√≥tulos vari√°veis.</p>
<pre class="r"><code>#Funcao de correlacoes
my_fn &lt;- function(data, mapping, method=&quot;lm&quot;, ...){
  p &lt;- ggplot(data = data, mapping = mapping) + 
    geom_point() + 
    geom_smooth(method=method, ...)
  p
}
data(tips, package = &quot;reshape&quot;)
#Correla√ßoes cruzadas
ggpairs(tips, lower = list(continuous = my_fn))</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Existem muitos recursos ocultos dentro dos <code>ggpairs()</code> e muitos exemplos podem ser conferidos na internet para obter o m√°ximo do <code>ggpairs()</code>.</p>
</div>
<div id="ggallyggscatmat" class="section level2">
<h2><code>GGally::ggscatmat</code></h2>
<p>A principal fun√ß√£o √© <code>ggscatmat</code>. √â semelhante a <code>ggpairs()</code>, mas funciona apenas para dados multivariados puramente num√©ricos.</p>
<p>√â mais r√°pido que ggpairs, porque √© necess√°rio fazer menos escolhas.</p>
<p>Ele cria uma matriz com diagramas de dispers√£o na diagonal inferior, densidades na diagonal e correla√ß√µes escritas na diagonal superior.</p>
<p>A sintaxe √© inserir o conjunto de dados, as colunas que deseja tra√ßar, uma coluna de cores e um n√≠vel alfa.</p>
<pre class="r"><code>data(flea)
ggscatmat(flea, columns = 2:4, color=&quot;species&quot;, alpha=0.8)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
</div>
</div>
<div id="ggfottify" class="section level1">
<h1>ggfottify</h1>
<p>Outra op√ß√£o interessante para avaliar o ajuste dos modelos √© o pacote <a href="https://cran.r-project.org/web/packages/ggfortify/index.html">ggfottify</a>. Ele disponibiliza uma interface de tra√ßado (como a fun√ß√£o <code>plot(modelo_ajustado)</code>) de an√°lise e gr√°ficos em um estilo unificado, por√©m usando <code>ggplot2</code>.</p>
<p>Vamos ent√£o dar in√≠cio carregando o pacote:</p>
<pre class="r"><code>library(ggfortify)</code></pre>
<p>Veja a seguir alguns dos gr√°ficos dispon√≠veis no R para a an√°lise de res√≠duos:</p>
<pre class="r"><code>autoplot(flea_model, which = 1:6, ncol = 3, label.size = 3)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>Especificando as op√ß√µes de plot</p>
<p>Algumas propriedades desses gr√°ficos podem ser alteradas. Por exemplo, a op√ß√£o <code>colour = 'dodgerblue3'</code> √© para pontos de dados, o <code>smooth.colour = 'black'</code> √© para linhas de suaviza√ß√£o e <code>ad.colour = 'blue'</code> √© para op√ß√µes adicionais.</p>
<p>Veja ainda que ncol e nrow controlam o layout.</p>
<pre class="r"><code>autoplot(flea_model, which = 1:6, colour = &#39;dodgerblue3&#39;,
         smooth.colour = &#39;black&#39;, smooth.linetype = &#39;dashed&#39;,
         ad.colour = &#39;blue&#39;,
         label.size = 3, label.n = 5, label.colour = &#39;blue&#39;,
         ncol = 3)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>Al√©m disso, voc√™ pode usar nomes de colunas para essas propriedades, vamos separar os grupos de machos e f√™meas por cores:</p>
<pre class="r"><code>autoplot(flea_model, which = 1:6, data = flea,
         colour = &#39;species&#39;, label.size = 3,
         ncol = 3)</code></pre>
<p><img src="/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>O que ser√° que os cr√¢nios da estat√≠stica fariam diante de tantos recursos?</p>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos/">Pacotes do R para avaliar o ajuste de modelos</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Estatistica</category>
      <category>Modelagem Estatistica</category>
      <category>R</category>
      <category>Teoria</category>
      <category>Tidyverse</category>
      <category domain="tag">gomesfellipe</category>
      <category domain="tag">R</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">modelagem estatistica</category>
      <category domain="tag">Estatistica</category>
      <category domain="tag">Correlacoes</category>
      <category domain="tag">R Markdown</category>
      <category domain="tag">regression</category>
      <category domain="tag">Teoria</category>
      <category domain="tag">modelos lineares</category>
      <category domain="tag">modelos generalizados</category>
      <category domain="tag">ggfortify</category>
      <category domain="tag">GGally</category>
    </item>
    <item>
      <title>Ajustando Modelos Bayesianos com JAGS</title>
      <link>https://gomesfellipe.github.io/post/2017-12-18-bayesiana-jags-mcmcplot/bayesiana-jags-mcmcplot/</link>
      <pubDate>Mon, 18 Dec 2017 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2017-12-18-bayesiana-jags-mcmcplot/bayesiana-jags-mcmcplot/</guid>
      <description>Infer√™ncia bayesiana Imagem da Internet
Quando estamos falando de Infer√™ncia nosso objetivo normalmente √© tentar verificar alguma informa√ß√£o sobre uma quantidade desconhecida.
Para isso devemos utilizar toda informa√ß√£o dispon√≠vel, seja ela objetiva ou subjetiva (isto √©, vinda de umam amostra ou de algum conhecimento pr√©veo ou intuitivo)
Segundo o ponto de vista Bayesiano essa informa√ß√£o subjetiva tamb√©m ser√° incorporada na an√°lise gra√ßas ao teorema de bayes.
Como no ponto de vista Bayesiano atribu√≠mos aleatoriedade ao par√¢metro, nossa ‚Äúcren√ßa‚Äù ser√° representada por uma distribui√ß√£o de probabilidade (ou modelo probabil√≠stico)</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="infer√™ncia-bayesiana" class="section level1">
<h1>Infer√™ncia bayesiana</h1>
<p><a href="https://www.flickr.com/photos/mattbuck007/3676624894/in/photolist-6ATEuo-9TK3TW">Imagem da Internet</a></p>
<p>Quando estamos falando de Infer√™ncia nosso objetivo normalmente √© tentar verificar alguma informa√ß√£o sobre uma quantidade desconhecida.</p>
<p>Para isso devemos utilizar <strong>toda</strong> informa√ß√£o dispon√≠vel, seja ela <strong>objetiva</strong> ou <strong>subjetiva</strong> (isto √©, vinda de umam amostra ou de algum conhecimento pr√©veo ou intuitivo)</p>
<p>Segundo o ponto de vista Bayesiano essa informa√ß√£o subjetiva tamb√©m ser√° incorporada na an√°lise gra√ßas ao <a href="https://pt.wikipedia.org/wiki/Teorema_de_Bayes">teorema de bayes</a>.</p>
<p>Como no ponto de vista Bayesiano atribu√≠mos aleatoriedade ao par√¢metro, nossa ‚Äúcren√ßa‚Äù ser√° representada por uma distribui√ß√£o de probabilidade (ou modelo probabil√≠stico)</p>
<p><em>Teorema de bayes</em>:
<span class="math display">\[
p(\theta|x)=\frac{p(x,\theta)}{p(x)}=\frac{p(x|\theta)p(\theta)}{p(x)}
\]</span></p>
<p>onde:</p>
<ul>
<li><span class="math inline">\(p(x|\theta)\)</span>: fun√ß√£o de verossimilhan√ßa (modelo)</li>
<li><span class="math inline">\(p(\theta)\)</span>: distribui√ß√£o a priori</li>
<li><span class="math inline">\(p(x)\)</span>: distribui√ß√£o marginal de <span class="math inline">\(x\)</span>.</li>
</ul>
<p>A estima√ß√£o muitas vezes envolve o c√°lculo de integrais nada simples analiticamente por√©m, alguns algor√≠timos como o amostrador de Gibbs pode relizar aproxima√ß√µes muito relevantes.</p>
</div>
<div id="modelo-linear-bayesiano" class="section level1">
<h1>Modelo linear bayesiano</h1>
<p>Para entender como funciona o modelo bayesiano, primeiramente vamos come√ßar com algo bem simples, suponha:</p>
<p><span class="math display">\[
Y_i \sim N(\mu_i,\tau)
\]</span>
onde <span class="math inline">\(\mu\)</span> √© definido como <span class="math inline">\(\mu_i= X \mathbf{\beta}\)</span>.</p>
<p>Incialmente vamos considerar que n√£o existe rela√ß√£o nenhuma, ent√£o utilizaremos a priori:</p>
<p><span class="math display">\[
\beta \sim N(0,\tau_{\beta})
\]</span></p>
<p>onde <span class="math inline">\(\tau\)</span> √© conhecido.</p>
<p>Nem sempre √© uma tarefa simples determinar a distribui√ß√£o posteri de um modelo bayesiano e √© neste ponto que o pacote <code>jags</code>ser√° bastante √∫til (existem outras alternativas como o <a href="https://cran.r-project.org/package=R2WinBUGS">WinBugs</a>, <a href="https://cran.r-project.org/package=R2OpenBUGS">OpenBugs</a>, <a href="https://cran.r-project.org/web/packages/rstan/index.html">Stan</a>, mas aqui resolvi trazer apenas o <a href="https://cran.r-project.org/package=rjags">jags</a> por possuir vantagens bem interessantes.)</p>
</div>
<div id="jags" class="section level1">
<h1>Jags</h1>
<p>O pacote <a href="https://cran.r-project.org/package=R2jags"><code>R2jags</code></a> √© exatamente o que seu nome significa: ‚Äú<em>Just Another Gibbs Sampler</em>‚Äù. Possui as mesmas funcionalidades do nosso querido <a href="https://cran.r-project.org/package=R2OpenBUGS">OpenBugs</a> possibilitando tamb√©m que seja utilizado inteiramente dentro do ambiente R.</p>
<p>Assim como o OpenBugs, ele tamb√©m trabalha chamando o <a href="mcmc-jags.sourceforge.net/">software oficial que precisa ser baixado no site</a>.</p>
<p>Para come√ßar a utilizar basta baixar o pacote e acess√°-lo na biblioteca:</p>
<pre class="r"><code>library(R2jags)</code></pre>
</div>
<div id="declarando-o-modelo" class="section level1">
<h1>Declarando o modelo</h1>
<p>A base de dados que ser√° utilizada para ajustar o modelo ser√° a base nativa do R chamada <code>trees</code>:</p>
<pre class="r"><code>X&lt;-trees[,1:2] #Matriz de vari√°veis explanat√≥rias
Y&lt;- trees[,3]  #Vetor da vari√°vel resposta
p &lt;- ncol(X)   #p √© o n√∫mero de par√¢metros do modelo (nesse caso √© o n√∫mero de colunas)
n &lt;- nrow(X)   #n √© o n√∫mero de observa√ß√µes do modelo</code></pre>
<p>O modelo deve estar declarado e salvo em um arquivo <code>.txt</code> (ou mesmo um outro arquivo <code>.r</code>) da seguinte maneira:</p>
<pre class="r"><code>### Declarando o modelo Bayesiano
sink(&quot;linreg.txt&quot;)
cat(&quot;
    model {
    
    # Prioris
    for(j in 1:p)
    {
    beta[j] ~ dnorm(mu.beta, tau.beta)       
    }
    sigma ~ dunif(0, 100)            
    tau &lt;- 1/ (sigma * sigma)
    
    # Verossimilhan√ßa
    for (i in 1:n) {
    y[i] ~ dnorm(mu[i], tau)
    mu[i] &lt;- inprod(X[i,], beta)
    }

    }
    &quot;,fill=TRUE)
sink()</code></pre>
<p>Uma vez que o modelo esta declarado, √© a hora de nomear os parametros da fun√ß√£o que far√° o ajuste do modelo</p>
<pre class="r"><code>#Parametros da Priori
mu.beta &lt;- 0
tau.beta &lt;- 0.001

#Set Working Directory
wd &lt;- getwd()

# Junte os dados em uma lista
win.data &lt;- list(X=X,y=Y,p=p,n=n,mu.beta=mu.beta,tau.beta=tau.beta)

# Fun√ß√£o de inicializa√ß√£o
inits &lt;- function(){ list(beta=rnorm(p), sigma = rlnorm(1))}

# Os parametros que desejamos estimar
params &lt;- c(&quot;beta&quot;,&quot;sigma&quot;,&quot;tau&quot;)

# Caracteristicas do MCMC
n.burnin &lt;- 500                    #N√∫mero de itera√ß√µes que ser√£o descartadas
n.thin &lt;- 10                       #para economizar mem√≥ria e tempo de computa√ß√£o se n.iter for grande
n.post &lt;- 5000  
n.chains &lt;- 3                      #N√∫mero de cadeias
n.iter &lt;- n.burnin + n.thin*n.post #N√∫mero de itera√ß√µes</code></pre>
</div>
<div id="implementando-o-modelo" class="section level1">
<h1>Implementando o modelo</h1>
<p>Ap√≥s ter em m√£os todos esses resultados, j√° podemos ajustar o modelo com o comando <code>jags()</code>, veja:</p>
<pre class="r"><code>bayes.mod.fit &lt;-jags(data = win.data,
                     inits = inits,
                     parameters = params,
                     model.file = &quot;linreg.txt&quot;,  # O arquivo &quot;linreg.txt&quot; deve estar no mesmo diret√≥rio
                     n.iter = n.iter,
                     n.thin=n.thin,
                     n.burnin=n.burnin,
                     n.chains=n.chains,
                     working.directory=wd,DIC = T)</code></pre>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 31
##    Unobserved stochastic nodes: 3
##    Total graph size: 166
## 
## Initializing model</code></pre>
<pre class="r"><code>print(bayes.mod.fit, dig = 3)</code></pre>
<pre><code>## Inference for Bugs model at &quot;linreg.txt&quot;, fit using jags,
##  3 chains, each with 50500 iterations (first 500 discarded), n.thin = 10
##  n.sims = 15000 iterations saved
##          mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff
## beta[1]    5.045   0.435   4.183   4.757   5.043   5.324   5.916 1.001 15000
## beta[2]   -0.478   0.078  -0.633  -0.527  -0.477  -0.427  -0.324 1.001 15000
## sigma      6.448   0.904   4.995   5.805   6.335   6.970   8.502 1.001 15000
## tau        0.025   0.007   0.014   0.021   0.025   0.030   0.040 1.001 15000
## deviance 201.924   2.682 198.881 199.970 201.244 203.149 208.856 1.001  7200
## 
## For each parameter, n.eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).
## 
## DIC info (using the rule, pD = var(deviance)/2)
## pD = 3.6 and DIC = 205.5
## DIC is an estimate of expected predictive error (lower deviance is better).</code></pre>
<p>Com os resultados em m√£os podemos avaliar o ajuste do modelo, o jags nos fornece os intervalos de credibilidade e o Rhat, que √© a converg√™ncia da cadeia, a princ√≠pio vamos apenas considerar o fato de que quanto mais pr√≥ximo de 1, melhor s√£o as estimativas.</p>
<p>N√£o vou me extender neste post com a interpreta√ß√£o do modelo pois o objetivo esta sendo mostrar a funcionalidade do jags em conjunto com o R.</p>
</div>
<div id="diagn√≥sticos-do-modelo-com-mcmcplots" class="section level1">
<h1>Diagn√≥sticos do modelo com <code>mcmcplots</code></h1>
<p>Para o diagn√≥stico do modelo podemos utilizar o pacote <code>mcmcplots</code> que fornece de maneira bem agrad√°vel os resultados gerados pelo amostrador, primeiramente vamos carregar o pacote:</p>
<pre class="r"><code>library(mcmcplots)</code></pre>
<p>Em seguida precisar informar para o <code>R</code> que o resultado do algor√≠timo se trata de um objeto mcmc, portanto:</p>
<pre class="r"><code>bayes.mod.fit.mcmc &lt;- as.mcmc(bayes.mod.fit)
summary(bayes.mod.fit.mcmc)</code></pre>
<pre><code>## 
## Iterations = 1:49991
## Thinning interval = 10 
## Number of chains = 3 
## Sample size per chain = 5000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##               Mean       SD  Naive SE Time-series SE
## beta[1]    5.04490 0.435344 3.555e-03      3.555e-03
## beta[2]   -0.47754 0.077588 6.335e-04      6.335e-04
## deviance 201.92383 2.682384 2.190e-02      2.144e-02
## sigma      6.44763 0.903646 7.378e-03      7.359e-03
## tau        0.02542 0.006784 5.539e-05      5.524e-05
## 
## 2. Quantiles for each variable:
## 
##               2.5%       25%       50%       75%     97.5%
## beta[1]    4.18250   4.75721   5.04333   5.32437   5.91642
## beta[2]   -0.63255  -0.52732  -0.47726  -0.42674  -0.32376
## deviance 198.88143 199.97019 201.24393 203.14881 208.85648
## sigma      4.99470   5.80492   6.33492   6.96990   8.50193
## tau        0.01383   0.02058   0.02492   0.02968   0.04008</code></pre>
<p>O pacote nos fornece alguns tipos de gr√°ficos para diagn√≥stico</p>
<pre class="r"><code>caterplot(bayes.mod.fit.mcmc)                #Observando todas as estimativas</code></pre>
<p><img src="/post/2017-12-18-bayesiana-jags-mcmcplot/bayesiana-jags-mcmcplot_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code>caterplot(bayes.mod.fit.mcmc,parms = params) #Observando as estimativas de todos os par√¢metros menos o desvio</code></pre>
<p><img src="/post/2017-12-18-bayesiana-jags-mcmcplot/bayesiana-jags-mcmcplot_files/figure-html/unnamed-chunk-8-2.png" width="672" /></p>
<pre class="r"><code>denplot(bayes.mod.fit.mcmc)                  #Densidade das estimativas de cada cadeia</code></pre>
<p><img src="/post/2017-12-18-bayesiana-jags-mcmcplot/bayesiana-jags-mcmcplot_files/figure-html/unnamed-chunk-8-3.png" width="672" /></p>
<pre class="r"><code>traplot(bayes.mod.fit.mcmc,greek = T)        #Avaliando a converg√™ncia</code></pre>
<p><img src="/post/2017-12-18-bayesiana-jags-mcmcplot/bayesiana-jags-mcmcplot_files/figure-html/unnamed-chunk-8-4.png" width="672" /></p>
<p>E por fim, para diagn√≥sticos r√°pidos, pode produzir arquivos html com tra√ßo, densidade e autocorrela√ß√£o.</p>
<p>O comando tra√ßa tudo em uma p√°gina e os arquivos ser√£o exibidos em seu navegador de internet padr√£o.</p>
<pre class="r"><code>mcmcplot(bayes.mod.fit.mcmc)</code></pre>
<p>Vai retornar um relat√≥rio resumido para todos os par√¢metros como nesta <a href="https://introndatalab.com/wp-content/uploads/manually/20150405/MCMC%20Plots%20%20result2_files/attack%5B1,1%5D.png">imagem da internet</a> como:</p>
<p><img src="/post/2017-12-18-bayesiana-jags-mcmcplot/bayesiana-jags-mcmcplot_files/imagem1.png" /></p>
<p>Como o objetivo do post √© trazer a funcionalidade do pacote, vou apenas deixar ilustrado quais s√£o algumas das fun√ß√µes mais comumente utilizadas para avaliar estat√≠sticamente o desempenho dos modelos.</p>
<p>Diagnosticos estat√≠sticos do modelo:</p>
<pre class="r"><code>#Mais diagnosticos:
gelman.plot(bayes.mod.fit.mcmc)</code></pre>
<p><img src="/post/2017-12-18-bayesiana-jags-mcmcplot/bayesiana-jags-mcmcplot_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code>geweke.diag(bayes.mod.fit.mcmc)</code></pre>
<pre><code>## [[1]]
## 
## Fraction in 1st window = 0.1
## Fraction in 2nd window = 0.5 
## 
##  beta[1]  beta[2] deviance    sigma      tau 
##  -1.6717   1.1790  -0.4485   0.1854  -0.6815 
## 
## 
## [[2]]
## 
## Fraction in 1st window = 0.1
## Fraction in 2nd window = 0.5 
## 
##  beta[1]  beta[2] deviance    sigma      tau 
##  0.37278 -0.36960 -0.24342 -0.08007  0.30725 
## 
## 
## [[3]]
## 
## Fraction in 1st window = 0.1
## Fraction in 2nd window = 0.5 
## 
##  beta[1]  beta[2] deviance    sigma      tau 
## -0.15725  0.19911 -0.08445 -0.34043  0.35357</code></pre>
<pre class="r"><code>geweke.plot(bayes.mod.fit.mcmc)</code></pre>
<p><img src="/post/2017-12-18-bayesiana-jags-mcmcplot/bayesiana-jags-mcmcplot_files/figure-html/unnamed-chunk-10-2.png" width="672" /><img src="/post/2017-12-18-bayesiana-jags-mcmcplot/bayesiana-jags-mcmcplot_files/figure-html/unnamed-chunk-10-3.png" width="672" /><img src="/post/2017-12-18-bayesiana-jags-mcmcplot/bayesiana-jags-mcmcplot_files/figure-html/unnamed-chunk-10-4.png" width="672" /></p>
<pre class="r"><code>raftery.diag(bayes.mod.fit.mcmc)</code></pre>
<pre><code>## [[1]]
## 
## Quantile (q) = 0.025
## Accuracy (r) = +/- 0.005
## Probability (s) = 0.95 
##                                                 
##           Burn-in  Total Lower bound  Dependence
##           (M)      (N)   (Nmin)       factor (I)
##  beta[1]  20       39950 3746         10.70     
##  beta[2]  20       36200 3746          9.66     
##  deviance 20       37410 3746          9.99     
##  sigma    20       38030 3746         10.20     
##  tau      20       36800 3746          9.82     
## 
## 
## [[2]]
## 
## Quantile (q) = 0.025
## Accuracy (r) = +/- 0.005
## Probability (s) = 0.95 
##                                                 
##           Burn-in  Total Lower bound  Dependence
##           (M)      (N)   (Nmin)       factor (I)
##  beta[1]  20       38030 3746         10.20     
##  beta[2]  20       36800 3746          9.82     
##  deviance 20       37410 3746          9.99     
##  sigma    20       37410 3746          9.99     
##  tau      20       35610 3746          9.51     
## 
## 
## [[3]]
## 
## Quantile (q) = 0.025
## Accuracy (r) = +/- 0.005
## Probability (s) = 0.95 
##                                                 
##           Burn-in  Total Lower bound  Dependence
##           (M)      (N)   (Nmin)       factor (I)
##  beta[1]  20       37410 3746          9.99     
##  beta[2]  20       38030 3746         10.20     
##  deviance 20       37410 3746          9.99     
##  sigma    30       40620 3746         10.80     
##  tau      20       39300 3746         10.50</code></pre>
<pre class="r"><code>heidel.diag(bayes.mod.fit.mcmc)</code></pre>
<pre><code>## [[1]]
##                                        
##          Stationarity start     p-value
##          test         iteration        
## beta[1]  passed       1         0.292  
## beta[2]  passed       1         0.455  
## deviance passed       1         0.733  
## sigma    passed       1         0.881  
## tau      passed       1         0.816  
##                                      
##          Halfwidth Mean     Halfwidth
##          test                        
## beta[1]  passed      5.0481 0.012089 
## beta[2]  passed     -0.4780 0.002155 
## deviance passed    201.8829 0.073069 
## sigma    passed      6.4367 0.024544 
## tau      passed      0.0255 0.000187 
## 
## [[2]]
##                                        
##          Stationarity start     p-value
##          test         iteration        
## beta[1]  passed       1         0.246  
## beta[2]  passed       1         0.249  
## deviance passed       1         0.967  
## sigma    passed       1         0.950  
## tau      passed       1         0.770  
##                                      
##          Halfwidth Mean     Halfwidth
##          test                        
## beta[1]  passed      5.0386 0.011955 
## beta[2]  passed     -0.4765 0.002134 
## deviance passed    201.9023 0.068414 
## sigma    passed      6.4571 0.025014 
## tau      passed      0.0253 0.000188 
## 
## [[3]]
##                                        
##          Stationarity start     p-value
##          test         iteration        
## beta[1]  passed       1         0.657  
## beta[2]  passed       1         0.690  
## deviance passed       1         0.544  
## sigma    passed       1         0.813  
## tau      passed       1         0.873  
##                                      
##          Halfwidth Mean     Halfwidth
##          test                        
## beta[1]  passed      5.0480 0.012156 
## beta[2]  passed     -0.4781 0.002163 
## deviance passed    201.9863 0.076685 
## sigma    passed      6.4491 0.025385 
## tau      passed      0.0254 0.000188</code></pre>
</div>
<div id="diagnostico-de-convergencia-rapida-superdiag" class="section level1">
<h1>Diagnostico de convergencia rapida: <code>superdiag</code></h1>
<p>Uma fun√ß√£o muito conveniente para analisar representa√ß√µes num√©ricas de diagn√≥sticos em um ajuste √© o pacote <code>superdiag</code> de Tsai, Gill e Rapkin, 2012 que tr√°s uma s√©rie de estat√≠sticas para avaliar o desempenho dos ajustes do modelo.</p>
<pre class="r"><code>library(superdiag)
superdiag(bayes.mod.fit.mcmc, burnin = 100)</code></pre>
<pre><code>## Number of chains = 3 
## Number of iterations = 5000 per chain before discarding the burn-in period
## Burn-in period = 100 per chain
## Sample size in total = 14703 
## 
## ****************** The Geweke diagnostic: ******************
## Windows:
##            chain 1 chain 2 chain 3
## From start     0.1  0.5420  0.2999
## From stop      0.5  0.3511  0.6893
## 
## Z-scores:
##           chain 1 chain 2  chain 3
## beta[1]  -1.85586  0.3331 -1.66699
## beta[2]   1.57605 -0.2271  1.53584
## deviance  0.02463  0.3356 -1.14324
## sigma    -0.15363 -0.8820 -0.33962
## tau      -0.09745  0.9937  0.01232
## 
## *************** The Gelman-Rubin diagnostic: ***************
## Potential scale reduction factors:
##          Point est. Upper C.I.
## beta[1]      1.0001      1.001
## beta[2]      1.0000      1.000
## deviance     1.0009      1.002
## sigma        1.0002      1.001
## tau          0.9999      1.000
## 
## Multivariate psrf: 1.0005
## 
## ************* The Heidelberger-Welch diagnostic ************
## Chain 1:
## epsilon=0.1, alpha=0.05                                       
##          Stationarity start     p-value
##          test         iteration        
## beta[1]  passed       1         0.1576 
## beta[2]  passed       1         0.2864 
## deviance passed       1         0.8399 
## sigma    passed       1         0.8207 
## tau      passed       1         0.7405 
##                                       
##          Halfwidth Mean      Halfwidth
##          test                         
## beta[1]  passed      5.04671 0.012211 
## beta[2]  passed     -0.47775 0.002177 
## deviance passed    201.89097 0.074094 
## sigma    passed      6.43566 0.024772 
## tau      passed      0.02549 0.000189 
## 
## Chain 2:
## epsilon=0.079, alpha=0.1                                       
##          Stationarity start     p-value
##          test         iteration        
## beta[1]  passed       1         0.3032 
## beta[2]  passed       1         0.3259 
## deviance passed       1         0.9562 
## sigma    passed       1         0.7462 
## tau      passed       1         0.5362 
##                                       
##          Halfwidth Mean      Halfwidth
##          test                         
## beta[1]  passed      5.03850 0.0120853
## beta[2]  passed     -0.47646 0.0021574
## deviance passed    201.90084 0.0693125
## sigma    passed      6.45467 0.0252168
## tau      passed      0.02536 0.0001894
## 
## Chain 3:
## epsilon=0.054, alpha=0.005                                       
##          Stationarity start     p-value
##          test         iteration        
## beta[1]  passed       1         0.5489 
## beta[2]  passed       1         0.5665 
## deviance passed       1         0.5038 
## sigma    passed       1         0.8038 
## tau      passed       1         0.8898 
##                                       
##          Halfwidth Mean      Halfwidth
##          test                         
## beta[1]  passed      5.04719 0.0122925
## beta[2]  passed     -0.47794 0.0021858
## deviance passed    201.98956 0.0775537
## sigma    passed      6.44893 0.0256817
## tau      passed      0.02544 0.0001937
## 
## *************** The Raftery-Lewis diagnostic ***************
## Chain 1:
## Convergence eps = 0.001
## Quantile (q) = 0.025
## Accuracy (r) = +/- 0.005
## Probability (s) = 0.95 
##                                                 
##           Burn-in  Total Lower bound  Dependence
##           (M)      (N)   (Nmin)       factor (I)
##  beta[1]  30       40170 3746         10.70     
##  beta[2]  20       36340 3746          9.70     
##  deviance 20       38200 3746         10.20     
##  sigma    20       38200 3746         10.20     
##  tau      20       36950 3746          9.86     
## 
## Chain 2:
## Convergence eps = 5e-04
## Quantile (q) = 0.25
## Accuracy (r) = +/- 0.001
## Probability (s) = 0.99 
## 
## You need a sample size of at least 1244044 with these values of q, r and s
## 
## Chain 3:
## Convergence eps = 0.005
## Quantile (q) = 0.25
## Accuracy (r) = +/- 5e-04
## Probability (s) = 0.999 
## 
## You need a sample size of at least 8120675 with these values of q, r and s
## 
## ************* The Hellinger distance diagnostic ************
## Between chains: 
##              Min     Max
## beta[1]  0.01735 0.02915
## beta[2]  0.02015 0.02620
## deviance 0.03155 0.03413
## sigma    0.01858 0.02731
## tau      0.01538 0.02810
## 
## Within chain 1:
##              980    1960    2940    3920
## beta[1]  0.05231 0.03952 0.04017 0.04259
## beta[2]  0.04261 0.05034 0.04320 0.04782
## deviance 0.05880 0.04060 0.06297 0.04311
## sigma    0.03871 0.03667 0.06465 0.04285
## tau      0.03668 0.03996 0.03633 0.04083
## 
## Within chain 2:
##              980    1960    2940    3920
## beta[1]  0.03098 0.04075 0.04281 0.03887
## beta[2]  0.03050 0.03770 0.03887 0.04216
## deviance 0.04541 0.03992 0.03390 0.04730
## sigma    0.04660 0.03876 0.03090 0.02866
## tau      0.03648 0.03773 0.02967 0.03589
## 
## Within chain 3:
##              980    1960    2940    3920
## beta[1]  0.03356 0.03988 0.03146 0.02986
## beta[2]  0.03425 0.04729 0.03175 0.03219
## deviance 0.05894 0.03553 0.05018 0.04509
## sigma    0.04392 0.04245 0.03858 0.03760
## tau      0.04089 0.03458 0.04512 0.03047</code></pre>
<p>Para finalizar, outra fun√ß√£o que pode ser √∫til pata atualizando o modelo, se necess√°rio - por exemplo, se n√£o houver converg√™ncia ou pouca convergencia:</p>
<pre class="r"><code>bayes.mod.fit.upd &lt;- update(bayes.mod.fit, n.iter=1000)
bayes.mod.fit.upd &lt;- autojags(bayes.mod.fit)</code></pre>
</div>
<div id="muito-a-estudar" class="section level1">
<h1>Muito a estudar</h1>
<p>Assim como toda a Estat√≠stica, infer√™ncia bayesiana n√£o funciona se a teoria n√£o for aplicada corretamente. √â uma ferramenta muito poderosa e necessita ser usada com cautela pois demanda bastante o uso de metodologias estat√≠sticas.</p>
<p>Como dizia o tio Ben: ‚Äúgrandes poderes trazem grandes responsabilidades‚Äù ent√£o vamos tomar cuidado com os resultados que encontramos.</p>
</div>
<div id="referencias" class="section level1">
<h1>Referencias</h1>
<p><a href="http://recologia.com.br/2012/12/uma-primeira-olhada-em-estatistica-bayesiana-e-linguagem-bugs/">Uma primeira olhada em estat√≠stica bayesiana e linguagem BUGS por Augusto Ribas - blog Recologia</a></p>
<p><a href="http://www.users.csbsju.edu/~mgass/robert.pdf">John K. Kruschke 2014 Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan.2nd Edition. Academic Press / Elsevier.</a></p>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2017-12-18-bayesiana-jags-mcmcplot/bayesiana-jags-mcmcplot/">Ajustando Modelos Bayesianos com JAGS</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Estatistica</category>
      <category>R</category>
      <category>Teoria</category>
      <category>Bayes</category>
      <category>Infer√™ncia Bayesiana</category>
      <category>Modelagem Estatistica</category>
      <category domain="tag">Estatistica</category>
      <category domain="tag">gomesfellipe</category>
      <category domain="tag">R</category>
      <category domain="tag">jags</category>
      <category domain="tag">bayes</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">modelagem estatistica</category>
    </item>
  </channel>
</rss>