&lt;?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>pratica on Fellipe Gomes - Data Science Blog</title>
    <link>https://gomesfellipe.github.io/tags/pratica/</link>
    <description>√öltimos posts sobre Data Science, Machine Learning e R</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>pt-br</language>
    <managingEditor>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</managingEditor>
    <webMaster>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</webMaster>
    <lastBuildDate>Tue, 30 May 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://gomesfellipe.github.io/tags/pratica/" rel="self" type="application/rss+xml" />
    <item>
      <title>Solu√ß√£o Final - ML Olympiad [1¬∫ lugar]</title>
      <link>https://gomesfellipe.github.io/post/2023-05-30-solucao-final-ensure-healthy-lives-kaggle-competition/</link>
      <pubDate>Tue, 30 May 2023 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2023-05-30-solucao-final-ensure-healthy-lives-kaggle-competition/</guid>
      <description>Confira a estrat√©gia aplicada para esta competi√ß√£o</description>
      <content:encoded>&lt;![CDATA[
        
<link href="https://gomesfellipe.github.io/post/2023-05-30-solucao-final-ensure-healthy-lives-kaggle-competition/index_files/vembedr/css/vembedr.css" rel="stylesheet" />

<div id="TOC">
<ul>
<li><a href="#introdu%C3%A7%C3%A3o" id="toc-introdu√ß√£o">Introdu√ß√£o</a></li>
<li><a href="#defini%C3%A7%C3%A3o-do-problema-de-neg%C3%B3cio" id="toc-defini√ß√£o-do-problema-de-neg√≥cio">Defini√ß√£o do problema de neg√≥cio</a></li>
<li><a href="#solu%C3%A7%C3%B5es" id="toc-solu√ß√µes">Solu√ß√µes</a></li>
<li><a href="#estrat%C3%A9gia-anal%C3%ADtica" id="toc-estrat√©gia-anal√≠tica">Estrat√©gia anal√≠tica</a>
<ul>
<li><a href="#decis%C3%B5es-sobre-a-target" id="toc-decis√µes-sobre-a-target">Decis√µes sobre a target</a></li>
<li><a href="#processamento-dos-dados" id="toc-processamento-dos-dados">Processamento dos Dados</a></li>
<li><a href="#dados-externos" id="toc-dados-externos">Dados Externos</a></li>
<li><a href="#feature-engineering" id="toc-feature-engineering">Feature Engineering</a></li>
<li><a href="#modelos" id="toc-modelos">Modelos</a></li>
<li><a href="#ensemble" id="toc-ensemble">Ensemble</a></li>
<li><a href="#post-processing" id="toc-post-processing">Post Processing</a></li>
</ul></li>
<li><a href="#considera%C3%A7%C3%B5es-finais" id="toc-considera√ß√µes-finais">Considera√ß√µes Finais</a></li>
<li><a href="#sobre-o-autor" id="toc-sobre-o-autor">Sobre o Autor</a></li>
</ul>
</div>

<div id="introdu√ß√£o" class="section level1">
<h1>Introdu√ß√£o</h1>
<p>O <a href="https://www.meetup.com/TensorFlowSP/events/284607061/">TFUG - TensorFlow Users Group de S√£o Paulo</a> lan√ßou uma nova <a href="https://www.kaggle.com/competitions/ml-olympiad-ensure-healthy-lives">competi√ß√£o no Kaggle</a> onde o objetivo era desenvolver modelos para previs√£o de diagn√≥stico de s√≠ndromes respirat√≥rias, que √© um tema relacionado com um dos 17 t√≥picos de Desenvolvimento Sustent√°vel das Na√ß√µes Unidas - <em>Boa sa√∫de e bem-estar</em>.</p>
<p>Como um cientista de dados, acredito que seja muito importante continuarmos aprimorando nossas habilidades e conhecimentos. Competi√ß√µes como essa s√£o muito divertidas e possibilitam que testemos nossos limites em um ambiente competitivo e colaborativo, al√©m de ser uma grande oportunidade para nos desafiarmos e aprender uns com os outros.</p>
<p>Tive o enorme prazer de conquistar o primeiro lugar, dessa vez com meu grande amigo <a href="https://www.linkedin.com/in/kaike-wesley-reis">Kaike</a>, parceiro de competi√ß√µes de longa data que trouxe grande sinergia para a <a href="https://www.kaggle.com/code/gomes555/ml-olypiads-1-lugar-blending">solu√ß√£o final</a> com a contribui√ß√£o de seu modelo (compartilhado abertamente no Kaggle).</p>
<p>Aqui est√£o alguns dos pr√™mios recebidos:</p>
<center>
<img src="/post/2023-05-30-solucao-final-ensure-healthy-lives-kaggle-competition/premio2.png" style="width:80.0%" />
</center>
<p>Como nesta competi√ß√£o havia bastante trabalho a ser feito e tivemos apenas 1 m√™s para trabalhar na solu√ß√£o, foi preciso fazer uma boa gest√£o do c√≥digo e do tempo de desenvolvimento.</p>
</div>
<div id="defini√ß√£o-do-problema-de-neg√≥cio" class="section level1">
<h1>Defini√ß√£o do problema de neg√≥cio</h1>
<p>O objetivo desta competi√ß√£o consistiu em predizer qual o agente causador da s√≠ndrome respirat√≥ria aguda grave com base nos dados e sintomas dos pacientes.</p>
<p>Esta tarefa pode ser enquadrada como um problema supervisionado de classifica√ß√£o multinomial (com m√∫ltiplos outputs) na qual as previs√µes s√£o, de certa forma, dependentes da entrada umas das outras (o paciente s√≥ pode ter registrado uma das doen√ßas).</p>
<p>A valida√ß√£o da solu√ß√£o foi feita utilizando a m√©trica Macro (or Mean) F1-Score, que √© basicamente a m√©dia do F1 calculado sobre as previs√µes de cada nota.</p>
</div>
<div id="solu√ß√µes" class="section level1">
<h1>Solu√ß√µes</h1>
<p>Ambas solu√ß√µes (minha e do Kaike) foram compartilhadas no Kaggle:</p>
<ul>
<li><a href="https://www.kaggle.com/code/gomes555/ml-olympiad-1-lugar-catboost-pos-process">ML Olympiad - 1¬∫ Lugar - Catboost + Pos Process</a> (Fellipe)</li>
<li><a href="https://www.kaggle.com/code/kaikewreis/ml-olypiads-1-lugar-lightgbm-binary-ensemble">ML Olypiads - 1¬∫ Lugar - LightGBM Binary Ensemble</a> (Kaike)</li>
<li><a href="https://www.kaggle.com/code/gomes555/ml-olympiad-1-lugar-blending">ML Olympiad - 1¬∫ Lugar - Blending</a> (combina√ß√£o das solu√ß√µes em um emsemble)</li>
</ul>
<p>Disponibilizamos tamb√©m a solu√ß√£o em formato de v√≠deo, gravado em um meetup com dura√ß√£o de 1 hora e meia para o canal do <a href="https://www.youtube.com/@tensorflowugsp">TensorFlow UGSP</a> no Youtube no link: <a href="https://youtu.be/6HPJn38NF3w" class="uri">https://youtu.be/6HPJn38NF3w</a></p>
<center>
<div class="vembedr">
<div>
<iframe src="https://www.youtube.com/embed/6HPJn38NF3w" width="533" height="300" frameborder="0" allowfullscreen="" data-external="1"></iframe>
</div>
</div>
</center>
</div>
<div id="estrat√©gia-anal√≠tica" class="section level1">
<h1>Estrat√©gia anal√≠tica</h1>
<p>Nas se√ß√µes abaixo apresento o racional por tr√°s da minha solu√ß√£o, como chegamos nos 5 melhores modelos individuais (para cada doen√ßa respirat√≥ria) que utilizei em um ensemble para chegar ao primeiro lugar, bem como a estrat√©gia de p√≥s processamento que com que o score melhorasse significativamente.</p>
<div id="decis√µes-sobre-a-target" class="section level2">
<h2>Decis√µes sobre a target</h2>
<p>A primeira decis√£o importante era definir como enquadrar o problema; se utilizar√≠amos 1 modelo multiclasse ou diferentes modelos para cada classe.</p>
<p>Em todos os testes que fizemos, os modelos individuais superaram o F1-Score Macro de um modelo √∫nico. Como 3 das classes eram bastante desbalanceadas, acredito que modelos especializados nesses casos conseguiram captar melhor suas nuances.</p>
</div>
<div id="processamento-dos-dados" class="section level2">
<h2>Processamento dos Dados</h2>
<p>Como optamos por unificar os resultados apenas na reta final, meu pr√©-processamento foi muito diferente do feito pelo Kaike e isso foi fundamental para que as estimativas dos nossos modelos tivessem baixa correla√ß√£o. N√£o focarei aqui no meu pr√©-processamento, pois n√£o acho que foi o diferencial para atingir um score superior a 0.6 (quem tiver curiosidade est√° tudo bem documentado nos notebooks compartilhados).</p>
</div>
<div id="dados-externos" class="section level2">
<h2>Dados Externos</h2>
<p>O fato de n√£o termos as informa√ß√µes do ano em que esses dados foram coletados dificultou na busca de bases externas, pois indicadores socioecon√¥micos e de sa√∫de variam bastante ao longo do tempo.</p>
<p>Fizemos alguns testes utilizando o <a href="https://basedosdados.org/dataset/mundo-onu-adh">Atlas do Desenvolvimento Humano (ADH)</a>, mas n√£o tivemos muito sucesso, pois esses dados est√£o muito defasados (1991-2010). Tamb√©m tentamos acrescentar a informa√ß√£o de <a href="https://github.com/kelvins/Municipios-Brasileiros/">latitude e longitude de cada munic√≠pio</a>, mas isso n√£o trouxe uma melhora substancial no nosso score.</p>
</div>
<div id="feature-engineering" class="section level2">
<h2>Feature Engineering</h2>
<p>Outra etapa em que investimos bastante tempo foi para criar novas vari√°veis.</p>
<p>Novamente, nossa engenharia de recursos foi feita de maneira separada para que nossos modelos aprendessem aspectos diferentes dos dados. Abaixo, compartilho algumas das features que desenvolvi apenas para o meu modelo:</p>
<ul>
<li>Presen√ßa de sintomas relacionados √† Target;</li>
<li>Se tomografia era t√≠pica do COVID;</li>
<li>Intervalo de idade com mais casos;</li>
<li>Idade discretizada;</li>
<li>Diferen√ßa entre a semana de notifica√ß√£o e primeiros sintomas;</li>
<li>Novas features baseadas nas contagens de algumas features categ√≥ricas;</li>
<li>etc.</li>
</ul>
</div>
<div id="modelos" class="section level2">
<h2>Modelos</h2>
<p>Al√©m de pr√©-processamentos e feature engineering diferentes, tamb√©m utilizamos modelos e mecanismos de tunning diferentes, o que ajudou para que nossas estimativas tivessem baixa correla√ß√£o. Eu usei o Catboost como modelo final, j√° o Kaike optou por um LightGBM com tuning de hiperparametros.</p>
</div>
<div id="ensemble" class="section level2">
<h2>Ensemble</h2>
<p>Calculamos a m√©dia das probabilidades previstas de cada modelo para cada classe antes de selecionar a classe que tivesse a maior probabilidade.</p>
<p>Como nossas previs√µes tinham baixa correla√ß√£o, conseguimos ser bem sucedidos no ensemble combinando nossas submiss√µes com score ~0.6 alcan√ßando ~0.61 na tabela p√∫blica.</p>
</div>
<div id="post-processing" class="section level2">
<h2>Post Processing</h2>
<p>Acredito que o <strong>diferencial</strong> dessa competi√ß√£o estava no p√≥s processamento.</p>
<p>Quando avaliamos o score do modelo de cada classe, tamb√©m calculamos um threshold que maximizava os respectivos F1.</p>
<p>Observamos que nosso modelo para a classe 5 apresentava um F1 muito superior √†s demais classes com esse threshold otimizado, ent√£o fizemos o seguinte:</p>
<ol style="list-style-type: decimal">
<li>Calculamos as probabilidades individuais para cada classe;</li>
<li>Selecionamos a classe que tinha maior probabilidade estimada em cada inst√¢ncia;</li>
<li>Pegamos a classifica√ß√£o bin√°ria da classe 5 com o threshold otimizado e aplicamos a seguinte condi√ß√£o: Se o modelo da classe 5 estimou que y5[i]==1, ent√£o yfinal[i] √© 5, caso contr√°rio, use a classe de maior probabilidade entre as outras 4. (Em outras palavras: <code>np.where(y5_test_class==1, 5, sub.CLASSI_FIN)</code>)</li>
</ol>
</div>
</div>
<div id="considera√ß√µes-finais" class="section level1">
<h1>Considera√ß√µes Finais</h1>
<p>Foi uma competi√ß√£o muito interessante e desafiadora. Agrade√ßo imensamente ao <a href="https://www.meetup.com/TensorFlowSP/events/284607061/">TFUG</a> por organizar o evento e a todos os participantes que contribu√≠ram para o aprendizado coletivo.Foi uma √≥tima oportunidade de aprendizado e troca de experi√™ncias.</p>
<p>Espero que minha solu√ß√£o possa ser √∫til para outros projetos e desafios futuros.</p>
</div>
<div id="sobre-o-autor" class="section level1">
<h1>Sobre o Autor</h1>
<p>Me chamo Fellipe Gomes, sou cientista de dados e apaixonado por aprendizado de m√°quina. Compartilho meu conhecimento por meio de artigos, tutoriais e projetos de c√≥digo aberto. Se quiser saber mais sobre meu trabalho, sinta-se √† vontade para conferir meu <a href="https://www.linkedin.com/in/fellipe-gomes-06943264/">LinkedIn</a> e <a href="https://github.com/fellipe-gomes">GitHub</a>.</p>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2023-05-30-solucao-final-ensure-healthy-lives-kaggle-competition/">Solu√ß√£o Final - ML Olympiad [1¬∫ lugar]</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">catboost</category>
      <category domain="tag">classification</category>
      <category domain="tag">data-mining</category>
      <category domain="tag">data-science</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">machine-learning</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">pratica</category>
      <category domain="tag">r</category>
    </item>
    <item>
      <title>Solu√ß√£o Final - ML Olympiad [2¬∫ lugar]</title>
      <link>https://gomesfellipe.github.io/post/2022-04-20-solucao-final-education-quality-kaggle-competition/</link>
      <pubDate>Tue, 19 Apr 2022 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2022-04-20-solucao-final-education-quality-kaggle-competition/</guid>
      <description>Confira a estrat√©gia aplicada para esta competi√ß√£o</description>
      <content:encoded>&lt;![CDATA[
        

<div id="TOC">
<ul>
<li><a href="#introdu%C3%A7%C3%A3o" id="toc-introdu√ß√£o">Introdu√ß√£o</a></li>
<li><a href="#defini%C3%A7%C3%A3o-do-problema-de-neg%C3%B3cio" id="toc-defini√ß√£o-do-problema-de-neg√≥cio">Defini√ß√£o do problema de neg√≥cio</a></li>
<li><a href="#an%C3%A1lise-explorat%C3%B3ria-em-r" id="toc-an√°lise-explorat√≥ria-em-r">An√°lise Explorat√≥ria (em R)</a>
<ul>
<li><a href="#estrutura-da-base" id="toc-estrutura-da-base">Estrutura da base</a></li>
<li><a href="#ano-da-base-de-dados" id="toc-ano-da-base-de-dados">Ano da base de dados</a></li>
<li><a href="#target" id="toc-target">Target</a></li>
</ul></li>
<li><a href="#machine-learning-em-python" id="toc-machine-learning-em-python">Machine Learning (em Python)</a>
<ul>
<li><a href="#importar-dependencias" id="toc-importar-dependencias">Importar dependencias</a></li>
<li><a href="#carregar-dados" id="toc-carregar-dados">Carregar dados</a></li>
<li><a href="#modelagem" id="toc-modelagem">Modelagem</a></li>
</ul></li>
<li><a href="#submiss%C3%A3o" id="toc-submiss√£o">Submiss√£o</a></li>
<li><a href="#considera%C3%A7%C3%B5es-finais" id="toc-considera√ß√µes-finais">Considera√ß√µes Finais</a></li>
</ul>
</div>

<div id="introdu√ß√£o" class="section level1">
<h1>Introdu√ß√£o</h1>
<p>No final de Janeiro desde ano (2022) o <a href="https://www.meetup.com/TensorFlowSP/events/284607061/">TFUG - TensorFlow Users Group de S√£o Paulo</a> lan√ßou uma competi√ß√£o no Kaggle para prever as notas do enem que tem rela√ß√£o com um dos 17 t√≥picos de Desenvolvimento Sustent√°vel das Na√ß√µes Unidas - <em>Educa√ß√£o de Qualidade</em>.</p>
<p>Al√©m de divertido, o desafio foi repleto de possibilidades e bastante desafiador! Todos os competidores que trabalharam duro em pleno m√™s de carnaval est√£o de parab√©ns! üòÖ üòÇ</p>
<p>Aqui est√£o alguns dos pr√™mios recebidos:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/premio.png" style="width:80.0%" />
</center>
<p>Como nesta competi√ß√£o havia bastante trabalho a ser feito e tivemos apenas 1 m√™s para trabalhar na solu√ß√£o, foi preciso fazer uma boa gest√£o do c√≥digo e do tempo de desenvolvimento.</p>
<p>Nas se√ß√µes abaixo apresento o racional por tr√°s da minha solu√ß√£o bem como os 5 melhores modelos individuais (para cada nota) que utilizei em um ensemble para chegar ao segundo lugar.</p>
</div>
<div id="defini√ß√£o-do-problema-de-neg√≥cio" class="section level1">
<h1>Defini√ß√£o do problema de neg√≥cio</h1>
<p>O objetivo desta competi√ß√£o consistiu em prever as notas dos alunos(as) nas provas: Ci√™ncias da Natureza, Ci√™ncias Humanas, Linguagens e C√≥digos, Matem√°tica e Reda√ß√£o.</p>
<p>Apesar das notas serem calculadas de maneira independente, a partir de modelos de <a href="http://portal.mec.gov.br/ultimas-noticias/389-ensino-medio-2092297298/17319-teoria-de-resposta-ao-item-avalia-habilidade-e-minimiza-o-chute">TRI (Teoria de Resposta ao Item)</a> que levam em considera√ß√£o a performance em um caderno espec√≠fico e na dificuldade de cada quest√£o, o mesmo aluno realiza todas as provas em um curto per√≠odo de tempo.</p>
<p>Portanto, esta tarefa pode ser enquadrada como um problema supervisionado de regress√£o com m√∫ltiplos outputs na qual as previs√µes s√£o, de certa forma, dependentes da entrada umas das outras.</p>
<p>A valida√ß√£o da solu√ß√£o foi feita utilizando a m√©trica Mean Columnwise Root Mean Squared Error ‚Äì MCRMSE, que √© basicamente a m√©dia do RMSE calculado sobre as previs√µes de cada nota.</p>
</div>
<div id="an√°lise-explorat√≥ria-em-r" class="section level1">
<h1>An√°lise Explorat√≥ria (em R)</h1>
<p>Convido o leitor a conferir o <a href="https://gomesfellipe.github.io/post/2021-11-01-solucao-final-porto-seguro-data-challenge/">notebook publicado no Kaggle</a> com a an√°lise explorat√≥ria completa. Aqui irei trazer apenas alguns dos principais insights que encontrei durante a etapa de an√°lise explorat√≥ria.</p>
<div id="estrutura-da-base" class="section level2">
<h2>Estrutura da base</h2>
<p>Veja a seguir qual a estrutura geral da base de dados:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/02_df_status.png" style="width:95.0%" />
</center>
<p>√â not√≥rio que existem dados faltantes e que parece haver algum padr√£o. Vejamos com mais detalhse:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/03_missing.png" style="width:95.0%" />
</center>
<div class="w3-panel w3-pale-blue w3-border">
<p>¬† üí° Insights!</p>
<p>Existem dados <em>missing</em> nas 5 targets que queremos prever e note que existe uma rela√ß√£o tanto entre as provas de Matem√°tica e Ci√™ncias da Natuerza quanto nas de Ci√™ncias Humanas, Linguagens e C√≥digos e Reda√ß√£o, o que parece ocorrer devido a aus√™ncia do aluno incrito em comparecer a realiza√ß√£o da prova no respectivo dia.</p>
</div>
</div>
<div id="ano-da-base-de-dados" class="section level2">
<h2>Ano da base de dados</h2>
<p>Essa informa√ß√£o n√£o estava explicitamente dispon√≠vel, mas ap√≥s analisar a idade dos participantes em rela√ß√£o ao ano em que conclu√≠ram o ensino m√©dio, foi poss√≠vel identificar que tratavam-se dos dados de 2019, veja:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/05_ano_concluiu.png" style="width:95.0%" />
</center>
<p>Essa informa√ß√£o poderia ser √∫til na hora de buscar dados externos (permitido nesta competi√ß√£o).</p>
<div class="w3-panel w3-pale-blue w3-border">
<p>¬† üí° Insights!</p>
<p>‚Üí Aten√ß√£o aos outliers: √â no m√≠nimo estranho uma pessoa que formou em 2007 ter 17 anos;</p>
<p>‚Üí Como ningu√©m concluiu a escola no ano de 2019 e a m√©dia das idades vai diminuindo quanto mais pr√≥ximo de 2018, parece que estes dados s√£o de 2019. Essa inform√ß√£o poderia ser √∫til na hora de procurar por bases externas.</p>
</div>
</div>
<div id="target" class="section level2">
<h2>Target</h2>
<p>A primeira decis√£o importante era definir como enquadrar o problema; se seriam m√∫ltiplos modelos independentes ou modelos com sa√≠das dependentes.</p>
<p>Primeiramente vejamos como eram as distribui√ß√µes das notas por caderno:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/07_distribuicao_target.png" style="width:95.0%" />
</center>
<p>Ao olhar estas distribui√ß√µes foram surgindo v√°rias id√©ias! Cheguei at√© a tentar modelos estat√≠sticos GAM considerando a resposta como uma distribui√ß√£o Beta (transformando as targets no intervalo [0,1]) mas acabou n√£o apresentando bons resultados para a competi√ß√£o.. acho que seria necess√°rio um pouco mais de prepara√ß√£o nos dados.</p>
<p>Apesar das notas do enem serem calculadas via TRI (Teoria de Resposta ao Item) que considera as notas independentes, parece existir alguma correla√ß√£o entre as notas, veja:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/08_correlacao_notas.png" style="width:95.0%" />
</center>
<p>As targets da nota de L√≠nguas e C√≥digos e Ci√™ncias Humanas pareciam possuir uma correla√ß√£o ‚Äúinteressante‚Äù, mas, ap√≥s testar modelos de m√∫ltiplas respostas dependentes para cada dia (com e sem a nota da reda√ß√£o), em nenhum de meus testes superou (de maneira consistente) o desempenho de modelos que considerassem as sa√≠das independentes. Portanto foquei em criar 5 modelos independentes.</p>
</div>
</div>
<div id="machine-learning-em-python" class="section level1">
<h1>Machine Learning (em Python)</h1>
<p>Toda a rotina de pr√©-processamento dos dados, feature engineering, modelagem, ensamble e p√≥s-processamento foi realizada utilizando a linguagem Python para cada uma das 5 notas. Trouxe apenas o modelo final neste post mas, para chegar at√© aqui foram necess√°rio muitos testes!</p>
<div id="importar-dependencias" class="section level2">
<h2>Importar dependencias</h2>
<p>Carregar pacotes Python:</p>
<pre class="python"><code># data prep
import numpy as np 
import pandas as pd 
# pre process
from sklearn.preprocessing import MinMaxScaler
# modeling
from sklearn.model_selection import train_test_split
from catboost import CatBoostRegressor
# plots
import seaborn as sns
import matplotlib.pyplot as plt</code></pre>
<p>Confira a baixo as fun√ß√µes desenvolvidas para a solu√ß√£o deste problema</p>
<details>
<summary>
(<em>Clique aqui para expandir as fun√ß√µes</em>)
</summary>
<pre class="python"><code>def prep_data_questionarios(df):
  &#39;&#39;&#39;
  Converte dados de questionario para ordinal
  &#39;&#39;&#39;
    # escolaridade pai
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}
    df.loc[:, &#39;Q001&#39;] = df.loc[:, &#39;Q001&#39;].map(to_map).astype(int)

    # escolaridade mae
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}
    df.loc[:, &#39;Q002&#39;] = df.loc[:, &#39;Q002&#39;].map(to_map).astype(int) 

    # ocupacao pai
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: -1}
    df.loc[:, &#39;Q003&#39;] = df.loc[:, &#39;Q003&#39;].map(to_map).astype(int) 

    # ocupacao mae
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: -1}
    df.loc[:, &#39;Q004&#39;] = df.loc[:, &#39;Q004&#39;].map(to_map).astype(int) 

    # renda da familia
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;:5, &#39;G&#39;:6, &#39;H&#39;:7, &#39;I&#39;:8,
              &#39;J&#39;:9, &#39;K&#39;:10,&#39;L&#39;:11, &#39;M&#39;:12, &#39;N&#39;:13, &#39;O&#39;:14, &#39;P&#39;:15, &#39;Q&#39;:16}
    df.loc[:, &#39;Q006&#39;] = df.loc[:, &#39;Q006&#39;].map(to_map).astype(int) 

    # empregado domestico
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3}
    df.loc[:, &#39;Q007&#39;] = df.loc[:, &#39;Q007&#39;].map(to_map).astype(int) 

    # banheiro
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q008&#39;] = df.loc[:, &#39;Q008&#39;].map(to_map).astype(int) 

    # qnt de quartos
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q009&#39;] = df.loc[:, &#39;Q009&#39;].map(to_map).astype(int) 

    # qnt de carros
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q010&#39;] = df.loc[:, &#39;Q010&#39;].map(to_map).astype(int) 

    # qnt de motocicleta
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q011&#39;] = df.loc[:, &#39;Q011&#39;].map(to_map).astype(int) 

    # qnt de geladeira
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q012&#39;] = df.loc[:, &#39;Q012&#39;].map(to_map).astype(int) 

    # qnt de freezer
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q013&#39;] = df.loc[:, &#39;Q013&#39;].map(to_map).astype(int) 

    # qnt de maquina de lavar roupa
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q014&#39;] = df.loc[:, &#39;Q014&#39;].map(to_map).astype(int) 

    # qnt de maquina de secar roupa
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q015&#39;] = df.loc[:, &#39;Q015&#39;].map(to_map).astype(int) 

    # qnt de microondas
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q016&#39;] = df.loc[:, &#39;Q016&#39;].map(to_map).astype(int) 

    # qnt de maquina de lavar louca
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q017&#39;] = df.loc[:, &#39;Q017&#39;].map(to_map).astype(int) 

    # tem aspirador de po
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1}
    df.loc[:, &#39;Q018&#39;] = df.loc[:, &#39;Q018&#39;].map(to_map).astype(int) 

    # qtd tv colorida
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q019&#39;] = df.loc[:, &#39;Q019&#39;].map(to_map).astype(int) 

    # tem dvd
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1}
    df.loc[:, &#39;Q020&#39;] = df.loc[:, &#39;Q020&#39;].map(to_map).astype(int) 

    # tem tv por assinatura
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1}
    df.loc[:, &#39;Q021&#39;] = df.loc[:, &#39;Q021&#39;].map(to_map).astype(int) 

    # qtd telefone celular
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q022&#39;] = df.loc[:, &#39;Q022&#39;].map(to_map).astype(int) 

    # qtd telefone fixo
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1}
    df.loc[:, &#39;Q023&#39;] = df.loc[:, &#39;Q023&#39;].map(to_map).astype(int) 

    # qtd computador
    to_map =  {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q024&#39;] = df.loc[:, &#39;Q024&#39;].map(to_map).astype(int) 

    # tem acesso a internet
    to_map =  {&#39;A&#39;:0, &#39;B&#39;:1}
    df.loc[:, &#39;Q025&#39;] = df.loc[:, &#39;Q025&#39;].map(to_map).astype(int) 
    
    return(df)
  
def fe_questionario(df):
  &#39;&#39;&#39;
  Gerar novas features artificiais baseadas nos dados de questionario
  &#39;&#39;&#39;
    df.loc[:, &quot;Q021+Q006&quot;] = df[&quot;Q021&quot;] + df[&quot;Q006&quot;]
    df.loc[:, &quot;Q018+Q006&quot;] = df[&quot;Q018&quot;] + df[&quot;Q006&quot;]
    df.loc[:, &quot;Q018+Q008&quot;] = df[&quot;Q018&quot;] + df[&quot;Q008&quot;]
    df.loc[:, &quot;Q010+Q018&quot;] = df[&quot;Q010&quot;] + df[&quot;Q018&quot;]
    df.loc[:, &quot;Q018+Q024&quot;] = df[&quot;Q018&quot;] + df[&quot;Q024&quot;]
    
    df.loc[:, &quot;Q018*Q006&quot;] = df[&quot;Q018&quot;] * df[&quot;Q006&quot;]
    df.loc[:, &quot;Q010*Q018&quot;] = df[&quot;Q010&quot;] * df[&quot;Q018&quot;]
    
    return df
  
def fe_mun(data):
    &#39;&#39;&#39;
    Gerar novas features a partir das localizacoes de municipio
    &#39;&#39;&#39;
    for c in list(data.columns[data.dtypes==&#39;category&#39;]):
        data.loc[:, c] = data.loc[:, c].astype(&#39;object&#39;)
    
    data.loc[:, &#39;FE_MUNICIPIO_PROVA_x_MUNICIPIO_RESIDENCIA&#39;] = np.where(data.NO_MUNICIPIO_PROVA == data.NO_MUNICIPIO_RESIDENCIA , 1, 0)
    data.loc[:, &#39;FE_MUNICIPIO_PROVA_x_MUNICIPIO_NASCIMENTO&#39;] = np.where(data.NO_MUNICIPIO_PROVA == data.NO_MUNICIPIO_NASCIMENTO , 1, 0)
    data.loc[:, &#39;FE_MUNICIPIO_PROVA_x_MUNICIPIO_ESC&#39;] = np.where(data.NO_MUNICIPIO_PROVA == data.NO_MUNICIPIO_ESC , 1, 0)
    data.loc[:, &#39;FE_MUNICIPIO_RESIDENCIA_x_MUNICIPIO_NASCIMENTO&#39;] = np.where(data.NO_MUNICIPIO_RESIDENCIA == data.NO_MUNICIPIO_NASCIMENTO , 1, 0)
    data.loc[:, &#39;FE_MUNICIPIO_RESIDENCIA_x_MUNICIPIO_ESC&#39;] = np.where(data.NO_MUNICIPIO_RESIDENCIA == data.NO_MUNICIPIO_ESC , 1, 0)
    data.loc[:, &#39;FE_MUNICIPIO_NASCIMENTO_x_MUNICIPIO_ESC&#39;] = np.where(data.NO_MUNICIPIO_RESIDENCIA == data.NO_MUNICIPIO_ESC , 1, 0)
    
    for c in list(data.columns[data.dtypes==&#39;object&#39;]):
        data.loc[:, c] = data.loc[:, c].astype(&#39;category&#39;)
    
    return data
  
def fe_in(df):
    &#39;&#39;&#39;
    Gerar features a partir das indicadoras
    &#39;&#39;&#39;
    df.loc[:, &#39;IN_DEFICIT_ATENCAO+IN_TEMPO_ADICIONAL&#39;] = df[&quot;IN_DEFICIT_ATENCAO&quot;] + df[&quot;IN_TEMPO_ADICIONAL&quot;]
    df.loc[:, &#39;IN_LEDOR+IN_TRANSCRICAO&#39;] = df[&quot;IN_LEDOR&quot;] + df[&quot;IN_TRANSCRICAO&quot;]

    return df
  
def prep_co_escola(df):
    &#39;&#39;&#39;
    Converter codigo da escola para categorico
    &#39;&#39;&#39;
    df.loc[:, &#39;CO_ESCOLA&#39;] = [str(x) for x in df.CO_ESCOLA]
    df.loc[:, &#39;CO_ESCOLA&#39;] = np.where(df[&#39;CO_ESCOLA&#39;]==&#39;nan&#39;, np.nan, df[&#39;CO_ESCOLA&#39;])
    df.loc[:, &#39;CO_ESCOLA&#39;] = df.loc[:, &#39;CO_ESCOLA&#39;].astype(&#39;category&#39;)
    
    return df
  
def fe_extra(df):
    &#39;&#39;&#39;
    Gerar novas features 
    &#39;&#39;&#39;
    df.loc[:, &quot;FE_IDADE_DISCRETA&quot;] = pd.cut(df.NU_IDADE, (0, 15, 18, 23, 36, 60, 120), labels=[&#39;ADOLESCENTE&#39;,&#39;ADOLESCENTE_2&#39;, &#39;JOVEM&#39;,&#39;JOVEM_2&#39;, &#39;ADULTO&#39;, &#39;IDOSO&#39;]).astype(&#39;category&#39;)
    df.loc[:, &#39;FE_OCUPACAO_PAIS&#39;] = df.Q003 + df.Q004
    df.loc[:, &#39;FE_ESCOLARIDADE_PAIS&#39;] = df.Q001 + df.Q002
    df.loc[:, &#39;FE_RENDA_POR_PESSOA&#39;] = df.Q006 / df.Q005
    df.loc[:, &#39;FE_CELULAR_POR_PESSOA&#39;] = df.Q022 / df.Q005
    df.loc[:, &#39;FE_COMPUTADOR_POR_PESSOA&#39;] = df.Q024 / df.Q005
    df.loc[:, &#39;FE_VISAO_RUIM&#39;] = df[[&#39;IN_BAIXA_VISAO&#39;, &#39;IN_CEGUEIRA&#39;, &#39;IN_VISAO_MONOCULAR&#39;, &#39;IN_SURDO_CEGUEIRA&#39;]].max(axis=1)
    df.loc[:, &#39;FE_AUDICAO_RUIM&#39;] = df[[&#39;IN_SURDEZ&#39;, &#39;IN_DEFICIENCIA_AUDITIVA&#39;, &#39;IN_SURDO_CEGUEIRA&#39;]].max(axis=1)
    df.loc[:, &#39;FE_TDAH_MAIS_TEMPO&#39;] = df.IN_TEMPO_ADICIONAL + df.IN_DEFICIT_ATENCAO
    df.loc[:, &#39;FE_TDAH_MEDICADO&#39;] = np.where((df.IN_DEFICIT_ATENCAO==1)&amp;(df.IN_MEDICAMENTOS==1), 1, 0)
    df.loc[:, &#39;FE_RECURSO_VISAO&#39;] =  df[[&#39;IN_BRAILLE&#39;, &#39;IN_AMPLIADA_24&#39;, &#39;IN_AMPLIADA_18&#39;, &#39;IN_LEDOR&#39;, &#39;IN_MAQUINA_BRAILE&#39;, &#39;IN_LAMINA_OVERLAY&#39;]].max(axis=1)
    df.loc[:, &#39;FE_RECURSO_SURDEZ&#39;] =  df[[&#39;IN_LIBRAS&#39;, &#39;IN_LEITURA_LABIAL&#39;, &#39;IN_TRANSCRICAO&#39;]].max(axis=1)
    acess = [&#39;IN_ACESSO&#39;, &#39;IN_MESA_CADEIRA_RODAS&#39;, &#39;IN_MESA_CADEIRA_SEPARADA&#39;, &#39;IN_APOIO_PERNA&#39;, &#39;IN_CADEIRA_ESPECIAL&#39;, &#39;IN_CADEIRA_CANHOTO&#39;, &#39;IN_CADEIRA_ACOLCHOADA&#39;, &#39;IN_MOBILIARIO_OBESO&#39;, &#39;IN_SALA_INDIVIDUAL&#39;, &#39;IN_SALA_ESPECIAL&#39;, &#39;IN_SALA_ACOMPANHANTE&#39;, &#39;IN_MOBILIARIO_ESPECIFICO&#39;, &#39;IN_MATERIAL_ESPECIFICO&#39;]
    df.loc[:, &#39;FE_ACESSIBILIDADE&#39;] =  df[acess].max(axis=1)

    return df</code></pre>
</details>
<p>¬†</p>
<p>Carregar features artificiais extra√≠das atrav√©s de um modelo KNN. N√£o apresentarei o c√≥digo aqui (talvez fique para um pr√≥ximo post) mas a id√©ia √© basicamente a seguinte:</p>
<div class="w3-panel w3-pale-yellow w3-border">
<p>¬† üß™ Feature Extraction com KNN</p>
<p>Ajuste um <code>KNeighborsRegressor</code> encontrando os K-vizinhos mais pr√≥ximos de cada inst√¢ncia out-of-fold via valida√ß√£o cruzada (para evitar data leak) nos dados de treino e depois ajuste um modelo em todos os dados de treino para obter os K-vizinhos mais pr√≥ximos nos dados de teste.</p>
</div>
<p>Quem sabe no futuro fa√ßo um post compartilhando esta estrat√©gia com mais detalhes.</p>
<pre class="python"><code>knn_train = pd.read_csv(&quot;../input/knn/KNN_feat_train_CH_LC.csv&quot;)
knn_test = pd.read_csv(&quot;../input/knn/KNN_feat_test_CH_LC.csv&quot;)

knn_train_cn_mt = pd.read_csv(&quot;../input/knn/KNN_feat_train_CN_MT.csv&quot;)
knn_test_cn_mt = pd.read_csv(&quot;../input/knn/KNN_feat_test_CN_MT.csv&quot;)

knn_train_rd = pd.read_csv(&quot;../input/knn/KNN_feat_train_RD.csv&quot;)
knn_test_rd = pd.read_csv(&quot;../input/knn/KNN_feat_test_RD.csv&quot;)</code></pre>
</div>
<div id="carregar-dados" class="section level2">
<h2>Carregar dados</h2>
<p>Importar uma vers√£o do dataset no formato <code>.parquet</code> que foi compactada com um truque para otimizar o consumo de mem√≥ria disponibilizada pelos organizadores <a href="https://www.kaggle.com/code/caneiro/mlo-make-parquet">neste notebook</a>.</p>
<pre class="python"><code>train = pd.read_parquet(&#39;train.parquet&#39;)
test = pd.read_parquet(&#39;test.parquet&#39;)
sub = pd.read_csv(&#39;../input/qualityeducation/sample_submission.csv&#39;)</code></pre>
<p>Definir objetos com targets</p>
<pre class="python"><code>targets = [&#39;NU_NOTA_LC&#39;, &#39;NU_NOTA_CH&#39;, &#39;NU_NOTA_CN&#39;,  &#39;NU_NOTA_MT&#39;, &#39;NU_NOTA_REDACAO&#39;]
presencas = [&#39;TP_PRESENCA_LC&#39;, &#39;TP_PRESENCA_CH&#39;, &#39;TP_PRESENCA_CN&#39;, &#39;TP_PRESENCA_MT&#39;, &#39;TP_STATUS_REDACAO&#39;]</code></pre>
<div class="w3-panel w3-pale-yellow w3-border">
<p>¬† ‚ö†Ô∏è Aten√ß√£o:</p>
<p>A feature de presen√ßa √© muito importante no p√≥s-processamento para atribuir nota zero aos alunos que n√£o foram realizar a prova mas n√£o faz sentido mant√™-la nos dados de treino pois ser√° sempre constante.</p>
</div>
<div id="dados-externos" class="section level3">
<h3>Dados externos</h3>
<p>Dados Externos utilizados:</p>
<ol style="list-style-type: decimal">
<li><a href="https://basedosdados.org/dataset/mundo-onu-adh">Atlas do Desenvolvimento Humano (ADH)</a></li>
</ol>
<p>Esta base tinha muita informa√ß√£o legal mas sua cobertura temporal estava bastante defasada (1991 - 2010) o que pode adicionar algum ru√≠do ao modelo.</p>
<p>As features selecionadas (sem muito crit√©rio) desta base foram:</p>
<pre class="python"><code>extra1 = pd.read_csv(&quot;municipio.csv&quot;)

extra1 = extra1[extra1.ano==2010]

features_extra1 = [&#39;expectativa_vida&#39;, &#39;razao_dependencia&#39;, &#39;expectativa_anos_estudo&#39;,
&#39;taxa_analfabetismo_11_a_14&#39;, &#39;taxa_analfabetismo_15_a_17&#39;, &#39;taxa_analfabetismo_18_mais&#39;,
&#39;taxa_atraso_0_basico&#39;, &#39;taxa_atraso_0_fundamental&#39;, &#39;taxa_atraso_0_medio&#39;,
&#39;taxa_freq_bruta_medio&#39;, &#39;taxa_freq_liquida_medio&#39;,
&#39;taxa_freq_medio_18_24&#39;, &#39;taxa_freq_medio_6_14&#39;, &#39;indice_gini&#39;,&#39;prop_pobreza_extrema&#39;, &#39;prop_pobreza&#39;,
&#39;prop_renda_10_ricos&#39;, &#39;prop_renda_20_pobres&#39;, &#39;razao_10_ricos_40_pobres&#39;,&#39;renda_pc&#39; , &#39;renda_pc_quintil_1&#39;,
&#39;indice_theil&#39;, &#39;prop_trabalhadores_conta_proria&#39;, 
&#39;prop_empregadores&#39;, &#39;prop_ocupados_agropecuaria&#39;, &#39;prop_ocupados_comercio&#39;,
&#39;prop_ocupados_construcao&#39;, &#39;prop_ocupados_formalizacao&#39;, &#39;prop_ocupados_medio&#39;,
&#39;prop_ocupados_servicos&#39;, &#39;prop_ocupados_superior&#39;,
&#39;prop_ocupados_renda_0&#39;, &#39;renda_media_ocupados&#39;, &#39;indice_treil_trabalho&#39;,
&#39;taxa_ocupados_carteira&#39;, &#39;taxa_agua_encanada&#39;, 
&#39;taxa_banheiro_agua_encanada&#39;, &#39;taxa_coleta_lixo&#39;, &#39;taxa_energia_eletrica&#39;,
&#39;taxa_agua_esgoto_inadequados&#39;, &#39;taxa_criancas_dom_sem_fund&#39;,
&#39;pea&#39;, &#39;indice_escolaridade&#39;, &#39;indice_frequencia_escolar&#39;, 
&#39;idhm&#39;, &#39;idhm_e&#39;, &#39;idhm_l&#39;, &#39;idhm_r&#39;]
extra1 = extra1[[&#39;id_municipio&#39;]+features_extra1]

train = pd.merge(train, extra1, how=&#39;left&#39;, left_on=&#39;CO_MUNICIPIO_RESIDENCIA&#39;, right_on=&#39;id_municipio&#39;)
test = pd.merge(test, extra1, how=&#39;left&#39;, left_on=&#39;CO_MUNICIPIO_RESIDENCIA&#39;, right_on=&#39;id_municipio&#39;)</code></pre>
<ol start="2" style="list-style-type: decimal">
<li><a href="https://www.gov.br/inep/pt-br/acesso-a-informacao/dados-abertos/microdados/censo-escolar">Microdados do Censo Escolar da Educaca√ß√£o B√°sica</a></li>
</ol>
<p>Base dispon√≠vel no mesmo site dos dados da competi√ß√£o e que tr√°s informa√ß√µes muito ricas das escolas do Brasil. Infelizmente quase 75% da informa√ß√£o da escola do aluno era missing ent√£o esta base n√£o conseguiu alavancar os ganhos do modelo de maneira consider√°vel.</p>
<p>Nesta base foquei principalmente nas features utilizadas para calcular o IIE (√çndice de Estrutura da Escola) que se baseia nos seguintes componentes:</p>
<table>
<colgroup>
<col width="32%" />
<col width="24%" />
<col width="42%" />
</colgroup>
<thead>
<tr class="header">
<th>Componente 1: Pedag√≥gica (IEE_Pedag√≥gico):</th>
<th>Componente 2: B√°sica (IEE_B√°sico):</th>
<th>Componente 3: Tecnol√≥gica (IEE_Tecnol√≥gico):</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Qualifica√ß√£o do docente (forma√ß√£o acad√™mica dos professores)</td>
<td>√Ågua filtrada (bin√°ria)</td>
<td>N√∫mero de computadores por aluno (computadores dispon√≠veis para uso dos alunos)</td>
</tr>
<tr class="even">
<td>N√∫mero de alunos por sala</td>
<td>Acesso √† rede p√∫blica de energia (bin√°ria)</td>
<td>N√∫mero de equipamentos multim√≠dia por aluno</td>
</tr>
<tr class="odd">
<td>N√∫mero de funcion√°rios por aluno</td>
<td>Acesso √† rede p√∫blica de esgoto (bin√°ria)</td>
<td>Acesso a internet (bin√°ria)</td>
</tr>
<tr class="even">
<td>Quadra de esportes coberta (bin√°ria)</td>
<td>Coleta peri√≥dica de lixo (bin√°ria)</td>
<td>Laborat√≥rio de Ci√™ncias (bin√°ria)</td>
</tr>
<tr class="odd">
<td>Biblioteca (bin√°ria)</td>
<td>Banheiro dentro do pr√©dio (bin√°ria)</td>
<td>Laborat√≥rio de Inform√°tica (bin√°ria)</td>
</tr>
</tbody>
</table>
<ul>
<li><a href="https://leosalesblog.wordpress.com/2018/02/03/escola-ruim-aluno-ruim-entendendo-a-relacao-entre-estrutura-escolar-e-desempenho-no-enem/">Fonte</a></li>
</ul>
<pre class="python"><code># Importar dados
extra2 = pd.read_csv(&#39;microdados_ed_basica_2021.csv&#39;, error_bad_lines=False, sep=&#39;;&#39;, encoding=&#39;latin1&#39;, dtype={&#39;CO_ORGAO_REGIONAL&#39;: &#39;str&#39;})
extra2 = extra2[extra2.isnull().sum(axis=1) / extra2.shape[1] &lt; .9]

# Tratamento nas features
extra2.loc[:, &#39;QT_TOTAL_ALUNOS&#39;] = extra2[[&#39;QT_MAT_BAS_ND&#39;, &#39;QT_MAT_BAS_BRANCA&#39;, &#39;QT_MAT_BAS_PRETA&#39;, &#39;QT_MAT_BAS_PARDA&#39;, &#39;QT_MAT_BAS_AMARELA&#39;, &#39;QT_MAT_BAS_INDIGENA&#39;]].sum(axis=1).fillna(0)
extra2.loc[:, &#39;QT_TOTAL_PROFESSORES&#39;] = (extra2.QT_DOC_BAS + extra2.QT_DOC_INF + extra2.QT_DOC_INF_CRE + extra2.QT_DOC_INF_PRE + extra2.QT_DOC_FUND + extra2.QT_DOC_FUND_AI + extra2.QT_DOC_FUND_AF + extra2.QT_DOC_MED + extra2.QT_DOC_PROF + extra2.QT_DOC_PROF_TEC + extra2.QT_DOC_EJA + extra2.QT_DOC_EJA_FUND + extra2.QT_DOC_EJA_MED + extra2.QT_DOC_ESP + extra2.QT_DOC_ESP_CC + extra2.QT_DOC_ESP_CE).fillna(0)
extra2.loc[:, &#39;QT_SALAS_UTILIZADAS&#39;] = (extra2.loc[:, &#39;QT_TOTAL_ALUNOS&#39;] / extra2.QT_SALAS_UTILIZADAS).fillna(0)
extra2.loc[:, &#39;QT_COMP_DISP_ALUNO&#39;] = extra2.QT_DESKTOP_ALUNO + extra2.QT_COMP_PORTATIL_ALUNO + extra2.QT_TABLET_ALUNO

# Selecao de faetures importantes
features_extra2 = [&#39;CO_ENTIDADE&#39;, &#39;QT_SALAS_UTILIZADAS&#39;, &#39;QT_TOTAL_PROFESSORES&#39;, &#39;IN_QUADRA_ESPORTES_COBERTA&#39;, &#39;IN_BIBLIOTECA&#39;,
       &#39;IN_AGUA_POTAVEL&#39;, &#39;IN_ENERGIA_REDE_PUBLICA&#39;, &#39;IN_ESGOTO_REDE_PUBLICA&#39;, &#39;IN_LIXO_SERVICO_COLETA&#39;, &#39;IN_BANHEIRO&#39;,
       &#39;QT_COMP_DISP_ALUNO&#39;, &#39;QT_EQUIP_MULTIMIDIA&#39;, &#39;IN_INTERNET&#39;, &#39;IN_LABORATORIO_CIENCIAS&#39;, &#39;IN_LABORATORIO_INFORMATICA&#39;]
extra2 = extra2[features_extra2]

# Remover outliers
for c in list(extra2.iloc[:, 1:].columns):
    trs = extra2.loc[extra2[c]!=88888, c].quantile(.99)
    extra2.loc[(extra2[c]==88888)|(extra2[c]&gt;trs), c] = trs
    
#Normalizar para calcular IEE
scaler = MinMaxScaler()
to_iee = scaler.fit_transform(extra2.iloc[:, 1:])
to_iee = pd.DataFrame(to_iee, columns=extra2.iloc[:, 1:].columns)

# Calcular IEE e componentes
extra2.loc[:, &#39;COMP1&#39;] = to_iee[[&#39;QT_SALAS_UTILIZADAS&#39;, &#39;QT_TOTAL_PROFESSORES&#39;, &#39;IN_QUADRA_ESPORTES_COBERTA&#39;, &#39;IN_BIBLIOTECA&#39;]].sum(axis=1)
extra2.loc[:, &#39;COMP2&#39;] = to_iee[[&#39;IN_AGUA_POTAVEL&#39;, &#39;IN_ENERGIA_REDE_PUBLICA&#39;, &#39;IN_ESGOTO_REDE_PUBLICA&#39;, &#39;IN_LIXO_SERVICO_COLETA&#39;, &#39;IN_BANHEIRO&#39;]].sum(axis=1)
extra2.loc[:, &#39;COMP3&#39;] = to_iee[[&#39;QT_COMP_DISP_ALUNO&#39;, &#39;QT_EQUIP_MULTIMIDIA&#39;, &#39;IN_INTERNET&#39;, &#39;IN_LABORATORIO_CIENCIAS&#39;, &#39;IN_LABORATORIO_INFORMATICA&#39;]].sum(axis=1)
extra2.loc[:, &#39;IEE&#39;] = extra2.COMP1 + extra2.COMP2 + extra2.COMP3

train = pd.merge(train, extra2, how=&#39;left&#39;, left_on=&#39;CO_ESCOLA&#39;, right_on=&#39;CO_ENTIDADE&#39;).drop(&#39;CO_ENTIDADE&#39;, axis=1)
test = pd.merge(test, extra2, how=&#39;left&#39;, left_on=&#39;CO_ESCOLA&#39;, right_on=&#39;CO_ENTIDADE&#39;).drop(&#39;CO_ENTIDADE&#39;, axis=1)</code></pre>
</div>
</div>
<div id="modelagem" class="section level2">
<h2>Modelagem</h2>
<p>Testei muitos modelos e muitas abordagens (inclusive com finalidade de estudo). Foram modelos estat√≠sticos (GAM considerando a distribui√ß√£o Beta(0,1)), redes neurais (TabNet) e √°rvores mas no final das contas os que tiveram melhor custo/benef√≠cio foram o LightGBM e o CatBoost.</p>
<p>Sobre o tuning, tomei a decis√£o de n√£o investir muito em otimiza√ß√£o autom√°tica de hiperpar√¢metros pois o tempo era curto e os ganhos seriam pequenos comparados com o potencial ganho com a variedade de features que poderiam ser geradas, ent√£o fiz apenas alguns testes manuais conforme via necessidade.</p>
<div id="pre-processing" class="section level4">
<h4>Pre processing</h4>
<p>A etapa que investi bastante tempo foi para criar novas vari√°veis. A seguir trago algumas features constru√≠das que foram utilizadas em determinados modelos, a partir dos dados dispon√≠veis:</p>
<ul>
<li>Renda somada dos pais;</li>
<li>N√≠vel de ocupa√ß√£o somado dos pais;</li>
<li>Renda dividido pelo n√∫mero de pessoas na casa;</li>
<li>Quantidade de celulares por pessoa na casa;</li>
<li>Quantidade de computadores por pessoa na casa;</li>
<li>Se a pessoa possui vis√£o ruim (se possui baixa vis√£o, cegueira ou monocular);</li>
<li>Se a pessoa possui audi√ß√£o ruim (Surdez, defici√™ncia auditiva);</li>
<li>Se o aluno possui TDAH e toma medicamento controlado;</li>
<li>Se o aluno possui TDAH e teve mais tempo de prova;</li>
<li>Se precisou de recurso de vis√£o ou audi√ß√£o (libras, baile, etc);</li>
<li>Se o munic√≠pio que nasceu √© o mesmo da escola;</li>
<li>Se o munic√≠pio que fez a prova √© o mesmo da escola;</li>
<li>Se o munic√≠pio da prova √© o mesmo da resid√™ncia;</li>
<li>Nota m√©dia dos alunos da respectiva escola nas outras provas (*);</li>
<li>Renda m√©dia dos alunos da respectiva escola (*).</li>
</ul>
<p>(*) Estas features precisaram ser calculadas de maneira muito cuidadosa para n√£o causar algum tipo de data leak!</p>
</div>
<div id="post-processing" class="section level4">
<h4>Post Processing</h4>
<p>Essa base tinha uma pegadinha que fazia muita diferen√ßa no resultado final. Existem duas possibilidades de um aluno tirar zero em uma prova: errar tudo ou n√£o comparecer.</p>
<p>Como temos a informa√ß√£o da presen√ßa do aluno na prova (o que na pr√°tica seria meio estranho) bastava dar zero para os alunos faltantes na hora de prever nos dados de teste para submeter.</p>
</div>
<div id="linguagens-e-c√≥digos" class="section level3">
<h3>Linguagens e C√≥digos</h3>
<p>Definir finalidade de algumas colunas:</p>
<pre class="python"><code># colunas que serao dropadas
to_drop = [&#39;IN_PROVA_DEITADO&#39;,
            &#39;NU_INSCRICAO&#39;,
            &#39;CO_MUNICIPIO_ESC&#39;,
            &#39;CO_UF_NASCIMENTO&#39;,
            &#39;CO_UF_RESIDENCIA&#39;,
            &#39;CO_UF_ESC&#39;,
            &#39;CO_UF_PROVA&#39;,
            &#39;CO_MUNICIPIO_PROVA&#39;,
            &#39;CO_MUNICIPIO_RESIDENCIA&#39;,
            &#39;CO_MUNICIPIO_NASCIMENTO&#39;]

# definir target e presenca
target = &quot;NU_NOTA_LC&quot;
presenca = &quot;TP_PRESENCA_LC&quot;

# demais notas para dropar (menos ch)
notas = list(set(targets)-set([target, &#39;NU_NOTA_CH&#39;]))</code></pre>
<p>Pr√©-processamento nos dados de treino</p>
<pre class="python"><code>X = train.copy()
X.loc[:, &#39;knn_feature&#39;] = knn_train.knn_oof
X = X.drop(to_drop, axis=1) 
X = X[X[presenca]==1]
X = X[~X[target].isnull()]

X = X.loc[:, ~X.columns.isin([target]+[presenca]+notas)]
X.loc[:, &#39;FE_RENDA&#39;] = X.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000, &#39;C&#39;:1500, &#39;D&#39;:2000,
&#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, &#39;I&#39;:6000, &#39;J&#39;:7000,&#39;K&#39;:8000,&#39;L&#39;:9000,
&#39;M&#39;:10000, &#39;N&#39;:12000, &#39;O&#39;:15000, &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
X = prep_data_questionarios(X)
X = fe_mun(X)
X = fe_questionario(X)
X = fe_in(X)
X = prep_co_escola(X)
X = fe_extra(X)

y = train.loc[(train[presenca]==1)&amp;(~train[target].isnull()), target].astype(np.float64)</code></pre>
<p>Pr√©-processamento nos dados de teste</p>
<pre class="python"><code>X_test = test.copy()
X_test.loc[:, &#39;knn_feature&#39;] = knn_test.knn_test
X_test = X_test.drop(to_drop, axis=1) 

X_test = X_test.loc[:, ~X_test.columns.isin([presenca])]
X_test.loc[:, &#39;FE_RENDA&#39;] = X_test.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000, &#39;C&#39;:1500, &#39;D&#39;:2000,
&#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, &#39;I&#39;:6000, &#39;J&#39;:7000, &#39;K&#39;:8000,&#39;L&#39;:9000,
&#39;M&#39;:10000, &#39;N&#39;:12000, &#39;O&#39;:15000, &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
X_test = prep_data_questionarios(X_test)
X_test = fe_mun(X_test)
X_test = fe_questionario(X_test)
X_test = fe_in(X_test)
X_test = prep_co_escola(X_test)
X_test = fe_extra(X_test)</code></pre>
<p>Feature engineering separada para evitar data leak:</p>
<pre class="python"><code># calcular estatisticas nos dados de treino
co_escola_renda_media = X.groupby(&#39;CO_ESCOLA&#39;).FE_RENDA.mean()
co_escola_idade_media = X.groupby(&#39;CO_ESCOLA&#39;).NU_IDADE.mean()
co_escola_nota_ch = X.groupby(&#39;CO_ESCOLA&#39;).NU_NOTA_CH.mean()
X = X.drop(&#39;NU_NOTA_CH&#39;, axis=1)

# instanciar objeto com as estatisticas por escola
co_escola_aux = pd.DataFrame({
    &#39;CO_ESCOLA&#39;: co_escola_renda_media.index,
    &#39;FE_ESCOLA_RENDA_MEDIA&#39;: co_escola_renda_media,
    &#39;FE_IDADE_MEDIA&#39;: co_escola_idade_media,
    &#39;FE_NOTA_CH&#39;: co_escola_nota_ch
}).reset_index(drop=True)

# Concatenar estatisticas nas bases de treino e teste
X = pd.merge(X, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)
X_test = pd.merge(X_test, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)

# Codigo da escola para categorico
X.loc[:, &#39;CO_ESCOLA&#39;] = X.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)
X_test.loc[:, &#39;CO_ESCOLA&#39;] = X_test.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)

# Features de contagem
X.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X_test.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X.NO_MUNICIPIO_RESIDENCIA.map({x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X_test.NO_MUNICIPIO_RESIDENCIA.map({ x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X.NO_MUNICIPIO_NASCIMENTO.map({ x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X_test.NO_MUNICIPIO_NASCIMENTO.map({x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})

X.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X_test.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})</code></pre>
<p>Ajustar modelo:</p>
<pre class="python"><code>cat_feat = X.columns[X.dtypes==&#39;category&#39;]
cat_indices = [X.columns.get_loc(x) for x in cat_feat]

for c in list(cat_feat):
    X.loc[:, c] = X.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)
    X_test.loc[:, c] = X_test.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)

X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.1, random_state=SEED)
    
clf = CatBoostRegressor(random_state=314,
                            cat_features=cat_indices,
                            verbose=0,
                            loss_function = &quot;RMSE&quot;,
                            od_type = &quot;Iter&quot;,
                            od_wait = 100,
                            iterations=3000,
                            use_best_model=True)

clf.fit(X, y, eval_set = (X_eval, y_eval), verbose=False, plot=True)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/lc_catboost.png" style="width:95.0%" />
</center>
<p>Salvar previs√µes:</p>
<pre class="python"><code>sub.loc[:, &#39;NU_NOTA_LC&#39;] = clf.predict(X_test)
# alunos que nao foram fazer a prova tiraram zero
sub.loc[test.TP_PRESENCA_LC!=1, &#39;NU_NOTA_LC&#39;] = 0</code></pre>
<p>Comparar distribui√ß√£o da target nos dados de treino com rela√ß√£o √†s previs√µes do modelo:</p>
<pre class="python"><code>sns.kdeplot(train.loc[:, target], shade=True, color=&#39;r&#39;, clip=[0,1000])
sns.kdeplot(sub.loc[:, target], shade=True, color=&#39;b&#39;, clip=[0,1000])
plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
plt.title(target)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/lc_pred.png" style="width:50.0%" />
</center>
</div>
<div id="ci√™ncias-humanas" class="section level3">
<h3>Ci√™ncias Humanas</h3>
<p>Novas features desenvolvidas especificamente para este modelo:</p>
<pre class="python"><code>def fe_ch(df):
    
    df.loc[:, &#39;FE_RENDA&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000,
    &#39;C&#39;:1500, &#39;D&#39;:2000, &#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, &#39;I&#39;:6000,
    &#39;J&#39;:7000, &#39;K&#39;:8000,&#39;L&#39;:9000, &#39;M&#39;:10000, &#39;N&#39;:12000, &#39;O&#39;:15000, 
    &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
    df.loc[:, &#39;FE_NU_IDADE*TP_ANO_CONCLUIU&#39;] = df.TP_ANO_CONCLUIU * df.NU_IDADE
    df.loc[:, &#39;FE_Q002+Q024&#39;] = df.loc[:, &#39;Q002&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, 
    &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}).astype(int) + 
    df.loc[:, &#39;Q024&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}).astype(int) 
    df.loc[:, &#39;FE_SCORE&#39;] = (1/df.TP_ANO_CONCLUIU) + np.sqrt(df.NU_IDADE) +
    np.where(df.TP_ESCOLA==3, 1, 0)
    
    return df</code></pre>
<p>Definir finalidade de algumas colunas:</p>
<pre class="python"><code># colunas que serao dropadas
to_drop = [&#39;IN_PROVA_DEITADO&#39;,
           &#39;NU_INSCRICAO&#39;,
           &#39;CO_MUNICIPIO_ESC&#39;,
           &#39;CO_UF_NASCIMENTO&#39;,
           &#39;CO_UF_RESIDENCIA&#39;,
           &#39;CO_UF_ESC&#39;,
           &#39;CO_UF_PROVA&#39;,
           &#39;CO_MUNICIPIO_PROVA&#39;,
           &#39;CO_MUNICIPIO_RESIDENCIA&#39;,
          &#39;CO_MUNICIPIO_NASCIMENTO&#39;]

# definir target e presenca
target = &quot;NU_NOTA_CH&quot;
presenca = &quot;TP_PRESENCA_CH&quot;

# demais notas para dropar (menos lc)
notas = list(set(targets)-set([target, &#39;NU_NOTA_LC&#39;]))</code></pre>
<p>Pr√©-processamento nos dados de treino</p>
<pre class="python"><code>X = train.copy()
X.loc[:, &#39;knn_feature&#39;] = knn_train.knn_oof
X = X.drop(to_drop, axis=1)
X = X[X[presenca]==1]
X = X[~X[target].isnull()]

X = X.loc[:, ~X.columns.isin([target]+[presenca]+notas)]
X = fe_ch(X)
X = prep_data_questionarios(X)
X = fe_mun(X)
X = fe_questionario(X)
#X = fe_in(X)
X = prep_co_escola(X)
X = fe_extra(X)

y = train.loc[(train[presenca]==1)&amp;(~train[target].isnull()), target].astype(np.float64)</code></pre>
<p>Pr√©-processamento nos dados de teste</p>
<pre class="python"><code>X_test = test.copy()
X_test.loc[:, &#39;knn_feature&#39;] = knn_test.knn_test
X_test = X_test.drop(to_drop, axis=1) 

X_test = X_test.loc[:, ~X_test.columns.isin([presenca])]
X_test = fe_ch(X_test)
X_test = prep_data_questionarios(X_test)
X_test = fe_mun(X_test)
X_test = fe_questionario(X_test)
#X_test = fe_in(X_test)
X_test = prep_co_escola(X_test)
X_test = fe_extra(X_test)</code></pre>
<p>Feature engineering separada para evitar data leak:</p>
<pre class="python"><code># calcular estatisticas nos dados de treino
co_escola_renda_media = X.groupby(&#39;CO_ESCOLA&#39;).FE_RENDA.mean()
co_escola_idade_media = X.groupby(&#39;CO_ESCOLA&#39;).NU_IDADE.mean()
co_escola_nota_lc = X.groupby(&#39;CO_ESCOLA&#39;).NU_NOTA_LC.mean()
X = X.drop(&#39;NU_NOTA_LC&#39;, axis=1)

# instanciar objeto com as estatisticas por escola
co_escola_aux = pd.DataFrame({
    &#39;CO_ESCOLA&#39;: co_escola_renda_media.index,
    &#39;FE_ESCOLA_RENDA_MEDIA&#39;: co_escola_renda_media,
    &#39;FE_IDADE_MEDIA&#39;: co_escola_idade_media,
    &#39;FE_NOTA_LC&#39;: co_escola_nota_lc
}).reset_index(drop=True)

# Concatenar estatisticas nas bases de treino e teste
X = pd.merge(X, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)
X_test = pd.merge(X_test, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)

# Codigo da escola para categorico
X.loc[:, &#39;CO_ESCOLA&#39;] = X.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)
X_test.loc[:, &#39;CO_ESCOLA&#39;] = X_test.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)

# Features de contagem
X.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X_test.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X.NO_MUNICIPIO_RESIDENCIA.map({x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X_test.NO_MUNICIPIO_RESIDENCIA.map({ x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X.NO_MUNICIPIO_NASCIMENTO.map({ x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X_test.NO_MUNICIPIO_NASCIMENTO.map({x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})

X.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X_test.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})</code></pre>
<p>Ajustar modelo:</p>
<pre class="python"><code>%%time

cat_feat = X.columns[X.dtypes==&#39;category&#39;]
cat_indices = [X.columns.get_loc(x) for x in cat_feat]

for c in list(cat_feat):
    X.loc[:, c] = X.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)
    X_test.loc[:, c] = X_test.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)

X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.1, random_state=SEED)

clf = CatBoostRegressor(random_state=314,
                            cat_features=cat_indices,
                            verbose=0,
                            loss_function = &quot;RMSE&quot;,
                            od_type = &quot;Iter&quot;,
                            od_wait = 100,iterations=3000,
                            use_best_model=True)

clf.fit(X, y, eval_set = (X_eval, y_eval), verbose=False, plot=True)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/ch_catboost.png" style="width:95.0%" />
</center>
<p>Salvar previs√µes:</p>
<pre class="python"><code>sub.loc[:, &#39;NU_NOTA_CH&#39;] = clf.predict(X_test)
# alunos que nao foram fazer a prova tiraram zero
sub.loc[test.TP_PRESENCA_CH!=1, &#39;NU_NOTA_CH&#39;] = 0</code></pre>
<p>Comparar distribui√ß√£o da target nos dados de treino com rela√ß√£o √†s previs√µes do modelo:</p>
<pre class="python"><code>sns.kdeplot(train.loc[:, target], shade=True, color=&#39;r&#39;, clip=[0,1000])
sns.kdeplot(sub.loc[:, target], shade=True, color=&#39;b&#39;, clip=[0,1000])
plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
plt.title(target)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/ch_pred.png" style="width:50.0%" />
</center>
</div>
<div id="ci√™ncias-da-natureza" class="section level3">
<h3>Ci√™ncias da Natureza</h3>
<p>Novas features desenvolvidas especificamente para este modelo:</p>
<pre class="python"><code>def fe_cn(df):
    df.loc[:, &#39;FE_RENDA&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000,
    &#39;C&#39;:1500, &#39;D&#39;:2000, &#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, 
    &#39;I&#39;:6000, &#39;J&#39;:7000, &#39;K&#39;:8000,&#39;L&#39;:9000, &#39;M&#39;:10000, &#39;N&#39;:12000, 
    &#39;O&#39;:15000, &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
    df.loc[:, &#39;FE_NU_IDADE*TP_ANO_CONCLUIU&#39;] = df.TP_ANO_CONCLUIU * df.NU_IDADE
    df.loc[:, &#39;FE_Q002+Q024&#39;] = df.loc[:, &#39;Q002&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2,
    &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}).astype(int) + 
    df.loc[:, &#39;Q024&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}).astype(int) 
    df.loc[:, &#39;FE_SCORE&#39;] = (1/df.TP_ANO_CONCLUIU) + np.sqrt(df.NU_IDADE) + np.where(df.TP_ESCOLA==3, 1, 0)
    
    df.loc[:, &#39;FE_UF_ESCOLA&#39;] = df.SG_UF_ESC.map({
      &#39;AM&#39;:&#39;Norte&#39;, &#39;RR&#39;:&#39;Norte&#39;, &#39;AP&#39;:&#39;Norte&#39;, &#39;PA&#39;:&#39;Norte&#39;, &#39;TO&#39;:&#39;Norte&#39;, &#39;RO&#39;:&#39;Norte&#39;, &#39;AC&#39;:&#39;Norte&#39;,
      &#39;MA&#39;:&#39;Nordeste&#39;, &#39;PI&#39;:&#39;Nordeste&#39;, &#39;CE&#39;:&#39;Nordeste&#39;, &#39;RN&#39;:&#39;Nordeste&#39;, &#39;PE&#39;:&#39;Nordeste&#39;, &#39;PB&#39;:&#39;Nordeste&#39;, &#39;SE&#39;:&#39;Nordeste&#39;, &#39;AL&#39;:&#39;Nordeste&#39;, &#39;BA&#39;:&#39;Nordeste&#39;,
      &#39;MT&#39;: &#39;CentroOeste&#39;, &#39;MS&#39;: &#39;CentroOeste&#39;, &#39;GO&#39;: &#39;CentroOeste&#39;,
      &#39;SP&#39;: &#39;Sudeste&#39;, &#39;RJ&#39;: &#39;Sudeste&#39;, &#39;ES&#39;: &#39;Sudeste&#39;, &#39;MG&#39;: &#39;Sudeste&#39;,
      &#39;PR&#39;: &#39;Sul&#39;, &#39;RS&#39;: &#39;Sul&#39;, &#39;SC&#39;: &#39;Sul&#39;}).astype(&#39;category&#39;)
    return df</code></pre>
<p>Definir finalidade de algumas colunas:</p>
<pre class="python"><code># colunas que serao dropadas
to_drop = [&#39;IN_PROVA_DEITADO&#39;,
           &#39;NU_INSCRICAO&#39;,
           &#39;CO_MUNICIPIO_ESC&#39;,
           &#39;CO_UF_NASCIMENTO&#39;,
           &#39;CO_UF_RESIDENCIA&#39;,
           &#39;CO_UF_ESC&#39;,
           &#39;CO_UF_PROVA&#39;,
           &#39;CO_MUNICIPIO_PROVA&#39;,
           &#39;CO_MUNICIPIO_RESIDENCIA&#39;,
          &#39;CO_MUNICIPIO_NASCIMENTO&#39;]

# definir target e presenca
target = &quot;NU_NOTA_CN&quot;
presenca = &quot;TP_PRESENCA_CN&quot;

# demais notas para dropar (menos mt)
notas = list(set(targets)-set([target, &#39;NU_NOTA_MT&#39;]))</code></pre>
<p>Pr√©-processamento nos dados de treino</p>
<pre class="python"><code>X = train.copy()
X = X.drop(to_drop, axis=1) 
X = X[X[presenca]==1]
X = X[~X[target].isnull()]

X = X.loc[:, ~X.columns.isin([target]+[presenca]+notas)]
X = fe_cn(X)
X = prep_data_questionarios(X)
X = fe_mun(X)
X = fe_questionario(X)
X = fe_in(X)
X = prep_co_escola(X)
X = fe_extra(X)

y = train.loc[(train[presenca]==1)&amp;(~train[target].isnull()), target].astype(np.float64)</code></pre>
<p>Pr√©-processamento nos dados de teste</p>
<pre class="python"><code>X_test = test.copy()
X_test = X_test.drop(to_drop, axis=1) 

X_test = X_test.loc[:, ~X_test.columns.isin([presenca])]
X_test = fe_cn(X_test)
X_test = prep_data_questionarios(X_test)
X_test = fe_mun(X_test)
X_test = fe_questionario(X_test)
X_test = fe_in(X_test)
X_test = prep_co_escola(X_test)
X_test = fe_extra(X_test)</code></pre>
<p>Feature engineering separada para evitar data leak:</p>
<pre class="python"><code># calcular estatisticas nos dados de treino
co_escola_renda_media = X.groupby(&#39;CO_ESCOLA&#39;).FE_RENDA.mean()
co_escola_idade_media = X.groupby(&#39;CO_ESCOLA&#39;).NU_IDADE.mean()
co_escola_nota_mt = X.groupby(&#39;CO_ESCOLA&#39;).NU_NOTA_MT.mean()
X = X.drop(&#39;NU_NOTA_MT&#39;, axis=1)

# instanciar objeto com as estatisticas por escola
co_escola_aux = pd.DataFrame({
    &#39;CO_ESCOLA&#39;: co_escola_renda_media.index,
    &#39;FE_ESCOLA_RENDA_MEDIA&#39;: co_escola_renda_media,
    &#39;FE_IDADE_MEDIA&#39;: co_escola_idade_media,
    &#39;FE_NOTA_MT&#39;: co_escola_nota_mt
}).reset_index(drop=True)

# Concatenar estatisticas nas bases de treino e teste
X = pd.merge(X, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)
X_test = pd.merge(X_test, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)

# Codigo da escola para categorico
X.loc[:, &#39;CO_ESCOLA&#39;] = X.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)
X_test.loc[:, &#39;CO_ESCOLA&#39;] = X_test.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)

# Features de contagem
X.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X_test.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X.NO_MUNICIPIO_RESIDENCIA.map({x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X_test.NO_MUNICIPIO_RESIDENCIA.map({ x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X.NO_MUNICIPIO_NASCIMENTO.map({ x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X_test.NO_MUNICIPIO_NASCIMENTO.map({x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})

X.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X_test.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})</code></pre>
<p>Ajustar modelo:</p>
<pre class="python"><code>%%time

cat_feat = X.columns[X.dtypes==&#39;category&#39;]
cat_indices = [X.columns.get_loc(x) for x in cat_feat]

for c in list(cat_feat):
    X.loc[:, c] = X.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)
    X_test.loc[:, c] = X_test.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)

X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.1, random_state=SEED)

clf = CatBoostRegressor(random_state=314,
                            cat_features=cat_indices,
                            verbose=0,
                            loss_function = &quot;RMSE&quot;,
                            od_type = &quot;Iter&quot;,
                            od_wait = 100,iterations=3000,
                            use_best_model=True)

clf.fit(X, y, eval_set = (X_eval, y_eval), verbose=False, plot=True)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/cn_catboost.png" style="width:95.0%" />
</center>
<p>Salvar previs√µes:</p>
<pre class="python"><code>sub.loc[:, &#39;NU_NOTA_CN&#39;] = clf.predict(X_test)
# alunos que nao foram fazer a prova tiraram zero
sub.loc[test.TP_PRESENCA_CN!=1, &#39;NU_NOTA_CN&#39;] = 0</code></pre>
<p>Comparar distribui√ß√£o da target nos dados de treino com rela√ß√£o √†s previs√µes do modelo:</p>
<pre class="python"><code>sns.kdeplot(train.loc[:, target], shade=True, color=&#39;r&#39;, clip=[0,1000])
sns.kdeplot(sub.loc[:, target], shade=True, color=&#39;b&#39;, clip=[0,1000])
plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
plt.title(target)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/cn_pred.png" style="width:50.0%" />
</center>
</div>
<div id="matem√°tica" class="section level3">
<h3>Matem√°tica</h3>
<p>Novas features desenvolvidas especificamente para este modelo:</p>
<pre class="python"><code>def fe_mt(df):
    
    df.loc[:, &#39;FE_RENDA&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000, &#39;C&#39;:1500, &#39;D&#39;:2000, &#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, &#39;I&#39;:6000, &#39;J&#39;:7000, &#39;K&#39;:8000,&#39;L&#39;:9000, &#39;M&#39;:10000, &#39;N&#39;:12000, &#39;O&#39;:15000, &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
    df.loc[:, &#39;FE_NU_IDADE*TP_ANO_CONCLUIU&#39;] = df.TP_ANO_CONCLUIU * df.NU_IDADE
    df.loc[:, &#39;FE_Q002+Q024&#39;] = df.loc[:, &#39;Q002&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}).astype(int) + df.loc[:, &#39;Q024&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}).astype(int) 
    df.loc[:, &#39;FE_SCORE&#39;] = (1/df.TP_ANO_CONCLUIU) + np.sqrt(df.NU_IDADE) + np.where(df.TP_ESCOLA==3, 1, 0)
    
    df.loc[:, &#39;FE_UF_ESCOLA&#39;] = df.SG_UF_ESC.map({&#39;AM&#39;:&#39;Norte&#39;, &#39;RR&#39;:&#39;Norte&#39;, &#39;AP&#39;:&#39;Norte&#39;, &#39;PA&#39;:&#39;Norte&#39;, &#39;TO&#39;:&#39;Norte&#39;, &#39;RO&#39;:&#39;Norte&#39;, &#39;AC&#39;:&#39;Norte&#39;,
                &#39;MA&#39;:&#39;Nordeste&#39;, &#39;PI&#39;:&#39;Nordeste&#39;, &#39;CE&#39;:&#39;Nordeste&#39;, &#39;RN&#39;:&#39;Nordeste&#39;, &#39;PE&#39;:&#39;Nordeste&#39;, &#39;PB&#39;:&#39;Nordeste&#39;, &#39;SE&#39;:&#39;Nordeste&#39;, &#39;AL&#39;:&#39;Nordeste&#39;, &#39;BA&#39;:&#39;Nordeste&#39;,
                &#39;MT&#39;: &#39;CentroOeste&#39;, &#39;MS&#39;: &#39;CentroOeste&#39;, &#39;GO&#39;: &#39;CentroOeste&#39;,
                &#39;SP&#39;: &#39;Sudeste&#39;, &#39;RJ&#39;: &#39;Sudeste&#39;, &#39;ES&#39;: &#39;Sudeste&#39;, &#39;MG&#39;: &#39;Sudeste&#39;,
                &#39;PR&#39;: &#39;Sul&#39;, &#39;RS&#39;: &#39;Sul&#39;, &#39;SC&#39;: &#39;Sul&#39;}).astype(&#39;category&#39;)
    
    
    return df</code></pre>
<p>Definir finalidade de algumas colunas:</p>
<pre class="python"><code># colunas que serao dropadas
to_drop = [&#39;IN_PROVA_DEITADO&#39;,
           &#39;NU_INSCRICAO&#39;,
           &#39;CO_MUNICIPIO_ESC&#39;,
           &#39;CO_UF_NASCIMENTO&#39;,
           &#39;CO_UF_RESIDENCIA&#39;,
           &#39;CO_UF_ESC&#39;,
           &#39;CO_UF_PROVA&#39;,
           &#39;CO_MUNICIPIO_PROVA&#39;,
           &#39;CO_MUNICIPIO_RESIDENCIA&#39;,
          &#39;CO_MUNICIPIO_NASCIMENTO&#39;]

# definir target e presenca
target = &quot;NU_NOTA_MT&quot;
presenca = &quot;TP_PRESENCA_MT&quot;

# demais notas para dropar (menos cn)
notas = list(set(targets)-set([target, &#39;NU_NOTA_CN&#39;]))</code></pre>
<p>Pr√©-processamento nos dados de treino</p>
<pre class="python"><code>X = train.copy()
X = X.drop(to_drop, axis=1) 
X = X[X[presenca]==1]
X = X[~X[target].isnull()]

X = X.loc[:, ~X.columns.isin([target]+[presenca]+notas)]
X = fe_mt(X)
X = prep_data_questionarios(X)
X = fe_mun(X)
#X = fe_questionario(X)
#X = fe_in(X)
X = prep_co_escola(X)
X = fe_extra(X)

y = train.loc[(train[presenca]==1)&amp;(~train[target].isnull()), target].astype(np.float64)</code></pre>
<p>Pr√©-processamento nos dados de teste</p>
<pre class="python"><code>X_test = test.copy()
X_test = X_test.drop(to_drop, axis=1) 

X_test = X_test.loc[:, ~X_test.columns.isin([presenca])]
X_test = fe_mt(X_test)
X_test = prep_data_questionarios(X_test)
X_test = fe_mun(X_test)
#X_test = fe_questionario(X_test)
#X_test = fe_in(X_test)
X_test = prep_co_escola(X_test)
X_test = fe_extra(X_test)</code></pre>
<p>Feature engineering separada para evitar data leak:</p>
<pre class="python"><code># calcular estatisticas nos dados de treino
co_escola_renda_media = X.groupby(&#39;CO_ESCOLA&#39;).FE_RENDA.mean()
co_escola_idade_media = X.groupby(&#39;CO_ESCOLA&#39;).NU_IDADE.mean()
co_escola_nota_cn = X.groupby(&#39;CO_ESCOLA&#39;).NU_NOTA_CN.mean()
X = X.drop(&#39;NU_NOTA_CN&#39;, axis=1)

# instanciar objeto com as estatisticas por escola
co_escola_aux = pd.DataFrame({
    &#39;CO_ESCOLA&#39;: co_escola_renda_media.index,
    &#39;FE_ESCOLA_RENDA_MEDIA&#39;: co_escola_renda_media,
    &#39;FE_IDADE_MEDIA&#39;: co_escola_idade_media,
    &#39;FE_NOTA_CN&#39;: co_escola_nota_cn
}).reset_index(drop=True)

# Concatenar estatisticas nas bases de treino e teste
X = pd.merge(X, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)
X_test = pd.merge(X_test, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)

# Codigo da escola para categorico
X.loc[:, &#39;CO_ESCOLA&#39;] = X.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)
X_test.loc[:, &#39;CO_ESCOLA&#39;] = X_test.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)

# Features de contagem
X.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X_test.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X.NO_MUNICIPIO_RESIDENCIA.map({x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X_test.NO_MUNICIPIO_RESIDENCIA.map({ x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X.NO_MUNICIPIO_NASCIMENTO.map({ x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X_test.NO_MUNICIPIO_NASCIMENTO.map({x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})

X.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X_test.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})</code></pre>
<p>Ajustar modelo:</p>
<pre class="python"><code>%%time

cat_feat = X.columns[X.dtypes==&#39;category&#39;]
cat_indices = [X.columns.get_loc(x) for x in cat_feat]

for c in list(cat_feat):
    X.loc[:, c] = X.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)
    X_test.loc[:, c] = X_test.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)

X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.1, random_state=SEED)
    
clf = CatBoostRegressor(random_state=314,
                            cat_features=cat_indices,
                            verbose=0,
                            loss_function = &quot;RMSE&quot;,
                            od_type = &quot;Iter&quot;,
                            od_wait = 100,iterations=3000,
                            use_best_model=True)

clf.fit(X, y, eval_set = (X_eval, y_eval), verbose=False, plot=True)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/mt_catboost.png" style="width:95.0%" />
</center>
<p>Salvar previs√µes:</p>
<pre class="python"><code>sub.loc[:, &#39;NU_NOTA_MT&#39;] = clf.predict(X_test)
# alunos que nao foram fazer a prova tiraram zero
sub.loc[test.TP_PRESENCA_CN!=1, &#39;NU_NOTA_MT&#39;] = 0</code></pre>
<p>Comparar distribui√ß√£o da target nos dados de treino com rela√ß√£o √†s previs√µes do modelo:</p>
<pre class="python"><code>sns.kdeplot(train.loc[:, target], shade=True, color=&#39;r&#39;, clip=[0,1000])
sns.kdeplot(sub.loc[:, target], shade=True, color=&#39;b&#39;, clip=[0,1000])
plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
plt.title(target)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/mt_pred.png" style="width:50.0%" />
</center>
</div>
<div id="reda√ß√£o" class="section level3">
<h3>Reda√ß√£o</h3>
<p>Novas features desenvolvidas especificamente para este modelo:</p>
<pre class="python"><code>def fe_rd(df):
    
    df.loc[:, &#39;FE_RENDA&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000, &#39;C&#39;:1500, &#39;D&#39;:2000, &#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, &#39;I&#39;:6000, &#39;J&#39;:7000, &#39;K&#39;:8000,&#39;L&#39;:9000, &#39;M&#39;:10000, &#39;N&#39;:12000, &#39;O&#39;:15000, &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
    df.loc[:, &#39;FE_NU_IDADE*TP_ANO_CONCLUIU&#39;] = df.TP_ANO_CONCLUIU * df.NU_IDADE
    df.loc[:, &#39;FE_Q002+Q024&#39;] = df.loc[:, &#39;Q002&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}).astype(int) + df.loc[:, &#39;Q024&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}).astype(int) 
    df.loc[:, &#39;FE_SCORE&#39;] = (1/df.TP_ANO_CONCLUIU) + np.sqrt(df.NU_IDADE) + np.where(df.TP_ESCOLA==3, 1, 0)
    
    df.loc[:, &#39;FE_RENDA_FAMILIA_+_IDADE&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;:5, &#39;G&#39;:6, &#39;H&#39;:7, &#39;I&#39;:8, &#39;J&#39;:9, &#39;K&#39;:10,&#39;L&#39;:11, &#39;M&#39;:12, &#39;N&#39;:13, &#39;O&#39;:14, &#39;P&#39;:15, &#39;Q&#39;:16}).astype(int) + df.NU_IDADE        
    df.loc[:, &#39;FE_RENDA_FAMILIA_+_ANO_CONCLUIU&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;:5, &#39;G&#39;:6, &#39;H&#39;:7, &#39;I&#39;:8, &#39;J&#39;:9, &#39;K&#39;:10,&#39;L&#39;:11, &#39;M&#39;:12, &#39;N&#39;:13, &#39;O&#39;:14, &#39;P&#39;:15, &#39;Q&#39;:16}).astype(int)+ df.TP_ANO_CONCLUIU  
    
    return df</code></pre>
<p>Definir finalidade de algumas colunas:</p>
<pre class="python"><code># colunas que serao dropadas
to_drop = [&#39;IN_PROVA_DEITADO&#39;,
           &#39;NU_INSCRICAO&#39;,
           &#39;CO_MUNICIPIO_ESC&#39;,
           &#39;CO_UF_NASCIMENTO&#39;,
           &#39;CO_UF_RESIDENCIA&#39;,
           &#39;CO_UF_ESC&#39;,
           &#39;CO_UF_PROVA&#39;,
           &#39;CO_MUNICIPIO_PROVA&#39;,
           &#39;CO_MUNICIPIO_RESIDENCIA&#39;,
          &#39;CO_MUNICIPIO_NASCIMENTO&#39;]

# definir target e presenca
target = &quot;NU_NOTA_REDACAO&quot;
presenca = &quot;TP_STATUS_REDACAO&quot;

# demais notas para dropar 
notas = list(set(targets)-set([target]))</code></pre>
<p>Pr√©-processamento nos dados de treino</p>
<pre class="python"><code>X = train.copy()
X = X.drop(to_drop, axis=1) 
X = X[X[presenca]==1]
X = X[~X[target].isnull()]


X = X.loc[:, ~X.columns.isin([target]+[presenca]+notas)]
X = fe_rd(X)
X = prep_data_questionarios(X)
X = fe_mun(X)
#X = fe_questionario(X)
X = fe_in(X)
X = prep_co_escola(X)
X = fe_extra(X)

y = train.loc[(train[presenca]==1)&amp;(~train[target].isnull()), target].astype(np.float64)</code></pre>
<p>Pr√©-processamento nos dados de teste</p>
<pre class="python"><code>X_test = test.copy()
X_test = X_test.drop(to_drop, axis=1) 

X_test = X_test.loc[:, ~X_test.columns.isin([presenca])]
X_test = fe_rd(X_test)
X_test = prep_data_questionarios(X_test)
X_test = fe_mun(X_test)
#X_test = fe_questionario(X_test)
X_test = fe_in(X_test)
X_test = prep_co_escola(X_test)
X_test = fe_extra(X_test)</code></pre>
<p>Feature engineering separada para evitar data leak:</p>
<pre class="python"><code># calcular estatisticas nos dados de treino
co_escola_renda_media = X.groupby(&#39;CO_ESCOLA&#39;).FE_RENDA.mean()
co_escola_idade_media = X.groupby(&#39;CO_ESCOLA&#39;).NU_IDADE.mean()

# instanciar objeto com as estatisticas por escola
co_escola_aux = pd.DataFrame({
    &#39;CO_ESCOLA&#39;: co_escola_renda_media.index,
    &#39;FE_ESCOLA_RENDA_MEDIA&#39;: co_escola_renda_media,
    &#39;FE_IDADE_MEDIA&#39;: co_escola_idade_media
}).reset_index(drop=True)

# Concatenar estatisticas nas bases de treino e teste
X = pd.merge(X, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)
X_test = pd.merge(X_test, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)

# Codigo da escola para categorico
X.loc[:, &#39;CO_ESCOLA&#39;] = X.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)
X_test.loc[:, &#39;CO_ESCOLA&#39;] = X_test.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)

# Features de contagem
X.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X_test.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X.NO_MUNICIPIO_RESIDENCIA.map({x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X_test.NO_MUNICIPIO_RESIDENCIA.map({ x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X.NO_MUNICIPIO_NASCIMENTO.map({ x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X_test.NO_MUNICIPIO_NASCIMENTO.map({x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})

X.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X_test.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})</code></pre>
<p>Ajustar modelo:</p>
<pre class="python"><code>%%time

cat_feat = X.columns[X.dtypes==&#39;category&#39;]
cat_indices = [X.columns.get_loc(x) for x in cat_feat]

for c in list(cat_feat):
    X.loc[:, c] = X.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)
    X_test.loc[:, c] = X_test.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)

X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.1, random_state=SEED)

clf = CatBoostRegressor(random_state=314,
                            cat_features=cat_indices,
                            verbose=0,
                            loss_function = &quot;RMSE&quot;,
                            od_type = &quot;Iter&quot;,
                            od_wait = 100,iterations=3000,
                            use_best_model=True)

clf.fit(X, y, eval_set = (X_eval, y_eval), verbose=False, plot=True)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/redacao_catboost.png" style="width:95.0%" />
</center>
<p>Salvar previs√µes:</p>
<pre class="python"><code>sub.loc[:, &#39;NU_NOTA_REDACAO&#39;] = clf.predict(X_test)
# alunos que nao foram fazer a prova tiraram zero
sub.loc[test.TP_STATUS_REDACAO!=1, &#39;NU_NOTA_REDACAO&#39;] = 0</code></pre>
<p>Comparar distribui√ß√£o da target nos dados de treino com rela√ß√£o √†s previs√µes do modelo:</p>
<pre class="python"><code>sns.kdeplot(train.loc[:, target], shade=True, color=&#39;r&#39;, clip=[0,1000])
sns.kdeplot(sub.loc[:, target], shade=True, color=&#39;b&#39;, clip=[0,1000])
plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
plt.title(target)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/redacao_pred.png" style="width:50.0%" />
</center>
</div>
</div>
</div>
<div id="submiss√£o" class="section level1">
<h1>Submiss√£o</h1>
<p>Veja a seguir como ficou a distribui√ß√£o das previs√µes comparada √† distribui√ß√£o da target nos dados de treino:</p>
<pre class="python"><code>plt.figure(figsize=(16, 5))

notas = [&#39;NU_NOTA_CH&#39;, &#39;NU_NOTA_CN&#39;, &#39;NU_NOTA_MT&#39;, &#39;NU_NOTA_LC&#39;, &#39;NU_NOTA_REDACAO&#39;]

for i in range(len(notas)):

    plt.subplot(1, 5, i+1)
    sns.kdeplot(train.loc[:, notas[i]], shade=True, color=&#39;r&#39;, clip=[0,1000])
    sns.kdeplot(sub.loc[:, notas[i]], shade=True, color=&#39;b&#39;, clip=[0,1000])
    plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
    plt.title(notas[i])
plt.tight_layout()
plt.show()</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/all_pred.png" style="width:95.0%" />
</center>
<p>Acredito que talvez um tuning do modelo poderia trazer mais qualidade √†s previs√µes mas com o tempo limitado n√£o pude investir muito nesta etapa.</p>
</div>
<div id="considera√ß√µes-finais" class="section level1">
<h1>Considera√ß√µes Finais</h1>
<p>Em resumo, essas foram as principais id√©ias para a solu√ß√£o da competi√ß√£o e acredito que um dos segredos era focar em feature engineering por 2 motivos:</p>
<ul>
<li>A base era muito grande e o processo de tuning seria muito custoso (a n√£o ser que tenha um √≥timo computador a disposi√ß√£o);</li>
<li>Os atributos n√£o eram an√¥nimos, o que d√° muita informa√ß√£o de contexto.</li>
</ul>
<p>Agrade√ßo aos organizadores e √† todos os participantes que tornaram esta competi√ß√£o t√£o divertida! Por mais competi√ß√µes como esta, que valorizam a comunidade brasileira de Data Science!</p>
<p>Espero que tenham gostado e at√© logo!</p>
<p>Abra√ßos!</p>
<p>Fellipe Gomes</p>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2022-04-20-solucao-final-education-quality-kaggle-competition/">Solu√ß√£o Final - ML Olympiad [2¬∫ lugar]</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">catboost</category>
      <category domain="tag">data-mining</category>
      <category domain="tag">data-science</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">machine-learning</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">pratica</category>
      <category domain="tag">r</category>
      <category domain="tag">regressao</category>
    </item>
    <item>
      <title>Solu√ß√£o Final - Porto Seguro Data Challenge [3¬∫ lugar]</title>
      <link>https://gomesfellipe.github.io/post/2021-11-01-solucao-final-porto-seguro-data-challenge/</link>
      <pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2021-11-01-solucao-final-porto-seguro-data-challenge/</guid>
      <description>Confira a estrat√©gia aplicada para a competi√ß√£o de machine learning do Porto Seguro hospedada no Kaggle</description>
      <content:encoded>&lt;![CDATA[
        

<div id="TOC">
<ul>
<li><a href="#introdu%C3%A7%C3%A3o" id="toc-introdu√ß√£o">Introdu√ß√£o</a></li>
<li><a href="#defini%C3%A7%C3%A3o-do-problema-de-neg%C3%B3cio" id="toc-defini√ß√£o-do-problema-de-neg√≥cio">Defini√ß√£o do problema de neg√≥cio</a></li>
<li><a href="#an%C3%A1lise-explorat%C3%B3ria-em-r" id="toc-an√°lise-explorat√≥ria-em-r">An√°lise Explorat√≥ria (em R)</a></li>
<li><a href="#machine-learning-em-python" id="toc-machine-learning-em-python">Machine Learning (em Python)</a>
<ul>
<li><a href="#importar-depend%C3%AAncias" id="toc-importar-depend√™ncias">Importar depend√™ncias</a></li>
<li><a href="#stage-0-feature-extraction-com-knn" id="toc-stage-0-feature-extraction-com-knn">Stage 0: Feature Extraction com KNN</a></li>
<li><a href="#stage-1-tuning-xgboost-com-optuna" id="toc-stage-1-tuning-xgboost-com-optuna">Stage 1: Tuning XGBoost com Optuna</a></li>
<li><a href="#stage-2-calcular-out-of-fold-shap-values" id="toc-stage-2-calcular-out-of-fold-shap-values">Stage 2: Calcular Out-Of-Fold SHAP values</a></li>
<li><a href="#stage-3-modelo-final-com-autogluon" id="toc-stage-3-modelo-final-com-autogluon">Stage 3: Modelo Final com AutoGluon</a></li>
</ul></li>
<li><a href="#conclus%C3%A3o" id="toc-conclus√£o">Conclus√£o</a></li>
<li><a href="#refer%C3%AAncias" id="toc-refer√™ncias">Refer√™ncias</a></li>
</ul>
</div>

<style>
.column {
float: left;
width: 50%;
padding: 10px;
}

.column4 {
float: left;
width: 20%;
padding: 10px;
}

.column8 {
float: left;
width: 80%;
padding: 10px;
}

.row:after {
content: "";
display: table;
clear: both;
}

.center {
display: flex;
justify-content: center;
align-items: center;
height: 200px;
}
</style>
<hr />
<div id="introdu√ß√£o" class="section level1">
<h1>Introdu√ß√£o</h1>
<div class="row">
<div class="column8">
<p>Em Agosto e 2021 a Porto Seguro lan√ßou um desafio no Kaggle que consistia em estimar a propens√£o de aquisi√ß√£o de novos produtos. Tratava-se de um problema de classifica√ß√£o e foi bem desafiador principalmente por 2 motivos:</p>
<ol style="list-style-type: decimal">
<li>Todas as features da base de ddos eram anonimas;</li>
<li>A m√©trica de avalia√ß√£o foi a F1 Score (sens√≠vel √† um ponto de corte)</li>
</ol>
</div>
<div class="column4">
<div class="float">
<img src="https://media.giphy.com/media/Ie2Hs3A0uJRtK/giphy.gif" alt="Via Giphy" />
<div class="figcaption"><a href="https://media.giphy.com/media/Ie2Hs3A0uJRtK/giphy.gif">Via Giphy</a></div>
</div>
</div>
</div>
<p>Depois de 2 longos meses e dezenas de notebooks desenvolvidos, muitas submiss√µes frustradas e muitas horas a menos de sono, cheguei em uma solu√ß√£o final que envole um <em>blending</em> de modelos e <em>pseudo-labels</em> e quando a competi√ß√£o acabou, percebi que uma solu√ß√£o mais simples de implementar teria um resultado privado ainda maior do que o notebook que selecionei. üòÖ</p>
<div class="w3-panel w3-sand w3-border">
<p>‚ö†Ô∏è Aten√ß√£o!</p>
<p>Neste post abordarei uma solu√ß√£o mais simples e eficiente mas caso tenha interesse em conferir a solu√ß√£o final completa (um grande frankstein), j√° est√° <a href="https://github.com/gomesfellipe/porto_seguro_data_challenge">publica la no github</a>.</p>
</div>
<p>Este notebook √© uma <a href="https://www.kaggle.com/gomes555/3st-place-simplified-solution-0-6967-private">reescritura do meu notebook publicado no Kaggle em linguagem Python</a>. Para quem acompanha meus posts de R pode achar meio estranho este notebook mas convido-o a tentar entender a solu√ß√£o pois foi desenvolvida pela perspectiva de um usu√°rio nativo de R.</p>
<p>Espero que gostem! ü§ò</p>
</div>
<div id="defini√ß√£o-do-problema-de-neg√≥cio" class="section level1">
<h1>Defini√ß√£o do problema de neg√≥cio</h1>
<p>Segundo a descri√ß√£o da competi√ß√£o:</p>
<blockquote>
<p>Voc√™ provavelmente j√° recebeu uma liga√ß√£o de telemarketing oferecendo um produto que voc√™ n√£o precisa. Essa situa√ß√£o de estresse √© minimizada quando voc√™ oferece um produto que o cliente realmente precisa. <br /><br /> Nessa competi√ß√£o voc√™ ser√° desafiado a construir um modelo que prediz a probabilidade de aquisi√ß√£o de um produto.</p>
</blockquote>
<p>Sobre a m√©trica de avalia√ß√£o:</p>
<p>O crit√©rio utilizado para defini√ß√£o da melhor solu√ß√£o ser√° o F1-Score, veja sua formula:</p>
<p><span class="math display">\[
F_1 = 2 \times \frac{precision \times recall}{precision + recall}  
\]</span></p>
<p>Note que tanto a <em>Precision</em> quanto a <em>Recall</em> precisam de um ponto de corte para obter as classes e por isso busquei otmizar as m√©tricas <em>ROC-AUC</em> e <em>Log Loss</em> para obter estimativas de probabilidades com qualidade para finalmente calcular os pontos de corte que maximizam a <em>F1</em>.</p>
</div>
<div id="an√°lise-explorat√≥ria-em-r" class="section level1">
<h1>An√°lise Explorat√≥ria (em R)</h1>
<p>Antes de partir para modelagem fiz uma an√°lise explorat√≥ria utilizando a linguagem R. Neste post tratarei de maneira bem breve e quem tiver interesse em conferir mais detalhes bem como os c√≥digos dos gr√°ficos basta acessar o <a href="https://www.kaggle.com/gomes555/porto-seguro-r-an-lise-explorat-ria-dos-dados">notebook que deixei aberto no Kaggle</a>.</p>
<p>Veja alguns gr√°ficos:</p>
<center>
<img src="/post/2021-11-01-solucao-final-porto-seguro-data-challenge/aed.png" style="width:90.0%" />
</center>
<!-- <div class="w3-panel w3-light-blue w3-border"> -->
<p><strong>üìå Interpreta√ß√£o:</strong> <br></p>
<ul>
<li><strong>Categ√≥ricas</strong>:
<div style="color: rgb(0, 0, 0);">
<ul>
<li>
<strong>Qualitativo nominal</strong>: Possuem muitas classes, poderia ser o nome do produto, regi√£o, um texto o que torna o desafio ainda maior para criar novas features;
</li>
<li>
<strong>Qualitativo ordinal</strong>: Basicamente deixei como veio pois j√° tava como numerico;
</li>
</ul>
</div></li>
<li><strong>Num√©ricas</strong>:
<div style="color: rgb(0, 0, 0);">
<ul>
<li>
<strong>Quantitativo continua</strong>: Todas est√£o normalizadas (0, 1), algumas s√£o bimodais, algumas assim√©tricas a direita (pode ser tempo ate alguma coisa);
</li>
<li>
<strong>Quantitativo discreto</strong>: Sem muito o que fazer, observa√ß√£o apenas a feature <code>var52</code> que parece idade
</li>
</ul>
</div></li>
<li><strong>Dados missing</strong>: Parece haver algum padr√£o na maneira como os dados missing ocorrem e tentei substituir os <code>-999</code> por <code>NaN</code>, imputar a m√©dia, a mediana e via outros modelos
<!-- </div> --></li>
</ul>
<p>N√£o achei que seria muito produtivo ficar adivinhando o que poderia ser cada feature pois praticamente todos as transforma√ß√µes e novas features que gerei n√£o superavam o resultado do modelo ajustado nos dados da maneira que vinham portanto procurei investir mais tempo na modelagem mesmo.</p>
</div>
<div id="machine-learning-em-python" class="section level1">
<h1>Machine Learning (em Python)</h1>
<p>Veja a estrat√©gia de modelagem de maneira visual:</p>
</br>
<center>
<img src="/post/2021-11-01-solucao-final-porto-seguro-data-challenge/final_pipeline.png" style="width:60.0%" />
</center>
<p></br></p>
<div id="importar-depend√™ncias" class="section level2">
<h2>Importar depend√™ncias</h2>
<p>Carregar pacotes do Python</p>
<pre class="python"><code># general packages
import pandas as pd
import numpy as np
import time
# knn features
from gokinjo import knn_kfold_extract
from gokinjo import knn_extract
# ml tools
from sklearn.model_selection import StratifiedKFold, KFold
from sklearn.metrics import f1_score, log_loss, roc_auc_score
# models
from catboost import CatBoostClassifier
from xgboost import XGBClassifier
# optimization
import optuna
# interpretable ml
import shap
# automl
from autogluon.tabular import TabularPredictor
# ignore specific warnings
import warnings
warnings.filterwarnings(&quot;ignore&quot;, message=&quot;ntree_limit is deprecated, use `iteration_range` or model slicing instead.&quot;)</code></pre>
<p>Definir fun√ß√µes auxiliares para calcular o ponto de corte que maximiza a F1:</p>
<pre class="python"><code>def get_threshold(y_true, y_pred):
    thresholds = np.arange(0.0, 1.0, 0.01)
    f1_scores = []
    for thresh in thresholds:
        f1_scores.append(
            f1_score(y_true, [1 if m&gt;thresh else 0 for m in y_pred]))
    f1s = np.array(f1_scores)
    return thresholds[f1s.argmax()]
    
def custom_f1(y_true, y_pred):
    max_f1_threshold =  get_threshold(y_true, y_pred)
    y_pred = np.where(y_pred&gt;max_f1_threshold, 1, 0)
    return f1_score(y_true, y_pred) </code></pre>
<p>Carregar <a href="https://www.kaggle.com/c/porto-seguro-data-challenge/data">dados da competi√ß√£o</a>:</p>
<pre class="python"><code># load data
train = pd.read_csv(&#39;../input/porto-seguro-data-challenge/train.csv&#39;).drop(&#39;id&#39;, axis=1)
test = pd.read_csv(&#39;../input/porto-seguro-data-challenge/test.csv&#39;).drop(&#39;id&#39;, axis=1)
sample_submission = pd.read_csv(&#39;../input/porto-seguro-data-challenge/submission_sample.csv&#39;)
meta = pd.read_csv(&#39;../input/porto-seguro-data-challenge/metadata.csv&#39;)

# get data types
cat_nom = [x for x in meta.iloc[1:-1, :].loc[(meta.iloc[:,1]==&quot;Qualitativo nominal&quot;)].iloc[:,0]] 
cat_ord = [x for x in meta.iloc[1:-1, :].loc[(meta.iloc[:,1]==&quot;Qualitativo ordinal&quot;)].iloc[:,0]] 
num_dis = [x for x in meta.iloc[1:-1, :].loc[(meta.iloc[:,1]==&quot;Quantitativo discreto&quot;)].iloc[:,0]] 
num_con = [x for x in meta.iloc[1:-1, :].loc[(meta.iloc[:,1]==&quot;Quantitativo continua&quot;)].iloc[:,0]] </code></pre>
</div>
<div id="stage-0-feature-extraction-com-knn" class="section level2">
<h2>Stage 0: Feature Extraction com KNN</h2>
<p>Esta t√©cnica gera <span class="math inline">\(k \times c\)</span> novas features, onde <span class="math inline">\(c\)</span> √© o n√∫mero de classes da target. As novas features s√£o calculadas a partir das dist√¢ncias entre as observa√ß√µes e seus k vizinhos mais pr√≥ximos dentro de cada classe;</p>
<p>O valor para os <span class="math inline">\(K\)</span> vizinhos mais pr√≥ximos selecionado foi <span class="math inline">\(K=1\)</span> e para isso utilizei a biblioteca
<a href="https://github.com/momijiame/gokinjo"><code>gokinjo</code></a> que foi <a href="https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335">inspirada nas id√©ias apresentadas na solu√ß√£o vencedora do Otto Group Product Classification Challenge.</a></p>
<pre class="python"><code># convert to numpy because gokinjo expects np arrays
X = train[cat_nom+cat_ord+num_dis+num_con].to_numpy()
y = train.y.to_numpy()
X_test = test[cat_nom+cat_ord+num_dis+num_con].to_numpy()

# extract on train data
KNN_feat_train = knn_kfold_extract(X, y, k=1, normalize=&#39;standard&#39;)
print(&quot;KNN features for training set, shape: &quot;, np.shape(KNN_feat_train))

# extract on test data
KNN_feat_test = knn_extract(X, y, X_test, k=1, normalize=&#39;standard&#39;)
print(&quot;KNN features for test set, shape: &quot;, np.shape(KNN_feat_test))

# convert to dataframe
knn_feat_train = pd.DataFrame(KNN_feat_train, columns=[&quot;knn&quot;+str(x) for x in range(knn_feat_train.shape[1])])
knn_feat_test = pd.DataFrame(KNN_feat_test, columns=[&quot;knn&quot;+str(x) for x in range(knn_feat_test.shape[1])])</code></pre>
<pre><code>## KNN features for training set, shape:  (14123, 2)
## KNN features for test set, shape:  (21183, 2)</code></pre>
</div>
<div id="stage-1-tuning-xgboost-com-optuna" class="section level2">
<h2>Stage 1: Tuning XGBoost com Optuna</h2>
<p>Testei e otimizei muitos modelos como XGBoost, NGBoost, LightGBM, CatBoost, TabNet, HistGradientBoosting e algumas DNNs e em todos os casos (exceto DNNs) utilizei o Optuna para a sele√ß√£o dos hiperpar√¢metros.</p>
<p>Tamb√©m inclui nas tentativas iniciais de otimiza√ß√£o alguns m√©todos de remostrarem como Random Under Sampling, Smote, Tomek, Adasyn dentre outros mas n√£o tive muito sucesso.. apenas a combina√ß√£o Tomek + CatBoost pareceu trazer algum ganho.</p>
<p>Claro que minhas tentativas n√£o foram exautivas e devido ao tempo limitado acabei selecionando o XGBoost que foi o que apresentou as melhores m√©tricas depois de otimizado e tamb√©m o CatBoost com alguns hiperpar√¢metros fixos para serem a base deste pipeline.</p>
<p>Principais Informa√ß√µes üìå :</p>
<ul>
<li>Nenhum pr√©-processamento;</li>
<li>KFold K=10;</li>
<li>Otimiza√ß√£o de hiperpar√¢metros com Optuna;</li>
<li>Loss do XGBoost: Log Loss;</li>
<li>Loss do Otimizador: Log Loss;</li>
<li>Sem resampling;</li>
<li>Previs√£o final com a probabilidade m√©dia de 10 seeds diferentes</li>
</ul>
<pre class="python"><code>X_test = test[cat_nom+cat_ord+num_dis+num_con]
X = train[cat_nom+cat_ord+num_dis+num_con]
y = train.y

K=10
SEED=314
kf = KFold(n_splits=K, random_state=SEED, shuffle=True)</code></pre>
<pre class="python"><code>fixed_params = {
    &#39;random_state&#39;: 9,
    &quot;objective&quot;: &quot;binary:logistic&quot;,
    &quot;eval_metric&quot;: &#39;logloss&#39;,
    &#39;use_label_encoder&#39;:False,
    &#39;n_estimators&#39;:10000,
}

def objective(trial):
    
    hyperparams = {
        &#39;clf&#39;:{
        &quot;booster&quot;: trial.suggest_categorical(&quot;booster&quot;, [&quot;gbtree&quot;]),
        &quot;lambda&quot;: trial.suggest_float(&quot;lambda&quot;, 1e-8, 5.0, log=True),
        &quot;alpha&quot;: trial.suggest_float(&quot;alpha&quot;, 1e-8, 5.0, log=True)
        }
    }
    
    if hyperparams[&#39;clf&#39;][&quot;booster&quot;] == &quot;gbtree&quot; or hyperparams[&#39;clf&#39;][&quot;booster&quot;] == &quot;dart&quot;:
        hyperparams[&#39;clf&#39;][&quot;max_depth&quot;] = trial.suggest_int(&quot;max_depth&quot;, 1, 9)
        hyperparams[&#39;clf&#39;][&quot;eta&quot;] = trial.suggest_float(&quot;eta&quot;, 0.01, 0.1, log=True)
        hyperparams[&#39;clf&#39;][&quot;gamma&quot;] = trial.suggest_float(&quot;gamma&quot;, 1e-8, 1.0, log=True)
        hyperparams[&#39;clf&#39;][&quot;grow_policy&quot;] = trial.suggest_categorical(&quot;grow_policy&quot;, [&quot;depthwise&quot;, &quot;lossguide&quot;])
        hyperparams[&#39;clf&#39;][&#39;min_child_weight&#39;] = trial.suggest_int(&#39;min_child_weight&#39;, 5, 20)
        hyperparams[&#39;clf&#39;][&quot;subsample&quot;] = trial.suggest_float(&quot;subsample&quot;, 0.03, 1)
        hyperparams[&#39;clf&#39;][&quot;colsample_bytree&quot;] = trial.suggest_float(&quot;colsample_bytree&quot;, 0.03, 1)
        hyperparams[&#39;clf&#39;][&#39;max_delta_step&#39;] = trial.suggest_float(&#39;max_delta_step&#39;, 0, 10)
        
    if hyperparams[&#39;clf&#39;][&quot;booster&quot;] == &quot;dart&quot;:
        hyperparams[&#39;clf&#39;][&quot;sample_type&quot;] = trial.suggest_categorical(&quot;sample_type&quot;, [&quot;uniform&quot;, &quot;weighted&quot;])
        hyperparams[&#39;clf&#39;][&quot;normalize_type&quot;] = trial.suggest_categorical(&quot;normalize_type&quot;, [&quot;tree&quot;, &quot;forest&quot;])
        hyperparams[&#39;clf&#39;][&quot;rate_drop&quot;] = trial.suggest_float(&quot;rate_drop&quot;, 1e-8, 1.0, log=True)
        hyperparams[&#39;clf&#39;][&quot;skip_drop&quot;] = trial.suggest_float(&quot;skip_drop&quot;, 1e-8, 1.0, log=True)
    
    params = dict(**fixed_params, **hyperparams[&#39;clf&#39;])
    xgb_oof = np.zeros(X.shape[0])

    for fold, (train_idx, val_idx) in enumerate(kf.split(X=X, y=y)):
        X_train = X.iloc[train_idx]
        y_train = y.iloc[train_idx]
        X_val = X.iloc[val_idx]
        y_val = y.iloc[val_idx]
        
        model = XGBClassifier(**params)
        
        model.fit(X_train, y_train,
                  eval_set=[(X_val, y_val)],
                  early_stopping_rounds=150,
                  verbose=False)
    
        xgb_oof[val_idx] = model.predict_proba(X_val)[:,1]

        del model

    return log_loss(y, xgb_oof)</code></pre>
<p>Como no Kaggle existe o limite de aproximadamente 8h para executar um notebook, coloquei um limite de 7.5 horas para a busca de hiperpar√¢metros:</p>
<pre class="python"><code>study_xgb = optuna.create_study(direction=&#39;minimize&#39;)

study_xgb.optimize(objective, 
               timeout=60*60*7.5, 
               gc_after_trial=True)</code></pre>
<p>Resultados da busca:</p>
<pre class="python"><code>print(&#39;-&gt; Number of finished trials: &#39;, len(study_xgb.trials))
print(&#39;-&gt; Best trial:&#39;)
trial = study_xgb.best_trial
print(&#39;\tValue: {}&#39;.format(trial.value))
print(&#39;-&gt; Params: &#39;)
trial.params</code></pre>
<pre><code>## -&gt; Number of finished trials:  197
## -&gt; Best trial:
## 	Value: 0.3028443879614926
## -&gt; Params: 
## {&#39;booster&#39;: &#39;gbtree&#39;,
##  &#39;lambda&#39;: 9.012384508756378e-07,
##  &#39;alpha&#39;: 0.7472040331088792,
##  &#39;max_depth&#39;: 5,
##  &#39;eta&#39;: 0.01507605562231303,
##  &#39;gamma&#39;: 1.0214961302342215e-08,
##  &#39;grow_policy&#39;: &#39;lossguide&#39;,
##  &#39;min_child_weight&#39;: 5,
##  &#39;subsample&#39;: 0.9331005225916879,
##  &#39;colsample_bytree&#39;: 0.25392142363325004,
##  &#39;max_delta_step&#39;: 5.685109389498008}</code></pre>
<p>Acompanhar o hist√≥rico de cada etapa da otimiza√ß√£o:</p>
<pre class="python"><code>plot_optimization_history(study_xgb)</code></pre>
<center>
<img src="/post/2021-11-01-solucao-final-porto-seguro-data-challenge/optimization_hist.png" style="width:90.0%" />
</center>
<p>Avaliar as combina√ß√µes de hiperpar√¢metros mais bem sucedidas:</p>
<pre class="python"><code>optuna.visualization.plot_parallel_coordinate(study_xgb)</code></pre>
<center>
<img src="/post/2021-11-01-solucao-final-porto-seguro-data-challenge/parallel_plot.png" style="width:90.0%" />
</center>
<p>Quais hiperpar√¢metros tiveram mais impacto na modelagem:</p>
<pre class="python"><code>plot_param_importances(study_xgb)</code></pre>
<center>
<img src="/post/2021-11-01-solucao-final-porto-seguro-data-challenge/param_imp.png" style="width:90.0%" />
</center>
<p>Ap√≥s as 7.5 horas de busca, a melhor combina√ß√£o encontrada para o XGBoost foi a seguinte:</p>
<pre class="python"><code># After 7.5 hours...
study_xgb = {&#39;booster&#39;: &#39;gbtree&#39;,
 &#39;lambda&#39;: 9.012384508756378e-07,
 &#39;alpha&#39;: 0.7472040331088792,
 &#39;max_depth&#39;: 5,
 &#39;eta&#39;: 0.01507605562231303,
 &#39;gamma&#39;: 1.0214961302342215e-08,
 &#39;grow_policy&#39;: &#39;lossguide&#39;,
 &#39;min_child_weight&#39;: 5,
 &#39;subsample&#39;: 0.9331005225916879,
 &#39;colsample_bytree&#39;: 0.25392142363325004,
 &#39;max_delta_step&#39;: 5.685109389498008}</code></pre>
<p>Preparar lista de hiperpar√¢metros do XGBoost:</p>
<pre class="python"><code>final_params_xgb = dict()
final_params_xgb[&#39;clf&#39;]=dict(**fixed_params, **study_xgb)</code></pre>
</div>
<div id="stage-2-calcular-out-of-fold-shap-values" class="section level2">
<h2>Stage 2: Calcular Out-Of-Fold SHAP values</h2>
<p>Ap√≥s obter a melhor combina√ß√£o de hiperpar√¢metros para o XGBoost e encontrar resultados formid√°veis com o CatBoost modificando apenas alguns hiperpar√¢metros, resolvi tentar utilizar a informa√ß√£o adquirida pelo <em>SHAP values</em> desses modelos como entrada para novos modelos.</p>
<p>Algumas vantagens de se usar o shap values como um m√©todo de encoder dos dados, <a href="https://www.kaggle.com/pavelvod/gbm-supervised-pretraining">segundo este notebook publicado no Kaggle</a> (muito interessante por sinal):</p>
<ul>
<li>Normaliza os dados;</li>
<li>Mais ou menos Linearizado pois as <em>features</em> s√£o transformadas em suas import√¢ncias;</li>
<li>Recursos categ√≥ricos codificados de maneira mais inteligente (A codifica√ß√£o n√£o √© linear e depende de outros recursos da amostra);</li>
<li>Tratamento mais inteligente para valores <em>missing</em>.</li>
</ul>
<p>Para evitar <em>data leak</em>, o <em>SHAP values</em> foi calculado em cima dos dados <em>out-of-fold</em> para os dados de treino e a m√©dia da previs√£o de todos os <em>fold</em> nos dados de teste.</p>
<p>Definir estrat√©gia de valida√ß√£o cruzada:</p>
<pre class="python"><code>X_test = test[cat_nom+cat_ord+num_dis+num_con]
X = train[cat_nom+cat_ord+num_dis+num_con]
y = train.y

K=15 # number of bins with Sturge‚Äôs rule
SEED=123
kf = StratifiedKFold(n_splits=K, random_state=SEED, shuffle=True)</code></pre>
<div id="xgboost" class="section level3">
<h3>XGBoost</h3>
<p>Obter <em>out-of-fold</em> SHAP do modelo XGBoost tunado:</p>
<pre class="python"><code>shap1_oof = np.zeros((X.shape[0], X.shape[1]))
shap1_test = np.zeros((X_test.shape[0], X_test.shape[1]))
model_shap1_oof = np.zeros(X.shape[0])

for fold, (train_idx, val_idx) in enumerate(kf.split(X=X, y=y)):
    print(f&quot;‚ûú FOLD :{fold}&quot;)
    X_train = X.iloc[train_idx]
    y_train = y.iloc[train_idx]
    X_val = X.iloc[val_idx]
    y_val = y.iloc[val_idx]
    
    start = time.time()
    
    model = XGBClassifier(**final_params_xgb[&#39;clf&#39;])
    
    model.fit(X_train, y_train,
              eval_set=[(X_val, y_val)],
              early_stopping_rounds=150,
              verbose=False)
    
    model_shap1_oof[val_idx] += model.predict_proba(X_val)[:,1]
    
    print(&quot;Final F1     :&quot;, custom_f1(y_val, model_shap1_oof[val_idx]))
    print(&quot;Final AUC    :&quot;, roc_auc_score(y_val, model_shap1_oof[val_idx]))
    print(&quot;Final LogLoss:&quot;, log_loss(y_val, model_shap1_oof[val_idx]))

    explainer = shap.TreeExplainer(model)
    shap1_oof[val_idx] = explainer.shap_values(X_val)
    shap1_test += explainer.shap_values(X_test) / K

    print(f&quot;elapsed: {time.time()-start:.2f} sec\n&quot;)
    
shap1_oof = pd.DataFrame(shap1_oof, columns = [x+&quot;_shap1&quot; for x in X.columns])
shap1_test = pd.DataFrame(shap1_test, columns = [x+&quot;_shap1&quot; for x in X_test.columns])

print(&quot;Final F1     :&quot;, custom_f1(y, model_shap1_oof))
print(&quot;Final AUC    :&quot;, roc_auc_score(y, model_shap1_oof))
print(&quot;Final LogLoss:&quot;, log_loss(y, model_shap1_oof))</code></pre>
<pre><code>## ‚ûú FOLD :0
## Final F1     : 0.7032967032967034
## Final AUC    : 0.902330627099664
## Final LogLoss: 0.2953604946129216
## elapsed: 62.58 sec
## 
## ‚ûú FOLD :1
## Final F1     : 0.6193853427895981
## Final AUC    : 0.8613101903695408
## Final LogLoss: 0.34227429854659686
## elapsed: 45.96 sec
## 
## ‚ûú FOLD :2
## Final F1     : 0.6793478260869567
## Final AUC    : 0.8945898656215007
## Final LogLoss: 0.3085819148842589
## elapsed: 58.84 sec
## 
## ‚ûú FOLD :3
## Final F1     : 0.7073791348600509
## Final AUC    : 0.9058020716685331
## Final LogLoss: 0.2881665477053405
## elapsed: 62.24 sec
## 
## ‚ûú FOLD :4
## Final F1     : 0.7239583333333334
## Final AUC    : 0.9053121500559911
## Final LogLoss: 0.29320601468396107
## elapsed: 93.74 sec
## 
## ‚ûú FOLD :5
## Final F1     : 0.7009803921568627
## Final AUC    : 0.9076567749160134
## Final LogLoss: 0.2872539995859452
## elapsed: 73.34 sec
## 
## ‚ûú FOLD :6
## Final F1     : 0.6736292428198434
## Final AUC    : 0.8822788353863381
## Final LogLoss: 0.320014158050091
## elapsed: 55.16 sec
## 
## ‚ûú FOLD :7
## Final F1     : 0.7135416666666666
## Final AUC    : 0.9016657334826428
## Final LogLoss: 0.29617989833438774
## elapsed: 74.49 sec
## 
## ‚ûú FOLD :8
## Final F1     : 0.7135135135135134
## Final AUC    : 0.8893825776158104
## Final LogLoss: 0.29351621553572266
## elapsed: 93.71 sec
## 
## ‚ûú FOLD :9
## Final F1     : 0.7391304347826086
## Final AUC    : 0.9064054944284814
## Final LogLoss: 0.28033187155768635
## elapsed: 95.65 sec
## 
## ‚ûú FOLD :10
## Final F1     : 0.684863523573201
## Final AUC    : 0.9031046324199313
## Final LogLoss: 0.29823173886367804
## elapsed: 64.70 sec
## 
## ‚ûú FOLD :11
## Final F1     : 0.704225352112676
## Final AUC    : 0.8882052000840984
## Final LogLoss: 0.30525241732057884
## elapsed: 50.06 sec
## 
## ‚ûú FOLD :12
## Final F1     : 0.6666666666666666
## Final AUC    : 0.8905529469479291
## Final LogLoss: 0.313654842143217
## elapsed: 78.45 sec
## 
## ‚ûú FOLD :13
## Final F1     : 0.6500000000000001
## Final AUC    : 0.8745111780783517
## Final LogLoss: 0.3300786509821235
## elapsed: 59.54 sec
## 
## ‚ûú FOLD :14
## Final F1     : 0.7135416666666666
## Final AUC    : 0.9063284042329526
## Final LogLoss: 0.29314716930177404
## elapsed: 70.28 sec
## 
## Final F1     : 0.6822461331540014
## Final AUC    : 0.8945288307257988
## Final LogLoss: 0.30301717097927483</code></pre>
</div>
<div id="catboost" class="section level3">
<h3>CatBoost</h3>
<p>Obter <em>out-of-fold</em> SHAP do modelo CatBoost + features extrat√≠das via KNN:</p>
<pre class="python"><code>X = pd.concat([X, knn_feat_train], axis=1)
X_test = pd.concat([X_test, knn_feat_test], axis=1)</code></pre>
<pre class="python"><code>shap2_oof = np.zeros((X.shape[0], X.shape[1]))
shap2_test = np.zeros((X_test.shape[0], X_test.shape[1]))
model_shap2_oof = np.zeros(X.shape[0])

for fold, (train_idx, val_idx) in enumerate(kf.split(X=X, y=y)):
    print(f&quot;‚ûú FOLD :{fold}&quot;)
    X_train = X.iloc[train_idx]
    y_train = y.iloc[train_idx]
    X_val = X.iloc[val_idx]
    y_val = y.iloc[val_idx]
    
    start = time.time()
    
    model = CatBoostClassifier(random_seed=SEED,
                               verbose = 0,
                               n_estimators=10000,
                               loss_function= &#39;Logloss&#39;,
                               use_best_model=True,
                               eval_metric= &#39;Logloss&#39;)
    
    model.fit(X_train, y_train, 
              eval_set = [(X_val,y_val)], 
              early_stopping_rounds = 100,
              verbose = False)
    
    model_shap2_oof[val_idx] += model.predict_proba(X_val)[:,1]
    
    print(&quot;Final F1     :&quot;, custom_f1(y_val, model_shap2_oof[val_idx]))
    print(&quot;Final AUC    :&quot;, roc_auc_score(y_val, model_shap2_oof[val_idx]))
    print(&quot;Final LogLoss:&quot;, log_loss(y_val, model_shap2_oof[val_idx]))

    explainer = shap.TreeExplainer(model)
    shap2_oof[val_idx] = explainer.shap_values(X_val)
    shap2_test += explainer.shap_values(X_test) / K

    print(f&quot;elapsed: {time.time()-start:.2f} sec\n&quot;)
    
shap2_oof = pd.DataFrame(shap2_oof, columns = [x+&quot;_shap&quot; for x in X.columns])
shap2_test = pd.DataFrame(shap2_test, columns = [x+&quot;_shap&quot; for x in X_test.columns])

print(&quot;Final F1     :&quot;, custom_f1(y, model_shap2_oof))
print(&quot;Final AUC    :&quot;, roc_auc_score(y, model_shap2_oof))
print(&quot;Final LogLoss:&quot;, log_loss(y, model_shap2_oof))</code></pre>
<pre><code>## ‚ûú FOLD :0
## Final F1     : 0.6972010178117048
## Final AUC    : 0.8954157334826428
## Final LogLoss: 0.29952314366911725
## elapsed: 22.84 sec
## 
## ‚ûú FOLD :1
## Final F1     : 0.6348448687350835
## Final AUC    : 0.8628429451287795
## Final LogLoss: 0.3407490151943705
## elapsed: 12.59 sec
## 
## ‚ûú FOLD :2
## Final F1     : 0.6809651474530831
## Final AUC    : 0.8949538073908175
## Final LogLoss: 0.3066089330852162
## elapsed: 18.03 sec
## 
## ‚ûú FOLD :3
## Final F1     : 0.702247191011236
## Final AUC    : 0.9107992721164613
## Final LogLoss: 0.2877216893570601
## elapsed: 15.66 sec
## 
## ‚ûú FOLD :4
## Final F1     : 0.7131367292225201
## Final AUC    : 0.9018687010078387
## Final LogLoss: 0.2976481761596595
## elapsed: 29.35 sec
## 
## ‚ûú FOLD :5
## Final F1     : 0.7055837563451777
## Final AUC    : 0.909231522956327
## Final LogLoss: 0.28834373773423566
## elapsed: 15.35 sec
## 
## ‚ûú FOLD :6
## Final F1     : 0.6631578947368421
## Final AUC    : 0.8796402575587906
## Final LogLoss: 0.32303153676573987
## elapsed: 19.13 sec
## 
## ‚ûú FOLD :7
## Final F1     : 0.6997389033942559
## Final AUC    : 0.901637737961926
## Final LogLoss: 0.2985978485411335
## elapsed: 23.30 sec
## 
## ‚ûú FOLD :8
## Final F1     : 0.6965699208443271
## Final AUC    : 0.8825565912117177
## Final LogLoss: 0.3009859242847037
## elapsed: 20.19 sec
## 
## ‚ûú FOLD :9
## Final F1     : 0.7435897435897436
## Final AUC    : 0.9042469689536757
## Final LogLoss: 0.28276851015512977
## elapsed: 24.39 sec
## 
## ‚ûú FOLD :10
## Final F1     : 0.6767676767676767
## Final AUC    : 0.902712173242694
## Final LogLoss: 0.29999812838692497
## elapsed: 16.14 sec
## 
## ‚ûú FOLD :11
## Final F1     : 0.7013698630136986
## Final AUC    : 0.8865022075828719
## Final LogLoss: 0.3081393413008847
## elapsed: 13.50 sec
## 
## ‚ûú FOLD :12
## Final F1     : 0.6630434782608696
## Final AUC    : 0.8920456934613498
## Final LogLoss: 0.31338640296724246
## elapsed: 24.48 sec
## 
## ‚ûú FOLD :13
## Final F1     : 0.6485148514851485
## Final AUC    : 0.8689887167986544
## Final LogLoss: 0.3369797070301582
## elapsed: 17.17 sec
## 
## ‚ûú FOLD :14
## Final F1     : 0.7108753315649867
## Final AUC    : 0.8994743850304856
## Final LogLoss: 0.301420230674656
## elapsed: 16.51 sec
## 
## Final F1     : 0.6823234134098244
## Final AUC    : 0.892656043550729
## Final LogLoss: 0.305726567456891</code></pre>
<pre class="python"><code>train = pd.concat([train, shap1_oof], axis=1)
test = pd.concat([test, shap1_test], axis=1)

train = pd.concat([train, shap2_oof], axis=1)
test = pd.concat([test, shap2_test], axis=1)</code></pre>
</div>
</div>
<div id="stage-3-modelo-final-com-autogluon" class="section level2">
<h2>Stage 3: Modelo Final com AutoGluon</h2>
<p>AutoGluon √© um <a href="https://github.com/awslabs/autogluon">AutoML desenvolvido pela Amazon</a> muito f√°cil de utilizar (no melhor estilo <code>sklearn</code> com m√©todos <code>.fit()</code> e <code>.predict()</code>).</p>
<p>Principais Informa√ß√µes üìå :</p>
<ul>
<li>Inputs: Dataset original + knn features + Shapt values do XGBoost tunado e do CatBoost;</li>
<li>Loss do XGBoost: Log Loss;</li>
<li>Loss do CatBoost: AUC;</li>
<li>Loss do AutoGluon: Log Loss;</li>
<li>Tempo de processamento: 7h30m</li>
</ul>
<div class="w3-panel w3-pale-green w3-border">
<p><strong>üí° Insight</strong> <br></p>
<p>Um recurso muito √∫til do AutoGluon √© poder acessar as previs√µes out-of-folds, o que facilita no c√°lculo do <em>threshold</em> que maximiza a <em>F1 Score</em>.</p>
</div>
<pre class="python"><code>predictor = TabularPredictor(label=&quot;y&quot;,
                             problem_type=&#39;binary&#39;,
                             eval_metric=&quot;log_loss&quot;,
                             path=&#39;./AutoGlon/&#39;,
                             verbosity=1)

predictor.fit(train, presets=&#39;best_quality&#39;, time_limit=60*60*7.5) 

results = predictor.fit_summary()</code></pre>
<pre><code>## *** Summary of fit() ***
## Estimated performance of each model:
##                       model  score_val  pred_time_val      fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order
## 0       WeightedEnsemble_L2  -0.299310      30.410467   8888.826963                0.001654           2.456810            2       True         14
## 1           CatBoost_BAG_L1  -0.301038       3.051793   2376.887100                3.051793        2376.887100            1       True          7
## 2       WeightedEnsemble_L3  -0.301722     194.034947  22907.669139                0.001541           2.008858            3       True         26
## 3         LightGBMXT_BAG_L2  -0.302135     131.534432  17201.299530                1.378576         389.400378            2       True         15
## 4         LightGBMXT_BAG_L1  -0.302562       3.570399    969.385833                3.570399         969.385833            1       True          3
## 5           CatBoost_BAG_L2  -0.302646     131.912474  17619.939451                1.756617         808.040299            2       True         19
## 6           LightGBM_BAG_L2  -0.303002     131.422007  17281.518763                1.266150         469.619612            2       True         16
## 7           LightGBM_BAG_L1  -0.303264       2.964433   1038.037160                2.964433        1038.037160            1       True          4
## 8            XGBoost_BAG_L1  -0.303471       4.475003   2036.551052                4.475003        2036.551052            1       True         11
## 9    NeuralNetFastAI_BAG_L1  -0.304455      19.917584   3434.894841               19.917584        3434.894841            1       True         10
## 10           XGBoost_BAG_L2  -0.304499     132.757505  17834.135370                2.601648        1022.236218            2       True         23
## 11   NeuralNetFastAI_BAG_L2  -0.306339     142.018741  18777.287244               11.862885        1965.388093            2       True         22
## 12     LightGBMLarge_BAG_L2  -0.306606     131.701429  18260.504603                1.545573        1448.605452            2       True         25
## 13    NeuralNetMXNet_BAG_L2  -0.308237     177.769179  19273.211899               47.613322        2461.312748            2       True         24
## 14     LightGBMLarge_BAG_L1  -0.309686       3.042399   2629.185346                3.042399        2629.185346            1       True         13
## 15    ExtraTreesEntr_BAG_L2  -0.314045     132.017535  16815.886061                1.861679           3.986910            2       True         21
## 16  RandomForestEntr_BAG_L2  -0.314454     132.061970  16843.769642                1.906114          31.870490            2       True         18
## 17    ExtraTreesGini_BAG_L2  -0.314960     132.123651  16816.087081                1.967794           4.187930            2       True         20
## 18    NeuralNetMXNet_BAG_L1  -0.317156      81.677096   4258.886806               81.677096        4258.886806            1       True         12
## 19  RandomForestGini_BAG_L2  -0.321702     132.035970  16835.326491                1.880114          23.427339            2       True         17
## 20    ExtraTreesEntr_BAG_L1  -0.323283       1.794093      4.051307                1.794093           4.051307            1       True          9
## 21  RandomForestEntr_BAG_L1  -0.324296       1.966043     33.380685                1.966043          33.380685            1       True          6
## 22    ExtraTreesGini_BAG_L1  -0.325897       1.796291      3.748723                1.796291           3.748723            1       True          8
## 23  RandomForestGini_BAG_L1  -0.328218       1.778995     22.705248                1.778995          22.705248            1       True          5
## 24    KNeighborsDist_BAG_L1  -1.070156       2.010938      2.075571                2.010938           2.075571            1       True          2
## 25    KNeighborsUnif_BAG_L1  -1.071373       2.110790      2.109480                2.110790           2.109480            1       True          1
## Number of models trained: 26
## Types of models trained:
## {&#39;StackerEnsembleModel_RF&#39;, &#39;StackerEnsembleModel_NNFastAiTabular&#39;, &#39;WeightedEnsembleModel&#39;, &#39;StackerEnsembleModel_XGBoost&#39;, &#39;StackerEnsembleModel_CatBoost&#39;, &#39;StackerEnsembleModel_KNN&#39;, &#39;StackerEnsembleModel_LGB&#39;, &#39;StackerEnsembleModel_XT&#39;, &#39;StackerEnsembleModel_TabularNeuralNet&#39;}
## Bagging used: True  (with 10 folds)
## Multi-layer stack-ensembling used: True  (with 3 levels)
## Feature Metadata (Processed):
## (raw dtype, special dtypes):
## (&#39;float&#39;, [])     : 152 | [&#39;var55&#39;, &#39;var56&#39;, &#39;var57&#39;, &#39;var58&#39;, &#39;var59&#39;, ...]
## (&#39;int&#39;, [])       :  48 | [&#39;var1&#39;, &#39;var2&#39;, &#39;var3&#39;, &#39;var4&#39;, &#39;var5&#39;, ...]
## (&#39;int&#39;, [&#39;bool&#39;]) :   6 | [&#39;var27&#39;, &#39;var31&#39;, &#39;var44&#39;, &#39;var49&#39;, &#39;var50&#39;, ...]
## Plot summary of models saved to file: ./AutoGlon/SummaryOfModels.html
## *** End of fit() summary ***</code></pre>
<p>Nota: Os resultados podem variar devido √† natureza estoc√°stica do algoritmo ou procedimento de avalia√ß√£o.</p>
<pre class="python"><code># get final predictions
y_oof = predictor.get_oof_pred_proba().iloc[:,1]
y_pred = predictor.predict_proba(test).iloc[:,1]</code></pre>
<pre class="python"><code>final_threshold = get_threshold(train.y, y_oof)
final_threshold</code></pre>
<pre><code>## 0.31</code></pre>
<pre class="python"><code>print(&quot;Final F1     :&quot;, custom_f1(y, y_oof))
print(&quot;Final AUC    :&quot;, roc_auc_score(y, y_oof))
print(&quot;Final LogLoss:&quot;, log_loss(y, y_oof))</code></pre>
<pre><code>## Final F1     : 0.6846193682030037
## Final AUC    : 0.8961328807692966
## Final LogLoss: 0.2993098559321765</code></pre>
<p>Ap√≥s submiss√£o:</p>
<center>
<img src="/post/2021-11-01-solucao-final-porto-seguro-data-challenge/final_sub.png" style="width:90.0%" />
</center>
</div>
</div>
<div id="conclus√£o" class="section level1">
<h1>Conclus√£o</h1>
<p>Gostaria de agradecer imensamente ao time do Porto Seguro pela iniciativa, pois esse tipo de competi√ß√£o (t√£o detalhada e desafiadora) n√£o tem sido muito comum no Brasil e √© muito importante para fomentar a comunidade brasileira de ci√™ncia de dados!</p>
<p>Sabemos que o ‚Äúmundo real‚Äù √© diferente do mundo das competi√ß√µes (onde buscamos o melhor score a todo custo) por√©m, na minha vis√£o, n√£o deixa de ser um √≥timo exerc√≠cio para treinar o racioc√≠nio anal√≠tico.. al√©m de ser muito empolgante e divertido!</p>
<p>Tive o enorme prazer de trocar id√©ias e conhecer pessoas fora da curva bem como me tornar f√£ de alguns competidores! A cada semana q passava o n√≠vel estava cada vez mais alto!</p>
<p>Com certeza este pipeline poderia ser muito melhor, sinto que poderia ter gasto mais tempo com <em>feature engineering</em> e tido mais paciencia com alguns modelos. Tentei fazer o melhor que pude com o tempo dispon√≠vel e me sinto muito grato pela experi√™ncia de apresentar os resultados e aprender bastante com a solu√ß√£o dos top colocados.</p>
<p>N√£o acaba por aqui! Agora √© hora de voltar aos estudos, continuar praticando com as <a href="https://www.kaggle.com/c/tabular-playground-series-nov-2021/overview">TPS‚Äôs do Kaggle</a> e, quem sabe, ir melhor na pr√≥xima!</p>
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias</h1>
<ul>
<li><a href="https://github.com/momijiame/gokinjo" class="uri">https://github.com/momijiame/gokinjo</a></li>
<li><a href="https://www.kaggle.com/melanie7744/tps6-boost-your-score-with-knn-features" class="uri">https://www.kaggle.com/melanie7744/tps6-boost-your-score-with-knn-features</a></li>
<li><a href="https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335" class="uri">https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335</a></li>
<li><a href="https://www.kaggle.com/pavelvod/gbm-supervised-pretraining" class="uri">https://www.kaggle.com/pavelvod/gbm-supervised-pretraining</a></li>
</ul>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2021-11-01-solucao-final-porto-seguro-data-challenge/">Solu√ß√£o Final - Porto Seguro Data Challenge [3¬∫ lugar]</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">catboost</category>
      <category domain="tag">data-science</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">knn</category>
      <category domain="tag">machine-learning</category>
      <category domain="tag">optuna</category>
      <category domain="tag">pratica</category>
      <category domain="tag">python</category>
      <category domain="tag">shap</category>
      <category domain="tag">threshold-movel</category>
    </item>
    <item>
      <title>Otimizando pipelines que envolvem dados desbalanceados</title>
      <link>https://gomesfellipe.github.io/post/2021-06-28-imbalanced-workflowsets/</link>
      <pubDate>Mon, 28 Jun 2021 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2021-06-28-imbalanced-workflowsets/</guid>
      <description>Utilizaremos o framework tidymodels para machine learning em R com o aux√≠lio do pacote workflowsets para otimizar pipelines de dados desbalanceados</description>
      <content:encoded>&lt;![CDATA[
        

<div id="TOC">
<ul>
<li><a href="#o-problema-envolvendo-dados-desbalanceados" id="toc-o-problema-envolvendo-dados-desbalanceados">O problema envolvendo dados desbalanceados</a></li>
<li><a href="#objetivo" id="toc-objetivo">Objetivo</a></li>
<li><a href="#depend%C3%AAncias" id="toc-depend√™ncias">Depend√™ncias</a></li>
<li><a href="#preparar-dados" id="toc-preparar-dados">Preparar dados</a></li>
<li><a href="#breve-an%C3%A1lise-explorat%C3%B3ria" id="toc-breve-an√°lise-explorat√≥ria">Breve an√°lise explorat√≥ria</a></li>
<li><a href="#modelagem" id="toc-modelagem">Modelagem</a>
<ul>
<li><a href="#baselines" id="toc-baselines">Baselines</a></li>
<li><a href="#preparar-pipeline-de-dados-com-workflowsets" id="toc-preparar-pipeline-de-dados-com-workflowsets">Preparar Pipeline de dados com <code>workflowsets</code></a></li>
<li><a href="#benchmark" id="toc-benchmark">Benchmark</a></li>
</ul></li>
<li><a href="#conclus%C3%A3o" id="toc-conclus√£o">Conclus√£o</a></li>
<li><a href="#refer%C3%AAncias" id="toc-refer√™ncias">Refer√™ncias</a></li>
</ul>
</div>

<style>
.column {
float: left;
width: 50%;
padding: 10px;
}

.column4 {
float: left;
width: 33%;
padding: 10px;
}

.column8 {
float: left;
width: 66%;
padding: 10px;
}

.row:after {
content: "";
display: table;
clear: both;
}

.center {
display: flex;
justify-content: center;
align-items: center;
height: 200px;
}
</style>
<div id="o-problema-envolvendo-dados-desbalanceados" class="section level1">
<h1>O problema envolvendo dados desbalanceados</h1>
<p>A tarefa de classifica√ß√£o com dados desbalanceados √© muito comum na vida real podendo variar desde um leve vi√©s at√© um enorme desequil√≠brio na distribui√ß√£o da classe de interesse. Problemas mais comuns envolvem:</p>
<ul>
<li>Detec√ß√£o de fraude;</li>
<li>Previs√£o de inadimpl√™ncia;</li>
<li>Identificador de <em>spam</em>;</li>
<li>Busca por anomalias/outliers;</li>
<li>Detec√ß√£o de poss√≠veis roubos/furtos/vulnerabilidades;</li>
<li>Previs√£o de <em>churn</em>;</li>
<li>etc</li>
</ul>
<div class="row">
<div class="column8">
<p>Este tipo de tarefa representa um enorme desafio para modelagem preditiva pois a maioria dos algoritmos de machine learning foram projetados sob suposi√ß√£o de haver um n√∫mero igual de exemplos para cada classe de interesse.</p>
<p>E isso √© um grande problema pois normalmente estamos interessados em prever a classe minorit√°ria e para isso √© preciso tomar uma s√©rie de decis√µes, como por exemplo: m√©trica utilizada, m√©todo para valida√ß√£o cruzada, ado√ß√£o (ou n√£o) do uso de m√©todos de reamostragem, quais algoritmos utilizar, qual ser√° o threshold, etc</p>
</div>
<div class="column4">
<p></br>
<img src="https://media.giphy.com/media/JPV8lNtI59zaWyL4pf/giphy.gif" alt="Via Giphy" /></p>
</div>
</div>
<p>Lidar com dados desbalanceados √© um assunto longo portanto tentarei dar mais aten√ß√£o apenas em um <em>hack</em> para encontrar a melhor forma de se aplicar o balanceamento dos dados. N√£o pretendo me aprofundar na teoria envolvida na escolha das m√©tricas neste post, caso o leitor deseje se aprofundar sobre a teoria envolvida com classifica√ß√£o que envolve dados desbalanceados, sugiro a leitura do livro: <a href="https://machinelearningmastery.com/imbalanced-classification-with-python/">Imbalanced Classification with Python - Choose Better Metrics, Balance Skewed Classes and Apply Cost-Sensitive Learning</a> e consultar os links de refer√™ncia no final do post).</p>
</div>
<div id="objetivo" class="section level1">
<h1>Objetivo</h1>
<p>Utilizaremos neste post o pacote <code>workflowsets</code> a fim de otimizar o pipeline de reamostragem da base para lidar com o desbalanceamento dos dados.</p>
<p>Para efeitos de compara√ß√£o, utilizarei como refer√™ncia o (excelente) <a href="https://juliasilge.com/blog/sliced-aircraft/">post escrito recentemente pela Julia Silge</a> em seu blog que tamb√©m aborda o problema de dados desbalanceados utilizando um conjunto de dados de uma <a href="https://www.kaggle.com/c/sliced-s01e02-xunyc5">competi√ß√£o do Kaggle</a>. Utilizarei a mesma configura√ß√£o de pr√©-processamento adotado em seu post para que a compara√ß√£o seja justa.</p>
<p>Portanto, nosso objetivo de modelagem ser√° prever se uma colis√£o com animais selvagens resultou em danos a aeronave.</p>
<div class="w3-panel w3-pale-green w3-border">
<p>‚ö†Ô∏è Este dataset √© rico em possibilidades para diferentes tipos de pr√© processamentos e por isso convido o leitor a analis√°-lo com maior profundidade e tamb√©m a compartilhar seus resultados!</p>
</div>
</div>
<div id="depend√™ncias" class="section level1">
<h1>Depend√™ncias</h1>
<p>Primeiro vamos carregar as bibliotecas necess√°rias e algumas fun√ß√µes desenvolvidas para o post</p>
<pre class="r"><code>library(tidyverse)    # ds toolkit
library(tidymodels)   # ml toolkit
library(baguette)     # bag_tree
library(themis)       # imbalanced
library(workflowsets) # opt pipelines
library(patchwork)    # arrange plots 

doParallel::registerDoParallel()
theme_set(theme_bw())</code></pre>
<details>
<summary>
(<em>Clique aqui para ver as fun√ß√µes</em> <code>print_table</code> <em>e</em> <code>conf_mat_plot</code> <em>importadas</em>)
</summary>
<pre class="r"><code># Para o print de tabelas
print_table &lt;- function(x, round=0, cv=F, wf=F, bm=F, ...){ 
  
  if(round&gt;0) x &lt;- x %&gt;% mutate_if(is.numeric, ~round(.x, round))
  
  if(cv==T){
    columns_spec = list(
      .metric = reactable::colDef(minWidth = 75),
      .estimator = reactable::colDef(minWidth = 70),
      .config = reactable::colDef(minWidth = 120)
    )
  } else if(wf==T){
    columns_spec = list(
      wflow_id = reactable::colDef(minWidth = 100),
      .metric = reactable::colDef(minWidth = 100),
      preprocessor = reactable::colDef(minWidth = 110),
      rank = reactable::colDef(minWidth = 50),
      n = reactable::colDef(minWidth = 50)
    )
  }else if (bm==T){
    columns_spec = list(
      wflow_id = reactable::colDef(minWidth = 130),
      model = reactable::colDef(minWidth = 80)
    )
  }else{
    columns_spec = NULL
  }
  
  reactable::reactable(x, striped = T, bordered = T,
                       highlight = T, pagination = F, resizable = T, 
                       columns = columns_spec, ...)
  
}

# Para plot da matriz de confusao e distribuicoes de probabilidade
conf_mat_plot &lt;- function(x, null_model = FALSE){
  p1 &lt;- 
    x %&gt;%
    select(.pred_class, damaged) %&gt;%
    table() %&gt;% 
    conf_mat() %&gt;% 
    autoplot(type = &quot;heatmap&quot;)+
    labs(title = &quot;Matriz de confus√£o&quot;)
  
  p2 &lt;- 
    x  %&gt;%
    ggplot() +
    geom_density(aes(x = .pred_damage, fill = damaged), 
                 alpha = 0.5)+
    labs(title = &quot;Distribui√ß√µes de probabilidade previstas&quot;,
         subtitle = &quot;por classe&quot;)+ 
    scale_x_continuous(limits = 0:1)+
    scale_fill_brewer(palette=&quot;Set1&quot;)
  
  p1 | p2
} </code></pre>
</details>
<p>¬†</p>
<p>Em seguida vamos importar os dados provenientes da competi√ß√£o Inclass do Kaggle <a href="https://www.kaggle.com/c/sliced-s01e02-xunyc5">SLICED s01e02 - Predict whether an aircraft strike with wildlife causes damage</a>. Para mais informa√ß√µes consulte a <a href="https://www.kaggle.com/c/sliced-s01e02-xunyc5/data">documenta√ß√£o e dicion√°rio dos dados</a>.</p>
<pre class="r"><code>df &lt;- read_csv(&quot;train.csv&quot;)</code></pre>
<p>Note que carregamos apenas os dados de treino pois os dados de teste n√£o possuem a target.</p>
</div>
<div id="preparar-dados" class="section level1">
<h1>Preparar dados</h1>
<p>Tratar a vari√°vel target <code>damaged</code> e avaliar sua distribui√ß√£o:</p>
<pre class="r"><code>df &lt;- df %&gt;% 
  mutate(damaged = if_else(damaged==1, &quot;damage&quot;, &quot;not_damage&quot;) %&gt;% 
           factor(levels = c(&quot;damage&quot;, &quot;not_damage&quot;)))</code></pre>
<details>
<summary>
(<em>Clique aqui para ver o c√≥digo do gr√°fico abaixo</em>)
</summary>
<pre class="r"><code>p1 &lt;- df %&gt;% 
  count(damaged) %&gt;% 
  ggplot(aes(x=rev(damaged), y=n, fill=damaged))+
  geom_bar(stat = &quot;identity&quot;)+
  scale_fill_brewer(palette=&quot;Set1&quot;)+
  theme(legend.position = &quot;bottom&quot;)+
  labs(y=&quot;N√∫mero de inst√¢ncias&quot;, x = &quot;&quot;)

p2 &lt;- df %&gt;% 
  count(damaged) %&gt;% 
  arrange(desc(damaged)) %&gt;%
  mutate(prop = n / sum(n)) %&gt;%
  mutate(ypos = cumsum(prop)- 0.5*prop )%&gt;% 
  ggplot(aes(x=&quot;&quot;, y=prop, fill=damaged)) +
  geom_bar(stat=&quot;identity&quot;, width=1) +
  coord_polar(&quot;y&quot;, start=0) +
  theme_void() + 
  theme(legend.position=&quot;none&quot;) +
  geom_text(aes(y = ypos,
                label = paste(scales::comma(n, big.mark = &quot;.&quot;),
                              scales::comma(n/sum(n), big.mark = &quot;.&quot;, 
                                            suffix = &quot;%&quot; ),sep = &quot;\n&quot;)
                
  ), 
  color = &quot;white&quot;, size=6) +
  scale_fill_brewer(palette=&quot;Set1&quot;)</code></pre>
</details>
<p>¬†</p>
<pre class="r"><code>p1 + p2 </code></pre>
<center>
<img src="/post/2021-06-28-imbalanced-workflowsets/unnamed-chunk-6-1.png" style="width:80.0%" />
</center>
<p>Veja que estamos diante de um problema que existem aproximadamente 9 casos de dano para cada 100 eventos observados.</p>
</div>
<div id="breve-an√°lise-explorat√≥ria" class="section level1">
<h1>Breve an√°lise explorat√≥ria</h1>
<p>Vamos iniciar a explorat√≥ria com uma avalia√ß√£o geral dos dados brutos</p>
<pre class="r"><code>DataExplorer::plot_intro(df, ggtheme = theme_bw(), 
                         theme_config = list(legend.position = &quot;bottom&quot;))</code></pre>
<center>
<img src="/post/2021-06-28-imbalanced-workflowsets/unnamed-chunk-7-1.png" style="width:80.0%" />
</center>
<p>Primeira informa√ß√£o que chama aten√ß√£o √© que quase 1/4 desses dados √© faltante. Vamos olhar a estrutura dessa base de maneira mais aprofundada:</p>
<pre class="r"><code>df %&gt;% 
  sample_frac(0.01) %&gt;% 
  visdat::vis_dat()</code></pre>
<center>
<img src="/post/2021-06-28-imbalanced-workflowsets/unnamed-chunk-8-1.png" style="width:80.0%" />
</center>
<p>Parece existir algum padr√£o nos dados faltantes (que coocorrem em diveros atributos). Al√©m disso algumas colunas est√£o quase inteiramente vazias e ser√£o descartadas no processo de modelagem.</p>
<p>Uma vis√£o geral das classes das features categ√≥ricas:</p>
<pre class="r"><code>df %&gt;%
  select(-damaged, -id)%&gt;%
  mutate_all(as.factor) %&gt;%
  inspectdf::inspect_cat() %&gt;% 
  inspectdf::show_plot()</code></pre>
<center>
<img src="/post/2021-06-28-imbalanced-workflowsets/unnamed-chunk-9-1.png" style="width:80.0%" />
</center>
<p>Algumas features possuem muitas classes e caso seja feita a transforma√ß√£o <em>one-hot-encoding</em> (estrat√©gia amplamente utilizada para lidar com features categ√≥ricas) sem algum cuidado, o desempenho da maioria dos modelos de machine learning pode ser prejudicado por tornar a base anal√≠tica muito esparsa.</p>
<p>Uma vis√£o geral das classes das features num√©ricas em rela√ß√£o a target:</p>
<pre class="r"><code>num_columns &lt;- c(df %&gt;% select_if(is.numeric) %&gt;% colnames(), &#39;damaged&#39;)
df%&gt;% 
  select_at(num_columns) %&gt;% 
  select(-id) %&gt;%
  gather(key, value, -damaged) %&gt;%
  ggplot(aes(y=damaged, x=value))+
  geom_boxplot()+
  facet_wrap(~key, ncol=5, scales = &quot;free_x&quot;)+
  labs(x = &quot;&quot;, y=&quot;&quot;)</code></pre>
<center>
<img src="/post/2021-06-28-imbalanced-workflowsets/unnamed-chunk-10-1.png" style="width:80.0%" />
</center>
<p>Parece que algumas features possuem comportamentos diferentes quando avaliados segundo a target. Al√©m disso √© poss√≠vel notar que as features <code>aircraft_mass</code>, <code>distance</code>, <code>engine4_position</code>, <code>engines</code>, <code>height</code> e <code>speed</code> apresentam outliers.</p>
</div>
<div id="modelagem" class="section level1">
<h1>Modelagem</h1>
<p>Finalmente chegamos a modelagem!</p>
<p>Primeiro vamos definir um esquema de reamostragem (com estratifica√ß√£o) que ser√° utilizado para avaliar os modelos e as m√©tricas de qualidade.</p>
<pre class="r"><code>set.seed(123)

bird_folds &lt;- vfold_cv(df, v = 5, strata = damaged)
bird_metrics &lt;- metric_set(mn_log_loss, accuracy, sensitivity, specificity)</code></pre>
<p>Nossos conjuntos de pipelines necessitar√£o de um pr√©-processador base que ser√° comum a todos como camada inicial. Para isso utilizaremos o mesmo definido no post de refer√™ncia.</p>
<pre class="r"><code>base_rec &lt;- recipe(damaged ~ ., data = df) %&gt;%
  step_select( damaged, flight_impact, precipitation,
               visibility, flight_phase, engines, incident_year,
               incident_month, species_id, engine_type,
               aircraft_model, species_quantity, height, speed) %&gt;% 
  step_novel(all_nominal_predictors()) %&gt;%
  step_other(all_nominal_predictors(), threshold = 0.01) %&gt;%
  step_unknown(all_nominal_predictors()) %&gt;%
  step_impute_median(all_numeric_predictors()) %&gt;%
  step_zv(all_predictors())</code></pre>
<div id="baselines" class="section level2">
<h2>Baselines</h2>
<p>Para efeitos de compara√ß√£o, vamos ajustar 2 modelos que ser√£o utilizados como baselines para saber se a complexidade que estamos adicionando no modelo est√° realmente trazendo algum ganho na performance do modelo. Os modelos ser√£o:</p>
<ul>
<li>Modelo nulo: um modelo que sempre prev√™ a classe majorit√°ria;</li>
<li>Modelo de base: <a href="https://bradleyboehmke.github.io/HOML/bagging.html">Bagged Decision Tree</a> sem adicionar pr√©-processamento para compensar o desequil√≠brio de classe.</li>
</ul>
<div id="modelo-nulo" class="section level3">
<h3>Modelo nulo</h3>
<p>Avaliando modelo nulo via valida√ß√£o cruzada:</p>
<pre class="r"><code>null_spec &lt;- null_model(mode = &quot;classification&quot;) %&gt;% 
  set_engine(&quot;parsnip&quot;)

null_wf &lt;-
  workflow() %&gt;%
  add_recipe(base_rec) %&gt;%
  add_model(null_spec)

null_rs &lt;-
  fit_resamples(
    object = null_wf,
    resamples = bird_folds,
    metrics = bird_metrics,
    control = control_resamples(save_pred = TRUE)
  ) 

collect_metrics(null_rs) %&gt;% print_table(round = 5, cv = T) </code></pre>
<p><img src="/post/2021-06-28-imbalanced-workflowsets/tab1.png" /></p>
<p>Qualquer modelo com desempenho pior do que este deve ser descartado. Vejamos a matriz de confus√£o:</p>
<pre class="r"><code>collect_predictions(null_rs) %&gt;% 
  conf_mat_plot()</code></pre>
<center>
<img src="/post/2021-06-28-imbalanced-workflowsets/unnamed-chunk-14-1.png" style="width:80.0%" />
</center>
</div>
<div id="modelo-de-base" class="section level3">
<h3>Modelo de base</h3>
<p>Agora vamos ajusta o modelo <em>Bagged Decision Tree</em> sem o pr√©-processamento para compensar o desequil√≠brio de classe:</p>
<pre class="r"><code>bag_spec &lt;-
  bag_tree(min_n = 10) %&gt;%
  set_engine(&quot;rpart&quot;, times = 25) %&gt;%
  set_mode(&quot;classification&quot;)

imb_wf &lt;-
  workflow() %&gt;%
  add_recipe(base_rec) %&gt;%
  add_model(bag_spec)

set.seed(123)
imb_rs &lt;-
  fit_resamples(
    imb_wf,
    resamples = bird_folds,
    metrics = bird_metrics,
    control = control_resamples(save_pred = TRUE)
  )

collect_metrics(imb_rs) %&gt;% print_table(round = 5, cv = T)</code></pre>
<p><img src="/post/2021-06-28-imbalanced-workflowsets/tab2.png" /></p>
<p>Apesar do elevado n√∫mero de falsos negativos, este modelo j√° esta com um desempenho razo√°vel em compara√ß√£ao ao modelo nulo e o n√∫mero de verdadeiros positivos j√° √© quase o dobro do n√∫mero de falsos positivos. Veja na matriz de confus√£o abaixo:</p>
<pre class="r"><code>collect_predictions(imb_rs) %&gt;% 
  conf_mat_plot()</code></pre>
<center>
<img src="/post/2021-06-28-imbalanced-workflowsets/unnamed-chunk-16-1.png" style="width:80.0%" />
</center>
</div>
</div>
<div id="preparar-pipeline-de-dados-com-workflowsets" class="section level2">
<h2>Preparar Pipeline de dados com <code>workflowsets</code></h2>
<p>A escolha do m√©todo de amostragem dos dados √© t√£o importante quanto a escolha do modelo preditivo que ser√° utilizado pois o desempenho pode ser enganosamente otimista visto que o algoritmo de bagging n√£o esta usando nenhuma estrat√©gia de subamostragem aleat√≥ria da classe majorit√°ria em cada amostra de bootstrap para equilibrar as duas classes.</p>
<p>Existem muitos m√©todos para amostragem de dados e n√£o h√° um m√©todo √∫nico que seja melhor em todos os problemas de classifica√ß√£o (assim como n√£o existe o ‚Äúmelhor modelo‚Äù) portanto, utilizaremos este pacote para testar diferentes m√©todos e tamb√©m tunar seus hiperpar√¢metros.</p>
<div id="oversampling" class="section level3">
<h3>Oversampling</h3>
<p>Estes m√©todos duplicam ou sintetizam novos dados da classe minorit√°ria. Deve ser usado com cautela pois na vida real pode gerar alguns dados que n√£o condizem com a relidade ou criar tantas inst√¢ncias que acaba consumindo muito mais tempo de processamento.</p>
<div id="random-oversampling" class="section level4">
<h4>Random Oversampling</h4>
<p>Este m√©todo simplesmente duplica aleat√≥riamente exemplos da classe minorit√°ria. Vamos tunar esta propor√ß√£o buscando n√∫meros reais no intervalo [0.5,1].</p>
<pre class="r"><code>rec_up &lt;- base_rec %&gt;% 
  step_upsample(damaged, over_ratio = tune())

params_up &lt;- rec_up %&gt;% 
  parameters() %&gt;% update(over_ratio = mixture(c(0.5, 1)))</code></pre>
</div>
<div id="smote---synthetic-minority-oversampling-technique" class="section level4">
<h4>SMOTE - Synthetic Minority Oversampling Technique</h4>
<p>O SMOTE funciona gerando novos dados sint√©tios baseados em exemplos selecionando que est√£o ‚Äúpr√≥ximos‚Äù. Vamos tunar tanto a propor√ß√£o de dados que ser√£o gerados quanto a quantidade de vizinhos selecionados, buscando n√∫meros reais e inteiros no intervalo [0.5,1] e [1, 10], respectivamente.</p>
<pre class="r"><code>rec_smote &lt;- base_rec %&gt;%
  step_dummy(all_nominal_predictors()) %&gt;%
  step_smote(damaged, over_ratio = tune(), 
             neighbors = tune())

params_smote &lt;- rec_smote %&gt;% 
  parameters() %&gt;% update(over_ratio = mixture(c(0.5, 1)),
                          neighbors = neighbors())</code></pre>
</div>
<div id="adasyn---adaptive-synthetic-sampling" class="section level4">
<h4>ADASYN - Adaptive Synthetic Sampling</h4>
<p>O ADASYN √© uma extens√£o do SMOTE que busca propor melhorias. Vamos tunar os mesmos par√¢metros definidos no SMOTE.</p>
<pre class="r"><code>rec_adasyn &lt;- base_rec %&gt;%
  step_dummy(all_nominal_predictors()) %&gt;%
  step_adasyn(damaged, 
              over_ratio = tune(), 
              neighbors = tune())

params_adasyn &lt;- rec_adasyn %&gt;% 
  parameters() %&gt;% update(over_ratio = mixture(c(0.5, 1)),
                          neighbors = neighbors())</code></pre>
</div>
</div>
<div id="undersampling" class="section level3">
<h3>Undersampling</h3>
<p>S√£o t√©cnicas que excluem ou selecionam um subconjunto de exemplos da classe majorit√°ria e existem dezenas (se n√£o centenas) desses m√©todos. Neste post utilizaremos s√≥ 3 mas existem outros implementados em outras bibliotecas (em R e em Python).</p>
<div id="random-undersampling" class="section level4">
<h4>Random Undersampling</h4>
<p>Este √© o m√©todo mais simples e envolve a exclus√£o aleat√≥ria de algumas inst√¢ncias da classe majorit√°ria. Vamos tunar esta propor√ß√£o de frequ√™ncias da minorit√°ria para a majorit√°ria.</p>
<pre class="r"><code>rec_down &lt;- base_rec %&gt;% 
  step_downsample(damaged, under_ratio = tune())

params_down &lt;- rec_down %&gt;% 
  parameters() %&gt;% update(under_ratio = deg_free())</code></pre>
</div>
<div id="near-miss-undersampling" class="section level4">
<h4>Near Miss Undersampling</h4>
<p>Este algoritmo se baseia em m√©todos de KNN selecionando exemplos da classe majorit√°ria que tem menor dist√¢ncia m√©dia dos k exemplos mais pr√≥ximos. Vamos tunar tanto a propor√ß√£o quanto o n√∫mero de vizinhos utilizados.</p>
<pre class="r"><code>rec_nearmiss &lt;- base_rec %&gt;%
  step_dummy(all_nominal_predictors()) %&gt;%
  step_nearmiss(damaged, 
                under_ratio = tune(), 
                neighbors = tune())

params_nearmiss &lt;- rec_nearmiss %&gt;% 
  parameters() %&gt;% update(under_ratio = deg_free(),
                          neighbors = neighbors())</code></pre>
</div>
<div id="tomek-links-undersampling" class="section level4">
<h4>Tomek Links Undersampling</h4>
<p>Este algoritmo que tenta excluir inst√¢ncias que sejam pr√≥ximas e que possuam classes diferentes, buscando diminuir a ambiguidade dos dados. N√£o vamos tunar nenhum hiperpar√¢metro aqui.</p>
<pre class="r"><code>rec_tomek &lt;- base_rec %&gt;%
  step_dummy(all_nominal_predictors()) %&gt;%
  step_tomek(damaged)</code></pre>
</div>
</div>
<div id="preparar-pipeline-de-dados" class="section level3">
<h3>Preparar pipeline de dados</h3>
<p>Agora que todos pipelines de dados candidatos est√£o definidos, vamos combinar tudo em um √∫nico objeto com <code>workflow_set</code>:</p>
<pre class="r"><code>chi_models &lt;- 
  workflow_set(
    preproc = list(upsample = rec_up,
                   smote = rec_smote,
                   adasyn = rec_adasyn,
                   downsample = rec_down,
                   nearmiss = rec_nearmiss,
                   tomek = rec_tomek),
    models = list(bag_spec = bag_spec),
    cross = TRUE
  )</code></pre>
<p>Utilizar a fun√ß√£o <code>option_add</code> para adicionar as informa√ß√µes dos intervalos definidos para cada hiperpar√¢metro:</p>
<pre class="r"><code>chi_models &lt;- chi_models %&gt;% 
  option_add(param_info = params_up, id = &quot;upsample_bag_spec&quot;)  %&gt;% 
  option_add(param_info = params_smote, id = &quot;smote_bag_spec&quot;) %&gt;% 
  option_add(param_info = params_adasyn, id = &quot;adasyn_bag_spec&quot;) %&gt;% 
  option_add(param_info = params_down, id = &quot;downsample_bag_spec&quot;) %&gt;% 
  option_add(param_info = params_nearmiss, id = &quot;nearmiss_bag_spec&quot;)</code></pre>
<p>Finalmente, vamos ajustar todos os modelos utilizando o m√©todo simples para fazer a busca dos melhores hiperpar√¢metros em grids de 20 valores aleat√≥rios e calcular os scores via valida√ß√£o cruzada (esta parte pode demorar bastante tempo):</p>
<pre class="r"><code>set.seed(123)
chi_models &lt;- 
  chi_models %&gt;% 
  workflow_map(&quot;tune_grid&quot;,
               resamples = bird_folds, 
               grid = 20, 
               metrics = bird_metrics,
               control = control_resamples(save_pred = TRUE),
               verbose = TRUE)</code></pre>
<p>Vejamos os resultados:</p>
<pre class="r"><code>rank_results(chi_models, rank_metric = &quot;mn_log_loss&quot;, select_best = TRUE) %&gt;% 
  select(-.config) %&gt;%
  mutate(wflow_id = str_remove(wflow_id, &quot;_bag_spec&quot;)) %&gt;% 
  print_table(round = 5, wf=T, height = 300, filterable = T)</code></pre>
<p>Matriz de confus√£o do modelo com menor <em>logloss</em>:</p>
<pre class="r"><code>collect_predictions(chi_models) %&gt;% 
  filter(wflow_id == &quot;tomek_bag_spec&quot;) %&gt;% 
  conf_mat_plot()</code></pre>
<center>
<img src="/post/2021-06-28-imbalanced-workflowsets/unnamed-chunk-29-1.png" style="width:80.0%" />
</center>
</div>
</div>
<div id="benchmark" class="section level2">
<h2>Benchmark</h2>
<p>Comparando os resultados dos modelos ajustados:</p>
<details>
<summary>
(<em>Clique aqui para ver o c√≥digo que cria o objeto</em> <code>benchmark</code>)
</summary>
<pre class="r"><code>benchmark &lt;- bind_rows(
  mutate(collect_metrics(null_rs), wflow_id = &quot;default&quot;, model = &quot;null_model&quot;) %&gt;% 
    select(.metric, mean, wflow_id, model) %&gt;% 
    spread(.metric, mean)
  ,
  mutate(collect_metrics(imb_rs), wflow_id = &quot;default&quot;, model = &quot;bag_tree&quot;) %&gt;% 
    select(.metric, mean, wflow_id, model) %&gt;% 
    spread(.metric, mean)
  ,
  rank_results(chi_models, rank_metric = &quot;mn_log_loss&quot;, select_best = TRUE) %&gt;% 
    filter(wflow_id==&quot;smote_bag_spec&quot;) %&gt;% 
    select(.metric, mean, wflow_id, model) %&gt;% 
    spread(.metric, mean)
  ,
  rank_results(chi_models, rank_metric = &quot;mn_log_loss&quot;, select_best = TRUE) %&gt;% 
    filter(rank==1) %&gt;% 
    select(.metric, mean, wflow_id, model) %&gt;% 
    spread(.metric, mean)
) </code></pre>
</details>
<pre class="r"><code>benchmark  %&gt;%
  print_table(round = 5, bm = T)</code></pre>
<p>Como no post da Julia, a logloss e a precis√£o dos modelos que utilizaram m√©todos de balanceamento dos dados pioraram em rela√ß√£o ao modelo de <em>Bagged Decision Tree</em> sem o uso desses pipelines. Apesar da piora em rela√ß√£o ao modelo de base nota-se que outros m√©todos como <em>Tomek Links</em> e <em>Adasyn</em> se sa√≠ram ligeiramente melhores do que o <em>Smote</em> (al√©m disso vimos que o <em>Smote</em> com sua configura√ß√£o <em>default</em> n√£o necessariamente produriz√° os melhores resultados).</p>
<p>Este tipo de performance √© muito comum e at√© esperado visto que estamos avaliando o modelo atrav√©s de uma √∫nica m√©trica (com os mesmos pontos de corte e com o mesmo algoritmo). Normalmente no mundo real monitoramos diversas m√©tricas e experimentamos mais configura√ß√µes de hiperpar√¢metros de diferentes modelos com diferentes pipelines.</p>
</div>
</div>
<div id="conclus√£o" class="section level1">
<h1>Conclus√£o</h1>
<p>Assim como n√£o existe melhor modelo, n√£o existe melhor t√©cnica de balanceamento de dados. Portanto, na busca de melhores resultados n√≥s podemos tentar otimizar qual abordagem ser√° uyilizada bem como seus hiperpar√¢metros (em conjunto com os hiperpar√¢metros dos modelos em quest√£o).</p>
<p>Esta abordagem em R √© nova para mim (estou mais acostumado a utilizar em Python com o m√©todo <code>sklearn.pipeline.Pipeline</code> em conjunto com a biblioteca <a href="https://pypi.org/project/imblearn/">imblearn</a>) ent√£o qualquer cr√≠tica e sugest√£o de melhoria ser√° muito bem vinda! Basta entrar em contato ou deixar aqui nos coment√°rios!</p>
<p>Bons estudos e espero que gostem! üöÄ</p>
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias</h1>
<ul>
<li><a href="https://www.tidyverse.org/blog/2021/03/workflowsets-0-0-1/" class="uri">https://www.tidyverse.org/blog/2021/03/workflowsets-0-0-1/</a></li>
<li><a href="https://www.kaggle.com/c/sliced-s01e02-xunyc5" class="uri">https://www.kaggle.com/c/sliced-s01e02-xunyc5</a></li>
<li><a href="https://juliasilge.com/blog/sliced-aircraft/" class="uri">https://juliasilge.com/blog/sliced-aircraft/</a></li>
<li><a href="https://topepo.github.io/caret/subsampling-for-class-imbalances.html" class="uri">https://topepo.github.io/caret/subsampling-for-class-imbalances.html</a></li>
<li><a href="https://machinelearningmastery.com/bagging-and-random-forest-for-imbalanced-classification/" class="uri">https://machinelearningmastery.com/bagging-and-random-forest-for-imbalanced-classification/</a></li>
<li><a href="https://machinelearningmastery.com/what-is-imbalanced-classification/" class="uri">https://machinelearningmastery.com/what-is-imbalanced-classification/</a></li>
<li><a href="https://machinelearningmastery.com/framework-for-imbalanced-classification-projects/" class="uri">https://machinelearningmastery.com/framework-for-imbalanced-classification-projects/</a></li>
<li><a href="https://machinelearningmastery.com/data-sampling-methods-for-imbalanced-classification/" class="uri">https://machinelearningmastery.com/data-sampling-methods-for-imbalanced-classification/</a></li>
</ul>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2021-06-28-imbalanced-workflowsets/">Otimizando pipelines que envolvem dados desbalanceados</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">imbalanced</category>
      <category domain="tag">imbalanced-data</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">machine-learning</category>
      <category domain="tag">pratica</category>
      <category domain="tag">r</category>
      <category domain="tag">random-forest</category>
      <category domain="tag">tidymodels</category>
      <category domain="tag">tidyverse</category>
      <category domain="tag">tunning</category>
    </item>
    <item>
      <title>Como automatizar relat√≥rios longos e repetitivos com RMarkdown</title>
      <link>https://gomesfellipe.github.io/post/2019-09-13-relatorios-automaticos-com-rmarkdown/relatorios-automaticos-com-rmarkdown/</link>
      <pubDate>Fri, 13 Sep 2019 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2019-09-13-relatorios-automaticos-com-rmarkdown/relatorios-automaticos-com-rmarkdown/</guid>
      <description>Veja como fazer um relat√≥rio estat√≠stico &#34;extenso e repetitivo&#34; sem utilizar copiar e colar nenhuma vez</description>
      <content:encoded>&lt;![CDATA[
        


<div id="problema-de-neg√≥cio" class="section level1">
<h1>Problema de neg√≥cio</h1>
<p>Uma tarefa comum no dia a dia de um estat√≠stico (ou cientista de dados) √© a elabora√ß√£o de relat√≥rios para passsar ao restante da equipe e/ou tomadores de decis√£o os resultados encontrados e muitas vezes essa tarefa pode parecer desgastante quando os relat√≥rios s√£o muitos extensos e repetitivos.</p>
<p>Com a linguagem R, escrever relat√≥rios estat√≠sticos utilizando <a href="https://rmarkdown.rstudio.com/">RMarkdown</a> acaba sendo a escolha padr√£o por ser t√£o simples transformar as an√°lises em documentos, apresenta√ß√µes e dashboards de alta qualidade com poucas linhas de c√≥digo.</p>
<p>Assim, combinando conceitos de programa√ß√£o, como o <a href="https://pt.wikipedia.org/wiki/Loop_(programa%C3%A7%C3%A3o)">Loop</a> no R e a linguagem <a href="https://pt.wikipedia.org/wiki/Markdown">Markdown</a> para produ√ß√£o de relat√≥rios, temos uma poderosa ferramenta para <a href="https://pt.wikipedia.org/wiki/Automa%C3%A7%C3%A3o">Automa√ß√£o</a> de relat√≥rios.</p>
<div id="entendendo-o-problema" class="section level2">
<h2>Entendendo o problema</h2>
<p>Suponha que o seguinte gr√°fico seja apresentado √† voc√™:</p>
<center>
<img src="/post/2019-09-13-relatorios-automaticos-com-rmarkdown/figure-html/unnamed-chunk-1-1.gif" style="width:80.0%" />
</center>
<p>Este gr√°fico animado apresenta a evolu√ß√£o da esperan√ßa de vida ao nascer (em anos) em rela√ß√£o ao PIB per capita (em US$, ajustado pela infla√ß√£o) de 141 pa√≠ses dos 5 continentes durante o per√≠odo de 1952 at√© 2007, a cada 5 anos.</p>
<p>Entraremos em mais detalhes sobre as informa√ß√µes dete gr√°fico a seguir.</p>
</div>
</div>
<div id="fonte-dos-dados" class="section level1">
<h1>Fonte dos dados</h1>
<p>Os dados utilizados neste problema foram importados atrav√©s do pacote <a href="https://cran.r-project.org/web/packages/gapminder/index.html">gapminder</a> que √© um projeto que utiliza dados do site <a href="https://www.gapminder.org/">Gapminder.org</a>.</p>
<p>Segundo sua <a href="https://www.gapminder.org/about-gapminder/">descri√ß√£o no site</a>:</p>
<blockquote>
<p>‚ÄúGapminder √© uma funda√ß√£o independente sueca sem afilia√ß√µes pol√≠ticas, religiosas ou econ√¥micas. (‚Ä¶)‚Äù</p>
</blockquote>
<p>No site √© poss√≠vel obter dados gratuitos para se obter estat√≠sticas confi√°veis e al√©m dos disso a Funda√ß√£o Gapminder apresenta alguns outros projetos como o <a href="https://www.gapminder.org/dollar-street/matrix">Dollar Street</a> que apresenta 30.000 fotos de 264 fam√≠lias em 50 pa√≠ses classificados por renda.</p>
<p>Na p√°gina do projeto √© poss√≠vel ver e comparar os mais variados aspectos da popula√ß√£o ao redor do mundo que v√£o desde casas, itens mais amados, carros at√© banheiros, comida de pets e bebidas alco√≥licas.</p>
<p>O pacote fornece dados da Funda√ß√£o Gapminder como: valores de expectativa de vida, PIB per capta e popula√ß√£o, a cada cinco anos, de 1952 a 2007 (total de 12 anos). Veja as primeiras 5 linhas da base de dados contidos no pacote:</p>
<pre class="r"><code># Base de dados utilizada
head(gapminder)</code></pre>
<pre><code>## # A tibble: 6 x 6
##   country     continent  year lifeExp      pop gdpPercap
##   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;
## 1 Afghanistan Asia       1952    28.8  8425333      779.
## 2 Afghanistan Asia       1957    30.3  9240934      821.
## 3 Afghanistan Asia       1962    32.0 10267083      853.
## 4 Afghanistan Asia       1967    34.0 11537966      836.
## 5 Afghanistan Asia       1972    36.1 13079460      740.
## 6 Afghanistan Asia       1977    38.4 14880372      786.</code></pre>
<p>Essa base de dados possui 1705 linhas de 6 vari√°veis, onde:</p>
<ul>
<li><code>country</code>: factor com 142 levels</li>
<li><code>continent</code>: factor com 5 levels</li>
<li><code>year</code>: sequencia de 1952 at√© 2007 a cada 5 anos</li>
<li><code>lifeExp</code>: esperan√ßa de vida ao nascer, em anos</li>
<li><code>pop</code>: popula√ß√£o</li>
<li><code>gdpPercap</code>: PIB per capita (em US$, ajustado pela infla√ß√£o)</li>
</ul>
</div>
<div id="comportamento-geral-dos-dados" class="section level1">
<h1>Comportamento geral dos dados</h1>
<p>Antes de come√ßar a fazer os relat√≥rios para cada ano, vamos reproduzir a anima√ß√£o apresentada para n√≥s com o comportamento temporal utilizando o pacote <a href="https://github.com/thomasp85/gganimate">gganimate</a>:</p>
<pre class="r"><code># Carregar pacotes
library(ggplot2)
library(dplyr)
library(gapminder)
library(scales)
library(gganimate)

# Definir tema:
theme_set(theme_bw())

# Funcao para customizar legendas:
custom_legend &lt;- function(x){comma(x, big.mark = &quot;.&quot;,decimal.mark = &quot;,&quot;)}

# Comportamento geral:
gapminder %&gt;% 
  filter(country!=&quot;Kuwait&quot;) %&gt;% # remover 1 pais outlier
  ggplot(aes(x = gdpPercap, y = lifeExp, size = pop, 
             label = country, color = continent, alpha= log(gdpPercap))) %+%
  geom_point(show.legend = F) %+%
  geom_text(show.legend = F, size = 3, nudge_y = -0.7) %+%
  scale_size_continuous(labels = custom_legend) %+%
  scale_x_continuous(labels = custom_legend) %+%
  geom_smooth(se=F, color = &quot;black&quot;, show.legend = F, method = &quot;lm&quot;) %+% 
  transition_time(year) %+%
  scale_color_brewer(palette = &quot;Dark2&quot;) %+%
  labs(title = &quot;Year: {frame_time}&quot;)</code></pre>
<center>
<img src="/post/2019-09-13-relatorios-automaticos-com-rmarkdown/unnamed-chunk-1-1.gif" style="width:80.0%" />
</center>
<p>Analisando esta anima√ß√£o √© poss√≠vel notar:</p>
<ul>
<li>Jap√£o √© o pa√≠s que possui a maior expectativa de vida ao longo de todos os anos;</li>
<li>Os pa√≠ses do cont√≠nente africano s√£o os que apresentam expectativa de vida mais baixa e pior <code>gdpPercap</code>.</li>
<li>A Ar√°bia Saudita teve sua <code>gdpPercap</code> aumentada at√© 1978 por√©m a partir da√≠ diminiu bastante.</li>
<li>O pa√≠s com maior <code>gdpPercap</code> e expectativa de vida na Am√©rica √© o Estados Unidos;</li>
<li>A Noroega foi o pa√≠s que mais se descatou com os valores mais elevados e est√°veis ao longo destes 55 anos.</li>
</ul>
<p>Obs[1]: <a href="https://www.google.com/search?q=Kuwait&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=0ahUKEwj53ZXJ4bPjAhVAD7kGHQvwCWgQ_AUIESgC&amp;biw=1574&amp;bih=943">Kuwait</a> foi removida para este gr√°fico animado pois √© um pa√≠s outlier. Segundo o <a href="https://pt.wikipedia.org/wiki/Kuwait">Wikip√©dia</a>:</p>
<blockquote>
<p>‚ÄúO Kuwait tem um PIB (PPC) de US$ 167,9 bilh√µes[96] e uma renda per capita de US$ 81 800,[96] o que o torna o quinto pa√≠s mais rico do mundo.[52] O √≠ndice de desenvolvimento humano (IDH) do Kuwait √© de 0,816, um dos mais elevados do Oriente M√©dio e do mundo √°rabe. Com uma taxa de crescimento do PIB de 5,7%, o Kuwait tem uma das economias que mais crescem na regi√£o.[96]‚Äù</p>
</blockquote>
<p>Para quem tiver curiosidade, os dados de <code>Kuwait</code> podem ser obtidos da seguinte forma:</p>
<pre class="r"><code>gapminder %&gt;% filter(country == &quot;Kuwait&quot;)</code></pre>
<pre><code>## # A tibble: 12 x 6
##    country continent  year lifeExp     pop gdpPercap
##    &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;     &lt;dbl&gt;
##  1 Kuwait  Asia       1952    55.6  160000   108382.
##  2 Kuwait  Asia       1957    58.0  212846   113523.
##  3 Kuwait  Asia       1962    60.5  358266    95458.
##  4 Kuwait  Asia       1967    64.6  575003    80895.
##  5 Kuwait  Asia       1972    67.7  841934   109348.
##  6 Kuwait  Asia       1977    69.3 1140357    59265.
##  7 Kuwait  Asia       1982    71.3 1497494    31354.
##  8 Kuwait  Asia       1987    74.2 1891487    28118.
##  9 Kuwait  Asia       1992    75.2 1418095    34933.
## 10 Kuwait  Asia       1997    76.2 1765345    40301.
## 11 Kuwait  Asia       2002    76.9 2111561    35110.
## 12 Kuwait  Asia       2007    77.6 2505559    47307.</code></pre>
</div>
<div id="resolvendo-o-problema-de-neg√≥cio" class="section level1">
<h1>Resolvendo o problema de neg√≥cio</h1>
<p>Para resolver o problema de se fazer uma an√°lise sobre a expectativa de vida, PIB per capta e popula√ß√£o, para cada continente, para cada ano dispon√≠vel, (ou seja, analisar de 1952 a 2007 a cada cinco anos) faremos um total de 12 relat√≥rios.</p>
<p>Isso √© muito para se arriscar usar <code>ctrl+c</code> e <code>ctrl+v</code> 12 vezes e depois caso precise de alguma mudan√ßa, alterar o relat√≥rio 12 vezes.</p>
<p>Portanto utilizaremos uma estrat√©gia parecida com a que apresentei no √∫ltimo post sobre como <a href="https://gomesfellipe.github.io/post/2019-04-05-split-apply-combine/split-apply-combine/">Hackear o R com a estrat√©cia Split-Appy-Combine</a>.</p>
<p>Primeiramente vamos separar nosso dataset por ano utilizando a fun√ß√£o <code>tidyr::nest()</code>:</p>
<pre class="r"><code>library(tidyr) # funcao nest

# separar por ano:
nested_gapminder &lt;- gapminder %&gt;% nest(-year)</code></pre>
<p>Selecionei um dos anos como exemplo e utilizei os objetos <code>nested_gapminder$year[1]</code> e <code>nested_gapminder$data[[1]]</code> para desenvolver uma fun√ß√£o que realizasse todas as an√°lises que eu precisasse.</p>
<p>Essa fun√ß√£o foi salva em um script separado chamado <code>analise.R</code> e pode ser encontrada <a href="">neste link</a>. Para caregar a fun√ß√£o localmente basta utilizar a fun√ß√£o <code>source()</code>, veja;</p>
<pre class="r"><code>source(&quot;analise_gapminder.R&quot;)</code></pre>
<p>Veja nas se√ß√µes a seguir os outputs da fun√ß√£o antes de encapsul√°-la em um arquivo RMarkdown (.Rmd) para fazer o looping:</p>
<div id="resultados-para-o-ano-2007" class="section level2">
<h2>Resultados para o ano 2007</h2>
<p>A seguir vamos criar o objeto <code>x</code> que ser√° o data set referente ao ano <code>title</code>. Em seguida vamos aplicar a fun√ß√£o carregada anteriormente para obter os resultados das an√°lises e salvar no objeto <code>resutls</code></p>
<pre class="r"><code>library(magrittr) # pipe %$%

# Obter resultados
x       &lt;- nested_gapminder %&gt;% filter(year == 2007) %&gt;% unnest()
title   &lt;- nested_gapminder %&gt;% filter(year == 2007) %$% year
results &lt;- analise_gapminder(x, title)</code></pre>
<p>Vejamos como o Brasil esta em rela√ß√£o aos outros pa√≠ses com um gr√°fico que resume os resultados do modelo ajustado:</p>
<pre class="r"><code>results$grafico_geral_regressao</code></pre>
<center>
<img src="/post/2019-09-13-relatorios-automaticos-com-rmarkdown/unnamed-chunk-9-1.png" style="width:80.0%" />
</center>
<p>Comportamento dos dados por Continente</p>
<pre class="r"><code>results$grafico_por_continente</code></pre>
<center>
<img src="/post/2019-09-13-relatorios-automaticos-com-rmarkdown/unnamed-chunk-10-1.png" style="width:80.0%" />
</center>
<p>Ap√≥s ajustar o modelo de regress√£o, vamos obter algumas estat√≠sticas descritivas com mais gr√°ficos informativos!</p>
<p>O gr√°fico abaixo apresenta uma <a href="http://www.leg.ufpr.br/lib/exe/fetch.php/projetos:saudavel:loess.pdf">Regress√£o Local (LOESS)</a> com destaque nos pa√≠ses que tiveram <code>gdpPercap</code> e <code>lifeExp</code> acima da m√©dia</p>
<pre class="r"><code>results$grafico_zoom_acima_media</code></pre>
<center>
<img src="/post/2019-09-13-relatorios-automaticos-com-rmarkdown/unnamed-chunk-11-1.png" style="width:80.0%" />
</center>
<p>E agora podemos conferir um gr√°fico que apresenta uma <a href="http://www.leg.ufpr.br/lib/exe/fetch.php/projetos:saudavel:loess.pdf">Regress√£o Local (LOESS)</a> com destaque nos pa√≠ses que tiveram <code>gdpPercap</code> e <code>lifeExp</code> acima da m√©dia</p>
<pre class="r"><code>results$grafico_zoom_abaixo_media</code></pre>
<center>
<img src="/post/2019-09-13-relatorios-automaticos-com-rmarkdown/unnamed-chunk-12-1.png" style="width:80.0%" />
</center>
<p>Maravilha! Muitas informa√ß√µes interessantes mas n√£o resolvemos o problema por inteiro. Resta aplicar as mesmas an√°lises para os demais anos do nosso dataset.</p>
</div>
</div>
<div id="automatizar-as-analises-para-os-pr√≥ximos-anos" class="section level1">
<h1>Automatizar as analises para os pr√≥ximos anos</h1>
<p>A linha a seguir √© a que realiza toda a m√°gica!</p>
<p>A fun√ß√£o <code>knit_child()</code> compila o c√≥digo R e retorna uma sa√≠da pura (Latex, html ou word sem c√≥digo R), ent√£o se fizermos um looping da seguinte maneira teremos replicado nossas an√°lises para todos os demais anos:</p>
<pre><code>rmarkdown::render(&quot;gapminder_automatico_master.Rmd&quot;)</code></pre>
<p>Veja o conte√∫do do script <code>gapminder_automatico_master.Rmd</code>:</p>
<script src="https://gist.github.com/gomesfellipe/86af044b4e8a874756a2f4c379cfc01b.js"></script>
<p>Note que este script chama outro arquivo <code>.Rmd</code> chamado <code>gapminder_automatico_child.Rmd</code>, que tem o seguinte conte√∫do:</p>
<script src="https://gist.github.com/gomesfellipe/2a9d666e041907ca88dd2188cbc72924.js"></script>
<p>Veja os resultados do looping:</p>
<iframe src="/post/2019-09-13-relatorios-automaticos-com-rmarkdown/gapminder_automatico_master.pdf" width="600" height="827" style="border: none;">
</iframe>
</div>
<div id="conclus√£o" class="section level1">
<h1>Conclus√£o</h1>
<p>A Abordagem para criar chunks <em>filhos</em> de RMarkdown com a fun√ß√£o <code>knit_child()</code> abre muitas portas para an√°lises de dados! Neste post fizemos um exemplo simples de automa√ß√£o de relat√≥rios por√©m esses resultados podem ser cada vez mais customiz√°veis e utilizados em RPA - <a href="https://en.wikipedia.org/wiki/Robotic_process_automation">Robotic Process Automation</a> - de forma que seja poss√≠vel automatizar processos que antes s√≥ poderiam ser executados por humanos!</p>
</div>
<div id="referencias" class="section level1">
<h1>Referencias</h1>
<ul>
<li><a href="https://cran.r-project.org/web/packages/gganimate/vignettes/gganimate.html" class="uri">https://cran.r-project.org/web/packages/gganimate/vignettes/gganimate.html</a></li>
<li><a href="https://cran.r-project.org/web/packages/ggforce/vignettes/Visual_Guide.html" class="uri">https://cran.r-project.org/web/packages/ggforce/vignettes/Visual_Guide.html</a></li>
<li><a href="https://cran.r-project.org/web/packages/ggExtra/vignettes/ggExtra.html" class="uri">https://cran.r-project.org/web/packages/ggExtra/vignettes/ggExtra.html</a></li>
<li><a href="https://cran.r-project.org/web/packages/gapminder/gapminder.pdf" class="uri">https://cran.r-project.org/web/packages/gapminder/gapminder.pdf</a></li>
<li><a href="https://www.gapminder.org/data/" class="uri">https://www.gapminder.org/data/</a></li>
<li><a href="https://stackoverflow.com/questions/43873345/knit-child-in-a-loop-variable-as-title" class="uri">https://stackoverflow.com/questions/43873345/knit-child-in-a-loop-variable-as-title</a></li>
</ul>
</div>
<div id="apendice" class="section level1">
<h1>Apendice</h1>
<div id="fun√ß√£o-analise.r" class="section level2">
<h2>Fun√ß√£o <code>analise.R</code></h2>
<p>Veja o conte√∫do da fun√ß√£o <code>analise.R</code> preparada para esta analise:</p>
<pre class="r"><code># Funcao para analise por ano:
analise_gapminder &lt;- function(x, title){
  
  # Carregar dependencias:
  require(broom)
  require(ggforce)
  require(ggpmisc)
  require(ggExtra)
  
  # Funcao para customizar legendas:
  custom_legend &lt;- function(x){comma(x, big.mark = &quot;.&quot;,decimal.mark = &quot;,&quot;)}
  
  # Obter dados do Brasil:
  brazil &lt;- x %&gt;% filter(country == &quot;Brazil&quot;)
  
  # Resultados do ajuste de regressao ---------------------------------------
  mytable &lt;- 
    lm(lifeExp ~ gdpPercap, data = x) %&gt;% 
    tidy() %&gt;% 
    mutate_if(is.numeric, ~round(.x, 4)) %&gt;% 
    `colnames&lt;-`(c(&quot;Termo&quot;, &quot;Estimativa&quot;, &quot;Desv.Pad.&quot;, &quot;Estatistica&quot;, &quot;Valor p&quot;))
  
  # r2:
  r2 &lt;- round(summary(lm(lifeExp ~ gdpPercap, data = x))$r.squared,4)*100
  
  # residuos do modelo:
  res &lt;- lm(lifeExp ~ gdpPercap, data = x)$residuals
  
  # resutado para teste de kolmogorov-smirnov
  ks_test &lt;- ks.test(res, &quot;pnorm&quot;, mean(res), sd(res))$p.value %&gt;% round(5)
  
  # Grafico geral com regressao e boxplots ----------------------------------
  grafico_geral_regressao &lt;- 
    x %&gt;% 
    ggplot(aes(x = gdpPercap, y = lifeExp, size = pop, label = country, color = country)) %+%
    geom_point(show.legend = F) %+%
    geom_text(show.legend = F, size = 3, nudge_y = -0.5) %+%
    scale_size_continuous(labels = custom_legend) %+%
    scale_x_log10(labels = custom_legend) %+%
    scale_color_manual(values = country_colors) %+%
    geom_smooth(se=F, color = &quot;black&quot;, show.legend = F, method = &quot;lm&quot;) %+%
    annotate(&quot;segment&quot;, color=&quot;blue&quot;, arrow=arrow(length=unit(0.05,&quot;npc&quot;)),
             x=brazil$gdpPercap, xend=brazil$gdpPercap,
             y=brazil$lifeExp-6, yend=brazil$lifeExp-1) %+%
    annotate(&quot;text&quot;, color=&quot;blue&quot;, label = &quot;Brasil&quot;,
             x=brazil$gdpPercap, y=brazil$lifeExp-7) %+%
    labs(title = paste0(title, &quot;: lifeExp ~ gdpPercap&quot;),
         subtitle = &quot;Regress√£o linear e destaque no Brasil&quot;,
         caption = paste0(&quot;R¬≤ do modelo: &quot;, r2, &quot;\n&quot;,&quot;p valor para ks.test: &quot;, ks_test),
         x = &quot;gdpPercap (Transforma√ß√£o log10)&quot;) %+%
    annotate(geom = &quot;table&quot;, x = Inf, y = -Inf,
             label = list(mytable), 
             vjust = 0, hjust = 1) %&gt;%  
    ggMarginal(type = &quot;boxplot&quot;, fill=&quot;transparent&quot;,size = 10)
  
  # Comportamento separado por continente -----------------------------------
  grafico_por_continente &lt;- 
    x %&gt;% 
    filter(continent != &quot;Oceania&quot;) %&gt;% 
    ggplot(aes(x = gdpPercap, y = lifeExp, size = pop, label = country, color = country)) %+%
    geom_point(show.legend = F) %+%
    geom_text(show.legend = F, size = 3, nudge_y = -0.5) %+%
    facet_wrap(~continent, scales = &quot;free&quot;) %+%
    scale_x_continuous(labels = custom_legend) %+%
    scale_color_manual(values = country_colors) %+% 
    geom_smooth(method = &quot;lm&quot;, color = &quot;black&quot;, se=F, show.legend = F) %+%
    labs(title = paste0(title, &quot;: lifeExp ~ gdpPercap, por continente&quot;))
  
  # Acima da media ----------------------------------------------------------
  grafico_zoom_acima_media &lt;- 
    x %&gt;% 
    ggplot(aes(x = gdpPercap, y = lifeExp, size = pop, label = country, color = country)) %+%
    geom_point(show.legend = F) %+% 
    geom_text(show.legend = F, size = 3, nudge_y = -0.5) %+%
    scale_size_continuous(labels = custom_legend) %+%
    scale_x_continuous(labels = custom_legend) %+%
    scale_color_manual(values = country_colors) %+%
    facet_zoom(y = lifeExp   &gt; median(x$lifeExp),
               x = gdpPercap &gt; median(x$gdpPercap), split = T) %+%
    geom_smooth(se=F, color = &quot;red&quot;, show.legend = F, method = &quot;loess&quot;)  %+%
    labs(title = paste0(title, &quot;: lifeExp ~ gdpPercap com zoom nos pa√≠ses acima da mediana&quot;))
  
  # Abaixo da media ---------------------------------------------------------
  grafico_zoom_abaixo_media &lt;- 
    x %&gt;% 
    ggplot(aes(x = gdpPercap, y = lifeExp, size = pop, label = country, color = country)) %+%
    geom_point(show.legend = F) %+%
    geom_text(show.legend = F, size = 3, nudge_y = -0.5) %+%
    scale_size_continuous(labels = custom_legend) %+%
    scale_x_continuous(labels = custom_legend) %+%
    scale_color_manual(values = country_colors) %+%
    facet_zoom(y = lifeExp   &lt; median(x$lifeExp),
               x = gdpPercap &lt; median(x$gdpPercap), split = T) %+%
    geom_smooth(se=F, color = &quot;red&quot;, show.legend = F, method = &quot;loess&quot;)   %+%
    labs(title = paste0(title, &quot;: lifeExp ~ gdpPercap com zoom nos pa√≠ses abaixo da mediana&quot;))
  
  # Output ------------------------------------------------------------------
  list(
    brazil  = brazil,
    mytable = mytable,
    r2      = r2,
    grafico_geral_regressao   = grafico_geral_regressao,
    grafico_por_continente    = grafico_por_continente,
    grafico_zoom_acima_media  = grafico_zoom_acima_media,
    grafico_zoom_abaixo_media = grafico_zoom_abaixo_media,
    ks_test = ks_test
  )
}</code></pre>
</div>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2019-09-13-relatorios-automaticos-com-rmarkdown/relatorios-automaticos-com-rmarkdown/">Como automatizar relat√≥rios longos e repetitivos com RMarkdown</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">data-mining</category>
      <category domain="tag">estatistica</category>
      <category domain="tag">otimizacao</category>
      <category domain="tag">pratica</category>
      <category domain="tag">r</category>
      <category domain="tag">regressao</category>
      <category domain="tag">relatorios</category>
      <category domain="tag">reports</category>
      <category domain="tag">rmarkdown</category>
      <category domain="tag">rstudio</category>
    </item>
    <item>
      <title>Hackeando o R: estrat√©gia split-apply-combine</title>
      <link>https://gomesfellipe.github.io/post/2019-04-05-split-apply-combine/split-apply-combine/</link>
      <pubDate>Fri, 05 Apr 2019 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2019-04-05-split-apply-combine/split-apply-combine/</guid>
      <description>Veja como aplicar essa estrat√©gia de maneira eficiente utilizando os pacotes do tidyverse: dplyr&#43;tidyr&#43;purrr</description>
      <content:encoded>&lt;![CDATA[
        


<div id="o-m√©todo-split-apply-combine" class="section level1">
<h1>O m√©todo split-apply-combine</h1>
<p>Geralmente em uma an√°lise de dados precisamos compreender, al√©m do comportamento geral dos dados, o seu comportamento de acordo com alguns segmentos.</p>
<p>No famoso paper <a href="https://vita.had.co.nz/papers/plyr.pdf">The Split-Apply-Combine Strategy for Data Analysis</a>, <a href="http://hadley.nz/">Hadley Wickham</a> descreve a abordagem ‚Äúsplit-apply-combine‚Äù (dividir-aplicar-combinar) como uma das mais comuns em uma an√°lise de dados. Em R essa tarefa pode ser feita por diversos caminhos, veja alguns dos modos de se fazer utilizando fun√ß√µes base do R e abordagens mais antigas:</p>
<ul>
<li><code>split()</code> + <code>lapply()</code> + <code>do.call(rbind, ...)</code></li>
<li><code>ddply()</code> do pacote <code>plyr</code></li>
<li><code>group_by</code> + <code>do()</code></li>
<li><code>split()</code> + <code>map_dfr()</code></li>
</ul>
<p>Todos esses exemplos atendem √† maioria dos casos que deseja-se utilizar a abordagem ‚Äúsplit-apply-combine‚Äù, por√©m, veja por exemplo este <a href="https://community.rstudio.com/t/should-i-move-away-from-do-and-rowwise/2857">t√≥pico na community.rstudio.com</a> criado no final de 2017 em que ocorre um comunicado que a fun√ß√£o <code>do()</code> ser√° descontinuada</p>
<p>Ou ainda, confira quando foi o √∫ltimo lan√ßamento de atualiza√ß√£o do pacote <a href="https://cran.r-project.org/web/packages/plyr/index.html"><code>plyr</code> no CRAN</a> (foi em junho de 2016).</p>
<p>Com a proposta de mais efici√™ncia e legibilidade do c√≥digo, atualmente existem maneiras mais sofisticadas e modernas de se realizar esta tarefa com pacotes que foram atualizados j√° este ano de 2019. Veja nas se√ß√µes a seguir o aumento de produtividade que √© poss√≠vel se obter combinando os pacotes <code>dplyr</code>, <code>tidyr</code> e <code>purrr</code> da cole√ß√£o de pacotes do <a href="https://www.tidyverse.org/"><code>tidyverse</code></a>.</p>
<div id="usando-s√≥-o-dplyr" class="section level2">
<h2>Usando s√≥ o dplyr</h2>
<p>Usamos ‚Äúsplit-apply-combine‚Äù implicitamente o tempo todo quando utilizamos as fun√ß√µes <code>groupy_by()</code> + <code>summarise()</code> do pacote <a href="https://dplyr.tidyverse.org/"><code>dplyr</code></a></p>
<p>Poder√≠amos facilmente reproduzir o exemplo da imagem do post com os seguintes comandos:</p>
<pre class="r"><code>library(dplyr)
data &lt;- tibble(x = c(&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;C&quot;, &quot;C&quot;),
               y = c(0,1,2,3,4,5)) 

data %&gt;%                       # input data
  group_by(x) %&gt;%              # split
  summarise(data = mean(y))    # apply/combine</code></pre>
<pre><code>## # A tibble: 3 x 2
##   x      data
##   &lt;chr&gt; &lt;dbl&gt;
## 1 A       0.5
## 2 B       2.5
## 3 C       4.5</code></pre>
<p>Essa sequ√™ncia de c√≥digos aplica a abordagem implicitamente, agrupando os dados de acordo com a vari√°vel selecionada e em seguida aplicando a opera√ß√£o e combinando os resultados em uma matriz resumida</p>
</div>
<div id="usando-dplyr-tidyr-purrr" class="section level2">
<h2>Usando dplyr + tidyr + purrr</h2>
<p>Poder√≠amos ter realizado a mesma opera√ß√£o de forma expl√≠cita com o aux√≠lio das fun√ß√µes <code>nest()</code>, <code>map()</code>, <code>mutate()</code> e <code>unnest()</code> dos pacotes <code>dplyr</code> <code>tidyr</code> e <code>purrr</code>, veja:</p>
<pre class="r"><code># Pacotes necess√°rios
library(tidyr)
library(purrr)

# Dados
data &lt;- tibble(x = c(&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;C&quot;, &quot;C&quot;),
               y = c(0,1,2,3,4,5)) 
# Codigos
data %&gt;%                                     # Input Data
  nest(-x) %&gt;%                               # Split
  mutate(data = map(data, ~mean(.x$y))) %&gt;%  # Apply
  unnest()                                   # Combine</code></pre>
<pre><code>## # A tibble: 3 x 2
##   x      data
##   &lt;chr&gt; &lt;dbl&gt;
## 1 A       0.5
## 2 B       2.5
## 3 C       4.5</code></pre>
<p>Note que obtemos a mesma sa√≠da do c√≥digo anterior</p>
<div id="split-apply-combine-com-fun√ß√µes-complexas" class="section level3">
<h3>Split-Apply-Combine com fun√ß√µes complexas</h3>
<p>Voc√™ deve estar se perguntando:</p>
<p>‚Äú<em>T√°, eu tenho um atalho para usar a estrat√©gia ‚Äùsplit-apply-combine‚Äù com pacote <code>dplyr</code>, por que eu preciso usar os dados aninhados?</em>‚Äù</p>
<p>Trabalhar com dados aninhados permite aplicar qualquer tipo de fun√ß√£o em parti√ß√µes do conjunto de dados e juntar os resultados em um objeto do tipo <a href="https://tibble.tidyverse.org/"><code>tibble</code></a> cujo <code>print()</code> √© um <em>‚Äúm√©todo aprimorado que os torna mais f√°ceis de usar com grandes conjuntos de dados contendo objetos complexos‚Äù</em>.</p>
<p>Veja o seguinte exemplo:</p>
<p>Primeiramente, imagine que voc√™ queira calcular a m√©dia de <code>mpg</code> por <code>cyl</code> dos dados <code>mtcars</code> (nativos do R), bastaria utilizar a sequ√™ncia de c√≥digos:</p>
<pre class="r"><code>mtcars %&gt;%                     # input data
  group_by(cyl) %&gt;%            # split
  summarise(media = mean(mpg)) # apply/combine</code></pre>
<pre><code>## # A tibble: 3 x 2
##     cyl media
##   &lt;dbl&gt; &lt;dbl&gt;
## 1     4  26.7
## 2     6  19.7
## 3     8  15.1</code></pre>
<p>Vejamos a seguir o uso da estrat√©gia em situa√ß√µes mais complexas</p>
<div id="em-ajustes-de-modelos" class="section level4">
<h4>Em ajustes de modelos</h4>
<p>E se precis√°ssemos calcular algo mais elaborado, como por exemplo ajustar <span class="math inline">\(k=3\)</span> regress√µes lineares: <span class="math inline">\(y_k= b_{0_k} + b_{1_k}*x_k\)</span> (com <span class="math inline">\(y_k=\)</span> <code>mpg</code>, <span class="math inline">\(x_k=\)</span><code>disp</code> para cada <span class="math inline">\(k=\)</span><code>cyl</code>) para estudar os coeficientes estimados, o que aconteceria se utiliz√°ssemos o c√≥digo abaixo ?</p>
<p><em>Spoiler</em>: Note que pelo fato da sa√≠da da fun√ß√£o <code>lm</code> n√£o retornar apenas uma √∫nica vari√°vel para sumarizar obteremos um <code>Error</code>:</p>
<pre class="r"><code>mtcars %&gt;%                       # input data
  group_by(cyl) %&gt;%              # split
  summarise(lm = lm(mpg ~ disp)) # apply/combine</code></pre>
<pre><code>## Error: Problem with `summarise()` input `lm`.
## x Input `lm` must be a vector, not a `lm` object.
## ‚Ñπ Input `lm` is `lm(mpg ~ disp)`.
## ‚Ñπ The error occurred in group 1: cyl = 4.</code></pre>
<p>O erro nos diz: ‚Äú<em>A coluna <code>lm</code> deve ter o comprimento 1, n√£o 12</em>‚Äù ou seja, o resultado precisa ser um valor de resumo e n√£o todo o resultado do ajuste dos modelos.</p>
<p>Agora vejamos utilizando a abordagem <code>split-apply-combine</code> que ir√° nos permitir aplicar qualquer tipo de fun√ß√£o nos dados agrupados por pela vari√°vel <code>cyl</code>:</p>
<pre class="r"><code>as_tibble(mtcars) %&gt;%                                                      # input data
  nest(-cyl) %&gt;%                                                           # split
  mutate(lm = map(data, ~lm(mpg ~ disp, data = .x) %&gt;% broom::tidy())) %&gt;% # apply
  unnest(lm)                                                               # combine</code></pre>
<pre><code>## # A tibble: 6 x 7
##     cyl data               term        estimate std.error statistic    p.value
##   &lt;dbl&gt; &lt;list&gt;             &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
## 1     6 &lt;tibble [7 √ó 10]&gt;  (Intercept) 19.1       2.91        6.55  0.00124   
## 2     6 &lt;tibble [7 √ó 10]&gt;  disp         0.00361   0.0156      0.232 0.826     
## 3     4 &lt;tibble [11 √ó 10]&gt; (Intercept) 40.9       3.59       11.4   0.00000120
## 4     4 &lt;tibble [11 √ó 10]&gt; disp        -0.135     0.0332     -4.07  0.00278   
## 5     8 &lt;tibble [14 √ó 10]&gt; (Intercept) 22.0       3.35        6.59  0.0000259 
## 6     8 &lt;tibble [14 √ó 10]&gt; disp        -0.0196    0.00932    -2.11  0.0568</code></pre>
<p>Com o aux√≠lio do pacote <a href="https://cran.r-project.org/web/packages/broom/vignettes/broom.html"><code>broom</code></a> obtemos sa√≠das de dados arrumados e juntamos os resultados finais da regress√£o em uma √∫nica tabela de maneira pr√°tica.</p>
</div>
<div id="na-constru√ß√£o-de-gr√°ficos" class="section level4">
<h4>Na constru√ß√£o de gr√°ficos</h4>
<p>Veja um outro exemplo de uso aplicando uma fun√ß√£o para criar gr√°ficos, agora com ggplot:</p>
<pre class="r"><code>library(ggplot2)
library(gridExtra)

plot_list &lt;- 
  mtcars %&gt;%      # input data
  nest(-cyl) %&gt;%  # split/apply ‚Üì
  mutate(plots = map(data, ~ggplot(.x, aes(x=disp, y=mpg))+geom_point()+geom_smooth(method = &quot;lm&quot;))) %$% 
  plots # magrittr

# Combine para printar:
invoke(grid.arrange,plot_list, ncol=1) # ou: grid.arrange(grobs = plot_list, ncol=1)

# Combine para salvar:
walk2(paste0(&quot;plot&quot;,1:3,&quot;.png&quot;), plot_list, ~ggsave(.x,.y))</code></pre>
<p><img src="/post/2019-04-05-split-apply-combine/unnamed-chunk-6-1.png" /></p>
</div>
<div id="criando-tabelas" class="section level4">
<h4>Criando tabelas</h4>
<p>Por fim, um exemplo utilizando o pacote flextable.</p>
<p>Utilizaremos a fun√ß√£o <a href="https://github.com/gomesfellipe/functions/blob/master/flextable_custom.R"><code>flextable_custom()</code></a> que adaptei para gerar uma tabela j√° customizada com o pacote flextable e a fun√ß√£o <a href="https://github.com/gomesfellipe/functions/blob/master/save_flextable.R"><code>save_flextable()</code></a> inspirada em uma pergunta que fiz no stackoverflow sobre <a href="https://stackoverflow.com/questions/50225669/how-to-save-flextable-as-png-in-r">Como salvar uma tabela flextable como png no R?</a>.</p>
<p>Veja:</p>
<pre class="r"><code>library(flextable)
source(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/flextable_custom.R&quot;)
source(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/save_flextable.R&quot;)

tabela_list &lt;- 
  head(mtcars,7) %&gt;%           # input data
  nest(-cyl) %$% data %&gt;%      # apply                                       
  map(~flextable_custom(.x))   # apply / combine

# Veja a tabela:
tabela_list[[1]]

# Combine para salvar:
walk2(paste0(&quot;tab&quot;,1:3,&quot;.png&quot;), tabela_list, ~save_flextable(.y,.x))</code></pre>
<p><img src="/post/2019-04-05-split-apply-combine/img1.png" /></p>
</div>
</div>
</div>
</div>
<div id="conclus√£o" class="section level1">
<h1>Conclus√£o</h1>
<p>Vimos aqui como funciona a estrat√©gia e alguns exemplos de uso, por√©m, existem infinitas outras aplica√ß√µes para esse tipo de abordagem com os dados arrumados. Dependendo da tarefa esta abordagem pode ser bem produtiva e poupar muitas linhas de c√≥digo!</p>
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias</h1>
<p>Al√©m das refer√™ncias deixarem aqui algumas sugest√µes de leitura:</p>
<ul>
<li><a href="https://github.com/tidyverse/purrr" class="uri">https://github.com/tidyverse/purrr</a></li>
<li><a href="https://tibble.tidyverse.org/" class="uri">https://tibble.tidyverse.org/</a></li>
<li><a href="https://vita.had.co.nz/papers/plyr.pdf" class="uri">https://vita.had.co.nz/papers/plyr.pdf</a></li>
<li><a href="https://adv-r.hadley.nz/functionals.html#purrr-style" class="uri">https://adv-r.hadley.nz/functionals.html#purrr-style</a></li>
<li><a href="https://davisvaughan.github.io/furrr/" class="uri">https://davisvaughan.github.io/furrr/</a></li>
</ul>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2019-04-05-split-apply-combine/split-apply-combine/">Hackeando o R: estrat√©gia split-apply-combine</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category>Texto e NLP</category>
      <category domain="tag">data-mining</category>
      <category domain="tag">estatistica</category>
      <category domain="tag">flextable</category>
      <category domain="tag">pratica</category>
      <category domain="tag">r</category>
      <category domain="tag">rstudio</category>
      <category domain="tag">split-aply-combine</category>
      <category domain="tag">tabelas</category>
    </item>
    <item>
      <title>Seu app, RStudio e Shiny Server na nuvem do Google</title>
      <link>https://gomesfellipe.github.io/post/2018-10-27-server-cloud/server-cloud/</link>
      <pubDate>Sat, 27 Oct 2018 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2018-10-27-server-cloud/server-cloud/</guid>
      <description>Uma maneira pr√°tica de hospedar o app desenvolvido no post no Shiny Server e ter seu pr√≥prio RStudio Server na nuvem do Google em uma m√°quina virtual Ubuntu 16.04.</description>
      <content:encoded>&lt;![CDATA[
        


<div id="objetivo-do-post" class="section level1">
<h1>Objetivo do post <i class="fa fa-rocket"></i></h1>
<p>Uma das v√°rias maneiras de se implementar o <a href="https://www.rstudio.com/products/rstudio/download-server/">RStudio Server</a> e o <a href="https://www.rstudio.com/products/shiny/download-server/">Shiny Server</a> √© atrav√©s de servi√ßos de nuvem que fornecem m√°quinas virtuais. Empresas gigantes no mercado como Amazon Web Services (AWS), Microsoft, Google, IBM, Oracle etc t√™m investido pesado nestes servi√ßos e a escolha de qual cloud utilizar deve ser feita de acordo com a necessidade do usu√°rio pois cada uma delas oferecem diferentes pre√ßos com diferentes custos/benef√≠cios.</p>
<p>No final deste post teremos al√©m do nosso pr√≥prio RStudio Server, esse app no Shiny Server que exibe em um gr√°fico das cota√ß√µes encontradas via fun√ß√£o fun√ß√£o <a href="https://www.rdocumentation.org/packages/TTR/versions/0.23-4/topics/stockSymbols"><code>quantmod::stockSymbols()</code></a> com os pacotes <a href="http://jkunst.com/highcharter/"><code>highcharter</code></a> e <a href="https://www.quantmod.com/"><code>quantmod</code></a>:</p>
<p><img src="/post/2018-10-27-server-cloud/server-cloud_files/demoapp.gif" /></p>
<p><strong>Nota</strong>: Algum conhecimento em Linux e Git pode ser √∫til, particularmente ainda tenho muito a aprender sobre ambos e devem existir outras maneiras mais eficientes de se realizar estas tarefas. Essa √© uma forma relativamente simples e ao final do post deixarei os links que utilizei como refer√™ncia para escrever este post.</p>
<p><strong>Nota¬≤</strong>, segundo o desenvolvedor do pacote <code>highcharter</code>: ‚Äú<em>Highcharts (www.highcharts.com) √© um produto de software da Highsoft que n√£o √© livre para uso comercial e governamental</em>‚Äù.</p>
</div>
<div id="gloogle-cloud" class="section level1">
<h1>Gloogle cloud <i class="fa fa-cloud"></i></h1>
<p>A servi√ßo que escolhi para este exemplo foi o <a href="https://cloud.google.com/">Google Cloud</a>, que oferece um <a href="https://cloud.google.com/free/docs/frequently-asked-questions">per√≠odo de um ano de Teste Gr√°tis com $300</a> para utilizar nos servi√ßos <a href="https://cloud.google.com/free/docs/frequently-asked-questions?hl=pt_BR&amp;_ga=2.209128081.-2022746393.1538697551&amp;_gac=1.3934980.1540514862.EAIaIQobChMImJjS9vCi3gIVEgaRCh0j3ArWEAAYASAAEgJ8AfD_BwE#always-free">dentre outras op√ß√µes</a>, mas isso n√£o quer dizer que seja a melhor de todas, sinta-se a vontade para utilizar os mesmos passos apresentados aqui nos demais servi√ßos, entendendo, claro, suas peculiaridades.</p>
<p>Dentre algumas vantagens sobre o uso de nuvem, temos:</p>
<ul>
<li>Maior produtividade, pois possibilita r√°pido acesso √† sua √°rea de trabalho;</li>
<li>Permite o desenvolvedor trabalhar de qualquer lugar com acesso √† internet;</li>
<li>Permite colabora√ß√£o r√°pida, pois os membros da equipe podem contribuir e acessar projetos ao mesmo tempo em que os dados s√£o armazenados na nuvem em vez de em seus computadores</li>
<li>Protegido pelos principais especialistas seguran√ßa do Google;</li>
<li>Controle e flexibilidade, o desenvolvedor tem controle sobre a tecnologia que deseja usar, sobre os dados e caso decida n√£o usar mais o servi√ßo, √© poss√≠vel retirar seus dados da nuvem do Google.</li>
</ul>
<div id="compute-engine" class="section level2">
<h2>Compute Engine <img src="/post/2018-10-27-server-cloud/server-cloud_files/compute_engine.png" style="width:5.0%" /></h2>
<p>Compute Engine √© um dos produtos oferecidos pelo Google. Tamb√©m conhecido como <a href="https://pt.wikipedia.org/wiki/Infraestrutura_como_servi%C3%A7o">infraestrutura como servi√ßo (IaaS)</a>, pode ser usada para executar altas cargas de trabalho em larga escala em m√°quinas virtuais, no nosso caso este produto que possibilita o uso das m√°quinas virtuais.</p>
</div>
</div>
<div id="iniciar-uma-inst√¢ncia-na-google-cloud-platform" class="section level1">
<h1>Iniciar uma inst√¢ncia na Google Cloud Platform</h1>
<p>Primeiramente, acesse <a href="https://cloud.google.com/" class="uri">https://cloud.google.com/</a> e se inscreva no recurso de nuvem, ap√≥s isso acesse <a href="https://console.cloud.google.com" class="uri">https://console.cloud.google.com</a> e percorra o caminho:</p>
<p><code>Menu de navega√ß√£o</code> ‚Üí <code>Compute Engine</code> ‚Üí <code>Inst√¢ncias de VMs</code></p>
<p>Nesta p√°gina existe o <img src="/post/2018-10-27-server-cloud/server-cloud_files/button_criar_instancia.png" />, clique nele para iniciar a configura√ß√£o da VM. Na p√°gina que abrir ser√° solicitado a dar um nome a sua VM (pode ser qualquer nome que te agrade). O Google tem data centers em quase todo o mundo e para este caso selecionei a <code>Regi√£o</code> <em>us-east1 (Carolina do Sul)</em> e a <code>Zona</code> deixei o default <em>us-east1-b</em>.<a href="https://cloud.google.com/free/docs/frequently-asked-questions?hl=pt_BR&amp;_ga=2.183970469.-2022746393.1538697551&amp;_gac=1.36440980.1540514862.EAIaIQobChMImJjS9vCi3gIVEgaRCh0j3ArWEAAYASAAEgJ8AfD_BwE#always-free">‚Äúsaiba mais‚Äù do Google</a>.</p>
<p>Dependendo de qu√£o poderoso o servidor precisa ser √© poss√≠vel selecionar a capacidade da RAM e do disco r√≠gido na se√ß√£o <code>Tipo de M√°quina</code>, como este √© um exemplo para prova de conceito selecionei a op√ß√£o <em>micro(1 vCPU compartilhado) - 0.6 GB de mem√≥ria, f1-micro</em> pois custa cerca de $ 0,006 por hora e as ‚Äúprimeiras 744 horas para uso da inst√¢ncia de tipo f1-micro s√£o gratuitas‚Äù, segundo o Google. No <code>Disco de inicializa√ß√£o</code> selecionei a <code>Imagen do SO</code> <em>Ubuntu 16.04 LTS</em> que tenho um pouco mais de familiaridade e n√£o mexi nas demais configura√ß√µes dessa se√ß√£o, mas antes de finalizar, selecione a op√ß√£o <em>Permitir tr√°fego HTTP</em> na op√ß√£o de <code>Firewall</code> e clique no bot√£o <code>criar</code>.</p>
<p>Segundo o guia <a href="https://support.rstudio.com/hc/en-us/articles/200552306-Getting-Started">Acessando o RStudio Server Open-Source</a> e o <a href="http://rstudio.github.io/shiny-server/os/0.4.0/">Guia do Administrador - Shiny Server Open Source v0.4.1</a>o RStudio Server e o Shiny Server ser√£o escutados nas portas 8787 e 3838 respectivamente, portanto, antes de come√ßar a usar a VM √© necess√°rio Configurando regras de firewall para permitir acesso atrav√©s destas portas.</p>
<p>Navegue at√© a se√ß√£o <code>Rede VPC</code> e selecione a op√ß√£o <code>Regras de firewall</code>. Clique em <img src="/post/2018-10-27-server-cloud/server-cloud_files/criar_firewall.png" />. No formul√°rio que abrir d√™ um nome e uma descri√ß√£o √† regra do firewall (‚Äòrstudio‚Äô est√° bom), deixe os bot√µes de op√ß√£o <code>Dire√ß√£o de tr√°fego</code> e <code>A√ß√£o ao corresponder</code> como padr√£o (<em>Entrada</em> e <em>Permitir</em>). Para a op√ß√£o <code>Destinos</code>, escolhi <em>Conta de servi√ßo especificado</em>, sendo a <code>Conta de servi√ßo de destino</code> <em>Compute Engine default service account</em>. Se voc√™ quiser filtrar os IPs que podem acessar o aplicativo, insira-o no campo <code>Intervalos de IP de origem</code>, como estamos sob uma prova de conceito, inseri a rota quad-zero: 0.0.0.0/0, o que permite acesso de todos os IPs em todas as portas nessa m√°quina. Na op√ß√£o <code>Protocolos e Portos</code> selecione o bot√£o de op√ß√£o <em>Protocolos e portas espec√≠ficas</em> e no campo digite o tcp: 8787. Por fim, clique no bot√£o <code>criar</code>.</p>
<p>Fa√ßa o procedimento exatamente da mesma maneira, mas com um novo nome (‚Äòshiny‚Äô est√° bom) e digite ‚Äòtcp: 3838‚Äô em vez de ‚Äòtcp: 8787‚Äô. Clique em <code>criar</code> e note que agora abrimos duas portas para usar no aplicativo.</p>
<p>Nota: <em>Os passos para iniciar uma inst√¢ncia na Cloud do Google foram inspirados no seguinte post:</em> <a href="https://datarunsdeep.com.au/blog/building-r-shiny-app-google-cloud-display-bigquery-data" class="uri">https://datarunsdeep.com.au/blog/building-r-shiny-app-google-cloud-display-bigquery-data</a></p>
</div>
<div id="inicie-a-m√°quina-virtual" class="section level1">
<h1>Inicie a m√°quina virtual <i class="fa fa-desktop"></i></h1>
<p>√ìtimo! Agora a VM j√° deve estar pronta para uso e o jeito mais simples de utiliz√°-la √© clicando no bot√£o <img src="/post/2018-10-27-server-cloud/server-cloud_files/ssh.png" /> na p√°gina de <code>Inst√¢ncias de VMs</code> e aguardar a m√°quina iniciar</p>
<center>
<img src="/post/2018-10-27-server-cloud/server-cloud_files/vm_on.gif" style="width:80.0%" />
</center>
<p>Vamos √†s configura√ß√µes!</p>
<div id="obter-senha-para-super-usu√°rio" class="section level2">
<h2>Obter senha para super usu√°rio</h2>
<p>O primeiro passo ser√° obter a senha para usu√°rio <code>root</code></p>
<pre><code>sudo passwd</code></pre>
<p>Ap√≥s determinar a senha, alterne para este usu√°rio e conceda a permiss√£o de super usu√°rio para evitar trabalhar como <code>root</code>:</p>
<pre><code>su root
passwd gomes_fellipe1
gpasswd -a gomes_fellipe1 sudo
su gomes_fellipe1</code></pre>
<p>Agora meu usu√°rio tem permiss√£o de super usu√°rio e uma senha para utilizar para acessar o RStudio Server.</p>
</div>
<div id="exibir-sua-vm-em-um-navegador" class="section level2">
<h2>Exibir sua VM em um navegador</h2>
<p>Se voc√™ utilizar o comando</p>
<pre><code>curl ipinfo.io/ip</code></pre>
<p>o endere√ßo IP externo ser√° exibido no console, por√©m se digitar este caminho no browser ainda n√£o ser√° poss√≠vel acessar sua m√°quina. Para isso, instale o <a href="https://www.nginx.com/">Nginx</a> (que √© um servidor web leve) com os comandos:</p>
<pre><code>sudo apt-get update
sudo apt-get -y install nginx</code></pre>
<p>Agora se voc√™ digitar o endere√ßo IP externo da sua m√°quina no browser ser√° poss√≠vel ver a tela de sauda√ß√£o do nginx!</p>
</div>
<div id="criar-um-arquivo-swap" class="section level2">
<h2>Criar um arquivo swap</h2>
<p>Seguindo a recomenda√ß√£o do post <a href="http://www.simoncoulombe.com/2018/05/07/protected_free_shiny/">Implantando um servidor Shiny Server e RStudio seguro em uma m√°quina virtual gratuita do Google Cloud</a> e as instru√ß√µes do link <a href="https://digitizor.com/create-swap-file-ubuntu-linux/" class="uri">https://digitizor.com/create-swap-file-ubuntu-linux/</a>, podemos criar um arquivo <a href="https://pt.wikipedia.org/wiki/Mem%C3%B3ria_virtual">swap</a> pois a inst√¢ncia <em>f1-micro</em> n√£o tem RAM suficiente e pode ser que alguns pacotes n√£o possam ser instalados por falta de mem√≥ria.</p>
<pre><code>cd /
sudo dd if=/dev/zero of=swapfile bs=1M count=3000
sudo mkswap swapfile
sudo swapon swapfile
sudo nano etc/fstab

# Adicione a linha a seguir no arquivo que abrir
/swapfile none swap sw 0 0 
cat /proc/meminfo</code></pre>
</div>
<div id="instalar-o-r" class="section level2">
<h2>Instalar o R</h2>
<p>Para instalar o R apenas segui os passos apresentados em <a href="https://deanattali.com/2015/05/09/setup-rstudio-shiny-server-digital-ocean/" class="uri">https://deanattali.com/2015/05/09/setup-rstudio-shiny-server-digital-ocean/</a> com os c√≥digos:</p>
<pre><code>sudo sh -c &#39;echo &quot;deb http://cran.rstudio.com/bin/linux/ubuntu xenial/&quot; &gt;&gt; /etc/apt/sources.list&#39;

gpg --keyserver keyserver.ubuntu.com --recv-key E084DAB9
gpg -a --export E084DAB9 | sudo apt-key add -

sudo apt-get update
sudo apt-get -y install r-base</code></pre>
<p>Agora o R est√° pronto para uso, basta escrever <code>R</code> no console para inici√°-lo.</p>
<center>
<img src="/post/2018-10-27-server-cloud/server-cloud_files/R.png" />
</center>
<div id="depend√™ncias-e-pacotes" class="section level3">
<h3>Depend√™ncias e pacotes</h3>
<p>Antes de instalar outros pacotes podemos iniciar instalando o <code>devtools</code> pois esse pacote permite instalar a vers√£o do desenvolvedores de alguns pacotes do R que est√£o dispon√≠veis no GitHub. Para instalar as depend√™ncias do <code>devtools</code>:</p>
<pre><code>sudo apt-get -y install libcurl4-gnutls-dev libxml2-dev libssl-dev</code></pre>
<p>Tudo certo, agora j√° √© poss√≠vel instalar pacotes tanto do <a href="https://cran.r-project.org/">CRAN</a> quando do <a href="www.github.com">GitHub</a>. Al√©m do <code>devtools</code> instalaremos mais dois pacotes que ser√£o utilizados pelo aplicativo exemplo deste post:</p>
<pre><code>sudo su - -c &quot;R -e \&quot;install.packages(c(&#39;devtools&#39;,&#39;rmarkdown&#39;, &#39;quantmod&#39;), repos=&#39;http://cran.rstudio.com/&#39;)\&quot;&quot;
sudo su - -c &quot;R -e \&quot;devtools::install_github(&quot;jbkunst/highcharter&quot;)\&quot;&quot;</code></pre>
<p><strong>Importante:</strong> Note que ao instalar os pacotes do R no terminal os pacotes s√£o instalados como usu√°rio <code>root</code>, o que significa que os pacotes ser√£o instalados em uma biblioteca global e estar√£o dispon√≠veis para todos os usu√°rios da m√°quina.</p>
</div>
</div>
<div id="instalar-o-rstudio-server" class="section level2">
<h2>Instalar o RStudio Server</h2>
<p>Neste ponto j√° temos uma m√°quina virtual como R instalado e estamos preparados para dar in√≠cio ao nosso <strong>RStudio Server</strong>! Para instalar entre na p√°gina de download do RStudio em: <a href="https://www.rstudio.com/products/rstudio/download-server/" class="uri">https://www.rstudio.com/products/rstudio/download-server/</a> e siga os passos de instala√ß√£o. No meu caso bastou rodar:</p>
<pre><code>sudo apt-get install gdebi-core
wget https://download2.rstudio.org/rstudio-server-1.1.456-amd64.deb
sudo gdebi rstudio-server-1.1.456-amd64.deb</code></pre>
<p>Pronto, agora j√° possu√≠mos nosso pr√≥prio RStudio Server na nuvem! Para acess√°-lo basta utilizar a porta <code>8787</code> atrav√©s de seu IP externo (que pode ser obtido com o comando: <code>curl ipinfo.io/ip</code>, como mencionado anteriormente). Portanto, acesse algo como: <a href="http://35.237.234.123:8787" class="uri">http://35.237.234.123:8787</a> (este link direcionar√° para a m√°quina virtual que criei no exemplo deste post) e voc√™ ser√° direcionado para uma tela de login:</p>
<center>
<img src="/post/2018-10-27-server-cloud/server-cloud_files/rstudioserver.png" style="width:80.0%" />
</center>
<p>√â poss√≠vel fazer o login no RStudio Server com qualquer usu√°rio que esteja cadastrado em sua m√°quina virtual, no meu caso eu fa√ßo o login com o nome de usu√°rio <code>gomes_fellipe1</code> e a senha que defini para meu usu√°rio ao criar a m√°quina.</p>
<p>√â poss√≠vel criar um novo usu√°rio para acessar o RStudio Server via terminal da m√°quina virtual com:</p>
<pre><code>adduser novo_usuario</code></pre>
<p>Caso seu objetivo seja apenas o de ter seu pr√≥prio RStudio Server Parab√©ns! J√° est√° dispon√≠vel e pronto para uso! A seguir trataremos de como instalar e como efetuar algumas configura√ß√µes b√°sicas de um Shiny Server, mesmo que voc√™ n√£o seja desenvolvedor desse tipo de app sugiro que instale o servidor mesmo assim pois ele tamb√©m pode ser usado para hospedar seus documentos interativos como arquivos Rmarkdown!</p>
</div>
<div id="instalar-o-shiny-server" class="section level2">
<h2>Instalar o Shiny Server</h2>
<p>Aplicativos Shiny s√£o incr√≠veis e sua aplicabilidade √© tamanha que poderia dedicar uma s√©rie de posts exclusivamente para falar sobre essa ferramenta. Dedico uma enorme parte do meu tempo de trabalho atualmente no desenvolvimento de aplicativos Shiny e ao terminar todas as etapas de um <a href="http://r4ds.had.co.nz/introduction.html">‚Äút√≠pico projeto de ciencia de dados‚Äù</a> a etapa da <strong>entrega</strong> dos resultados pode ser bastante cr√≠tica e um aplicativo Shiny √© capaz de trazer interatividade √†s suas an√°lises.</p>
<p>Existem diversas op√ß√µes de entrega, entre elas:</p>
<ul>
<li>Fazer um r√°pido deploy no <a href="http://www.shinyapps.io/" class="uri">http://www.shinyapps.io/</a>;</li>
<li>Hospedar no now, como apresentado na p√°gina do <a href="https://www.curso-r.com/">curso-r</a> em: <a href="https://www.curso-r.com/blog/2018-03-05-shiny-now/" class="uri">https://www.curso-r.com/blog/2018-03-05-shiny-now/</a>;</li>
<li>Dockerizar o app com a imagem <a href="https://hub.docker.com/u/rocker/" class="uri">https://hub.docker.com/u/rocker/</a> e disponibiliz√°-lo na rede interna de sua empresa;</li>
<li>Criar um execut√°vel via R e Inno Setup (um instalador para programas do Windows), como ilustrado em: <a href="https://www.ficonsulting.com/filabs/RInno" class="uri">https://www.ficonsulting.com/filabs/RInno</a>;</li>
<li>Dentre tantas outras‚Ä¶</li>
</ul>
<p>No caso deste post, estamos criando nosso pr√≥prio servidor e assim teremos controle tanto do desempenho da m√°quina hospedada quanto do uso do Shiny Server propriamente dito.</p>
<p>Consulte a p√°gina de download do Shiny Server em: <a href="https://www.rstudio.com/products/shiny/download-server/" class="uri">https://www.rstudio.com/products/shiny/download-server/</a> para saber qual a vers√£o mais recente para instalar no seu computador (note que ser√° necess√°rio instalar o pacote do shiny como mencionado anteriormente), no meu caso foi:</p>
<pre><code>sudo su - -c &quot;R -e \&quot;install.packages(&#39;shiny&#39;, repos=&#39;https://cran.rstudio.com/&#39;)\&quot;&quot;
sudo apt-get install gdebi-core
wget https://download3.rstudio.org/ubuntu-14.04/x86_64/shiny-server-1.5.9.923-amd64.deb
sudo gdebi shiny-server-1.5.9.923-amd64.deb</code></pre>
<p>O Shiny Server est√° agora instalado e em execu√ß√£o. Supondo que correu tudo bem voc√™ j√° poder√° acessar a p√°gina inicial padr√£o do Shiny Server, que inclui algumas instru√ß√µes e dois aplicativos Shiny, veja em (na minha m√°quina de exemplo): <a href="http://35.237.234.123:3838" class="uri">http://35.237.234.123:3838</a> (Note que eu n√£o configurei um IP externo fixo neste app por se tratar de uma prova de conceito, como ele pode alterar para um IP aleat√≥rio quando eu reiniciar a m√°quina ent√£o fiz um backup no <a href="http://www.shinyapps.io/" class="uri">http://www.shinyapps.io/</a> no endere√ßo: <a href="https://gomesfellipe.shinyapps.io/app_acoes/" class="uri">https://gomesfellipe.shinyapps.io/app_acoes/</a> para manter este DEMO no caso dessa m√°quina espec√≠fica n√£o estar mais no ar futuramente)</p>
<p><img src="/post/2018-10-27-server-cloud/server-cloud_files/shinyserver.png" /></p>
<p>Alguns comandos √∫teis para configurar o status do servidor:</p>
<pre><code>sudo systemctl start shiny-server
sudo systemctl stop shiny-server
sudo systemctl restart shiny-server</code></pre>
<p><strong>Importante</strong>:</p>
<ul>
<li>O arquivo de configura√ß√£o do servidor est√° em: <code>/etc/shiny-server/shiny-server.conf</code>;</li>
<li>A p√°gina inicial do Shiny Server estar√° em: <code>/srv/shiny-server/index.html</code> (normalmente removemos esta p√°gina ou incluir um outro html para customiz√°-la);</li>
<li>Todos aplicativos Shiny com o nome <code>app.R</code> que voc√™ colocar em <code>/srv/shiny-server/</code> ser√£o entendidos pelo servidor como um aplicativo Shiny;</li>
<li>Todos aplicativos ser√£o executados como usu√°rio <code>shiny</code> o que implica que qualquer pacote necess√°rio em um aplicativo shiny dever√° estar na biblioteca global ou na biblioteca do usu√°rio <code>shiny</code>. Existem diversas formas de lidar com esse aspecto da configura√ß√£o inicial do servidor ent√£o deixarei algumas refer√™ncias ao final do post para quem tiver a necessidade de maior customiza√ß√£o e controle do servidor;</li>
<li>Para visualizar os logs durante o desenvolvimento do aplicativo ser√° necess√°rio adicionar as linhas <code>preserve_logs true;</code>e <code>sanitize_errors false;</code> ao arquivo de configura√ß√£o do servidor (e reiniciar o servidor em seguida) e assim os logs do servidor estar√£o dispon√≠veis em <code>/var/log/shiny-server.log</code>.</li>
</ul>
</div>
<div id="adicionar-aplicativos-ao-shiny-server-via-github" class="section level2">
<h2>Adicionar aplicativos ao Shiny Server via Github</h2>
<p>Como j√° foi dito, qualquer aplicativo Shiny colocado na pasta <code>/srv/shiny-server/</code> ser√° automaticamente exibido como um Shiny app na p√°gina principal do servidor. Al√©m da op√ß√£o de desenvolver um um aplicativo diretamente pelo RStudio Server e salvar nesta pasta podemos clonar algum app hospedado no <a href="https://github.com/">GitHub</a> ou no <a href="https://bitbucket.org">BitBucket</a> para esta pasta.</p>
<p>Primeiramente vamos instalar o git e realizar as configura√ß√µes iniciais:</p>
<pre><code>sudo apt-get -y install git
git config --global user.email &quot;you@example.com&quot;
git config --global user.name &quot;Your Name&quot;</code></pre>
<p>Agora vamos nos deslocar para a pasta <code>srv/shiny-server/</code> e remover a p√°gina <code>index.html</code> pois n√£o estamos mais interessados em ver a p√°gina de exemplo do Shiny Server:</p>
<pre><code>cd /srv/shiny-server
sudo rm index.html</code></pre>
<p>Em seguida j√° √© poss√≠vel clonar seu reposit√≥rio do <a href="https://github.com/">GitHub</a> ou do <a href="https://bitbucket.org">BitBucket</a> diretamente para esta pasta (Lembrete: o arquivo que executa o app deve ser nomeado como <code>app.R</code>!), veja:</p>
<pre><code>sudo git clone https://github.com/gomesfellipe/app_acoes.git</code></pre>
<p>O legal de administrar suas aplica√ß√µes via <a href="https://github.com/">GitHub</a> ou do <a href="https://bitbucket.org">BitBucket</a> √© que o c√≥digo hospedado pode ser trabalhado e ‚Äúcommitado‚Äù de qualquer outro lugar do mundo e para atualizar no servidor basta utilizar o comando <code>sudo git pull</code> na pasta do app que o diret√≥rio local ser√° atualizado com as modifica√ß√µes realizadas no reposit√≥rio remoto.</p>
<p>Pronto, se utilizar o comando <code>ls</code> j√° ser√° poss√≠vel ver a pasta clonada no diret√≥rio local:</p>
<center>
<img src="/post/2018-10-27-server-cloud/server-cloud_files/ls.png" />
</center>
<p>Ao acessar o app teremos:</p>
<center>
<iframe width="100%" height="725" scrolling="no" frameborder="no" src="https://gomesfellipe.shinyapps.io/app_acoes/">
</iframe>
</center>
<p>O c√≥digo para gerar este app √©:</p>
<script src="https://gist.github.com/gomesfellipe/f1f6469884bff2d2dce8729fd8bce719.js"></script>
<p><strong>Nota¬π</strong>: O c√≥digo do app e as depend√™ncias podem ser obtidas em: <a href="https://github.com/gomesfellipe/app_acoes" class="uri">https://github.com/gomesfellipe/app_acoes</a> e para execut√°-lo diretamente do seu terminal basta utilizar o comando:</p>
<pre class="r"><code>shiny::runGitHub(&quot;gomesfellipe/app_acoes&quot;)</code></pre>
<p><strong>Nota¬≤</strong>: Caso queira incluir seu aplicativo Shiny em alguma p√°gina Web ou relat√≥rio √© simples, utilizando um pouco de html √© s√≥ o inclu√≠ no seu arquivo RMarkdown utilizando algo parecido com:</p>
<pre><code>&lt;iframe width=&quot;450&quot; height=&quot;400&quot; scrolling=&quot;no&quot; frameborder=&quot;no&quot; src=&quot;sua_url&quot;&gt; &lt;/iframe&gt;</code></pre>
</div>
</div>
<div id="tudo-pronto" class="section level1">
<h1>Tudo Pronto? <i class="fa fa-exclamation-triangle"></i></h1>
<p>Definitivamente n√£o! Implementamos um app bem b√°sico mas existem intermin√°veis desafios que n√£o foram abordados nesse r√°pido post como a instala√ß√£o do Latex para o Shiny Server, o uso de cont√™ineres Docker, senha para os aplicativos shiny, definir um IP est√°tico, todas as outras depend√™ncias que podem ser requisitadas na instala√ß√£o de pacotes espec√≠ficos etc.. √â aquela hist√≥ria, quanto mais a gente estuda mais coisa aparece para estudar mas esse √© justamente o legal de tudo, que nunca fica f√°cil!</p>
</div>
<div id="refer√™ncias-e-sugest√µes" class="section level1">
<h1>Refer√™ncias e sugest√µes:</h1>
<ul>
<li><a href="https://cloud.google.com/getting-started/?hl=pt-br" class="uri">https://cloud.google.com/getting-started/?hl=pt-br</a></li>
<li><a href="https://support.rstudio.com/hc/en-us/articles/200552306-Getting-Started" class="uri">https://support.rstudio.com/hc/en-us/articles/200552306-Getting-Started</a></li>
<li><a href="http://rstudio.github.io/shiny-server/os/0.4.0/" class="uri">http://rstudio.github.io/shiny-server/os/0.4.0/</a></li>
<li><a href="https://deanattali.com/2015/05/09/setup-rstudio-shiny-server-digital-ocean/" class="uri">https://deanattali.com/2015/05/09/setup-rstudio-shiny-server-digital-ocean/</a></li>
<li><a href="https://datarunsdeep.com.au/blog/building-r-shiny-app-google-cloud-display-bigquery-data" class="uri">https://datarunsdeep.com.au/blog/building-r-shiny-app-google-cloud-display-bigquery-data</a></li>
<li><a href="https://shiny.rstudio.com/gallery/authentication-and-database.html" class="uri">https://shiny.rstudio.com/gallery/authentication-and-database.html</a></li>
<li><a href="http://www.simoncoulombe.com/2018/05/07/protected_free_shiny/" class="uri">http://www.simoncoulombe.com/2018/05/07/protected_free_shiny/</a></li>
<li><a href="https://www.brettory.com/2018/02/embedding-a-shiny-app-in-blogdown/" class="uri">https://www.brettory.com/2018/02/embedding-a-shiny-app-in-blogdown/</a></li>
<li><a href="https://www.keycdn.com/support/413-request-entity-too-large" class="uri">https://www.keycdn.com/support/413-request-entity-too-large</a></li>
<li><a href="https://auth0.com/blog/adding-authentication-to-shiny-server/" class="uri">https://auth0.com/blog/adding-authentication-to-shiny-server/</a></li>
<li><a href="https://gist.github.com/jjesusfilho/6a3ec38016f7e120e9a2cf2b49e42962" class="uri">https://gist.github.com/jjesusfilho/6a3ec38016f7e120e9a2cf2b49e42962</a></li>
<li><a href="https://gist.github.com/jjesusfilho/7b7001745cbb8f7b1ad36e7bfe5d43e8" class="uri">https://gist.github.com/jjesusfilho/7b7001745cbb8f7b1ad36e7bfe5d43e8</a></li>
<li><a href="https://curso-r.github.io/auth0/" class="uri">https://curso-r.github.io/auth0/</a></li>
</ul>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2018-10-27-server-cloud/server-cloud/">Seu app, RStudio e Shiny Server na nuvem do Google</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">bitcoin</category>
      <category domain="tag">estatistica</category>
      <category domain="tag">google</category>
      <category domain="tag">google-cloud</category>
      <category domain="tag">pratica</category>
      <category domain="tag">r</category>
      <category domain="tag">rstudio</category>
      <category domain="tag">rstudio-server</category>
      <category domain="tag">series-temporais</category>
      <category domain="tag">servidor</category>
    </item>
  </channel>
</rss>