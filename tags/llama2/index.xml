&lt;?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>llama2 on Fellipe Gomes - Data Science Blog</title>
    <link>https://gomesfellipe.github.io/tags/llama2/</link>
    <description>√öltimos posts sobre Data Science, Machine Learning e R</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>pt-br</language>
    <managingEditor>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</managingEditor>
    <webMaster>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</webMaster>
    <lastBuildDate>Fri, 27 Sep 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://gomesfellipe.github.io/tags/llama2/" rel="self" type="application/rss+xml" />
    <item>
      <title>Extra√ß√£o de informa√ß√µes de imagens com IA Generativa</title>
      <link>https://gomesfellipe.github.io/post/2024-09-27-image-text-to-text/</link>
      <pubDate>Fri, 27 Sep 2024 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2024-09-27-image-text-to-text/</guid>
      <description>Neste post, exploraremos como utilizar o modelo Llava para gerar r√≥tulos descritivos de imagens, usando dados do conjunto COCO-2017.</description>
      <content:encoded>&lt;![CDATA[
        


<div id="caso-de-uso-de-ia-generativa-extra√ß√£o-de-informa√ß√µes-de-imagens-com-o-modelo-llava" class="section level1">
<h1>Caso de Uso de IA Generativa: Extra√ß√£o de Informa√ß√µes de Imagens com o Modelo Llava</h1>
<p>GenAI refere-se a modelos de intelig√™ncia artificial capazes de gerar conte√∫do novo e criativo a partir de dados de entrada. Seu uso est√° revolucionando a maneira como processamos dados n√£o estruturados, como imagens, √°udios, textos, v√≠deos, etc. Trabalhar com modelos pr√©-treinados (i.e., que j√° foram treinados com grandes conjuntos de dados) e adapt√°-los para necessidades espec√≠ficas tem sido um divisor de √°guas.</p>
<p>Neste post, vamos explorar a utiliza√ß√£o do modelo Llava (Large Language and Vision Assistant) para extrair r√≥tulos descritivos de imagens e tamb√©m discutir como comparar a qualidade das previs√µes geradas com m√©tricas espec√≠ficas para avaliar a performance desse tipo de modelo.</p>
<div id="por-que-o-modelo-llava" class="section level2">
<h2>Por que o Modelo Llava?</h2>
<p>O modelo <a href="https://llava-vl.github.io/">Llava</a> √© uma alternativa de c√≥digo aberto ao <a href="https://chat-gpt-5.ai/capabilities-of-gpt-4v/">GPT-4 Vision</a> da OpenAI (que se destaca neste dom√≠nio, mas sua aplica√ß√£o √© restrita devido sua natureza propriet√°ria e comercial) que foi treinado em grandes conjuntos de dados multimodais, sendo capaz de compreender e gerar descri√ß√µes textuais para imagens.</p>
<p>Essa capacidade de ‚Äúconversar com imagens‚Äù tendo o mesmo ‚Äúpoder‚Äù de um LLM, possibilita seu uso em muitas solu√ß√µes desenvolvidas por cientistas de dados no mundo real, como:</p>
<ol style="list-style-type: decimal">
<li><strong>Classifica√ß√£o de produtos em e-commerce</strong>: gera√ß√£o de descri√ß√µes detalhadas de roupas, acess√≥rios, eletr√¥nicos, etc.</li>
<li><strong>Detec√ß√£o de defeitos em linhas de produ√ß√£o</strong>: identifica√ß√£o de falhas em produtos para automa√ß√£o e controle de qualidade.</li>
<li><strong>Diagn√≥stico m√©dico por imagens</strong>: auxiliar na detec√ß√£o precoce de doen√ßas a partir de descri√ß√µes detalhadas de imagens m√©dicas.</li>
<li><strong>Reconhecimento de placas de carros</strong>: transcri√ß√£o autom√°tica de textos de placas e caracter√≠sticas de ve√≠culos.</li>
<li><strong>Identifica√ß√£o de sinais de tr√¢nsito</strong>: aplica√ß√£o em ve√≠culos aut√¥nomos para navega√ß√£o e identifica√ß√£o de sinais.</li>
<li><strong>An√°lise de alimentos para calcular nutri√ß√£o</strong>: extra√ß√£o autom√°tica de informa√ß√µes nutricionais de fotos ou r√≥tulos de alimentos.</li>
<li><strong>Identifica√ß√£o de animais em c√¢meras de vida selvagem</strong>: gerar descri√ß√µes detalhadas de animais detectados, ajudando pesquisadores a automatizar o monitoramento da vida selvagem.</li>
<li><strong>Detec√ß√£o de aglomera√ß√µes em eventos</strong>: analisar imagens de c√¢meras de seguran√ßa para identificar a presen√ßa de grandes grupos de pessoas em eventos ou lugares p√∫blicos, √∫til em gest√£o de multid√µes ou para quest√µes de seguran√ßa.</li>
</ol>
</div>
<div id="dataset-coco-2017" class="section level2">
<h2>Dataset COCO-2017</h2>
<p>O <a href="https://cocodataset.org/">COCO</a> (Common Objects in Context) √© um dataset amplamente utilizado em vis√£o computacional. Ele √© um dos maiores conjuntos de imagens do dia a dia com objetos em diferentes contextos, com anota√ß√µes detalhadas fornecidas por humanos como tags, caixa delimitadora, pol√≠gono que segmenta a imagem detectando objetos bem como sua descri√ß√£o. Isso o torna ideal para testar o desempenho desse tipo de modelo para gera√ß√£o de legendas.</p>
<center>
<div style="display: flex; width: 100%;">
<div style="width: 50%;">
<p><img src="/post/2024-09-27-image-text-to-text/coco1.png" alt="Imagem 2" style="width: 100%;"></p>
</div>
<div style="width: 50%;">
<p><img src="/post/2024-09-27-image-text-to-text/coco2.png" alt="Imagem 2" style="width: 100%;"></p>
</div>
</div>
<center>
<small>
Imagem do COCO Dataset com e sem anota√ß√£o obtida na <a href="https://cocodataset.org/#explore">se√ß√£o explorat√≥ria</a> das imagens
</small>
</center>
</center>
</div>
</div>
<div id="preparando-o-ambiente" class="section level1">
<h1>Preparando o Ambiente</h1>
<p>Utilizei o ambiente do Kaggle para desenvolvimento deste notebook, que disponibiliza a utiliza√ß√£o de GPUs. Atrav√©s do Hardware Accelerator utilizaremos a <a href="https://www.kaggle.com/docs/efficient-gpu-usage">NVIDIA TESLA P100 GPU</a>.</p>
<details>
<summary>
<em>Expandir c√≥digo</em>
</summary>
<pre><code>%%capture
!pip -qqq install bitsandbytes accelerate rouge-score pycocoevalcap bert_score
!pip install -U nltk

import os
import re
import json
import pandas as pd
import numpy as np
from tqdm import tqdm

import seaborn as sns
import matplotlib.pyplot as plt

from PIL import Image
import requests
from io import BytesIO
from IPython.display import HTML
import base64

import torch
from transformers import pipeline, AutoProcessor, BitsAndBytesConfig

from nltk.translate.bleu_score import sentence_bleu
from rouge_score import rouge_scorer
from bert_score import score as bert_score
from nltk.translate.meteor_score import meteor_score

from transformers import logging
import warnings

logging.set_verbosity_error()
warnings.filterwarnings(&quot;ignore&quot;, &quot;use_inf_as_na&quot;)</code></pre>
</details>
<p><br></p>
</div>
<div id="carregar-dados" class="section level1">
<h1>Carregar dados</h1>
<p>Por fins de praticidade para este post, selecionei uma amostra de 10 imagens aleat√≥rias do dataset COCO - (Common Objects in Context) no site <a href="https://cocodataset.org" class="uri">https://cocodataset.org</a> (onde √© poss√≠vel ter uma descri√ß√£o detalhada do conjunto de dados, incluindo seu <a href="https://arxiv.org/abs/1405.0312">paper</a> para aprofundamento), para avaliar o desempenho do modelo.</p>
<details>
<summary>
<em>Expandir c√≥digo</em>
</summary>
<pre class="python"><code>df_sample = pd.DataFrame({
  &#39;coco_url&#39;: [
    &#39;http://images.cocodataset.org/train2017/000000058822.jpg&#39;,
    &#39;http://images.cocodataset.org/train2017/000000530396.jpg&#39;,
    &#39;http://images.cocodataset.org/train2017/000000097916.jpg&#39;,
    &#39;http://images.cocodataset.org/train2017/000000418492.jpg&#39;,
    &#39;http://images.cocodataset.org/train2017/000000022304.jpg&#39;,
    &#39;http://images.cocodataset.org/train2017/000000295999.jpg&#39;,
    &#39;http://images.cocodataset.org/train2017/000000406616.jpg&#39;,
    &#39;http://images.cocodataset.org/train2017/000000370926.jpg&#39;,
    &#39;http://images.cocodataset.org/train2017/000000005612.jpg&#39;,
    &#39;http://images.cocodataset.org/train2017/000000146436.jpg&#39;
  ],
  &#39;caption&#39;: [
    &#39;A laptop sitting on a desk with a cell phone and mouse.&#39;,
    &#39;A black bear walking through the grass field.&#39;,
    &#39;a person who is surfing in the ocean.&#39;,
    &#39;A young boy standing on a sandy beach holding a flag.&#39;,
    &#39;A man surfing on a wave in the ocean.&#39;,
    &#39;A herd of cows, grazing in a field.&#39;,
    &#39;There is a cutting board and knife with chopped apples and carrots.&#39;,
    &#39;A long yellow school bus is parked on a city street.\n&#39;,
    &#39;A black and white horse standing in the middle of a field.&#39;,
    &#39;A man in a red jacket looking at his phone.&#39;
    ]})
    
# Fun√ß√£o para verificar se o caminho √© uma URL
def is_url(path):
    return path.startswith(&#39;http://&#39;) or path.startswith(&#39;https://&#39;)

# Fun√ß√£o simplificada para gerar o thumbnail e convert√™-lo em base64 diretamente
def process_image(path):
    try:
        if is_url(path):
            # Se for uma URL, baixar a imagem
            response = requests.get(path)
            response.raise_for_status()  # Verifica se houve algum erro no download
            image = Image.open(BytesIO(response.content))  # Abrir a imagem do conte√∫do da resposta
        else:
            # Se for um caminho local, abrir a imagem diretamente
            image = Image.open(path)
        
        # Criar uma miniatura da imagem (thumbnail) com tamanho m√°ximo de 150x150
        image.thumbnail((150, 150), Image.LANCZOS)
        
        # Salvar a imagem em um buffer de mem√≥ria e convert√™-la para base64
        with BytesIO() as buffer:
            image.save(buffer, &#39;jpeg&#39;)
            image_base64 = base64.b64encode(buffer.getvalue()).decode()
        
        # Retornar a string HTML com a imagem embutida no formato base64
        return f&#39;&lt;img src=&quot;data:image/jpeg;base64,{image_base64}&quot;&gt;&#39;
    
    except Exception as e:
        # Em caso de erro, retornar uma string vazia ou uma mensagem de erro
        return f&quot;&lt;p&gt;Erro ao carregar imagem: {e}&lt;/p&gt;&quot;

# Aplicar o processamento de imagens diretamente no DataFrame
df_sample[&#39;image&#39;] = df_sample[&#39;coco_url&#39;].map(process_image)  # Pode ser URL ou caminho local

# Exibir as legendas e imagens formatadas em HTML
HTML(df_sample[[&#39;image&#39;, &#39;coco_url&#39;, &#39;caption&#39;]].head().to_html(escape=False))</code></pre>
</details>
<p><br></p>
<p><img src="/post/2024-09-27-image-text-to-text/df1.png" style="width: 100%;"></p>
<p>Caso voc√™ precise de mais imagens para testar, tamb√©m √© poss√≠vel encontrar uma <a href="https://www.kaggle.com/datasets/awsaf49/coco-2017-dataset/data">vers√£o disponibilizada no Kaggle</a> .</p>
</div>
<div id="carregar-modelo" class="section level1">
<h1>Carregar modelo</h1>
<p>Utilizaremos uma vers√£o de 7 bilh√µes de par√¢metros do modelo <a href="https://huggingface.co/llava-hf/llava-1.5-7b-hf">‚ÄúLLaVA 1.5‚Äù</a> (Language and Vision Assistant), dispon√≠vel no HuggingFace (Uma plataforma onde a comunidade de Machine Learning colabora com modelos, dados e aplica√ß√µes) treinada para tarefas de gera√ß√£o de texto a partir de imagens.</p>
<pre class="python"><code>%%time

model_id = &quot;llava-hf/llava-1.5-7b-hf&quot;

# Configura√ß√£o de quantiza√ß√£o do modelo, que permite reduzir o uso de mem√≥ria sem 
# comprometer muito a precis√£o. Aqui estamos configurando para usar quantiza√ß√£o em 4 bits.
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,  
    bnb_4bit_use_double_quant=True,  
    bnb_4bit_quant_type=&quot;nf4&quot;,  
    bnb_4bit_compute_dtype=torch.bfloat16  
)

# Cria√ß√£o de um pipeline de processamento de imagens para gera√ß√£o de texto
# O pipeline √© configurado para a tarefa &quot;image-to-text&quot;
pipe = pipeline(
    &quot;image-to-text&quot;, 
    model=model_id, 
    model_kwargs={
        &quot;quantization_config&quot;: quantization_config,
        &quot;low_cpu_mem_usage&quot;: True
    }
)

# Carregar o processador associado respons√°vel por pr√©-processar
# as imagens de entrada e preparar os dados para serem inseridos no modelo
processor = AutoProcessor.from_pretrained(model_id)</code></pre>
<pre><code>CPU times: user 28.7 s, sys: 28.1 s, total: 56.8 s
Wall time: 6min 26s</code></pre>
<div class="w3-panel w3-pale-blue w3-border">
<p>¬† <strong>üìå Nota:</strong> A quantiza√ß√£o √© uma t√©cnica para reduzir o tamanho do modelo, perdendo um pouco de performance para otimizar o desempenho e rodar em m√°quinas com mem√≥ria limitada.</p>
</div>
</div>
<div id="prompt-engineering" class="section level1">
<h1>Prompt Engineering</h1>
<p>Uma ampla variedade de <a href="https://www.promptingguide.ai/pt">t√©cnicas</a> poderiam ser aplicadas para desenvolver <a href="https://python.langchain.com/docs/how_to/multimodal_prompts/">prompts</a> mais eficazes (inclusive com <a href="https://python.langchain.com/docs/introduction/">LangChain</a>, como fiz no <a href="https://gomesfellipe.github.io/post/2024-05-26-detec-o-de-linguagem-t-xica-com-o-llm-gemma-e-langchain/">√∫ltimo post</a>) ou especializar o modelo com ajuste fino visando obter resultados otimizados. No entanto, como este n√£o √© o foco do post, usarei um prompt simples e direto para estabelecer um baseline para avaliar as capacidades do modelo com o m√≠nimo de esfor√ßo.</p>
<pre class="python"><code># Cada valor em &quot;content&quot; tem que ser uma lista de dicion√°rio com os tipos (&quot;text&quot;, &quot;image&quot;) 
conversation = [
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: [
          {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this image in a few words:&quot;},
          {&quot;type&quot;: &quot;image&quot;},
        ]
    },
]

# Formata a conversa (que pode incluir texto e imagens) no formato correto que o modelo entende.
prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)</code></pre>
<p>O <a href="https://colab.research.google.com/drive/1qsl6cd2c8gGtEW1xV5io7S8NHh-Cp1TV?usp=sharing#scrollTo=JvvtplWDRvfu">prompt</a> deve ser especificado no seguinte formato:</p>
<pre><code>USER: &lt;image&gt;
&lt;prompt&gt;
ASSISTANT:</code></pre>
</div>
<div id="infer√™ncia" class="section level1">
<h1>Infer√™ncia</h1>
<p>Com o modelo devidamente configurado e o prompt ajustado, estamos prontos para executar o pipeline de infer√™ncia. A vantagem de utilizar <a href="https://huggingface.co/docs/transformers/main_classes/pipelines#multimodal">pipelines</a> √© que eles abstraem boa parte da codifica√ß√£o complexa, proporcionando uma interface simples e eficiente. Essa API vers√°til √© dedicada a v√°rias tarefas, como NER (Reconhecimento de Entidades), An√°lise de Sentimentos, Extra√ß√£o de Features e Question Answering.</p>
<pre class="python"><code>for i in tqdm(range(df_sample.shape[0])):
    
    # preparar objetos do loop
    coco_url = df_sample.iloc[i][&#39;coco_url&#39;]
    caption = df_sample.iloc[i][&#39;caption&#39;]
    index = df_sample.iloc[i].name
    
    # Obter imagem
    response = requests.get(coco_url)
    image = Image.open(BytesIO(response.content))
    
    # Realizar a infer√™ncia usando o pipeline e o prompt gerado
    outputs = pipe(image, prompt=prompt, generate_kwargs={&quot;max_new_tokens&quot;: 32})
    
    # Processar o texto gerado para extrair a parte relevante
    result = outputs[0][&#39;generated_text&#39;].split(&#39;ASSISTANT:&#39;, 1)[1].strip()
    
    # Adicionar o resultado da infer√™ncia √† nova coluna &#39;llm&#39; do DataFrame
    df_sample.loc[index, &#39;llm&#39;] = result</code></pre>
<pre><code>100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:41&lt;00:00,  4.20s/it]</code></pre>
<p>Ap√≥s a execu√ß√£o do modelo, veja como ficaram os resultados:</p>
<details>
<summary>
<em>Expandir c√≥digo</em>
</summary>
<pre class="python"><code># Fun√ß√£o para destacar as palavras
def highlight_diff(caption, llm):
    # Divide as frases em palavras
    caption_words = caption.replace(&quot;.&quot;, &quot;&quot;).split()
    llm_words = llm.replace(&quot;.&quot;, &quot;&quot;).split()
    
    # Converte as palavras em conjuntos para encontrar a interse√ß√£o
    caption_set = set(caption_words)
    llm_set = set(llm_words)
    
    # Calcula as palavras que n√£o est√£o na interse√ß√£o
    caption_highlighted = &quot; &quot;.join([f&#39;&lt;span style=&quot;color:red&quot;&gt;{word}&lt;/span&gt;&#39; if word not in llm_set else word for word in caption_words])
    llm_highlighted = &quot; &quot;.join([f&#39;&lt;span style=&quot;color:red&quot;&gt;{word}&lt;/span&gt;&#39; if word not in caption_set else word for word in llm_words])
    
    return caption_highlighted, llm_highlighted

# Aplica a fun√ß√£o a cada linha do DataFrame e cria novas colunas
df_sample[&#39;highlighted_caption&#39;], df_sample[&#39;highlighted_llm&#39;] = zip(*df_sample.apply(lambda row: highlight_diff(row[&#39;caption&#39;], row[&#39;llm&#39;]), axis=1))

# Exibir o DataFrame formatado com HTML
HTML(df_sample[[&#39;image&#39;, &#39;highlighted_caption&#39;, &#39;highlighted_llm&#39;]].to_html(escape=False))</code></pre>
</details>
<p><br></p>
<p><img src="/post/2024-09-27-image-text-to-text/df2.png" alt="extra√ß√£o de r√≥tulos descritivos de imagens com Llava" style="width: 100%;"></p>
<p>Destaquei em vermelho as palavras que diferem entre a legenda original do dataset e a previs√£o gerada pelo nosso modelo de linguagem.</p>
<div class="w3-panel w3-pale-yellow w3-border">
<p>¬† üí≠ Apesar de algumas diferen√ßas sutis entre as duas vers√µes, como ‚Äòlooking at his phone‚Äô e ‚Äòlooking at his <span style="color:red;">cell</span> phone‚Äô, a ideia principal permanece bastante coerente com o que vemos nas imagens. Em alguns casos, como no item 3, a descri√ß√£o gerada pelo modelo, ‚Äòholding a <span style="color:red;">kite</span>‚Äô, parece at√© mais apropriada do que a fornecida pelo dataset, ‚Äòholding a <span style="color:red;">flag</span>‚Äô.</p>
</div>
<p>Agora, o pr√≥ximo passo ser√° quantificar essas diferen√ßas de maneira num√©rica.</p>
</div>
<div id="avaliar-modelo" class="section level1">
<h1>Avaliar modelo</h1>
<p>Para medir a precis√£o das legendas geradas, aplicaremos quatro m√©tricas amplamente usadas:</p>
<ul>
<li><strong><a href="https://aclanthology.org/P02-1040.pdf">BLEU</a> (Bilingual Evaluation Understudy Score)</strong>: Amplamente utilizada para medir a qualidade de tradu√ß√µes autom√°ticas, mede a <strong>sobreposi√ß√£o de n-gramas</strong> entre a tradu√ß√£o gerada por um modelo e as tradu√ß√µes de refer√™ncia, atribuindo uma pontua√ß√£o que varia de 0 a 1 (aplica tamb√©m um fator de penaliza√ß√£o para evitar que tradu√ß√µes curtas sejam favorecidas);</li>
<li><strong><a href="https://aclanthology.org/W04-1013.pdf">ROUGE-L</a> (Recall-Oriented Understudy for Gisting Evaluation)</strong>: Muito utilizado em tarefa de sumariza√ß√£o de textos, considera a sequ√™ncia mais longa de palavras que aparecem em ambas as refer√™ncias e previs√µes, medindo a capacidade de preservar a <strong>ordem das palavras</strong>;</li>
<li><strong><a href="https://www.cs.cmu.edu/~alavie/METEOR/">METEOR</a> (Metric for Evaluation of Translation with Explicit ORdering)</strong>: Baseada na m√©dia harm√¥nica da precis√£o e recall de n-gramas, com recall ponderado mais alto do que a precis√£o. Essa m√©trica METEOR foi projetada para corrigir alguns dos problemas (como encontrar sin√¥nimos) nas m√©tricas BLEU e ROGUE;</li>
<li><strong><a href="https://huggingface.co/spaces/evaluate-metric/bertscore">BERTScore</a></strong>: Usa embeddings (representa√ß√µes sem√¢nticas) obtidas a partir do modelo BERT para comparar a similaridade sem√¢ntica entre as descri√ß√µes geradas e as de refer√™ncia.</li>
</ul>
<details>
<summary>
<em>Expandir c√≥digo</em>
</summary>
<pre class="python"><code># Fun√ß√µes para calcular as m√©tricas
def calcular_bleu(referencias, previsao):
    return sentence_bleu([referencias.split(&quot; &quot;)], previsao.split(&quot; &quot;),weights = [1])

def calcular_rouge(referencias, previsao):
    scorer = rouge_scorer.RougeScorer([&#39;rougeL&#39;], use_stemmer=True)
    return scorer.score(referencias, previsao)[&#39;rougeL&#39;].fmeasure

def calcular_meteor(referencias, previsao):
    return meteor_score([referencias.split(&quot; &quot;)], previsao.split(&quot; &quot;))

def calcular_bertscore(referencias, previsao):
    P, R, F1 = bert_score([previsao], [referencias], lang=&quot;en&quot;, verbose=True)
    return F1.mean().item()</code></pre>
</details>
<p><br></p>
<pre class="python"><code>%%capture

# Avaliar as amostras no DataFrame
resultados = []
for i, row in df_sample.iterrows():
    
    referencias = row[&#39;caption&#39;].replace(&quot;.&quot;, &quot;&quot;)
    previsao = row[&#39;llm&#39;].replace(&quot;.&quot;, &quot;&quot;)
    
    bleu = calcular_bleu(referencias, previsao)
    rouge = calcular_rouge(referencias, previsao)
    meteor = calcular_meteor(referencias, previsao)
    bert = calcular_bertscore(referencias, previsao)
    
    resultados.append([referencias, previsao, bleu, rouge, meteor, bert])

# Converter os resultados para um DataFrame
df_resultados = pd.DataFrame(resultados, columns=[&#39;caption&#39;, &#39;llm&#39;, &#39;BLEU&#39;, &#39;ROUGE&#39;, &#39;METEOR&#39;, &#39;BERTScore&#39;])</code></pre>
<p>Vejamos os resultados:</p>
<details>
<summary>
<em>Expandir c√≥digo</em>
</summary>
<pre class="python"><code># Configurar o tema do Seaborn
sns.set_theme(style=&quot;white&quot;, rc={&quot;axes.facecolor&quot;: (0, 0, 0, 0)})

# Reformatar o DataFrame para o formato long
df_long = df_resultados[[&#39;BLEU&#39;, &#39;ROUGE&#39;, &#39;METEOR&#39;, &#39;BERTScore&#39;]].melt(var_name=&quot;M√©trica&quot;, value_name=&quot;Valor&quot;)

# Calcular a m√©dia de cada m√©trica
mean_values = df_long.groupby(&#39;M√©trica&#39;)[&#39;Valor&#39;].mean().reset_index()

# Inicializar o objeto FacetGrid
pal = sns.cubehelix_palette(len(df_long[&#39;M√©trica&#39;].unique()), rot=-.25, light=.7)
g = sns.FacetGrid(df_long, row=&quot;M√©trica&quot;, hue=&quot;M√©trica&quot;, aspect=6, height=1.5, palette=pal)

# Desenhar as densidades
g.map(sns.kdeplot, &quot;Valor&quot;, 
      bw_adjust=.5, clip_on=False, 
      fill=True, alpha=1, linewidth=1.5)
g.map(sns.kdeplot, &quot;Valor&quot;, clip_on=False, color=&quot;w&quot;, lw=2, bw_adjust=.5)

# Adicionar linha de refer√™ncia
g.refline(y=0, linewidth=2, linestyle=&quot;-&quot;, color=None, clip_on=False)

# Fun√ß√£o para rotular o gr√°fico
def label(x, color, label):
    ax = plt.gca()
    # Localizar a m√©dia correspondente √† m√©trica
    mean_value = mean_values[mean_values[&#39;M√©trica&#39;] == label][&#39;Valor&#39;].values[0]
    ax.text(0, .4, f&quot;{label} (M√©dia: {mean_value:.2f})&quot;, fontweight=&quot;bold&quot;, color=color,
            ha=&quot;left&quot;, va=&quot;center&quot;, transform=ax.transAxes, fontsize=20)

g.map(label, &quot;Valor&quot;)

# Ajustar espa√ßamento entre subplots manualmente
g.figure.subplots_adjust(hspace=0.2)

# Remover detalhes desnecess√°rios dos eixos
g.set_titles(&quot;&quot;)
g.set(yticks=[], ylabel=&quot;&quot;)
g.despine(bottom=True, left=True)

# Configurar o eixo x
g.set(xlim=(0.4, 1), xticks=np.arange(0.4, 1.05, 0.1))  # Limites e ticks do eixo x

# Remover r√≥tulos do eixo x em cada subplot
for ax in g.axes.flat:
    ax.set_xlabel(&quot;&quot;)  # Remover r√≥tulo do eixo x
    ax.tick_params(axis=&#39;x&#39;, labelsize=16)  # Aumentar o tamanho da fonte dos ticks do eixo x

# Exibir o gr√°fico
plt.show()</code></pre>
</details>
<p><br></p>
<center>
<img src="/post/2024-09-27-image-text-to-text/metrics.png" alt="metricas da extra√ß√£o de r√≥tulos descritivos de imagens com Llava" style="width: 80%;">
</center>
<div class="w3-panel w3-pale-yellow w3-border">
<p>¬† <strong>üìå Insights ao Avaliar as M√©tricas do Modelo: </strong></p>
<ul>
<li><p>As m√©tricas baseadas em <strong>n-grams e na correspond√™ncia de palavras</strong> mostraram desempenho <strong>subestimado</strong>. Embora o modelo tenha apresentado algumas varia√ß√µes na escolha das palavras, as frases geradas mantiveram um sentido geral muito semelhante ao que √© retratado nas imagens.</p></li>
<li><p>Por outro lado, a m√©trica baseada em <strong>embeddings</strong>, que avalia o significado <strong>sem√¢ntico</strong> das frases, apresentou resultados <strong>significativamente superiores</strong>. Essa abordagem se mostrou mais congruente em avaliar a similaridade das descri√ß√µes geradas e a descri√ß√£o informada do conte√∫do visual das imagens.</p></li>
<li><p>√â importante ressaltar que nosso <strong>prompt</strong> foi mantido na forma <strong>mais simples poss√≠vel</strong> e que o conjunto de dados abrange um <strong>escopo bastante amplo</strong>. Com isso, acredito que o modelo ainda tem muito potencial para oferecer resultados ainda mais robustos, sem a necessidade de ajustes finos, em tarefas mais espec√≠ficas.</p></li>
</ul>
</div>
</div>
<div id="conclus√£o" class="section level1">
<h1>Conclus√£o</h1>
<p>O uso da GenAI com o modelo Llava oferece uma solu√ß√£o eficiente para a extra√ß√£o de features de imagens em Python, possibilitando a cria√ß√£o de descri√ß√µes ricas e detalhadas. Ao comparar a qualidade das sa√≠das com m√©tricas como BLEU, podemos garantir que o modelo esteja oferecendo resultados satisfat√≥rios para as necessidades do projeto.</p>
<p>Se voc√™ deseja automatizar processos de an√°lise de imagens, explorar a cria√ß√£o de modelos customizados ou otimizar a organiza√ß√£o de dados visuais, a utiliza√ß√£o de GenAI com modelos como o Llava pode ser um divisor de √°guas em seus projetos.</p>
<p>Se este conte√∫do foi √∫til, continue acompanhando o blog para mais tutoriais sobre intelig√™ncia artificial e Python!</p>
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias</h1>
<ul>
<li><a href="https://huggingface.co/llava-hf/llava-1.5-7b-hf" class="uri">https://huggingface.co/llava-hf/llava-1.5-7b-hf</a></li>
<li><a href="https://github.com/haotian-liu/LLaVA" class="uri">https://github.com/haotian-liu/LLaVA</a></li>
<li><a href="https://colab.research.google.com/drive/1qsl6cd2c8gGtEW1xV5io7S8NHh-Cp1TV?usp=sharing#scrollTo=6Bx8iu9jOssW" class="uri">https://colab.research.google.com/drive/1qsl6cd2c8gGtEW1xV5io7S8NHh-Cp1TV?usp=sharing#scrollTo=6Bx8iu9jOssW</a></li>
<li><a href="https://cocodataset.org/#explore" class="uri">https://cocodataset.org/#explore</a></li>
<li><a href="https://www.kaggle.com/datasets/awsaf49/coco-2017-dataset/" class="uri">https://www.kaggle.com/datasets/awsaf49/coco-2017-dataset/</a></li>
</ul>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2024-09-27-image-text-to-text/">Extra√ß√£o de informa√ß√µes de imagens com IA Generativa</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">chatgpt</category>
      <category domain="tag">data-science</category>
      <category domain="tag">genai</category>
      <category domain="tag">ia-generativa</category>
      <category domain="tag">inteligencia-artificial</category>
      <category domain="tag">llama</category>
      <category domain="tag">llama2</category>
      <category domain="tag">llava</category>
      <category domain="tag">llm</category>
      <category domain="tag">lmm</category>
    </item>
    <item>
      <title>An√°lise de Sentimentos com um &#34;ChatGPT&#34; de C√≥digo Aberto</title>
      <link>https://gomesfellipe.github.io/post/2024-04-20-sentiment-analysis-llama2/</link>
      <pubDate>Sat, 20 Apr 2024 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2024-04-20-sentiment-analysis-llama2/</guid>
      <description>Como executar localmente o LLM pr√©-treinado de c√≥digo aberto Llama2 para realizar uma an√°lise de sentimentos em Python</description>
      <content:encoded>&lt;![CDATA[
        

<div id="TOC">
<ul>
<li><a href="#por-que-an%C3%A1lise-de-sentimentos" id="toc-por-que-an√°lise-de-sentimentos">Por que An√°lise de Sentimentos?</a></li>
<li><a href="#por-que-large-language-models" id="toc-por-que-large-language-models">Por que Large Language Models?</a></li>
<li><a href="#o-que-faremos-aqui" id="toc-o-que-faremos-aqui">O que faremos aqui?</a></li>
<li><a href="#m%C3%A3os-a-obra" id="toc-m√£os-a-obra">M√£os a obra!</a>
<ul>
<li><a href="#iniciar-ambiente-de-trabalho" id="toc-iniciar-ambiente-de-trabalho">Iniciar ambiente de trabalho</a></li>
<li><a href="#carregar-dados" id="toc-carregar-dados">Carregar dados</a></li>
<li><a href="#informa%C3%A7%C3%B5es-gerais" id="toc-informa√ß√µes-gerais">Informa√ß√µes gerais</a></li>
<li><a href="#an%C3%A1lise-explorat%C3%B3ria" id="toc-an√°lise-explorat√≥ria">An√°lise Explorat√≥ria</a></li>
<li><a href="#an%C3%A1lise-de-sentimentos" id="toc-an√°lise-de-sentimentos">An√°lise de Sentimentos</a></li>
</ul></li>
<li><a href="#resultado-final" id="toc-resultado-final">Resultado Final</a></li>
<li><a href="#conclus%C3%A3o-e-discuss%C3%A3o" id="toc-conclus√£o-e-discuss√£o">Conclus√£o e Discuss√£o</a></li>
<li><a href="#refer%C3%AAncias" id="toc-refer√™ncias">Refer√™ncias</a></li>
</ul>
</div>

<style>
.column4 {
  float: left;
  width: 33%;
  padding: 10px;
}
 
.column8 {
  float: left;
  width: 66%;
  padding: 10px;
}

.column6 {
  float: left;
  width: 50%;
  padding: 10px;
}

.row:after {
  content: "";
  display: table;
  clear: both;
}
</style>
<div id="por-que-an√°lise-de-sentimentos" class="section level2">
<h2>Por que An√°lise de Sentimentos?</h2>
<p>Compreender os sentimentos por tr√°s de grandes volumes de texto tornou-se essencial, pois em um mundo cada vez mais digitalizado, a capacidade de compreender as respostas e emo√ß√µes em larga escala das pessoas diante de produtos, eventos ou t√≥picos espec√≠ficos n√£o √© apenas valiosa por fornecer insights, mas tamb√©m se tornou uma necessidade para alavancar neg√≥cios e tornar-se cada vez mais competitivo.</p>
<blockquote>
<p>An√°lise de sentimento, tamb√©m chamada de minera√ß√£o de opini√£o, √© o campo de estudo que analisa as opini√µes, sentimentos, avalia√ß√µes, aprecia√ß√µes, atitudes e emo√ß√µes das pessoas em rela√ß√£o a entidades como produtos, servi√ßos, organiza√ß√µes, indiv√≠duos, quest√µes, eventos, t√≥picos e seus atributos. <a href="https://www.cambridge.org/de/universitypress/subjects/computer-science/artificial-intelligence-and-natural-language-processing/sentiment-analysis-mining-opinions-sentiments-and-emotions-2nd-edition?format=HB&amp;isbn=9781108486378">Liu 2020</a></p>
</blockquote>
</div>
<div id="por-que-large-language-models" class="section level2">
<h2>Por que Large Language Models?</h2>
<p>A abordagem comum para resolver problemas de NLP envolviam a aplica√ß√£o de <em>text mining</em>, <em>embeddings</em> como <em>word2vec</em> e <em>GloVe (Global Vectors for Word Representation)</em> e t√©cnicas de Machine Learning, onde modelos como <em>Random Forest</em>, <em>SVM</em>, <em>Naive Bayes</em>, <em>KNN</em>, <em>Ensembles</em> e at√© mesmo Regress√£o eram frequentemente utilizados para classificar textos. Al√©m disso, o uso de redes neurais recorrentes (<em>RNNs</em>) sempre foi uma alternativa valiosa, especialmente em situa√ß√µes que demandavam o processamento de dados sequenciais, sendo a <em>LSTM (Long Short-Term Memory)</em> uma variante eficaz para lidar com o desafio conhecido como <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem"><em>vanishing gradient</em></a>.</p>
<p>J√° no cen√°rio atual de modelos pr√©-treinados, o <em>BERT (Bidirectional Encoder Representations from Transformers)</em> tamb√©m teve bastante destaque nesse dom√≠nio antes da ascens√£o do <em>ChatGPT</em>, demonstrando a viabilidade como um m√©todo gerador de texto e mostraram o poder que as redes neurais t√™m para gerar longas sequ√™ncias de texto que antes pareciam inating√≠veis.</p>
<center>
<p><img src="/post/2024-04-20-sentiment-analysis-llama2/parameters_transformer_based_language_models.png" style="width:70.0%" /></br>
<small><a href="https://www.techtarget.com/searchenterpriseai/definition/GPT-3">GPT-3 supera seus antecessores em termos de contagem de par√¢metros</a></small></p>
</center>
<p>Embora j√° existam h√° algum tempo, os <em>LLMs</em> ganharam a m√≠dia atrav√©s do <em>ChatGPT</em>, interface de chat da OpenAI para modelos LLM GPT-3 lan√ßado em 2020, com 175 milh√µes de par√¢metros, que j√° teve uma s√©rie de avan√ßos significativos nos √∫ltimos anos como seu irm√£o maior, o GPT-4 lan√ßado em 2023 conta com incr√≠veis 100 <strong>tr√≠lh√µes</strong> de par√¢metros.</p>
<center>
<p><img src="/post/2024-04-20-sentiment-analysis-llama2/comparison-between-GPT-3-and-GPT-4.png" style="width:50.0%" /></br>
<small><a href="https://www.techtarget.com/searchenterpriseai/definition/GPT-3">The comparison between GPT-3 and GPT-4 based on the number of parameters used in their architecture</a></small></p>
</center>
<p>Modelos com mais de 100 bilh√µes de par√¢metros j√° podem ser considerados muito grandes, com conhecimento mundial muito rico. Esses modelos maiores conseguem ‚Äúaprender‚Äù ainda mais informa√ß√µes sobre muitas coisas sobre fisica, filosofia, ci√™ncia, programa√ß√£o, etc sendo cada vez mais √∫teis para ajudar em tarefas que envolvam conhecimento profundo ou raciocinio complexo, sendo um bom ‚Äúparceiro‚Äù para brainstorming.</p>
<div class="w3-panel w3-pale-red w3-border">
<p>¬† <strong>‚ö†Ô∏è Aten√ß√£o!</strong> </br>
Afirmar que maiores modelos s√£o sempre melhores n√£o √© verdade. O tempo de processamento, lat√™ncia e o custo tamb√©m ir√£o aumentar, por isso <a href="https://medium.com/@masteringllm/mistral-7b-is-187x-cheaper-compared-to-gpt-4-b8e5ee1c9fc2">abordagens alternativas</a> tamb√©m devem ser consideradas.</p>
</div>
<div id="como-funcionam-os-llms" class="section level3">
<h3>Como funcionam os LLMs?</h3>
<p>Os <em>LLMs</em> s√£o modelos de <em>Machine Learning</em> que usam algoritmos de <em>Deep Learning</em> para processar e compreender a linguagem natural, gerando texto de maneira eficaz. Esses modelos s√£o treinados com grandes volumes de dados da internet, adquirindo a capacidade de identificar padr√µes na composi√ß√£o de palavras e frases. A id√©ia b√°sica por tr√°s desses modelos √© que s√£o capazes de gerar texto prevendo repetidamente a pr√≥xima palavra oferecendo resultados r√°pidos e diversas aplica√ß√µes pr√°ticas em v√°rias √°reas</p>
<div id="aplica√ß√µes" class="section level4">
<h4>Aplica√ß√µes</h4>
<p>Diferentemente de uma ferramenta de busca como o Google, o ChatGPT n√£o recupera informa√ß√µes, mas cria frases e textos completos em tempo real com base no processamento de um imenso volume de dados, veja alguns exemplos de uso para diferentes tarefas:</p>
<div class="row">
<div id="escrita" class="section level5 column4">
<h5>‚úçÔ∏è <strong>Escrita:</strong></h5>
<ul>
<li>Colabora√ß√£o em brainstorming, sugerindo nomes;</li>
<li>Elabora√ß√£o de templates para comunicados e e-mails;</li>
<li>Tradu√ß√£o autom√°tica.</li>
</ul>
</div>
<div id="leitura" class="section level5 column4">
<h5>üìñ <strong>Leitura</strong>:</h5>
<ul>
<li>Revis√£o de textos;</li>
<li>Sumariza√ß√£o de artigos extensos;</li>
<li>An√°lise de sentimentos, possibilitando a cria√ß√£o de dashboards para acompnhamento ao longo do tempo.</li>
</ul>
</div>
<div id="conversa" class="section level5 column4">
<h5>üí¨ <strong>Conversa</strong>:</h5>
<ul>
<li>Di√°logos e aconselhamentos;</li>
<li>Coaching de carreira;</li>
<li>Planejamento de viagens;
Sugest√µes de receitas;</li>
<li>Conversa√ß√£o interativa com documentos PDF;</li>
<li>Atendimento ao cliente;</li>
<li>Realiza√ß√£o de pedidos.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div id="o-que-faremos-aqui" class="section level2">
<h2>O que faremos aqui?</h2>
<p>Nosso objetivo aqui √© realizar uma an√°lise de sentimentos para classificar senten√ßas como positivas ou negativas utilizando algum LLM pr√©-treinado. Embora a OpenAI j√° tenha sido uma organiza√ß√£o sem fins lucrativos que lan√ßava seus projetos como c√≥digo aberto, desde o lan√ßamento do ChatGPT ela se tornou uma empresa que mant√©m a propriedade de seus c√≥digos fonte. Isso significa que apesar da facilidade de criar aplica√ß√µes, modelos mais poderosos e relativamente baratos, desenvolvedores de IA n√£o podem modificar o GPT-3 para atender √†s nossa necessidades espec√≠ficas ou incorpor√°-lo em seus pr√≥prios projetos de maneira livre e gratuita. Portanto teremos de recorrer √† alternativas n√£o t√£o(*) <em>open source</em> como o <a href="https://huggingface.co/meta-llama"><em>Llama 2</em> da Meta</a> que permite total controle sobre o modelo, rodar em nosso pr√≥prio computador/servidor e n√≥s d√° o controle sobre a privacidade dos nossos dados.</p>
<div class="w3-panel w3-pale-red w3-border">
<p>(*) ‚ÄúC√≥digo aberto‚Äù ü§î </br>
N√£o √© totalmente c√≥digo aberto pois por mais que a Meta tenha disponibilizado o modelo treinado para uso livre, ele n√£o compartilha os dados de treinamento do modelo ou o c√≥digo usado para trein√°-lo.</p>
</div>
</div>
<div id="m√£os-a-obra" class="section level1">
<h1>M√£os a obra!</h1>
<div id="iniciar-ambiente-de-trabalho" class="section level2">
<h2>Iniciar ambiente de trabalho</h2>
<p>Primeiramente vamos carregar todas as dependencias necess√°rias para executar os c√≥digos a seguir:</p>
<pre class="python"><code>import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from PIL import Image
from nltk.corpus import stopwords
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score
from llama_cpp import Llama
from tqdm.notebook import tqdm
tqdm.pandas()</code></pre>
</div>
<div id="carregar-dados" class="section level2">
<h2>Carregar dados</h2>
<p>Utilizaremos uma vers√£o <a href="https://www.kaggle.com/datasets/luisfredgs/imdb-ptbr">traduzida do dataset IMdb para o portugu√™s</a>, um conjunto de dados do Internet Movie Database (IMDB), que √© uma das maiores e mais abrangentes bases de dados online sobre filmes e programas de televis√£o.</p>
<pre class="python"><code> #Importar todo conjunto de dados
df = pd.read_csv(&#39;input/imdb-reviews-pt-br.csv&#39;, index_col=&#39;id&#39;)
# Obter amostra de tamanho 100
_, df = train_test_split(df, test_size=100, random_state=42, shuffle=True)</code></pre>
</div>
<div id="informa√ß√µes-gerais" class="section level2">
<h2>Informa√ß√µes gerais</h2>
<p>Esse dataset inclui avalia√ß√µes e cr√≠ticas de filmes feitas por usu√°rios do IMDB, bem como informa√ß√µes sobre os pr√≥prios filmes, como t√≠tulo, ano de lan√ßamento, g√™nero, etc. Para nossa finalidade para tarefa de an√°lise de sentimentos, utilizaremos os seguintes dados:</p>
<table>
<colgroup>
<col width="6%" />
<col width="41%" />
<col width="40%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">id</th>
<th align="left">text_en</th>
<th align="left">text_pt</th>
<th align="center">sentiment</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">12534</td>
<td align="left">This was unusual: a modern-day film which‚Ä¶</td>
<td align="left">Isso era incomum: um filme moderno que era‚Ä¶</td>
<td align="center">pos</td>
</tr>
<tr class="even">
<td align="center">35447</td>
<td align="left">Some of my old friends suggested me to wat‚Ä¶</td>
<td align="left">Alguns dos meus velhos amigos sugeriram qu‚Ä¶</td>
<td align="center">neg</td>
</tr>
<tr class="odd">
<td align="center">20281</td>
<td align="left">What a pleasure. This is really a parody. ‚Ä¶</td>
<td align="left">Que prazer. Isto √© realmente uma par√≥dia. S‚Ä¶</td>
<td align="center">pos</td>
</tr>
<tr class="even">
<td align="center">‚Ä¶</td>
<td align="left">‚Ä¶</td>
<td align="left">‚Ä¶</td>
<td align="center">‚Ä¶</td>
</tr>
<tr class="odd">
<td align="center">34241</td>
<td align="left">WOW!I just was given this film from a frie‚Ä¶</td>
<td align="left">WOW! Acabei de receber este filme de um am‚Ä¶</td>
<td align="center">neg</td>
</tr>
<tr class="even">
<td align="center">12896</td>
<td align="left">This film offers many delights and surprise‚Ä¶</td>
<td align="left">Este filme oferece muitas del√≠cias e surp‚Ä¶</td>
<td align="center">pos</td>
</tr>
<tr class="odd">
<td align="center">19748</td>
<td align="left">Over the years Ive watched this movie many‚Ä¶</td>
<td align="left">Ao longo dos anos, assisti a esse filme mu‚Ä¶</td>
<td align="center">pos</td>
</tr>
</tbody>
</table>
<p>Onde:</p>
<ul>
<li><code>id</code>: Identificador;</li>
<li><code>text_en</code>: texto em ingl√™s;</li>
<li><code>text_pt</code>: texto em portugu√™s;</li>
<li><code>sentiment</code>: r√≥tulo do texto, que pode ser <code>pos</code> ou <code>neg</code>.</li>
</ul>
</div>
<div id="an√°lise-explorat√≥ria" class="section level2">
<h2>An√°lise Explorat√≥ria</h2>
<hr />
<div id="distribui√ß√£o-dos-sentimentos-na-amostra" class="section level3">
<h3>Distribui√ß√£o dos sentimentos na amostra</h3>
<p>Primeiro vamos entender como ficou distribu√≠da a propor√ß√£o dos sentimentos na amostra coletada:</p>
<details>
<summary>
<em>Clique aqui para ver o c√≥digo do gr√°fico</em>
</summary>
<pre class="python"><code># Contagem absoluta
contagem_absoluta = df[&#39;sentiment&#39;].value_counts()

# Contagem relativa
contagem_relativa = df[&#39;sentiment&#39;].value_counts(normalize=True) * 100

# Criar gr√°fico de barras
fig, ax = plt.subplots(figsize=(6, 4))
barras = plt.bar(contagem_absoluta.index, contagem_absoluta, color=[&#39;green&#39;, &#39;red&#39;])

# Adicionar texto nas barras
for barra, abs_value, rel_value in zip(barras, contagem_absoluta, contagem_relativa):
    yval = barra.get_height()
    ax.text(barra.get_x() + barra.get_width()/2, yval, f&#39;{abs_value} ({rel_value:.1f}%)&#39;,
            ha=&#39;center&#39;, va=&#39;bottom&#39;, color=&#39;black&#39;, fontsize=12)

# Adicionar r√≥tulos e t√≠tulo
plt.xlabel(&#39;Sentimento&#39;, fontsize=14)
plt.ylabel(&#39;Frequ√™ncia absoluta&#39;, fontsize=14)
plt.title(&#39;Quantidade de textos de cada sentimento \nem uma amostra de tamanho 100&#39;, fontsize=16, x=0.5, y=1.1)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)

# Remover bordas da parte superior e direita
ax.spines[&#39;top&#39;].set_visible(False)
ax.spines[&#39;right&#39;].set_visible(False)

# Ajustar layout
plt.tight_layout()

# Salvar imagem
plt.savefig(f&quot;img/freq_sentiment.png&quot;, bbox_inches=&#39;tight&#39;)

# Exibir o gr√°fico
plt.show()</code></pre>
</details>
<p>¬†</p>
<center>
<p><img src="/post/2024-04-20-sentiment-analysis-llama2/freq_sentiment.png" /></p>
</center>
<div class="w3-panel w3-pale-blue w3-border">
<p>¬† üìå <strong>Interpreta√ß√£o:</strong>
Coletei uma amostra aleat√≥ria simples de tamanho n=100 de todas as reviews que cont√©m aproximadamente metade de cada sentimento para diminuir o tempo computacional de execu√ß√£o no meu computador.</p>
</div>
</div>
<div id="palavras-mais-frequentes-para-cada-sentimento" class="section level3 tabset">
<h3>Palavras mais frequentes para cada sentimento</h3>
<p>N√∫vens de palavras das resenhas dos filmes que foram anotadas como positivos e como negativos nas duas linguas dispon√≠veis no dataset:</p>
<details>
<summary>
<em>Clique aqui para ver o c√≥digo das Wordclouds</em>
</summary>
<pre class="python"><code>def generate_wordcloud(df, language=&#39;en&#39;):
    # Definir stopwords para o idioma escolhido
    if language == &#39;en&#39;:
        stop_words_pos = stop_words_neg = set(stopwords.words(&#39;english&#39;))
        stop_words_pos.update([&quot;film&quot;, &quot;movie&quot;, &quot;one&quot;])
        stop_words_neg.update([&quot;character&quot;, &quot;like&quot;, &quot;really&quot;, &quot;make&quot;, &quot;see&quot;])
    elif language == &#39;pt&#39;:
        stop_words_pos = stop_words_neg = set(stopwords.words(&#39;portuguese&#39;))
        stop_words_pos.update([&quot;filme&quot;, &quot;filmes&quot;, &quot;todo&quot;, &quot;t√£o&quot;, &quot;pode&quot;, &quot;todos&quot;])
        stop_words_neg.update([&quot;filme&quot;, &quot;filmes&quot;, &quot;todo&quot;, &quot;t√£o&quot;, &quot;filme&quot;, &quot;coisa&quot;, &quot;realmente&quot;])
    else:
        raise ValueError(&quot;Language must be &#39;en&#39; or &#39;pt&#39;.&quot;)

    # Concatenar textos positivos e negativos
    txt_pos = &quot; &quot;.join(review for review in df[df.sentiment == &#39;pos&#39;][f&#39;text_{language}&#39;])
    txt_neg = &quot; &quot;.join(review for review in df[df.sentiment == &#39;neg&#39;][f&#39;text_{language}&#39;])

    # Carregar m√°scaras de imagem
    mask_pos = np.array(Image.open(f&quot;img/pos.png&quot;))
    mask_neg = np.array(Image.open(f&quot;img/neg.png&quot;))

    # Gerar nuvens de palavras positivas e negativas
    wordcloud_positivo = WordCloud(
        stopwords=stop_words_pos,
        random_state=42,
        background_color=&quot;white&quot;,
        color_func=lambda *args, **kwargs: &quot;green&quot;,
        contour_color=&#39;black&#39;,
        contour_width=1,
        max_font_size=100,
        min_font_size=15,
        max_words=200,
        mask=mask_pos
    ).generate(txt_pos)

    wordcloud_negativo = WordCloud(
        stopwords=stop_words_neg,
        random_state=42,
        background_color=&quot;white&quot;,
        color_func=lambda *args, **kwargs: &quot;red&quot;,
        contour_color=&#39;black&#39;,
        contour_width=1,
        max_font_size=100,
        min_font_size=15,
        max_words=200,
        mask=mask_neg
    ).generate(txt_neg)

    # Configura√ß√µes do plot
    plt.figure(figsize=(7, 14))

    # Plotar nuvem de palavras positivas
    plt.subplot(1, 2, 1)
    plt.imshow(wordcloud_positivo, interpolation=&#39;bilinear&#39;)
    plt.axis(&#39;off&#39;)
    plt.title(&#39;Positivo&#39;, fontsize=20, color=&#39;green&#39;)

    # Plotar nuvem de palavras negativas
    plt.subplot(1, 2, 2)
    plt.imshow(wordcloud_negativo, interpolation=&#39;bilinear&#39;)
    plt.axis(&#39;off&#39;)
    plt.title(&#39;Negativo&#39;, fontsize=20, color=&#39;red&#39;)

    # Ajustar layout
    plt.tight_layout()

    # Salvar a nuvem de palavras como imagem
    plt.savefig(f&quot;img/wordcloud_{language}.png&quot;, bbox_inches=&#39;tight&#39;)

    # Exibir a nuvem de palavras
    plt.show()
    
# Exemplo de uso para o idioma ingl√™s
generate_wordcloud(df, language=&#39;en&#39;)

# Exemplo de uso para o idioma portugu√™s
generate_wordcloud(df, language=&#39;pt&#39;)</code></pre>
</details>
<p>¬†</p>
<center>
<p><img src="/post/2024-04-20-sentiment-analysis-llama2/wordcloud_en.png" /></p>
<p>N√∫vem de palavras mais frequentes das resenhas em <strong>üá∫üá≤ Ingl√™s</strong></p>
</center>
<center>
<p><img src="/post/2024-04-20-sentiment-analysis-llama2/wordcloud_pt.png" /></p>
<p>N√∫vem de palavras mais frequentes das resenhas em <strong>üáßüá∑ Portugu√™s</strong></p>
</center>
<div class="w3-panel w3-pale-blue w3-border">
<p>¬† <strong>üìå Interpreta√ß√£o:</strong>
Como esperado, mesmo com a mudan√ßa na l√≠ngua, a frequ√™ncia das palavras √© exibida de maneira muito similar de acordo com cada sentimento.</p>
</div>
</div>
</div>
<div id="an√°lise-de-sentimentos" class="section level2">
<h2>An√°lise de Sentimentos</h2>
<hr />
<div id="fam√≠lia-llama-2-de-large-language-models-llms" class="section level3">
<h3>Fam√≠lia <em>Llama 2</em> de <em>Large Language Models</em> (<em>LLMs</em>)</h3>
<p>Nesta se√ß√£o, exploraremos o <a href="https://llama.meta.com/"><em>Llama 2</em></a>, um modelo de c√≥digo aberto, e discutiremos as vantagens e desvantagens em rela√ß√£o aos LLMs de c√≥digo fechado ou remotos.</p>
<div id="tamanho-do-modelo" class="section level4">
<h4>Tamanho do modelo</h4>
<p>Para saber qual modelo utilizar, primeiramente precisamos ter em mente algumas no√ß√µes sobre a quantidade de par√¢metros e tamanhos dos LLM. No geral:</p>
<div class="row">
<div class="column4">
<p><big><strong>1 Bilh√£o</strong>:</big></p>
<p>Bons em correspond√™ncia de padr√µes e algum conhecimento b√°sico do mundo (como por exemplo classificar avalia√ß√µes por sentimento)</p>
</div>
<div class="column4">
<p><big><strong>10 Bilh√µes</strong>:</big></p>
<p>Maior conhecimento mundial, conhecem mais fatos esot√©ricos sobre o mundo e melhoram em seguir instru√ß√µes b√°sicas (bom para chatbot para pedidos de comida);</p>
</div>
<div class="column4">
<p><big><strong>100+ Bilh√µes</strong>: </big></p>
<p>Muito grandes, com conhecimento mundial muito rico, saber√£o coisas sobre f√≠sica, filosofia, ci√™ncia e assim por diante e ser√£o melhores em racioc√≠nios complexos (tarefas que envolvem conhecimento profundo ou racioc√≠nio complexo, parceiro para brainstorming)</p>
</div>
</div>
<p>Para uma an√°lise de sentimentos simples, n√£o √© necess√°rio um modelo com 100 bilh√µes de par√¢metros. Modelos menores, como os com 7 bilh√µes de par√¢metros, podem ser suficientes e menos computacionalmente exigentes.</p>
</div>
<div id="c√≥digo-aberto-ou-fechado" class="section level4">
<h4>C√≥digo aberto ou fechado</h4>
<p>Embora pr√≥ximos, os LLMs de c√≥digo aberto ainda n√£o conseguem igualar o poder e a precis√£o dos aplicativos de c√≥digo fechado dispon√≠veis comercialmente, como <a href="https://openai.com/gpt-4">GPT-4</a> e <a href="https://gemini.google.com/app">Bard (Gemini)</a>. Mesmo sendo menos poderosos, existem alguns pr√≥s e contras pelos quais podemos pesar na hora de escolher a melhor op√ß√£o:</p>
<div class="row">
<div class="column6">
<p><big><strong>Open Source</strong></big></p>
<ul>
<li>Total controle sobre o modelo</li>
<li>Pode rodar em nosso pr√≥prio computador/servidor</li>
<li>Controle sobre a privacidade dos dados</li>
</ul>
</div>
<div class="column6">
<p><big><strong>Closed</strong></big></p>
<ul>
<li>F√°cil de criar aplica√ß√µes</li>
<li>Maiores e mais poderosos</li>
<li>Relativamente barato</li>
<li>Existe um certo risco de depender do fornecedor</li>
</ul>
</div>
</div>
<p>Utilizaremos a abordagem de c√≥digo aberto por ser mais pr√°tica para fins de estudos, pois al√©m de gratuita, n√£o exige internet, registros ou chaves de API.</p>
</div>
<div id="uso-remoto-ou-local" class="section level4">
<h4>Uso remoto ou local</h4>
<p>Podemos interagir com o modelo de linguagem grande (LLM) do Llama 2 via API da <a href="https://huggingface.co/">Hugging Face</a>, seguindo as instru√ß√µes do <a href="https://huggingface.co/meta-llama">reposit√≥rio oficial da Meta</a> ou podemos baixar os arquivos do modelo em formato GGML para o <a href="https://huggingface.co/meta-llama/Llama-2-7b-chat-hf">Llama 2 7B Chat do Meta Llama 2</a>. Os formatos GGML s√£o utilizados para infer√™ncia de CPU + GPU usando o principamente o pacote <a href="https://pypi.org/project/llama-cpp-python/">llama-cpp-python</a>.</p>
<p>Para mais informa√ß√µes sobre como configurar o modelo consulte <a href="https://swharden.com/blog/2023-07-29-ai-chat-locally-with-python/">este link</a></p>
<pre class="python"><code>def load_llama_model(model_path=&quot;./input/llama-2-7b-chat.ggmlv3.q2_K.bin&quot;, language=&#39;en&#39;, seed=42):
    # Determinar o tamanho da janela de contexto com base no idioma
    if language == &#39;en&#39;:
        context_window = df.text_en.map(len).max()
    elif language == &#39;pt&#39;:
        context_window = df.text_pt.map(len).max()
    else:
        raise ValueError(&quot;Language must be &#39;en&#39; or &#39;pt&#39;.&quot;)

    # Carregar o modelo Llama
    return Llama(model_path=model_path,
                 verbose=False,
                 n_ctx=context_window,
                 seed=seed)</code></pre>
<p>Para obter os melhores resultados, devemos ser o mais claro e espec√≠ficos poss√≠vel nas intera√ß√µes. Por√©m devemos iniciar com um prompt simples e r√°pido para ir direcionando o modelo na dire√ß√£o desejada e avaliando os resultados obtidos e ajustando gradualmente o prompt para refinar e aprimorar a resposta desejada</p>
<pre class="python"><code>def classify_sentiment_llama(text, llama_model):
    # Construir a prompt para o modelo Llama
    prompt = f&#39;&#39;&#39; \
    Q: Answer with just one word, \
    does the following text express a \
    positive or negative feeling? \
    {text} \
    A:&#39;&#39;&#39;
    # Obter a sa√≠da do modelo Llama
    output = llama_model(prompt, max_tokens=3)
    return output[&quot;choices&quot;][0][&quot;text&quot;]</code></pre>
<p>Com nosso prompt definido, j√° podemos carregar o modelo:</p>
<pre class="python"><code># Carregar o modelo Llama para o idioma desejado
llama_model = load_llama_model(language=&#39;en&#39;)</code></pre>
<pre><code>## llama.cpp: loading model from ./llama-2-7b-chat.ggmlv3.q2_K.bin
## llama_model_load_internal: format     = ggjt v3 (latest)
## llama_model_load_internal: n_vocab    = 32000
## llama_model_load_internal: n_ctx      = 4320
## llama_model_load_internal: n_embd     = 4096
## llama_model_load_internal: n_mult     = 256
## llama_model_load_internal: n_head     = 32
## llama_model_load_internal: n_head_kv  = 32
## llama_model_load_internal: n_layer    = 32
## llama_model_load_internal: n_rot      = 128
## llama_model_load_internal: n_gqa      = 1
## llama_model_load_internal: rnorm_eps  = 5.0e-06
## llama_model_load_internal: n_ff       = 11008
## llama_model_load_internal: freq_base  = 10000.0
## llama_model_load_internal: freq_scale = 1
## llama_model_load_internal: ftype      = 10 (mostly Q2_K)
## llama_model_load_internal: model size = 7B
## llama_model_load_internal: ggml ctx size =    0.08 MB
## llama_model_load_internal: mem required  = 2733.66 MB (+ 2160.00 MB per state)
## llama_new_context_with_model: kv self size  = 2160.00 MB
## llama_new_context_with_model: compute buffer total size =  295.35 MB</code></pre>
<p>Ap√≥s instanciar o modelo, basta aplic√°-lo em nossa base de dados. (apliquei o mesmo modelo tanto para as reviews e portugu√™s quanto em ingl√™s).</p>
<pre class="python"><code>df[&#39;sentiment_llm_en&#39;] = df.text_en.progress_apply(lambda x: classify_sentiment_llama(x, llama_model))</code></pre>
<p><img src="/post/2024-04-20-sentiment-analysis-llama2/load_en.png" /></p>
<p>Como este modelo √© o mais b√°sico e n√£o alteramos nenhum par√¢metro (como por exemplo <code>temperature</code>, que determina se o output ser√° mais aleat√≥rio ou mais previs√≠vel) pode ser que a sa√≠da n√£o saia padronizada e necessite de algum p√≥s-processamento. Vejamos como foram os outputs do LLM:</p>
<details>
<summary>
<em>Clique aqui para ver o c√≥digo do gr√°fico</em>
</summary>
<pre class="python"><code># Contagem da frequ√™ncia das classifica√ß√µes
sentiment_llm_counts = df.groupby(&#39;sentiment&#39;).sentiment_llm_en.value_counts().reset_index(name=&#39;n&#39;)

# Organizar as categorias pela frequ√™ncia total
order = df.sentiment_llm_en.value_counts().reset_index(name=&#39;n&#39;)
order = order.sort_values(by=&#39;n&#39;, ascending=False)[&#39;index&#39;]

# Configura√ß√µes de estilo do seaborn
sns.set(style=&quot;whitegrid&quot;)

# Criar o gr√°fico de barras
plt.figure(figsize=(12, 4))
ax = sns.barplot(x=sentiment_llm_counts.sentiment_llm_en, y=sentiment_llm_counts.n, hue=sentiment_llm_counts.sentiment, order=order, palette=[&quot;red&quot;, &quot;green&quot;])

# Adicionar r√≥tulos e t√≠tulo
plt.ylim([0, 25])
plt.xticks(fontsize=12, rotation=90)
plt.yticks(fontsize=12)
ax.set_xlabel(&#39;Anota√ß√£o de sentimento das resenhas&#39;, fontsize=14)
ax.set_ylabel(&#39;Frequ√™ncia&#39;, fontsize=14)
ax.set_title(&#39;Frequ√™ncia dos sentimentos classificados pelo LLM em Ingl√™s\nem rela√ß√£o aos sentimentos j√° anotados da base&#39;, fontsize=20)

# Adicionar anota√ß√µes nas barras
for p in ax.patches:
    ax.annotate(f&#39;{p.get_height()}&#39;, (p.get_x() + p.get_width() / 2., p.get_height()),
                ha=&#39;center&#39;, va=&#39;baseline&#39;, fontsize=10, color=&#39;black&#39;, xytext=(0, 5),
                textcoords=&#39;offset points&#39;)

plt.legend(loc=&quot;upper right&quot;, title = &quot;Label real&quot;)

# Remover bordas da parte superior e direita
ax.spines[&#39;top&#39;].set_visible(False)
ax.spines[&#39;right&#39;].set_visible(False)
ax.grid(False)

# Salvar a nuvem de palavras como imagem
plt.savefig(f&quot;img/freq_class_llm_en.png&quot;, bbox_inches=&#39;tight&#39;)

# Exibir o gr√°fico
plt.show()</code></pre>
</details>
<!-- &nbsp; -->
<center>
<img src="/post/2024-04-20-sentiment-analysis-llama2/freq_class_llm_en.png" />
</center>
<div class="w3-panel w3-pale-blue w3-border">
<p>¬† <strong>üìå Interpreta√ß√£o:</strong>
√â poss√≠vel observar que o modelo pr√©-treinado conseguiu reconhecer de maneira bastante coerente o sentimento dos trechos para as categorias <code>pos</code> e <code>neg</code>, por√©m, n√£o vieram padronizadas exatamente como solicitamos ao modelo.</p>
</div>
<p>Como a sa√≠da n√£o foi padronizada, vamos realizar algum p√≥s-processamento para padronizar as classes como <code>pos</code> ou <code>neg</code> para possibilitar avaliar o desempenho do modelo com base em m√©tricas de classifica√ß√£o.</p>
<pre class="python"><code>conditions = [
    (df.sentiment_llm_en.str.contains(&#39;(?i)(?:pos|fun)&#39;)),
    (df.sentiment_llm_en.str.contains(&#39;(?i)(?:neg|horrible|melanchol)&#39;))
]
pd.crosstab(df.sentiment, np.select(conditions, [&#39;pos&#39;, &#39;neg&#39;], default=&#39;other&#39;))</code></pre>
<p>Com os outputs padronizados em duas classes, podemos verificar como foi a acur√°cia do modelo.</p>
</div>
<div id="desempenho" class="section level4">
<h4>Desempenho</h4>
<p>Como estamos diante de um problema de classifica√ß√£o, avaliaremos o desempenho do modelo com matrizes de confus√£o para entender a as taxas de acerto e calcular a acur√°cia pois o dataset √© balanceado.</p>
<details>
<summary>
<em>Clique aqui para ver o c√≥digo do gr√°fico</em>
</summary>
<pre class="python"><code># Matrizes de Confus√£o
conditions = [
    (df.sentiment_llm_en.str.contains(&#39;(?i)(?:pos|fun|good|comedy)&#39;)),
    (df.sentiment_llm_en.str.contains(&#39;(?i)(?:neg|melanchol|absurd|horrible)&#39;))
]
cm_llm_en = confusion_matrix(df.sentiment, np.select(conditions, [&#39;pos&#39;, &#39;neg&#39;], default=&#39;other&#39;))
accuracy_llm_en = accuracy_score(df.sentiment, np.select(conditions, [&#39;pos&#39;, &#39;neg&#39;], default=&#39;other&#39;))

conditions = [
    (df.sentiment_llm_pt.str.contains(&#39;(?i)(?:pos)&#39;)),
    (df.sentiment_llm_pt.str.contains(&#39;(?i)(?:neg|horr√≠vel)&#39;))
]
cm_llm_pt = confusion_matrix(df.sentiment, np.select(conditions, [&#39;pos&#39;, &#39;neg&#39;], default=&#39;other&#39;))
accuracy_llm_pt = accuracy_score(df.sentiment, np.select(conditions, [&#39;pos&#39;, &#39;neg&#39;], default=&#39;other&#39;))

# Configura√ß√µes de estilo do seaborn
sns.set(font_scale=1.2)
plt.figure(figsize=(12, 5))

# Plotar Matriz de Confus√£o para o modelo de LLM em ingl√™s
plt.subplot(1, 2, 1)
sns.heatmap(cm_llm_en, annot=True, fmt=&#39;d&#39;, cmap=&#39;binary&#39;, cbar=False, vmin=0, vmax=50,
            xticklabels=[&#39;Negativo&#39;, &#39;Neutro&#39;, &#39;Positivo&#39;], yticklabels=[&#39;Negativo&#39;, &#39;Neutro&#39;, &#39;Positivo&#39;])
plt.title(f&#39;Matriz de Confus√£o (Vader - Ingl√™s)\nAcur√°cia: {accuracy_llm_en:.0%}&#39;, fontsize=22)
plt.xlabel(&#39;Previsto&#39;, fontsize=14)
plt.ylabel(&#39;Real&#39;, fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)

# Plotar Matriz de Confus√£o para o modelo de LLM em portugu√™s
plt.subplot(1, 2, 2)
sns.heatmap(cm_llm_pt, annot=True, fmt=&#39;d&#39;, cmap=&#39;binary&#39;, cbar=False,vmin=0, vmax=50,
            xticklabels=[&#39;Negativo&#39;, &#39;Neutro&#39;, &#39;Positivo&#39;], yticklabels=[&#39;Negativo&#39;, &#39;Neutro&#39;, &#39;Positivo&#39;])
plt.title(f&#39;Matriz de Confus√£o (Vader - Portugu√™s)\nAcur√°cia: {accuracy_llm_pt:.0%}&#39;, fontsize=22)
plt.xlabel(&#39;Previsto&#39;, fontsize=14)
plt.ylabel(&#39;Real&#39;, fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)

# Ajustar layout
plt.tight_layout()

# Salvar a nuvem de palavras como imagem
plt.savefig(f&quot;img/cm_llm.png&quot;, bbox_inches=&#39;tight&#39;)

# Exibir o gr√°fico
plt.show()</code></pre>
</details>
<!-- &nbsp; -->
<center>
<img src="/post/2024-04-20-sentiment-analysis-llama2/cm_llm2.png" />
</center>
<div class="w3-panel w3-pale-blue w3-border">
<p>¬† <strong>üìå Interpreta√ß√£o:</strong>
A acur√°cia geral para a l√≠ngua Inglesa foi superior quando aplicado o mesmo modelo para a l√≠ngua portuguesa. Vale lembrar que este modelo foi treinado em Ingl√™s e estamos utilizado a menor das op√ß√µes.</p>
</div>
<p>O desempenho deste modelo √© muito interessante, principalmente por j√° ser pr√© treinado, n√£o sendo necess√°rio gastar tanto tempo na sua constru√ß√£o mas para afirmar que este modelo √© bom precisamos entender qual seria o resultado para resolver este problemas se utilizassemos a abordagem mais simples poss√≠vel.</p>
</div>
</div>
<div id="vader" class="section level3">
<h3>Vader</h3>
<!-- ## Baseline -->
<p>O <em><strong>VADER</strong> (Valence Aware Dictionary and sEntiment Reasoner)</em> √© uma abordagem mais simples e r√°pida em compara√ß√£o aos LLMs. N√£o requer o treinamento de um modelo, mas depende de l√©xicos de palavras relacionadas a sentimentos. Pode ser facilmente utilizado via bibliotecas de c√≥digo aberto em Python, como <a href="https://pypi.org/project/vaderSentiment/">vaderSentiment</a> para ingl√™s e <a href="https://github.com/rafjaa/LeIA">LeIA (L√©xico para Infer√™ncia Adaptada)</a> para portugu√™s.</p>
<p>A abordagem √© direta: no l√©xico (uma cole√ß√£o de palavras), cada palavra j√° possui uma nota atribu√≠da. Ao passar um documento (frase), retorna um dicion√°rio com o escore de polaridade com base no escore das palavras no texto. O dicion√°rio inclui o valor do sentimento geral normalizado (<code>compound</code>), variando de -1 (extremamente negativo) a +1 (extremamente positivo). Esse valor pode ser usado para descrever o sentimento predominante no texto, considerando os seguintes limites:</p>
<ul>
<li>Sentimento <span style="color: green;">positivo</span>: <code>compound</code> &gt;= 0.05</li>
<li>Sentimento <span style="color: red;">negativo</span>: <code>compound</code> &lt;= -0.05</li>
<li>Sentimento <span style="color: orange;">neutro</span>: (<code>compound</code> &gt; -0.05) e (<code>compound</code> &lt; 0.05)</li>
</ul>
<details>
<summary>
<em>Clique aqui para ver a fun√ß√£o utilizada para classificar o sentimento com base no escore <code>compound</code></em>
</summary>
<pre class="python"><code># Fun√ß√£o para classificar o sentimento com base no compound score
def classify_sentiment_vader(text, language=&#39;en&#39;):

    # Definir m√©todo que ser√° utilizado
    if language==&#39;en&#39;:
        from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
    elif language == &#39;pt&#39;:
        from leia import SentimentIntensityAnalyzer
    else:
        raise ValueError(&quot;Language must be &#39;en&#39; or &#39;pt&#39;.&quot;)

    # Instanciar a ferramenta para an√°lise de sentimentos
    analyzer = SentimentIntensityAnalyzer()
    # Realiza a an√°lise de sentimentos e obt√©m o compound score
    compound_score = analyzer.polarity_scores(text)[&#39;compound&#39;]
    # Classifica o sentimento com base no compound score
    if compound_score &gt;= 0.05:
        return &#39;pos&#39;
    elif compound_score &lt;= -0.05:
        return &#39;neg&#39;
    else:
        return &#39;neu&#39;

# Criando uma nova coluna &#39;sentimento_vader&#39;
df[&#39;sentiment_vader_en&#39;] = df.text_en.apply(lambda x: classify_sentiment_vader(x, &#39;en&#39;))
df[&#39;sentiment_vader_pt&#39;] = df.text_pt.apply(lambda x: classify_sentiment_vader(x, &#39;pt&#39;))</code></pre>
</details>
<!-- &nbsp; -->
<p>A execu√ß√£o do c√≥digo √© bem r√°pida, sendo √∫til para refer√™ncia como baseline ou em casos em que temos baixo recurso computacional e um grande volume de dados para classificar.</p>
<div id="desempenho-1" class="section level4">
<h4>Desempenho</h4>
<p>Como estamos diante de um problema de classifica√ß√£o, avaliaremos o desempenho do modelo com matrizes de confus√£o para entender a as taxas de acerto e calcular a acur√°cia pois o dataset √© balanceado.</p>
<details>
<summary>
<em>Clique aqui para ver o c√≥digo do gr√°fico</em>
</summary>
<pre class="python"><code># Matrizes de Confus√£o
cm_vader_en = confusion_matrix(df.sentiment, df.sentiment_vader_en)
cm_vader_pt = confusion_matrix(df.sentiment, df.sentiment_vader_pt)

# Acur√°cias
accuracy_vader_en = accuracy_score(df.sentiment, df.sentiment_vader_en)
accuracy_vader_pt = accuracy_score(df.sentiment, df.sentiment_vader_pt)

# Configura√ß√µes de estilo do seaborn
sns.set(font_scale=1.2)
plt.figure(figsize=(12, 5))

# Plotar Matriz de Confus√£o para o m√©todo Vader em ingl√™s
plt.subplot(1, 2, 1)
sns.heatmap(cm_vader_en, annot=True, fmt=&#39;d&#39;, cmap=&#39;binary&#39;, cbar=False,vmin=0, vmax=50,
            xticklabels=[&#39;Negativo&#39;, &#39;Neutro&#39;, &#39;Positivo&#39;], yticklabels=[&#39;Negativo&#39;, &#39;Neutro&#39;, &#39;Positivo&#39;])
plt.title(f&#39;Matriz de Confus√£o (Vader - Ingl√™s)\nAcur√°cia: {accuracy_vader_en:.0%}&#39;, fontsize=22)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.xlabel(&#39;Previsto&#39;, fontsize=14)
plt.ylabel(&#39;Real&#39;, fontsize=14)

# Plotar Matriz de Confus√£o para o m√©todo Vader em portugu√™s
plt.subplot(1, 2, 2)
sns.heatmap(cm_vader_pt, annot=True, fmt=&#39;d&#39;, cmap=&#39;binary&#39;, cbar=False,vmin=0, vmax=50,
            xticklabels=[&#39;Negativo&#39;, &#39;Neutro&#39;, &#39;Positivo&#39;], yticklabels=[&#39;Negativo&#39;, &#39;Neutro&#39;, &#39;Positivo&#39;])
plt.title(f&#39;Matriz de Confus√£o (Vader - Portugu√™s)\nAcur√°cia: {accuracy_vader_pt:.0%}&#39;, fontsize=22)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.xlabel(&#39;Previsto&#39;, fontsize=14)
plt.ylabel(&#39;Real&#39;, fontsize=14)

# Ajustar layout
plt.tight_layout()

# Salvar a nuvem de palavras como imagem
plt.savefig(f&quot;img/cm_vader.png&quot;, bbox_inches=&#39;tight&#39;)

# Exibir o gr√°fico
plt.show()</code></pre>
</details>
<!-- &nbsp; -->
<center>
<img src="/post/2024-04-20-sentiment-analysis-llama2/cm_vader.png" />
</center>
<div class="w3-panel w3-pale-blue w3-border">
<p>¬† <strong>üìå Interpreta√ß√£o:</strong> A acur√°cia geral do m√©todo foi praticamente o mesmo para ambas as linguas. Na lingua inglesa observamos mais casos de falsos positivos (22%), j√° na lingua portuguesa observamos mais casos de falsos negativos (14%).</p>
</div>
<p>Essa abordagem √© boa para ser utilizada como baseline pois quase todas as abordagens tradicionais de Machine Learning para a tarefa de an√°lise de sentimentos necessitam de tempo para desenvolvimento, treino, valida√ß√£o e sustenta√ß√£o de modelos.</p>
</div>
</div>
</div>
</div>
<div id="resultado-final" class="section level1">
<h1>Resultado Final</h1>
<hr />
<p>Avaliamos o desempenho de ambas as abordagens para determinar se o uso do LLM justificou-se em compara√ß√£o com a abordagem mais simples para a execu√ß√£o da tarefa de an√°lise de sentimentos.</p>
<details>
<summary>
<em>Clique aqui para ver o c√≥digo do gr√°fico</em>
</summary>
<pre class="python"><code>models = (
    &quot;Ingl√™s&quot;,
    &quot;Portugu√™s&quot;,
)
weight_counts = {
    &quot;Vader&quot;: np.array([accuracy_vader_en,
                       accuracy_vader_pt]),
    &quot;LLM&quot;: np.array([accuracy_llm_en-accuracy_vader_en,
                     accuracy_llm_pt-accuracy_vader_pt]),
}

fig, ax = plt.subplots()
bottom = np.zeros(2)
colors=[&quot;#b4dbe6&quot;, &quot;#024b7a&quot;]
for (boolean, weight_count), col in zip(weight_counts.items(), colors):
    p = ax.bar(models, weight_count, width=0.5, label=boolean, bottom=bottom, color=col)
    bottom += weight_count

# Formatar eixos
plt.ylim([0, 1.1])
plt.xlabel(&#39;Idioma das resenhas dos filmes&#39;, fontsize=14)
plt.ylabel(&#39;Ganho de Acur√°cia&#39;, fontsize=14)
plt.title(&quot;Compara√ß√£o do ganho de acur√°cia \ndo LLM em rela√ß√£o ao Vader&quot;, fontsize=16, x=0.5)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)

# Legenda
ax.legend(loc=&quot;upper right&quot;, title=&#39;M√©todo utilizado&#39;)
#specify order of items in legend
handles, labels = plt.gca().get_legend_handles_labels()
order = [1, 0]
plt.legend([handles[idx] for idx in order],[labels[idx] for idx in order])

accs=[x*100 for x in [accuracy_vader_en, accuracy_vader_pt, accuracy_llm_en, accuracy_llm_pt]]
for p, acc in zip(ax.patches, accs):
    width, height = p.get_width(), p.get_height()
    x, y = p.get_xy()
    ax.text(x+width/2,
            y+(height/2) - 0.01,
            &#39;{:.0f} %&#39;.format(acc),
            horizontalalignment=&#39;center&#39;,
            verticalalignment=&#39;center&#39;,
            color=&#39;white&#39;, fontsize=18)

# Adicionar setas e textos na figura
plt.arrow(0.3, 0.62, 0, 0.16,
          head_width = 0.05,
          width = 0.015,
          color=&#39;black&#39;)
plt.text(0.2, 0.9, &#39;+20,0%&#39;, fontsize = 20)

plt.arrow(0.7, 0.63, 0, 0.09,
          head_width = 0.05,
          width = 0.015,
          color=&#39;black&#39;)
plt.text(0.6, 0.84, &#39;+11,53%&#39;, fontsize = 20)

# Remover bordas da parte superior e direita
ax.spines[&#39;top&#39;].set_visible(False)
ax.spines[&#39;right&#39;].set_visible(False)
ax.spines[&#39;bottom&#39;].set_visible(True)
ax.spines[&#39;left&#39;].set_visible(True)
ax.grid(visible=None)
ax.set_facecolor(&#39;white&#39;)

# Ajustar layout
plt.tight_layout()

# Salvar a nuvem de palavras como imagem
plt.savefig(f&quot;img/acc_comparation.png&quot;, bbox_inches=&#39;tight&#39;)

# Exibir o gr√°fico
plt.show()</code></pre>
</details>
<!-- &nbsp; -->
<center>
<img src="/post/2024-04-20-sentiment-analysis-llama2/acc_comparation.png" />
</center>
<div class="w3-panel w3-pale-blue w3-border">
<p>¬† <strong>üìå Interpreta√ß√£o:</strong> A acur√°cia geral foi consideravelmente maior para o modelo Llama2 em ambas as l√≠nguas, mesmo sendo treinado principalmente em dados da l√≠ngua inglesa.</p>
</div>
</div>
<div id="conclus√£o-e-discuss√£o" class="section level1">
<h1>Conclus√£o e Discuss√£o</h1>
<hr />
<p>Os avan√ßos tecnol√≥gicos na √°rea s√£o verdadeiramente impressionantes e evidenciam a r√°pida evolu√ß√£o da intelig√™ncia artificial. √â importante estarmos sempre atentos a essas mudan√ßas, pois a √°rea de LLMs est√° em constante crescimento e melhorias significativas s√£o desenvolvidas diariamente.</p>
<p>Em meio a tantos avan√ßos, tamb√©m √© importante reconhecer as limita√ß√µes desses modelos. Um dos desafios √© o corte de conhecimento (knowledge cutoffs), o que significa que o modelo √© treinado at√© uma determinada data, como 2022, portanto n√£o possui conhecimento sobre eventos ou desenvolvimentos que ocorreram ap√≥s essa data. Al√©m disso, os LLMs est√£o sujeitos a ‚Äúhallucinations‚Äù, ou seja, podem inventar informa√ß√µes em um tom muito confiante, o que pode levar a resultados imprecisos ou at√© mesmo prejudiciais.</p>
<p>Outras limita√ß√µes incluem restri√ß√µes no input e output dos modelos, o que pode tornar dif√≠cil lidar com grandes volumes de dados ou fornecer resultados completos de uma s√≥ vez. Al√©m disso, os LLMs geralmente n√£o funcionam bem com dados estruturados, como tabelas, e podem reproduzir vieses e toxicidade presentes na sociedade, o que levanta preocupa√ß√µes √©ticas e sociais importantes.</p>
<p>Portanto, enquanto exploramos esse vasto campo das redes neurais, √© essencial abordar essas limita√ß√µes e desenvolver solu√ß√µes que permitam o uso √©tico e respons√°vel dessas poderosas ferramentas de IA.</p>
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias</h1>
<hr />
<ul>
<li><a href="https://medium.com/mapegy-tech/large-scale-language-models-for-innovation-and-technology-intelligence-sentiment-analysis-on-news-2c1ed1f6f2ad">Large-scale language models for innovation and technology intelligence: sentiment analysis on news articles</a></li>
<li><a href="https://medium.com/luisfredgs/an%C3%A1lise-de-sentimentos-com-redes-neurais-recorrentes-lstm-a5352b21e6aa">An√°lise de sentimentos com redes neurais recorrentes LSTM</a></li>
<li><a href="https://www.coursera.org/programs/applied-intelligence-workera-vshgt/learn/generative-ai-for-everyone?authProvider=accenture-main">Generative AI for Everyone - Andrew Ng - Coursera Course</a></li>
<li><a href="https://swharden.com/blog/2023-07-29-ai-chat-locally-with-python/">Run Llama 2 Locally with Python</a></li>
</ul>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2024-04-20-sentiment-analysis-llama2/">An√°lise de Sentimentos com um &#34;ChatGPT&#34; de C√≥digo Aberto</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category>Texto e NLP</category>
      <category domain="tag">chatgpt</category>
      <category domain="tag">data-science</category>
      <category domain="tag">gam</category>
      <category domain="tag">inteligencia-artificial</category>
      <category domain="tag">llama2</category>
      <category domain="tag">llm</category>
      <category domain="tag">python</category>
      <category domain="tag">redes-neurais</category>
      <category domain="tag">sentiment-analysis</category>
    </item>
  </channel>
</rss>