&lt;?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>kaggle on Fellipe Gomes - Data Science Blog</title>
    <link>https://gomesfellipe.github.io/tags/kaggle/</link>
    <description>√öltimos posts sobre Data Science, Machine Learning e R</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>pt-br</language>
    <managingEditor>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</managingEditor>
    <webMaster>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</webMaster>
    <lastBuildDate>Sun, 26 May 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://gomesfellipe.github.io/tags/kaggle/" rel="self" type="application/rss+xml" />
    <item>
      <title>Detec√ß√£o de Linguagem T√≥xica com o LLM Gemma e LangChain</title>
      <link>https://gomesfellipe.github.io/post/2024-05-26-detec-o-de-linguagem-t-xica-com-o-llm-gemma-e-langchain/</link>
      <pubDate>Sun, 26 May 2024 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2024-05-26-detec-o-de-linguagem-t-xica-com-o-llm-gemma-e-langchain/</guid>
      <description>Neste post utilizaremos o modelo Gemma de IA generativa do Google com framework LangChain auxiliando na tarefa de prompt engineering</description>
      <content:encoded>&lt;![CDATA[
        


<div id="caso-de-uso-de-ia-generativa-detec√ß√£o-de-linguagem-t√≥xica-em-m√≠dias-sociais" class="section level1">
<h1>Caso de Uso de IA Generativa: Detec√ß√£o de Linguagem T√≥xica em M√≠dias Sociais</h1>
<hr />
<p>Neste post, realizaremos a tarefa de detec√ß√£o de linguagem t√≥xica em m√≠dias sociais usando o modelo <a href="https://ai.google.dev/gemma?hl=pt-br">Gemma</a> de IA generativa do Google com o framework <a href="https://www.langchain.com/">LangChain</a>. Vamos explorar como o texto de entrada afeta a sa√≠da do modelo e faremos alguma engenharia de prompts para direcion√°-lo √† tarefa necess√°ria.</p>
</div>
<div id="setup" class="section level1">
<h1>Setup</h1>
<p>Utilizaremos o ambiente do Kaggle para desenvolvimento deste notebook, que disponibiliza a utiliza√ß√£o de GPUs. Atrav√©s do <em>Hardware Accelerator</em> utilizaremos a <a href="https://www.kaggle.com/docs/efficient-gpu-usage">NVIDIA TESLA P100 GPU</a>.</p>
<div id="instalar-e-carregar-dependencias" class="section level2">
<h2>Instalar e carregar dependencias</h2>
<p>Vamos instalar as bibliotecas <code>accelerate</code> e <code>bitsandbytes</code> que possibilitam a quantiza√ß√£o de LLMs e algumas bibliotecas do framework LangChain</p>
<pre class="python"><code>!pip install accelerate
!pip install -i https://pypi.org/simple/ bitsandbytes
!pip install langchain langchain_huggingface langchain_community langchain_chroma</code></pre>
</div>
<div id="carregar-bibliotecas" class="section level2">
<h2>Carregar bibliotecas</h2>
<pre class="python"><code>import pandas as pd
import torch 
import re

from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
from langchain_huggingface import HuggingFacePipeline
from langchain_core.prompts.few_shot import PromptTemplate, FewShotPromptTemplate
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_chroma import Chroma</code></pre>
</div>
<div id="carregar-fun√ß√µes-auxiliares" class="section level2">
<h2>Carregar fun√ß√µes auxiliares</h2>
<p>Carregar uma fun√ß√£o para limpeza simples dos tweets.</p>
<details>
<summary>
<em>Clique aqui para ver os c√≥digos</em>
</summary>
<pre class="python"><code>def clean_tweet(text):
    &quot;&quot;&quot;
    src: https://github.com/lrdsouza/told-br-classifier
    &quot;&quot;&quot;
    text = text.replace(&#39;rt @user&#39;, &#39;&#39;)
    text = text.replace(&#39;@user&#39;, &#39;&#39;)
    pattern = re.compile(&#39;[^a-zA-Z0-9\s√°√©√≠√≥√∫√†√®√¨√≤√π√¢√™√Æ√¥√ª√£√µ√ß√Å√â√ç√ì√ö√Ä√à√å√í√ô√Ç√ä√é√î√õ√É√ï√á]&#39;)
    text = re.sub(r&#39;http\S+&#39;, &#39;&#39;, text)
    text = pattern.sub(r&#39; &#39;, text)
    text = text.replace(&#39;\n&#39;, &#39; &#39;)
    text = &#39; &#39;.join(text.split())
    return text</code></pre>
</details>
<p>¬†</p>
</div>
</div>
<div id="carregar-dados" class="section level1">
<h1>Carregar dados</h1>
<hr />
<p>Vamos utilizar o conjunto de dados <a href="https://github.com/JAugusto97/ToLD-Br">TolD-br</a>, um recurso interessante para o estudo da toxicidade em conte√∫dos online em portugu√™s brasileiro. Este dataset foi utilizado na competi√ß√£o <a href="https://www.kaggle.com/competitions/ml-olympiad-toxic-language-ptbr-detection">ML Olympiad - Toxic Language (PTBR) Detection</a>, organizada pelo <a href="https://www.youtube.com/@tensorflowugsp">TensorFlow UGSP</a> no Kaggle este ano. A competi√ß√£o convidou entusiastas de dados, cientistas e pesquisadores a desenvolverem modelos de machine learning capazes de classificar tweets em portugu√™s brasileiro como t√≥xicos ou n√£o t√≥xicos.</p>
<pre class="python"><code>train = pd.read_csv(&quot;/kaggle/input/ml-olympiad-toxic-language-ptbr-detection/train (2).csv&quot;)
test = pd.read_csv(&quot;/kaggle/input/ml-olympiad-toxic-language-ptbr-detection/test (4).csv&quot;)
sub = pd.read_csv(&quot;/kaggle/input/ml-olympiad-toxic-language-ptbr-detection/sample_submission.csv&quot;)</code></pre>
<p>Selecionar uma amostra para auxiliar no desenvolvimento do prompt para utiliza√ß√£o em novos dados:</p>
<pre class="python"><code>valid = train.sample(n=100, random_state=123)</code></pre>
<div id="preparar-dados" class="section level2">
<h2>Preparar dados</h2>
<p>Aplicar limpeza b√°sica para preparar os tweets.</p>
<details>
<summary>
<em>Clique aqui para ver os c√≥digos</em>
</summary>
<pre class="python"><code>train[&#39;text&#39;] = train.text.apply(lambda x: clean_tweet(x))
valid[&#39;text&#39;] = valid.text.apply(lambda x: clean_tweet(x))
test[&#39;text&#39;] = test.text.apply(lambda x: clean_tweet(x))</code></pre>
</details>
<p>¬†</p>
</div>
</div>
<div id="carregar-modelo" class="section level1">
<h1>Carregar Modelo</h1>
<hr />
<p>Neste notebook, faremos uso de um modelo da fam√≠lia <a href="https://ai.google.dev/gemma?hl=pt-br">Gemma</a>, desenvolvida pelo Google, que consiste em modelos leves e de c√≥digo aberto constru√≠dos com base em pesquisas e tecnologias empregadas no desenvolvimento dos modelos <a href="https://gemini.google.com/">Gemini</a></p>
<div class="w3-panel w3-pale-yellow w3-border">
<p>¬† <strong>üìå Nota:</strong> Para utilizar o modelo √© necess√°rio consentir com a <a href="https://www.kaggle.com/models/google/gemma/license/consent?returnUrl=%2Fmodels%2Fgoogle%2Fgemma%2Ftransformers">licen√ßa do Gemma</a> com o preenchimento de um formul√°rio dispon√≠vel na <a href="https://www.kaggle.com/models/google/gemma">p√°gina do modelo</a>.</p>
</div>
<p>Utilizaremos a implementa√ß√£o do <a href="https://huggingface.co/google/gemma-7b-it">Gemma-7b-instruct</a>, que √© uma variante ajustada por instru√ß√£o (IT) que pode ser usada para bate-papo e/ou seguir instru√ß√µes.</p>
<pre class="python"><code># Caminho para o modelo dispon√≠vel pelo ambiente do Kaggle
model_path = &quot;/kaggle/input/gemma/transformers/1.1-7b-it/1/&quot;

# Definir configuracoes de quantizacao para reduzir 
# o tamanho do modelo perdendo pouca performance
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=&quot;nf4&quot;,
    bnb_4bit_compute_dtype=torch.bfloat16
)

# Instanciar o tokenizador do LLM
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Instanciar o LLM
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    quantization_config=quantization_config,
    low_cpu_mem_usage=True, 
    device_map=&quot;auto&quot;
)</code></pre>
<p>Vamos carregar tamb√©m um modelo de embedding que utilizaremos para auxiliar na constru√ß√£o do nosso prompt:</p>
<pre class="python"><code>embeddings = HuggingFaceEmbeddings(model_name=&quot;all-MiniLM-L6-v2&quot;)</code></pre>
<div id="preparar-modelo" class="section level2">
<h2>Preparar modelo</h2>
<p>Os modelos da Hugging Face podem ser facilmente executados localmente utilizando a classe <code>HuggingFacePipeline</code>. O <a href="https://huggingface.co/models">Hugging Face Model Hub</a> √© um reposit√≥rio que abriga mais de 120 mil modelos, 20 mil conjuntos de dados e 50 mil aplicativos de demonstra√ß√£o (Spaces), todos de c√≥digo aberto e dispon√≠veis publicamente. Esta plataforma online permite que as pessoas colaborem facilmente e construam modelos de machine learning juntas.</p>
<pre class="python"><code># Instanciar um pipeline transformers
pipe = pipeline(
    model=model,
    tokenizer=tokenizer,
    task=&quot;text-generation&quot;,
    max_new_tokens=1,
)

# Passar o pipeline para a classe do LangChain
llm = HuggingFacePipeline(pipeline=pipe)</code></pre>
</div>
</div>
<div id="prompt-engineering" class="section level1">
<h1>Prompt Engineering</h1>
<hr />
<p>Para resolver este problema, criaremos um template de prompt que utiliza a estrat√©gia few-shot, que pode ser constru√≠do a partir de um conjunto de exemplos. O conjunto de exemplos ser√° din√¢mico, sendo constru√≠do com base em tweets que possuem a maior similaridade semantica com o tweet de entrada.</p>
<div id="preparar-exemplos" class="section level2">
<h2>Preparar exemplos</h2>
<p>Selecionaremos os exemplos candidatos do conjunto de dados de treino que n√£o estejam no dataset de valida√ß√£o. Cada exemplo de entrada deve ser um dicion√°rio onde:</p>
<ul>
<li><code>key</code>: o nome das vari√°veis de inputs do prompt;</li>
<li><code>values</code>: os valores dos inputs.</li>
</ul>
<pre class="python"><code># indices de instancias que nao estao no dataset de validacao (evitar leak)
idx_train_examples = train.loc[~train.index.isin(valid.index)].index

# organizar a lista com os exemplos candidatos
examples = [{&#39;tweet&#39;: train.text[i], 
             &#39;label&#39;: str(train.label[i])} for i in idx_train_examples]</code></pre>
</div>
<div id="criar-template-para-os-exemplos-com-prompttemplate" class="section level2">
<h2>Criar template para os exemplos com <code>PromptTemplate</code></h2>
<p>Agora precisamos instanciar um <code>PromptTemplate</code> para nosso prompt, que recebe um template das instru√ß√µes que desejamos passar para o LLM e os inputs que alimentam este template:</p>
<pre class="python"><code>example_template = &quot;&quot;&quot;
Tweet: {tweet}
Label: {label}
&quot;&quot;&quot;

# Instanciar o exemplo de prompt 
example_prompt = PromptTemplate(
    input_variables=[&quot;tweet&quot;, &quot;label&quot;], 
    template=example_template
)</code></pre>
</div>
<div id="inserir-exemplos-com-exampleselector" class="section level2">
<h2>Inserir exemplos com <code>ExampleSelector</code></h2>
<p>Agora vamos instanciar <code>SemanticSimilarityExampleSelector</code> para selecionar exemplos com base em sua semelhan√ßa com a entrada. Ele usa um modelo de embedding para calcular a similaridade entre a entrada e os exemplos de few-shot, bem como um armazenamento de vetores <a href="https://www.trychroma.com/">Chroma</a> para realizar a pesquisa do vizinho mais pr√≥ximo de maneira eficiente.</p>
<pre class="python"><code>example_selector = SemanticSimilarityExampleSelector.from_examples(
    examples = examples,
    embeddings = embeddings,
    vectorstore_cls = Chroma,
    k=3,
)</code></pre>
<p>Aumentar o n√∫mero de vizinhos mais pr√≥ximos n√£o garantir√° necessariamente resultados melhores. Normalmente k=6 no m√°ximo j√° √© suficiente. Se n√£o conseguir bons resultados assim, j√° seria mais indicado realizar um ajuste fino mesmo.</p>
</div>
<div id="preparar-o-fewshotprompttemplate" class="section level2">
<h2>Preparar o <code>FewShotPromptTemplate</code></h2>
<p>Finalmente, vamos definir a formata√ß√£o para a apresenta√ß√£o dos exemplos e, em seguida, usar <code>FewShotPromptTemplate</code> para para gerar o template final que ser√° utilizado como prompt com base nos valores de entrada.</p>
<pre class="python"><code>prefix = &quot;&quot;&quot;The following tweets are written in Brazilian Portuguese. \n\
You are a tweet classifier that identifies \
toxic language as 1 and non-toxic language as 0. \n\
Here are some examples:&quot;&quot;&quot;

suffix = &quot;&quot;&quot;
Tweet: {tweet} 
Label: &quot;&quot;&quot;

prompt = FewShotPromptTemplate(
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix=prefix,
    suffix=suffix,
    example_separator=&#39;\n&#39;,
    input_variables=[&quot;tweet&quot;],
)</code></pre>
<p>Vejamos como ser√° a formata√ß√£o do prompt para a classifica√ß√£o de cada tweet:</p>
<pre class="python"><code>print(prompt.format(tweet=valid.head(1).text.values[0]))</code></pre>
<pre><code>## The following tweets are written in Brazilian Portuguese. 
## You are a tweet classifier that identifies toxic language as 1 and non-toxic language as 0. 
## Here are some examples:
## 
## Tweet: caralho eu tenho q fazer alguma coisa mt importante mas eu esqueci o que √© ent√£o n deve ser importante
## Label: 1
## 
## Tweet: caralho as pessoas fazem me sentir a pessoa mais bosta e odiada poss√≠vel eu t√¥ bem
## Label: 0
## 
## Tweet: tenho quase certeza que isso e um homem escroto fingindo ser mulher kkkkkkkkk por um momento eu tbm pensei nisso
## Label: 0
## 
## Tweet: vei se um filho faz isso cmg eu pego o sandu√≠che e enfio no cu dele
## Label: </code></pre>
</div>
<div id="definir-custom-output-parsers" class="section level2">
<h2>Definir <code>Custom Output Parsers</code></h2>
<p>Para concluir a cadeia, vamos definir uma fun√ß√£o que funcione como um <a href="https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/custom/">Custom Output Parser</a>, que ser√° respons√°vel por pegar a sa√≠da do LLM e transform√°-la no formato mais adequado para nosso caso. Precisamos apenas do √∫ltimo caractere que ser√° retornado pelo LLM.</p>
<pre class="python"><code>def parse(response):
    &quot;&quot;&quot;Retorna apenas o ultimo caracter da sa√≠da do LLM&quot;&quot;&quot;
    return int(response[-1:])</code></pre>
</div>
<div id="definir-cadeira-langchain" class="section level2">
<h2>Definir Cadeira LangChain</h2>
<p><a href="https://python.langchain.com/v0.1/docs/modules/chains/">Chains</a> referem-se √† sequ√™ncias de chamadas - seja para um LLM, uma etapa de pr√©-processamento de dados, <a href="https://python.langchain.com/v0.1/docs/modules/tools/">tools</a>, ou ainda etapas de p√≥s-processamento do output gerado pelo modelo. As cadeias constru√≠das desta forma s√£o boas porque oferecem suporte nativo a streaming, ass√≠ncrono e infer√™ncia em batchs para uso.</p>
<pre class="python"><code>chain = prompt | llm | parse</code></pre>
<p>Vamos testar o comportamento da nossa cadeia em 1 tweet:</p>
<pre class="python"><code>print(f&quot;&quot;&quot;Tweet: {valid.head(1).text.values[0]}
Label: {valid.head(1).label.values[0]}
Predict: {chain.invoke({&#39;tweet&#39;:  valid.head(1).text.values[0]})}&quot;&quot;&quot;)</code></pre>
<pre><code>## Tweet: vei se um filho faz isso cmg eu pego o sandu√≠che e enfio no cu dele
## Label: 1
## Predict: 1</code></pre>
<p>Claramente um conte√∫do t√≥xico e que foi classificado corretamente. Mas como queremos realizar a chamada da nossa cadeia para diversos tweets do dataset de test, utilizaremos o m√©todo <code>.batch()</code> que executa a cadeia para uma lista de entradas:</p>
<pre class="python"><code>%%time
valid[&#39;predict&#39;] = chain.batch([{&#39;tweet&#39;: x} for x in valid.text])</code></pre>
<pre><code>## CPU times: user 1min 26s, sys: 24.8 s, total: 1min 51s
## Wall time: 1min 50s</code></pre>
</div>
<div id="avaliar-resultados" class="section level2">
<h2>Avaliar resultados</h2>
<p>Como a m√©trica de avalia√ß√£o da competi√ß√£o era a acur√°cia, vamos dar uma olhada em como ficou a matriz de confus√£o:</p>
<details>
<summary>
<em>Clique aqui para ver o c√≥digo do gr√°fico</em>
</summary>
<pre class="python"><code># Calcular m√©tricas
cm = confusion_matrix(valid.label, valid.predict)
acc=accuracy_score(valid.label, valid.predict)

# Configura√ß√µes de estilo do seaborn
sns.set(font_scale=1.2)
plt.figure(figsize=(5, 3))

# Plotar Matriz de Confus√£o para o m√©todo Vader em ingl√™s
sns.heatmap(cm, annot=True, fmt=&#39;d&#39;, cmap=&#39;binary&#39;, cbar=False,vmin=0, vmax=50,
            xticklabels=[&#39;N√£o T√≥xico&#39;, &#39;T√≥xico&#39;], yticklabels=[&#39;N√£o T√≥xico&#39;, &#39;T√≥xico&#39;])
plt.title(f&#39;Matriz de Confus√£o\nAcur√°cia: {acc:.0%}&#39;, fontsize=22)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.xlabel(&#39;Previsto&#39;, fontsize=14)
plt.ylabel(&#39;Real&#39;, fontsize=14)
plt.show()</code></pre>
</details>
<p>¬†</p>
<center>
<img src="/post/2024-05-26-detec-o-de-linguagem-t-xica-com-o-llm-gemma-e-langchain/cm.png" />
</center>
</div>
</div>
<div id="conclus√£o" class="section level1">
<h1>Conclus√£o</h1>
<hr />
<p>Embora nosso objetivo n√£o fosse alcan√ßar a perfei√ß√£o em termos de acur√°cia, at√© que o resultado foi satisfat√≥rio, dado o potencial dessa ferramenta para resolver uma ampla gama de problemas com poucas modifica√ß√µes nos c√≥digos. Existem muitos outros caminhos a serem explorados (inclusive recomendo assistir √† <a href="https://www.youtube.com/watch?v=bzU_STGxj7o&amp;t=6s">live no YouTube</a> em que os vencedores apresentaram solu√ß√µes muito mais eficientes), nosso foco aqui foi praticar, aplicar e documentar alguns conceitos interessantes e √∫teis sobre LLMs e LangChain.</p>
</div>
<div id="referencias" class="section level1">
<h1>Referencias</h1>
<hr />
<ul>
<li><a href="https://www.kaggle.com/competitions/ml-olympiad-toxic-language-ptbr-detection" class="uri">https://www.kaggle.com/competitions/ml-olympiad-toxic-language-ptbr-detection</a></li>
<li><a href="https://www.kaggle.com/models/google/gemma/transformers" class="uri">https://www.kaggle.com/models/google/gemma/transformers</a></li>
<li><a href="https://huggingface.co/google/gemma-1.1-7b-it" class="uri">https://huggingface.co/google/gemma-1.1-7b-it</a></li>
<li><a href="https://python.langchain.com/v0.1/docs/modules/model_io/prompts/few_shot_examples/" class="uri">https://python.langchain.com/v0.1/docs/modules/model_io/prompts/few_shot_examples/</a></li>
</ul>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2024-05-26-detec-o-de-linguagem-t-xica-com-o-llm-gemma-e-langchain/">Detec√ß√£o de Linguagem T√≥xica com o LLM Gemma e LangChain</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category>Texto e NLP</category>
      <category domain="tag">chatgpt</category>
      <category domain="tag">classification</category>
      <category domain="tag">data-science</category>
      <category domain="tag">gemma</category>
      <category domain="tag">google</category>
      <category domain="tag">inteligencia-artificial</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">llm</category>
      <category domain="tag">machine-learning</category>
      <category domain="tag">prophet</category>
    </item>
    <item>
      <title>Solu√ß√£o Final - ML Olympiad [1¬∫ lugar]</title>
      <link>https://gomesfellipe.github.io/post/2023-05-30-solucao-final-ensure-healthy-lives-kaggle-competition/</link>
      <pubDate>Tue, 30 May 2023 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2023-05-30-solucao-final-ensure-healthy-lives-kaggle-competition/</guid>
      <description>Confira a estrat√©gia aplicada para esta competi√ß√£o</description>
      <content:encoded>&lt;![CDATA[
        
<link href="https://gomesfellipe.github.io/post/2023-05-30-solucao-final-ensure-healthy-lives-kaggle-competition/index_files/vembedr/css/vembedr.css" rel="stylesheet" />

<div id="TOC">
<ul>
<li><a href="#introdu%C3%A7%C3%A3o" id="toc-introdu√ß√£o">Introdu√ß√£o</a></li>
<li><a href="#defini%C3%A7%C3%A3o-do-problema-de-neg%C3%B3cio" id="toc-defini√ß√£o-do-problema-de-neg√≥cio">Defini√ß√£o do problema de neg√≥cio</a></li>
<li><a href="#solu%C3%A7%C3%B5es" id="toc-solu√ß√µes">Solu√ß√µes</a></li>
<li><a href="#estrat%C3%A9gia-anal%C3%ADtica" id="toc-estrat√©gia-anal√≠tica">Estrat√©gia anal√≠tica</a>
<ul>
<li><a href="#decis%C3%B5es-sobre-a-target" id="toc-decis√µes-sobre-a-target">Decis√µes sobre a target</a></li>
<li><a href="#processamento-dos-dados" id="toc-processamento-dos-dados">Processamento dos Dados</a></li>
<li><a href="#dados-externos" id="toc-dados-externos">Dados Externos</a></li>
<li><a href="#feature-engineering" id="toc-feature-engineering">Feature Engineering</a></li>
<li><a href="#modelos" id="toc-modelos">Modelos</a></li>
<li><a href="#ensemble" id="toc-ensemble">Ensemble</a></li>
<li><a href="#post-processing" id="toc-post-processing">Post Processing</a></li>
</ul></li>
<li><a href="#considera%C3%A7%C3%B5es-finais" id="toc-considera√ß√µes-finais">Considera√ß√µes Finais</a></li>
<li><a href="#sobre-o-autor" id="toc-sobre-o-autor">Sobre o Autor</a></li>
</ul>
</div>

<div id="introdu√ß√£o" class="section level1">
<h1>Introdu√ß√£o</h1>
<p>O <a href="https://www.meetup.com/TensorFlowSP/events/284607061/">TFUG - TensorFlow Users Group de S√£o Paulo</a> lan√ßou uma nova <a href="https://www.kaggle.com/competitions/ml-olympiad-ensure-healthy-lives">competi√ß√£o no Kaggle</a> onde o objetivo era desenvolver modelos para previs√£o de diagn√≥stico de s√≠ndromes respirat√≥rias, que √© um tema relacionado com um dos 17 t√≥picos de Desenvolvimento Sustent√°vel das Na√ß√µes Unidas - <em>Boa sa√∫de e bem-estar</em>.</p>
<p>Como um cientista de dados, acredito que seja muito importante continuarmos aprimorando nossas habilidades e conhecimentos. Competi√ß√µes como essa s√£o muito divertidas e possibilitam que testemos nossos limites em um ambiente competitivo e colaborativo, al√©m de ser uma grande oportunidade para nos desafiarmos e aprender uns com os outros.</p>
<p>Tive o enorme prazer de conquistar o primeiro lugar, dessa vez com meu grande amigo <a href="https://www.linkedin.com/in/kaike-wesley-reis">Kaike</a>, parceiro de competi√ß√µes de longa data que trouxe grande sinergia para a <a href="https://www.kaggle.com/code/gomes555/ml-olypiads-1-lugar-blending">solu√ß√£o final</a> com a contribui√ß√£o de seu modelo (compartilhado abertamente no Kaggle).</p>
<p>Aqui est√£o alguns dos pr√™mios recebidos:</p>
<center>
<img src="/post/2023-05-30-solucao-final-ensure-healthy-lives-kaggle-competition/premio2.png" style="width:80.0%" />
</center>
<p>Como nesta competi√ß√£o havia bastante trabalho a ser feito e tivemos apenas 1 m√™s para trabalhar na solu√ß√£o, foi preciso fazer uma boa gest√£o do c√≥digo e do tempo de desenvolvimento.</p>
</div>
<div id="defini√ß√£o-do-problema-de-neg√≥cio" class="section level1">
<h1>Defini√ß√£o do problema de neg√≥cio</h1>
<p>O objetivo desta competi√ß√£o consistiu em predizer qual o agente causador da s√≠ndrome respirat√≥ria aguda grave com base nos dados e sintomas dos pacientes.</p>
<p>Esta tarefa pode ser enquadrada como um problema supervisionado de classifica√ß√£o multinomial (com m√∫ltiplos outputs) na qual as previs√µes s√£o, de certa forma, dependentes da entrada umas das outras (o paciente s√≥ pode ter registrado uma das doen√ßas).</p>
<p>A valida√ß√£o da solu√ß√£o foi feita utilizando a m√©trica Macro (or Mean) F1-Score, que √© basicamente a m√©dia do F1 calculado sobre as previs√µes de cada nota.</p>
</div>
<div id="solu√ß√µes" class="section level1">
<h1>Solu√ß√µes</h1>
<p>Ambas solu√ß√µes (minha e do Kaike) foram compartilhadas no Kaggle:</p>
<ul>
<li><a href="https://www.kaggle.com/code/gomes555/ml-olympiad-1-lugar-catboost-pos-process">ML Olympiad - 1¬∫ Lugar - Catboost + Pos Process</a> (Fellipe)</li>
<li><a href="https://www.kaggle.com/code/kaikewreis/ml-olypiads-1-lugar-lightgbm-binary-ensemble">ML Olypiads - 1¬∫ Lugar - LightGBM Binary Ensemble</a> (Kaike)</li>
<li><a href="https://www.kaggle.com/code/gomes555/ml-olympiad-1-lugar-blending">ML Olympiad - 1¬∫ Lugar - Blending</a> (combina√ß√£o das solu√ß√µes em um emsemble)</li>
</ul>
<p>Disponibilizamos tamb√©m a solu√ß√£o em formato de v√≠deo, gravado em um meetup com dura√ß√£o de 1 hora e meia para o canal do <a href="https://www.youtube.com/@tensorflowugsp">TensorFlow UGSP</a> no Youtube no link: <a href="https://youtu.be/6HPJn38NF3w" class="uri">https://youtu.be/6HPJn38NF3w</a></p>
<center>
<div class="vembedr">
<div>
<iframe src="https://www.youtube.com/embed/6HPJn38NF3w" width="533" height="300" frameborder="0" allowfullscreen="" data-external="1"></iframe>
</div>
</div>
</center>
</div>
<div id="estrat√©gia-anal√≠tica" class="section level1">
<h1>Estrat√©gia anal√≠tica</h1>
<p>Nas se√ß√µes abaixo apresento o racional por tr√°s da minha solu√ß√£o, como chegamos nos 5 melhores modelos individuais (para cada doen√ßa respirat√≥ria) que utilizei em um ensemble para chegar ao primeiro lugar, bem como a estrat√©gia de p√≥s processamento que com que o score melhorasse significativamente.</p>
<div id="decis√µes-sobre-a-target" class="section level2">
<h2>Decis√µes sobre a target</h2>
<p>A primeira decis√£o importante era definir como enquadrar o problema; se utilizar√≠amos 1 modelo multiclasse ou diferentes modelos para cada classe.</p>
<p>Em todos os testes que fizemos, os modelos individuais superaram o F1-Score Macro de um modelo √∫nico. Como 3 das classes eram bastante desbalanceadas, acredito que modelos especializados nesses casos conseguiram captar melhor suas nuances.</p>
</div>
<div id="processamento-dos-dados" class="section level2">
<h2>Processamento dos Dados</h2>
<p>Como optamos por unificar os resultados apenas na reta final, meu pr√©-processamento foi muito diferente do feito pelo Kaike e isso foi fundamental para que as estimativas dos nossos modelos tivessem baixa correla√ß√£o. N√£o focarei aqui no meu pr√©-processamento, pois n√£o acho que foi o diferencial para atingir um score superior a 0.6 (quem tiver curiosidade est√° tudo bem documentado nos notebooks compartilhados).</p>
</div>
<div id="dados-externos" class="section level2">
<h2>Dados Externos</h2>
<p>O fato de n√£o termos as informa√ß√µes do ano em que esses dados foram coletados dificultou na busca de bases externas, pois indicadores socioecon√¥micos e de sa√∫de variam bastante ao longo do tempo.</p>
<p>Fizemos alguns testes utilizando o <a href="https://basedosdados.org/dataset/mundo-onu-adh">Atlas do Desenvolvimento Humano (ADH)</a>, mas n√£o tivemos muito sucesso, pois esses dados est√£o muito defasados (1991-2010). Tamb√©m tentamos acrescentar a informa√ß√£o de <a href="https://github.com/kelvins/Municipios-Brasileiros/">latitude e longitude de cada munic√≠pio</a>, mas isso n√£o trouxe uma melhora substancial no nosso score.</p>
</div>
<div id="feature-engineering" class="section level2">
<h2>Feature Engineering</h2>
<p>Outra etapa em que investimos bastante tempo foi para criar novas vari√°veis.</p>
<p>Novamente, nossa engenharia de recursos foi feita de maneira separada para que nossos modelos aprendessem aspectos diferentes dos dados. Abaixo, compartilho algumas das features que desenvolvi apenas para o meu modelo:</p>
<ul>
<li>Presen√ßa de sintomas relacionados √† Target;</li>
<li>Se tomografia era t√≠pica do COVID;</li>
<li>Intervalo de idade com mais casos;</li>
<li>Idade discretizada;</li>
<li>Diferen√ßa entre a semana de notifica√ß√£o e primeiros sintomas;</li>
<li>Novas features baseadas nas contagens de algumas features categ√≥ricas;</li>
<li>etc.</li>
</ul>
</div>
<div id="modelos" class="section level2">
<h2>Modelos</h2>
<p>Al√©m de pr√©-processamentos e feature engineering diferentes, tamb√©m utilizamos modelos e mecanismos de tunning diferentes, o que ajudou para que nossas estimativas tivessem baixa correla√ß√£o. Eu usei o Catboost como modelo final, j√° o Kaike optou por um LightGBM com tuning de hiperparametros.</p>
</div>
<div id="ensemble" class="section level2">
<h2>Ensemble</h2>
<p>Calculamos a m√©dia das probabilidades previstas de cada modelo para cada classe antes de selecionar a classe que tivesse a maior probabilidade.</p>
<p>Como nossas previs√µes tinham baixa correla√ß√£o, conseguimos ser bem sucedidos no ensemble combinando nossas submiss√µes com score ~0.6 alcan√ßando ~0.61 na tabela p√∫blica.</p>
</div>
<div id="post-processing" class="section level2">
<h2>Post Processing</h2>
<p>Acredito que o <strong>diferencial</strong> dessa competi√ß√£o estava no p√≥s processamento.</p>
<p>Quando avaliamos o score do modelo de cada classe, tamb√©m calculamos um threshold que maximizava os respectivos F1.</p>
<p>Observamos que nosso modelo para a classe 5 apresentava um F1 muito superior √†s demais classes com esse threshold otimizado, ent√£o fizemos o seguinte:</p>
<ol style="list-style-type: decimal">
<li>Calculamos as probabilidades individuais para cada classe;</li>
<li>Selecionamos a classe que tinha maior probabilidade estimada em cada inst√¢ncia;</li>
<li>Pegamos a classifica√ß√£o bin√°ria da classe 5 com o threshold otimizado e aplicamos a seguinte condi√ß√£o: Se o modelo da classe 5 estimou que y5[i]==1, ent√£o yfinal[i] √© 5, caso contr√°rio, use a classe de maior probabilidade entre as outras 4. (Em outras palavras: <code>np.where(y5_test_class==1, 5, sub.CLASSI_FIN)</code>)</li>
</ol>
</div>
</div>
<div id="considera√ß√µes-finais" class="section level1">
<h1>Considera√ß√µes Finais</h1>
<p>Foi uma competi√ß√£o muito interessante e desafiadora. Agrade√ßo imensamente ao <a href="https://www.meetup.com/TensorFlowSP/events/284607061/">TFUG</a> por organizar o evento e a todos os participantes que contribu√≠ram para o aprendizado coletivo.Foi uma √≥tima oportunidade de aprendizado e troca de experi√™ncias.</p>
<p>Espero que minha solu√ß√£o possa ser √∫til para outros projetos e desafios futuros.</p>
</div>
<div id="sobre-o-autor" class="section level1">
<h1>Sobre o Autor</h1>
<p>Me chamo Fellipe Gomes, sou cientista de dados e apaixonado por aprendizado de m√°quina. Compartilho meu conhecimento por meio de artigos, tutoriais e projetos de c√≥digo aberto. Se quiser saber mais sobre meu trabalho, sinta-se √† vontade para conferir meu <a href="https://www.linkedin.com/in/fellipe-gomes-06943264/">LinkedIn</a> e <a href="https://github.com/fellipe-gomes">GitHub</a>.</p>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2023-05-30-solucao-final-ensure-healthy-lives-kaggle-competition/">Solu√ß√£o Final - ML Olympiad [1¬∫ lugar]</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">catboost</category>
      <category domain="tag">classification</category>
      <category domain="tag">data-mining</category>
      <category domain="tag">data-science</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">machine-learning</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">pratica</category>
      <category domain="tag">r</category>
    </item>
    <item>
      <title>Solu√ß√£o Final - ML Olympiad [2¬∫ lugar]</title>
      <link>https://gomesfellipe.github.io/post/2022-04-20-solucao-final-education-quality-kaggle-competition/</link>
      <pubDate>Tue, 19 Apr 2022 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2022-04-20-solucao-final-education-quality-kaggle-competition/</guid>
      <description>Confira a estrat√©gia aplicada para esta competi√ß√£o</description>
      <content:encoded>&lt;![CDATA[
        

<div id="TOC">
<ul>
<li><a href="#introdu%C3%A7%C3%A3o" id="toc-introdu√ß√£o">Introdu√ß√£o</a></li>
<li><a href="#defini%C3%A7%C3%A3o-do-problema-de-neg%C3%B3cio" id="toc-defini√ß√£o-do-problema-de-neg√≥cio">Defini√ß√£o do problema de neg√≥cio</a></li>
<li><a href="#an%C3%A1lise-explorat%C3%B3ria-em-r" id="toc-an√°lise-explorat√≥ria-em-r">An√°lise Explorat√≥ria (em R)</a>
<ul>
<li><a href="#estrutura-da-base" id="toc-estrutura-da-base">Estrutura da base</a></li>
<li><a href="#ano-da-base-de-dados" id="toc-ano-da-base-de-dados">Ano da base de dados</a></li>
<li><a href="#target" id="toc-target">Target</a></li>
</ul></li>
<li><a href="#machine-learning-em-python" id="toc-machine-learning-em-python">Machine Learning (em Python)</a>
<ul>
<li><a href="#importar-dependencias" id="toc-importar-dependencias">Importar dependencias</a></li>
<li><a href="#carregar-dados" id="toc-carregar-dados">Carregar dados</a></li>
<li><a href="#modelagem" id="toc-modelagem">Modelagem</a></li>
</ul></li>
<li><a href="#submiss%C3%A3o" id="toc-submiss√£o">Submiss√£o</a></li>
<li><a href="#considera%C3%A7%C3%B5es-finais" id="toc-considera√ß√µes-finais">Considera√ß√µes Finais</a></li>
</ul>
</div>

<div id="introdu√ß√£o" class="section level1">
<h1>Introdu√ß√£o</h1>
<p>No final de Janeiro desde ano (2022) o <a href="https://www.meetup.com/TensorFlowSP/events/284607061/">TFUG - TensorFlow Users Group de S√£o Paulo</a> lan√ßou uma competi√ß√£o no Kaggle para prever as notas do enem que tem rela√ß√£o com um dos 17 t√≥picos de Desenvolvimento Sustent√°vel das Na√ß√µes Unidas - <em>Educa√ß√£o de Qualidade</em>.</p>
<p>Al√©m de divertido, o desafio foi repleto de possibilidades e bastante desafiador! Todos os competidores que trabalharam duro em pleno m√™s de carnaval est√£o de parab√©ns! üòÖ üòÇ</p>
<p>Aqui est√£o alguns dos pr√™mios recebidos:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/premio.png" style="width:80.0%" />
</center>
<p>Como nesta competi√ß√£o havia bastante trabalho a ser feito e tivemos apenas 1 m√™s para trabalhar na solu√ß√£o, foi preciso fazer uma boa gest√£o do c√≥digo e do tempo de desenvolvimento.</p>
<p>Nas se√ß√µes abaixo apresento o racional por tr√°s da minha solu√ß√£o bem como os 5 melhores modelos individuais (para cada nota) que utilizei em um ensemble para chegar ao segundo lugar.</p>
</div>
<div id="defini√ß√£o-do-problema-de-neg√≥cio" class="section level1">
<h1>Defini√ß√£o do problema de neg√≥cio</h1>
<p>O objetivo desta competi√ß√£o consistiu em prever as notas dos alunos(as) nas provas: Ci√™ncias da Natureza, Ci√™ncias Humanas, Linguagens e C√≥digos, Matem√°tica e Reda√ß√£o.</p>
<p>Apesar das notas serem calculadas de maneira independente, a partir de modelos de <a href="http://portal.mec.gov.br/ultimas-noticias/389-ensino-medio-2092297298/17319-teoria-de-resposta-ao-item-avalia-habilidade-e-minimiza-o-chute">TRI (Teoria de Resposta ao Item)</a> que levam em considera√ß√£o a performance em um caderno espec√≠fico e na dificuldade de cada quest√£o, o mesmo aluno realiza todas as provas em um curto per√≠odo de tempo.</p>
<p>Portanto, esta tarefa pode ser enquadrada como um problema supervisionado de regress√£o com m√∫ltiplos outputs na qual as previs√µes s√£o, de certa forma, dependentes da entrada umas das outras.</p>
<p>A valida√ß√£o da solu√ß√£o foi feita utilizando a m√©trica Mean Columnwise Root Mean Squared Error ‚Äì MCRMSE, que √© basicamente a m√©dia do RMSE calculado sobre as previs√µes de cada nota.</p>
</div>
<div id="an√°lise-explorat√≥ria-em-r" class="section level1">
<h1>An√°lise Explorat√≥ria (em R)</h1>
<p>Convido o leitor a conferir o <a href="https://gomesfellipe.github.io/post/2021-11-01-solucao-final-porto-seguro-data-challenge/">notebook publicado no Kaggle</a> com a an√°lise explorat√≥ria completa. Aqui irei trazer apenas alguns dos principais insights que encontrei durante a etapa de an√°lise explorat√≥ria.</p>
<div id="estrutura-da-base" class="section level2">
<h2>Estrutura da base</h2>
<p>Veja a seguir qual a estrutura geral da base de dados:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/02_df_status.png" style="width:95.0%" />
</center>
<p>√â not√≥rio que existem dados faltantes e que parece haver algum padr√£o. Vejamos com mais detalhse:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/03_missing.png" style="width:95.0%" />
</center>
<div class="w3-panel w3-pale-blue w3-border">
<p>¬† üí° Insights!</p>
<p>Existem dados <em>missing</em> nas 5 targets que queremos prever e note que existe uma rela√ß√£o tanto entre as provas de Matem√°tica e Ci√™ncias da Natuerza quanto nas de Ci√™ncias Humanas, Linguagens e C√≥digos e Reda√ß√£o, o que parece ocorrer devido a aus√™ncia do aluno incrito em comparecer a realiza√ß√£o da prova no respectivo dia.</p>
</div>
</div>
<div id="ano-da-base-de-dados" class="section level2">
<h2>Ano da base de dados</h2>
<p>Essa informa√ß√£o n√£o estava explicitamente dispon√≠vel, mas ap√≥s analisar a idade dos participantes em rela√ß√£o ao ano em que conclu√≠ram o ensino m√©dio, foi poss√≠vel identificar que tratavam-se dos dados de 2019, veja:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/05_ano_concluiu.png" style="width:95.0%" />
</center>
<p>Essa informa√ß√£o poderia ser √∫til na hora de buscar dados externos (permitido nesta competi√ß√£o).</p>
<div class="w3-panel w3-pale-blue w3-border">
<p>¬† üí° Insights!</p>
<p>‚Üí Aten√ß√£o aos outliers: √â no m√≠nimo estranho uma pessoa que formou em 2007 ter 17 anos;</p>
<p>‚Üí Como ningu√©m concluiu a escola no ano de 2019 e a m√©dia das idades vai diminuindo quanto mais pr√≥ximo de 2018, parece que estes dados s√£o de 2019. Essa inform√ß√£o poderia ser √∫til na hora de procurar por bases externas.</p>
</div>
</div>
<div id="target" class="section level2">
<h2>Target</h2>
<p>A primeira decis√£o importante era definir como enquadrar o problema; se seriam m√∫ltiplos modelos independentes ou modelos com sa√≠das dependentes.</p>
<p>Primeiramente vejamos como eram as distribui√ß√µes das notas por caderno:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/07_distribuicao_target.png" style="width:95.0%" />
</center>
<p>Ao olhar estas distribui√ß√µes foram surgindo v√°rias id√©ias! Cheguei at√© a tentar modelos estat√≠sticos GAM considerando a resposta como uma distribui√ß√£o Beta (transformando as targets no intervalo [0,1]) mas acabou n√£o apresentando bons resultados para a competi√ß√£o.. acho que seria necess√°rio um pouco mais de prepara√ß√£o nos dados.</p>
<p>Apesar das notas do enem serem calculadas via TRI (Teoria de Resposta ao Item) que considera as notas independentes, parece existir alguma correla√ß√£o entre as notas, veja:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/08_correlacao_notas.png" style="width:95.0%" />
</center>
<p>As targets da nota de L√≠nguas e C√≥digos e Ci√™ncias Humanas pareciam possuir uma correla√ß√£o ‚Äúinteressante‚Äù, mas, ap√≥s testar modelos de m√∫ltiplas respostas dependentes para cada dia (com e sem a nota da reda√ß√£o), em nenhum de meus testes superou (de maneira consistente) o desempenho de modelos que considerassem as sa√≠das independentes. Portanto foquei em criar 5 modelos independentes.</p>
</div>
</div>
<div id="machine-learning-em-python" class="section level1">
<h1>Machine Learning (em Python)</h1>
<p>Toda a rotina de pr√©-processamento dos dados, feature engineering, modelagem, ensamble e p√≥s-processamento foi realizada utilizando a linguagem Python para cada uma das 5 notas. Trouxe apenas o modelo final neste post mas, para chegar at√© aqui foram necess√°rio muitos testes!</p>
<div id="importar-dependencias" class="section level2">
<h2>Importar dependencias</h2>
<p>Carregar pacotes Python:</p>
<pre class="python"><code># data prep
import numpy as np 
import pandas as pd 
# pre process
from sklearn.preprocessing import MinMaxScaler
# modeling
from sklearn.model_selection import train_test_split
from catboost import CatBoostRegressor
# plots
import seaborn as sns
import matplotlib.pyplot as plt</code></pre>
<p>Confira a baixo as fun√ß√µes desenvolvidas para a solu√ß√£o deste problema</p>
<details>
<summary>
(<em>Clique aqui para expandir as fun√ß√µes</em>)
</summary>
<pre class="python"><code>def prep_data_questionarios(df):
  &#39;&#39;&#39;
  Converte dados de questionario para ordinal
  &#39;&#39;&#39;
    # escolaridade pai
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}
    df.loc[:, &#39;Q001&#39;] = df.loc[:, &#39;Q001&#39;].map(to_map).astype(int)

    # escolaridade mae
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}
    df.loc[:, &#39;Q002&#39;] = df.loc[:, &#39;Q002&#39;].map(to_map).astype(int) 

    # ocupacao pai
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: -1}
    df.loc[:, &#39;Q003&#39;] = df.loc[:, &#39;Q003&#39;].map(to_map).astype(int) 

    # ocupacao mae
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: -1}
    df.loc[:, &#39;Q004&#39;] = df.loc[:, &#39;Q004&#39;].map(to_map).astype(int) 

    # renda da familia
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;:5, &#39;G&#39;:6, &#39;H&#39;:7, &#39;I&#39;:8,
              &#39;J&#39;:9, &#39;K&#39;:10,&#39;L&#39;:11, &#39;M&#39;:12, &#39;N&#39;:13, &#39;O&#39;:14, &#39;P&#39;:15, &#39;Q&#39;:16}
    df.loc[:, &#39;Q006&#39;] = df.loc[:, &#39;Q006&#39;].map(to_map).astype(int) 

    # empregado domestico
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3}
    df.loc[:, &#39;Q007&#39;] = df.loc[:, &#39;Q007&#39;].map(to_map).astype(int) 

    # banheiro
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q008&#39;] = df.loc[:, &#39;Q008&#39;].map(to_map).astype(int) 

    # qnt de quartos
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q009&#39;] = df.loc[:, &#39;Q009&#39;].map(to_map).astype(int) 

    # qnt de carros
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q010&#39;] = df.loc[:, &#39;Q010&#39;].map(to_map).astype(int) 

    # qnt de motocicleta
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q011&#39;] = df.loc[:, &#39;Q011&#39;].map(to_map).astype(int) 

    # qnt de geladeira
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q012&#39;] = df.loc[:, &#39;Q012&#39;].map(to_map).astype(int) 

    # qnt de freezer
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q013&#39;] = df.loc[:, &#39;Q013&#39;].map(to_map).astype(int) 

    # qnt de maquina de lavar roupa
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q014&#39;] = df.loc[:, &#39;Q014&#39;].map(to_map).astype(int) 

    # qnt de maquina de secar roupa
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q015&#39;] = df.loc[:, &#39;Q015&#39;].map(to_map).astype(int) 

    # qnt de microondas
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q016&#39;] = df.loc[:, &#39;Q016&#39;].map(to_map).astype(int) 

    # qnt de maquina de lavar louca
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q017&#39;] = df.loc[:, &#39;Q017&#39;].map(to_map).astype(int) 

    # tem aspirador de po
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1}
    df.loc[:, &#39;Q018&#39;] = df.loc[:, &#39;Q018&#39;].map(to_map).astype(int) 

    # qtd tv colorida
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q019&#39;] = df.loc[:, &#39;Q019&#39;].map(to_map).astype(int) 

    # tem dvd
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1}
    df.loc[:, &#39;Q020&#39;] = df.loc[:, &#39;Q020&#39;].map(to_map).astype(int) 

    # tem tv por assinatura
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1}
    df.loc[:, &#39;Q021&#39;] = df.loc[:, &#39;Q021&#39;].map(to_map).astype(int) 

    # qtd telefone celular
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q022&#39;] = df.loc[:, &#39;Q022&#39;].map(to_map).astype(int) 

    # qtd telefone fixo
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1}
    df.loc[:, &#39;Q023&#39;] = df.loc[:, &#39;Q023&#39;].map(to_map).astype(int) 

    # qtd computador
    to_map =  {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q024&#39;] = df.loc[:, &#39;Q024&#39;].map(to_map).astype(int) 

    # tem acesso a internet
    to_map =  {&#39;A&#39;:0, &#39;B&#39;:1}
    df.loc[:, &#39;Q025&#39;] = df.loc[:, &#39;Q025&#39;].map(to_map).astype(int) 
    
    return(df)
  
def fe_questionario(df):
  &#39;&#39;&#39;
  Gerar novas features artificiais baseadas nos dados de questionario
  &#39;&#39;&#39;
    df.loc[:, &quot;Q021+Q006&quot;] = df[&quot;Q021&quot;] + df[&quot;Q006&quot;]
    df.loc[:, &quot;Q018+Q006&quot;] = df[&quot;Q018&quot;] + df[&quot;Q006&quot;]
    df.loc[:, &quot;Q018+Q008&quot;] = df[&quot;Q018&quot;] + df[&quot;Q008&quot;]
    df.loc[:, &quot;Q010+Q018&quot;] = df[&quot;Q010&quot;] + df[&quot;Q018&quot;]
    df.loc[:, &quot;Q018+Q024&quot;] = df[&quot;Q018&quot;] + df[&quot;Q024&quot;]
    
    df.loc[:, &quot;Q018*Q006&quot;] = df[&quot;Q018&quot;] * df[&quot;Q006&quot;]
    df.loc[:, &quot;Q010*Q018&quot;] = df[&quot;Q010&quot;] * df[&quot;Q018&quot;]
    
    return df
  
def fe_mun(data):
    &#39;&#39;&#39;
    Gerar novas features a partir das localizacoes de municipio
    &#39;&#39;&#39;
    for c in list(data.columns[data.dtypes==&#39;category&#39;]):
        data.loc[:, c] = data.loc[:, c].astype(&#39;object&#39;)
    
    data.loc[:, &#39;FE_MUNICIPIO_PROVA_x_MUNICIPIO_RESIDENCIA&#39;] = np.where(data.NO_MUNICIPIO_PROVA == data.NO_MUNICIPIO_RESIDENCIA , 1, 0)
    data.loc[:, &#39;FE_MUNICIPIO_PROVA_x_MUNICIPIO_NASCIMENTO&#39;] = np.where(data.NO_MUNICIPIO_PROVA == data.NO_MUNICIPIO_NASCIMENTO , 1, 0)
    data.loc[:, &#39;FE_MUNICIPIO_PROVA_x_MUNICIPIO_ESC&#39;] = np.where(data.NO_MUNICIPIO_PROVA == data.NO_MUNICIPIO_ESC , 1, 0)
    data.loc[:, &#39;FE_MUNICIPIO_RESIDENCIA_x_MUNICIPIO_NASCIMENTO&#39;] = np.where(data.NO_MUNICIPIO_RESIDENCIA == data.NO_MUNICIPIO_NASCIMENTO , 1, 0)
    data.loc[:, &#39;FE_MUNICIPIO_RESIDENCIA_x_MUNICIPIO_ESC&#39;] = np.where(data.NO_MUNICIPIO_RESIDENCIA == data.NO_MUNICIPIO_ESC , 1, 0)
    data.loc[:, &#39;FE_MUNICIPIO_NASCIMENTO_x_MUNICIPIO_ESC&#39;] = np.where(data.NO_MUNICIPIO_RESIDENCIA == data.NO_MUNICIPIO_ESC , 1, 0)
    
    for c in list(data.columns[data.dtypes==&#39;object&#39;]):
        data.loc[:, c] = data.loc[:, c].astype(&#39;category&#39;)
    
    return data
  
def fe_in(df):
    &#39;&#39;&#39;
    Gerar features a partir das indicadoras
    &#39;&#39;&#39;
    df.loc[:, &#39;IN_DEFICIT_ATENCAO+IN_TEMPO_ADICIONAL&#39;] = df[&quot;IN_DEFICIT_ATENCAO&quot;] + df[&quot;IN_TEMPO_ADICIONAL&quot;]
    df.loc[:, &#39;IN_LEDOR+IN_TRANSCRICAO&#39;] = df[&quot;IN_LEDOR&quot;] + df[&quot;IN_TRANSCRICAO&quot;]

    return df
  
def prep_co_escola(df):
    &#39;&#39;&#39;
    Converter codigo da escola para categorico
    &#39;&#39;&#39;
    df.loc[:, &#39;CO_ESCOLA&#39;] = [str(x) for x in df.CO_ESCOLA]
    df.loc[:, &#39;CO_ESCOLA&#39;] = np.where(df[&#39;CO_ESCOLA&#39;]==&#39;nan&#39;, np.nan, df[&#39;CO_ESCOLA&#39;])
    df.loc[:, &#39;CO_ESCOLA&#39;] = df.loc[:, &#39;CO_ESCOLA&#39;].astype(&#39;category&#39;)
    
    return df
  
def fe_extra(df):
    &#39;&#39;&#39;
    Gerar novas features 
    &#39;&#39;&#39;
    df.loc[:, &quot;FE_IDADE_DISCRETA&quot;] = pd.cut(df.NU_IDADE, (0, 15, 18, 23, 36, 60, 120), labels=[&#39;ADOLESCENTE&#39;,&#39;ADOLESCENTE_2&#39;, &#39;JOVEM&#39;,&#39;JOVEM_2&#39;, &#39;ADULTO&#39;, &#39;IDOSO&#39;]).astype(&#39;category&#39;)
    df.loc[:, &#39;FE_OCUPACAO_PAIS&#39;] = df.Q003 + df.Q004
    df.loc[:, &#39;FE_ESCOLARIDADE_PAIS&#39;] = df.Q001 + df.Q002
    df.loc[:, &#39;FE_RENDA_POR_PESSOA&#39;] = df.Q006 / df.Q005
    df.loc[:, &#39;FE_CELULAR_POR_PESSOA&#39;] = df.Q022 / df.Q005
    df.loc[:, &#39;FE_COMPUTADOR_POR_PESSOA&#39;] = df.Q024 / df.Q005
    df.loc[:, &#39;FE_VISAO_RUIM&#39;] = df[[&#39;IN_BAIXA_VISAO&#39;, &#39;IN_CEGUEIRA&#39;, &#39;IN_VISAO_MONOCULAR&#39;, &#39;IN_SURDO_CEGUEIRA&#39;]].max(axis=1)
    df.loc[:, &#39;FE_AUDICAO_RUIM&#39;] = df[[&#39;IN_SURDEZ&#39;, &#39;IN_DEFICIENCIA_AUDITIVA&#39;, &#39;IN_SURDO_CEGUEIRA&#39;]].max(axis=1)
    df.loc[:, &#39;FE_TDAH_MAIS_TEMPO&#39;] = df.IN_TEMPO_ADICIONAL + df.IN_DEFICIT_ATENCAO
    df.loc[:, &#39;FE_TDAH_MEDICADO&#39;] = np.where((df.IN_DEFICIT_ATENCAO==1)&amp;(df.IN_MEDICAMENTOS==1), 1, 0)
    df.loc[:, &#39;FE_RECURSO_VISAO&#39;] =  df[[&#39;IN_BRAILLE&#39;, &#39;IN_AMPLIADA_24&#39;, &#39;IN_AMPLIADA_18&#39;, &#39;IN_LEDOR&#39;, &#39;IN_MAQUINA_BRAILE&#39;, &#39;IN_LAMINA_OVERLAY&#39;]].max(axis=1)
    df.loc[:, &#39;FE_RECURSO_SURDEZ&#39;] =  df[[&#39;IN_LIBRAS&#39;, &#39;IN_LEITURA_LABIAL&#39;, &#39;IN_TRANSCRICAO&#39;]].max(axis=1)
    acess = [&#39;IN_ACESSO&#39;, &#39;IN_MESA_CADEIRA_RODAS&#39;, &#39;IN_MESA_CADEIRA_SEPARADA&#39;, &#39;IN_APOIO_PERNA&#39;, &#39;IN_CADEIRA_ESPECIAL&#39;, &#39;IN_CADEIRA_CANHOTO&#39;, &#39;IN_CADEIRA_ACOLCHOADA&#39;, &#39;IN_MOBILIARIO_OBESO&#39;, &#39;IN_SALA_INDIVIDUAL&#39;, &#39;IN_SALA_ESPECIAL&#39;, &#39;IN_SALA_ACOMPANHANTE&#39;, &#39;IN_MOBILIARIO_ESPECIFICO&#39;, &#39;IN_MATERIAL_ESPECIFICO&#39;]
    df.loc[:, &#39;FE_ACESSIBILIDADE&#39;] =  df[acess].max(axis=1)

    return df</code></pre>
</details>
<p>¬†</p>
<p>Carregar features artificiais extra√≠das atrav√©s de um modelo KNN. N√£o apresentarei o c√≥digo aqui (talvez fique para um pr√≥ximo post) mas a id√©ia √© basicamente a seguinte:</p>
<div class="w3-panel w3-pale-yellow w3-border">
<p>¬† üß™ Feature Extraction com KNN</p>
<p>Ajuste um <code>KNeighborsRegressor</code> encontrando os K-vizinhos mais pr√≥ximos de cada inst√¢ncia out-of-fold via valida√ß√£o cruzada (para evitar data leak) nos dados de treino e depois ajuste um modelo em todos os dados de treino para obter os K-vizinhos mais pr√≥ximos nos dados de teste.</p>
</div>
<p>Quem sabe no futuro fa√ßo um post compartilhando esta estrat√©gia com mais detalhes.</p>
<pre class="python"><code>knn_train = pd.read_csv(&quot;../input/knn/KNN_feat_train_CH_LC.csv&quot;)
knn_test = pd.read_csv(&quot;../input/knn/KNN_feat_test_CH_LC.csv&quot;)

knn_train_cn_mt = pd.read_csv(&quot;../input/knn/KNN_feat_train_CN_MT.csv&quot;)
knn_test_cn_mt = pd.read_csv(&quot;../input/knn/KNN_feat_test_CN_MT.csv&quot;)

knn_train_rd = pd.read_csv(&quot;../input/knn/KNN_feat_train_RD.csv&quot;)
knn_test_rd = pd.read_csv(&quot;../input/knn/KNN_feat_test_RD.csv&quot;)</code></pre>
</div>
<div id="carregar-dados" class="section level2">
<h2>Carregar dados</h2>
<p>Importar uma vers√£o do dataset no formato <code>.parquet</code> que foi compactada com um truque para otimizar o consumo de mem√≥ria disponibilizada pelos organizadores <a href="https://www.kaggle.com/code/caneiro/mlo-make-parquet">neste notebook</a>.</p>
<pre class="python"><code>train = pd.read_parquet(&#39;train.parquet&#39;)
test = pd.read_parquet(&#39;test.parquet&#39;)
sub = pd.read_csv(&#39;../input/qualityeducation/sample_submission.csv&#39;)</code></pre>
<p>Definir objetos com targets</p>
<pre class="python"><code>targets = [&#39;NU_NOTA_LC&#39;, &#39;NU_NOTA_CH&#39;, &#39;NU_NOTA_CN&#39;,  &#39;NU_NOTA_MT&#39;, &#39;NU_NOTA_REDACAO&#39;]
presencas = [&#39;TP_PRESENCA_LC&#39;, &#39;TP_PRESENCA_CH&#39;, &#39;TP_PRESENCA_CN&#39;, &#39;TP_PRESENCA_MT&#39;, &#39;TP_STATUS_REDACAO&#39;]</code></pre>
<div class="w3-panel w3-pale-yellow w3-border">
<p>¬† ‚ö†Ô∏è Aten√ß√£o:</p>
<p>A feature de presen√ßa √© muito importante no p√≥s-processamento para atribuir nota zero aos alunos que n√£o foram realizar a prova mas n√£o faz sentido mant√™-la nos dados de treino pois ser√° sempre constante.</p>
</div>
<div id="dados-externos" class="section level3">
<h3>Dados externos</h3>
<p>Dados Externos utilizados:</p>
<ol style="list-style-type: decimal">
<li><a href="https://basedosdados.org/dataset/mundo-onu-adh">Atlas do Desenvolvimento Humano (ADH)</a></li>
</ol>
<p>Esta base tinha muita informa√ß√£o legal mas sua cobertura temporal estava bastante defasada (1991 - 2010) o que pode adicionar algum ru√≠do ao modelo.</p>
<p>As features selecionadas (sem muito crit√©rio) desta base foram:</p>
<pre class="python"><code>extra1 = pd.read_csv(&quot;municipio.csv&quot;)

extra1 = extra1[extra1.ano==2010]

features_extra1 = [&#39;expectativa_vida&#39;, &#39;razao_dependencia&#39;, &#39;expectativa_anos_estudo&#39;,
&#39;taxa_analfabetismo_11_a_14&#39;, &#39;taxa_analfabetismo_15_a_17&#39;, &#39;taxa_analfabetismo_18_mais&#39;,
&#39;taxa_atraso_0_basico&#39;, &#39;taxa_atraso_0_fundamental&#39;, &#39;taxa_atraso_0_medio&#39;,
&#39;taxa_freq_bruta_medio&#39;, &#39;taxa_freq_liquida_medio&#39;,
&#39;taxa_freq_medio_18_24&#39;, &#39;taxa_freq_medio_6_14&#39;, &#39;indice_gini&#39;,&#39;prop_pobreza_extrema&#39;, &#39;prop_pobreza&#39;,
&#39;prop_renda_10_ricos&#39;, &#39;prop_renda_20_pobres&#39;, &#39;razao_10_ricos_40_pobres&#39;,&#39;renda_pc&#39; , &#39;renda_pc_quintil_1&#39;,
&#39;indice_theil&#39;, &#39;prop_trabalhadores_conta_proria&#39;, 
&#39;prop_empregadores&#39;, &#39;prop_ocupados_agropecuaria&#39;, &#39;prop_ocupados_comercio&#39;,
&#39;prop_ocupados_construcao&#39;, &#39;prop_ocupados_formalizacao&#39;, &#39;prop_ocupados_medio&#39;,
&#39;prop_ocupados_servicos&#39;, &#39;prop_ocupados_superior&#39;,
&#39;prop_ocupados_renda_0&#39;, &#39;renda_media_ocupados&#39;, &#39;indice_treil_trabalho&#39;,
&#39;taxa_ocupados_carteira&#39;, &#39;taxa_agua_encanada&#39;, 
&#39;taxa_banheiro_agua_encanada&#39;, &#39;taxa_coleta_lixo&#39;, &#39;taxa_energia_eletrica&#39;,
&#39;taxa_agua_esgoto_inadequados&#39;, &#39;taxa_criancas_dom_sem_fund&#39;,
&#39;pea&#39;, &#39;indice_escolaridade&#39;, &#39;indice_frequencia_escolar&#39;, 
&#39;idhm&#39;, &#39;idhm_e&#39;, &#39;idhm_l&#39;, &#39;idhm_r&#39;]
extra1 = extra1[[&#39;id_municipio&#39;]+features_extra1]

train = pd.merge(train, extra1, how=&#39;left&#39;, left_on=&#39;CO_MUNICIPIO_RESIDENCIA&#39;, right_on=&#39;id_municipio&#39;)
test = pd.merge(test, extra1, how=&#39;left&#39;, left_on=&#39;CO_MUNICIPIO_RESIDENCIA&#39;, right_on=&#39;id_municipio&#39;)</code></pre>
<ol start="2" style="list-style-type: decimal">
<li><a href="https://www.gov.br/inep/pt-br/acesso-a-informacao/dados-abertos/microdados/censo-escolar">Microdados do Censo Escolar da Educaca√ß√£o B√°sica</a></li>
</ol>
<p>Base dispon√≠vel no mesmo site dos dados da competi√ß√£o e que tr√°s informa√ß√µes muito ricas das escolas do Brasil. Infelizmente quase 75% da informa√ß√£o da escola do aluno era missing ent√£o esta base n√£o conseguiu alavancar os ganhos do modelo de maneira consider√°vel.</p>
<p>Nesta base foquei principalmente nas features utilizadas para calcular o IIE (√çndice de Estrutura da Escola) que se baseia nos seguintes componentes:</p>
<table>
<colgroup>
<col width="32%" />
<col width="24%" />
<col width="42%" />
</colgroup>
<thead>
<tr class="header">
<th>Componente 1: Pedag√≥gica (IEE_Pedag√≥gico):</th>
<th>Componente 2: B√°sica (IEE_B√°sico):</th>
<th>Componente 3: Tecnol√≥gica (IEE_Tecnol√≥gico):</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Qualifica√ß√£o do docente (forma√ß√£o acad√™mica dos professores)</td>
<td>√Ågua filtrada (bin√°ria)</td>
<td>N√∫mero de computadores por aluno (computadores dispon√≠veis para uso dos alunos)</td>
</tr>
<tr class="even">
<td>N√∫mero de alunos por sala</td>
<td>Acesso √† rede p√∫blica de energia (bin√°ria)</td>
<td>N√∫mero de equipamentos multim√≠dia por aluno</td>
</tr>
<tr class="odd">
<td>N√∫mero de funcion√°rios por aluno</td>
<td>Acesso √† rede p√∫blica de esgoto (bin√°ria)</td>
<td>Acesso a internet (bin√°ria)</td>
</tr>
<tr class="even">
<td>Quadra de esportes coberta (bin√°ria)</td>
<td>Coleta peri√≥dica de lixo (bin√°ria)</td>
<td>Laborat√≥rio de Ci√™ncias (bin√°ria)</td>
</tr>
<tr class="odd">
<td>Biblioteca (bin√°ria)</td>
<td>Banheiro dentro do pr√©dio (bin√°ria)</td>
<td>Laborat√≥rio de Inform√°tica (bin√°ria)</td>
</tr>
</tbody>
</table>
<ul>
<li><a href="https://leosalesblog.wordpress.com/2018/02/03/escola-ruim-aluno-ruim-entendendo-a-relacao-entre-estrutura-escolar-e-desempenho-no-enem/">Fonte</a></li>
</ul>
<pre class="python"><code># Importar dados
extra2 = pd.read_csv(&#39;microdados_ed_basica_2021.csv&#39;, error_bad_lines=False, sep=&#39;;&#39;, encoding=&#39;latin1&#39;, dtype={&#39;CO_ORGAO_REGIONAL&#39;: &#39;str&#39;})
extra2 = extra2[extra2.isnull().sum(axis=1) / extra2.shape[1] &lt; .9]

# Tratamento nas features
extra2.loc[:, &#39;QT_TOTAL_ALUNOS&#39;] = extra2[[&#39;QT_MAT_BAS_ND&#39;, &#39;QT_MAT_BAS_BRANCA&#39;, &#39;QT_MAT_BAS_PRETA&#39;, &#39;QT_MAT_BAS_PARDA&#39;, &#39;QT_MAT_BAS_AMARELA&#39;, &#39;QT_MAT_BAS_INDIGENA&#39;]].sum(axis=1).fillna(0)
extra2.loc[:, &#39;QT_TOTAL_PROFESSORES&#39;] = (extra2.QT_DOC_BAS + extra2.QT_DOC_INF + extra2.QT_DOC_INF_CRE + extra2.QT_DOC_INF_PRE + extra2.QT_DOC_FUND + extra2.QT_DOC_FUND_AI + extra2.QT_DOC_FUND_AF + extra2.QT_DOC_MED + extra2.QT_DOC_PROF + extra2.QT_DOC_PROF_TEC + extra2.QT_DOC_EJA + extra2.QT_DOC_EJA_FUND + extra2.QT_DOC_EJA_MED + extra2.QT_DOC_ESP + extra2.QT_DOC_ESP_CC + extra2.QT_DOC_ESP_CE).fillna(0)
extra2.loc[:, &#39;QT_SALAS_UTILIZADAS&#39;] = (extra2.loc[:, &#39;QT_TOTAL_ALUNOS&#39;] / extra2.QT_SALAS_UTILIZADAS).fillna(0)
extra2.loc[:, &#39;QT_COMP_DISP_ALUNO&#39;] = extra2.QT_DESKTOP_ALUNO + extra2.QT_COMP_PORTATIL_ALUNO + extra2.QT_TABLET_ALUNO

# Selecao de faetures importantes
features_extra2 = [&#39;CO_ENTIDADE&#39;, &#39;QT_SALAS_UTILIZADAS&#39;, &#39;QT_TOTAL_PROFESSORES&#39;, &#39;IN_QUADRA_ESPORTES_COBERTA&#39;, &#39;IN_BIBLIOTECA&#39;,
       &#39;IN_AGUA_POTAVEL&#39;, &#39;IN_ENERGIA_REDE_PUBLICA&#39;, &#39;IN_ESGOTO_REDE_PUBLICA&#39;, &#39;IN_LIXO_SERVICO_COLETA&#39;, &#39;IN_BANHEIRO&#39;,
       &#39;QT_COMP_DISP_ALUNO&#39;, &#39;QT_EQUIP_MULTIMIDIA&#39;, &#39;IN_INTERNET&#39;, &#39;IN_LABORATORIO_CIENCIAS&#39;, &#39;IN_LABORATORIO_INFORMATICA&#39;]
extra2 = extra2[features_extra2]

# Remover outliers
for c in list(extra2.iloc[:, 1:].columns):
    trs = extra2.loc[extra2[c]!=88888, c].quantile(.99)
    extra2.loc[(extra2[c]==88888)|(extra2[c]&gt;trs), c] = trs
    
#Normalizar para calcular IEE
scaler = MinMaxScaler()
to_iee = scaler.fit_transform(extra2.iloc[:, 1:])
to_iee = pd.DataFrame(to_iee, columns=extra2.iloc[:, 1:].columns)

# Calcular IEE e componentes
extra2.loc[:, &#39;COMP1&#39;] = to_iee[[&#39;QT_SALAS_UTILIZADAS&#39;, &#39;QT_TOTAL_PROFESSORES&#39;, &#39;IN_QUADRA_ESPORTES_COBERTA&#39;, &#39;IN_BIBLIOTECA&#39;]].sum(axis=1)
extra2.loc[:, &#39;COMP2&#39;] = to_iee[[&#39;IN_AGUA_POTAVEL&#39;, &#39;IN_ENERGIA_REDE_PUBLICA&#39;, &#39;IN_ESGOTO_REDE_PUBLICA&#39;, &#39;IN_LIXO_SERVICO_COLETA&#39;, &#39;IN_BANHEIRO&#39;]].sum(axis=1)
extra2.loc[:, &#39;COMP3&#39;] = to_iee[[&#39;QT_COMP_DISP_ALUNO&#39;, &#39;QT_EQUIP_MULTIMIDIA&#39;, &#39;IN_INTERNET&#39;, &#39;IN_LABORATORIO_CIENCIAS&#39;, &#39;IN_LABORATORIO_INFORMATICA&#39;]].sum(axis=1)
extra2.loc[:, &#39;IEE&#39;] = extra2.COMP1 + extra2.COMP2 + extra2.COMP3

train = pd.merge(train, extra2, how=&#39;left&#39;, left_on=&#39;CO_ESCOLA&#39;, right_on=&#39;CO_ENTIDADE&#39;).drop(&#39;CO_ENTIDADE&#39;, axis=1)
test = pd.merge(test, extra2, how=&#39;left&#39;, left_on=&#39;CO_ESCOLA&#39;, right_on=&#39;CO_ENTIDADE&#39;).drop(&#39;CO_ENTIDADE&#39;, axis=1)</code></pre>
</div>
</div>
<div id="modelagem" class="section level2">
<h2>Modelagem</h2>
<p>Testei muitos modelos e muitas abordagens (inclusive com finalidade de estudo). Foram modelos estat√≠sticos (GAM considerando a distribui√ß√£o Beta(0,1)), redes neurais (TabNet) e √°rvores mas no final das contas os que tiveram melhor custo/benef√≠cio foram o LightGBM e o CatBoost.</p>
<p>Sobre o tuning, tomei a decis√£o de n√£o investir muito em otimiza√ß√£o autom√°tica de hiperpar√¢metros pois o tempo era curto e os ganhos seriam pequenos comparados com o potencial ganho com a variedade de features que poderiam ser geradas, ent√£o fiz apenas alguns testes manuais conforme via necessidade.</p>
<div id="pre-processing" class="section level4">
<h4>Pre processing</h4>
<p>A etapa que investi bastante tempo foi para criar novas vari√°veis. A seguir trago algumas features constru√≠das que foram utilizadas em determinados modelos, a partir dos dados dispon√≠veis:</p>
<ul>
<li>Renda somada dos pais;</li>
<li>N√≠vel de ocupa√ß√£o somado dos pais;</li>
<li>Renda dividido pelo n√∫mero de pessoas na casa;</li>
<li>Quantidade de celulares por pessoa na casa;</li>
<li>Quantidade de computadores por pessoa na casa;</li>
<li>Se a pessoa possui vis√£o ruim (se possui baixa vis√£o, cegueira ou monocular);</li>
<li>Se a pessoa possui audi√ß√£o ruim (Surdez, defici√™ncia auditiva);</li>
<li>Se o aluno possui TDAH e toma medicamento controlado;</li>
<li>Se o aluno possui TDAH e teve mais tempo de prova;</li>
<li>Se precisou de recurso de vis√£o ou audi√ß√£o (libras, baile, etc);</li>
<li>Se o munic√≠pio que nasceu √© o mesmo da escola;</li>
<li>Se o munic√≠pio que fez a prova √© o mesmo da escola;</li>
<li>Se o munic√≠pio da prova √© o mesmo da resid√™ncia;</li>
<li>Nota m√©dia dos alunos da respectiva escola nas outras provas (*);</li>
<li>Renda m√©dia dos alunos da respectiva escola (*).</li>
</ul>
<p>(*) Estas features precisaram ser calculadas de maneira muito cuidadosa para n√£o causar algum tipo de data leak!</p>
</div>
<div id="post-processing" class="section level4">
<h4>Post Processing</h4>
<p>Essa base tinha uma pegadinha que fazia muita diferen√ßa no resultado final. Existem duas possibilidades de um aluno tirar zero em uma prova: errar tudo ou n√£o comparecer.</p>
<p>Como temos a informa√ß√£o da presen√ßa do aluno na prova (o que na pr√°tica seria meio estranho) bastava dar zero para os alunos faltantes na hora de prever nos dados de teste para submeter.</p>
</div>
<div id="linguagens-e-c√≥digos" class="section level3">
<h3>Linguagens e C√≥digos</h3>
<p>Definir finalidade de algumas colunas:</p>
<pre class="python"><code># colunas que serao dropadas
to_drop = [&#39;IN_PROVA_DEITADO&#39;,
            &#39;NU_INSCRICAO&#39;,
            &#39;CO_MUNICIPIO_ESC&#39;,
            &#39;CO_UF_NASCIMENTO&#39;,
            &#39;CO_UF_RESIDENCIA&#39;,
            &#39;CO_UF_ESC&#39;,
            &#39;CO_UF_PROVA&#39;,
            &#39;CO_MUNICIPIO_PROVA&#39;,
            &#39;CO_MUNICIPIO_RESIDENCIA&#39;,
            &#39;CO_MUNICIPIO_NASCIMENTO&#39;]

# definir target e presenca
target = &quot;NU_NOTA_LC&quot;
presenca = &quot;TP_PRESENCA_LC&quot;

# demais notas para dropar (menos ch)
notas = list(set(targets)-set([target, &#39;NU_NOTA_CH&#39;]))</code></pre>
<p>Pr√©-processamento nos dados de treino</p>
<pre class="python"><code>X = train.copy()
X.loc[:, &#39;knn_feature&#39;] = knn_train.knn_oof
X = X.drop(to_drop, axis=1) 
X = X[X[presenca]==1]
X = X[~X[target].isnull()]

X = X.loc[:, ~X.columns.isin([target]+[presenca]+notas)]
X.loc[:, &#39;FE_RENDA&#39;] = X.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000, &#39;C&#39;:1500, &#39;D&#39;:2000,
&#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, &#39;I&#39;:6000, &#39;J&#39;:7000,&#39;K&#39;:8000,&#39;L&#39;:9000,
&#39;M&#39;:10000, &#39;N&#39;:12000, &#39;O&#39;:15000, &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
X = prep_data_questionarios(X)
X = fe_mun(X)
X = fe_questionario(X)
X = fe_in(X)
X = prep_co_escola(X)
X = fe_extra(X)

y = train.loc[(train[presenca]==1)&amp;(~train[target].isnull()), target].astype(np.float64)</code></pre>
<p>Pr√©-processamento nos dados de teste</p>
<pre class="python"><code>X_test = test.copy()
X_test.loc[:, &#39;knn_feature&#39;] = knn_test.knn_test
X_test = X_test.drop(to_drop, axis=1) 

X_test = X_test.loc[:, ~X_test.columns.isin([presenca])]
X_test.loc[:, &#39;FE_RENDA&#39;] = X_test.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000, &#39;C&#39;:1500, &#39;D&#39;:2000,
&#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, &#39;I&#39;:6000, &#39;J&#39;:7000, &#39;K&#39;:8000,&#39;L&#39;:9000,
&#39;M&#39;:10000, &#39;N&#39;:12000, &#39;O&#39;:15000, &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
X_test = prep_data_questionarios(X_test)
X_test = fe_mun(X_test)
X_test = fe_questionario(X_test)
X_test = fe_in(X_test)
X_test = prep_co_escola(X_test)
X_test = fe_extra(X_test)</code></pre>
<p>Feature engineering separada para evitar data leak:</p>
<pre class="python"><code># calcular estatisticas nos dados de treino
co_escola_renda_media = X.groupby(&#39;CO_ESCOLA&#39;).FE_RENDA.mean()
co_escola_idade_media = X.groupby(&#39;CO_ESCOLA&#39;).NU_IDADE.mean()
co_escola_nota_ch = X.groupby(&#39;CO_ESCOLA&#39;).NU_NOTA_CH.mean()
X = X.drop(&#39;NU_NOTA_CH&#39;, axis=1)

# instanciar objeto com as estatisticas por escola
co_escola_aux = pd.DataFrame({
    &#39;CO_ESCOLA&#39;: co_escola_renda_media.index,
    &#39;FE_ESCOLA_RENDA_MEDIA&#39;: co_escola_renda_media,
    &#39;FE_IDADE_MEDIA&#39;: co_escola_idade_media,
    &#39;FE_NOTA_CH&#39;: co_escola_nota_ch
}).reset_index(drop=True)

# Concatenar estatisticas nas bases de treino e teste
X = pd.merge(X, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)
X_test = pd.merge(X_test, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)

# Codigo da escola para categorico
X.loc[:, &#39;CO_ESCOLA&#39;] = X.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)
X_test.loc[:, &#39;CO_ESCOLA&#39;] = X_test.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)

# Features de contagem
X.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X_test.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X.NO_MUNICIPIO_RESIDENCIA.map({x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X_test.NO_MUNICIPIO_RESIDENCIA.map({ x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X.NO_MUNICIPIO_NASCIMENTO.map({ x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X_test.NO_MUNICIPIO_NASCIMENTO.map({x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})

X.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X_test.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})</code></pre>
<p>Ajustar modelo:</p>
<pre class="python"><code>cat_feat = X.columns[X.dtypes==&#39;category&#39;]
cat_indices = [X.columns.get_loc(x) for x in cat_feat]

for c in list(cat_feat):
    X.loc[:, c] = X.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)
    X_test.loc[:, c] = X_test.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)

X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.1, random_state=SEED)
    
clf = CatBoostRegressor(random_state=314,
                            cat_features=cat_indices,
                            verbose=0,
                            loss_function = &quot;RMSE&quot;,
                            od_type = &quot;Iter&quot;,
                            od_wait = 100,
                            iterations=3000,
                            use_best_model=True)

clf.fit(X, y, eval_set = (X_eval, y_eval), verbose=False, plot=True)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/lc_catboost.png" style="width:95.0%" />
</center>
<p>Salvar previs√µes:</p>
<pre class="python"><code>sub.loc[:, &#39;NU_NOTA_LC&#39;] = clf.predict(X_test)
# alunos que nao foram fazer a prova tiraram zero
sub.loc[test.TP_PRESENCA_LC!=1, &#39;NU_NOTA_LC&#39;] = 0</code></pre>
<p>Comparar distribui√ß√£o da target nos dados de treino com rela√ß√£o √†s previs√µes do modelo:</p>
<pre class="python"><code>sns.kdeplot(train.loc[:, target], shade=True, color=&#39;r&#39;, clip=[0,1000])
sns.kdeplot(sub.loc[:, target], shade=True, color=&#39;b&#39;, clip=[0,1000])
plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
plt.title(target)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/lc_pred.png" style="width:50.0%" />
</center>
</div>
<div id="ci√™ncias-humanas" class="section level3">
<h3>Ci√™ncias Humanas</h3>
<p>Novas features desenvolvidas especificamente para este modelo:</p>
<pre class="python"><code>def fe_ch(df):
    
    df.loc[:, &#39;FE_RENDA&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000,
    &#39;C&#39;:1500, &#39;D&#39;:2000, &#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, &#39;I&#39;:6000,
    &#39;J&#39;:7000, &#39;K&#39;:8000,&#39;L&#39;:9000, &#39;M&#39;:10000, &#39;N&#39;:12000, &#39;O&#39;:15000, 
    &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
    df.loc[:, &#39;FE_NU_IDADE*TP_ANO_CONCLUIU&#39;] = df.TP_ANO_CONCLUIU * df.NU_IDADE
    df.loc[:, &#39;FE_Q002+Q024&#39;] = df.loc[:, &#39;Q002&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, 
    &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}).astype(int) + 
    df.loc[:, &#39;Q024&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}).astype(int) 
    df.loc[:, &#39;FE_SCORE&#39;] = (1/df.TP_ANO_CONCLUIU) + np.sqrt(df.NU_IDADE) +
    np.where(df.TP_ESCOLA==3, 1, 0)
    
    return df</code></pre>
<p>Definir finalidade de algumas colunas:</p>
<pre class="python"><code># colunas que serao dropadas
to_drop = [&#39;IN_PROVA_DEITADO&#39;,
           &#39;NU_INSCRICAO&#39;,
           &#39;CO_MUNICIPIO_ESC&#39;,
           &#39;CO_UF_NASCIMENTO&#39;,
           &#39;CO_UF_RESIDENCIA&#39;,
           &#39;CO_UF_ESC&#39;,
           &#39;CO_UF_PROVA&#39;,
           &#39;CO_MUNICIPIO_PROVA&#39;,
           &#39;CO_MUNICIPIO_RESIDENCIA&#39;,
          &#39;CO_MUNICIPIO_NASCIMENTO&#39;]

# definir target e presenca
target = &quot;NU_NOTA_CH&quot;
presenca = &quot;TP_PRESENCA_CH&quot;

# demais notas para dropar (menos lc)
notas = list(set(targets)-set([target, &#39;NU_NOTA_LC&#39;]))</code></pre>
<p>Pr√©-processamento nos dados de treino</p>
<pre class="python"><code>X = train.copy()
X.loc[:, &#39;knn_feature&#39;] = knn_train.knn_oof
X = X.drop(to_drop, axis=1)
X = X[X[presenca]==1]
X = X[~X[target].isnull()]

X = X.loc[:, ~X.columns.isin([target]+[presenca]+notas)]
X = fe_ch(X)
X = prep_data_questionarios(X)
X = fe_mun(X)
X = fe_questionario(X)
#X = fe_in(X)
X = prep_co_escola(X)
X = fe_extra(X)

y = train.loc[(train[presenca]==1)&amp;(~train[target].isnull()), target].astype(np.float64)</code></pre>
<p>Pr√©-processamento nos dados de teste</p>
<pre class="python"><code>X_test = test.copy()
X_test.loc[:, &#39;knn_feature&#39;] = knn_test.knn_test
X_test = X_test.drop(to_drop, axis=1) 

X_test = X_test.loc[:, ~X_test.columns.isin([presenca])]
X_test = fe_ch(X_test)
X_test = prep_data_questionarios(X_test)
X_test = fe_mun(X_test)
X_test = fe_questionario(X_test)
#X_test = fe_in(X_test)
X_test = prep_co_escola(X_test)
X_test = fe_extra(X_test)</code></pre>
<p>Feature engineering separada para evitar data leak:</p>
<pre class="python"><code># calcular estatisticas nos dados de treino
co_escola_renda_media = X.groupby(&#39;CO_ESCOLA&#39;).FE_RENDA.mean()
co_escola_idade_media = X.groupby(&#39;CO_ESCOLA&#39;).NU_IDADE.mean()
co_escola_nota_lc = X.groupby(&#39;CO_ESCOLA&#39;).NU_NOTA_LC.mean()
X = X.drop(&#39;NU_NOTA_LC&#39;, axis=1)

# instanciar objeto com as estatisticas por escola
co_escola_aux = pd.DataFrame({
    &#39;CO_ESCOLA&#39;: co_escola_renda_media.index,
    &#39;FE_ESCOLA_RENDA_MEDIA&#39;: co_escola_renda_media,
    &#39;FE_IDADE_MEDIA&#39;: co_escola_idade_media,
    &#39;FE_NOTA_LC&#39;: co_escola_nota_lc
}).reset_index(drop=True)

# Concatenar estatisticas nas bases de treino e teste
X = pd.merge(X, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)
X_test = pd.merge(X_test, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)

# Codigo da escola para categorico
X.loc[:, &#39;CO_ESCOLA&#39;] = X.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)
X_test.loc[:, &#39;CO_ESCOLA&#39;] = X_test.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)

# Features de contagem
X.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X_test.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X.NO_MUNICIPIO_RESIDENCIA.map({x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X_test.NO_MUNICIPIO_RESIDENCIA.map({ x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X.NO_MUNICIPIO_NASCIMENTO.map({ x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X_test.NO_MUNICIPIO_NASCIMENTO.map({x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})

X.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X_test.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})</code></pre>
<p>Ajustar modelo:</p>
<pre class="python"><code>%%time

cat_feat = X.columns[X.dtypes==&#39;category&#39;]
cat_indices = [X.columns.get_loc(x) for x in cat_feat]

for c in list(cat_feat):
    X.loc[:, c] = X.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)
    X_test.loc[:, c] = X_test.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)

X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.1, random_state=SEED)

clf = CatBoostRegressor(random_state=314,
                            cat_features=cat_indices,
                            verbose=0,
                            loss_function = &quot;RMSE&quot;,
                            od_type = &quot;Iter&quot;,
                            od_wait = 100,iterations=3000,
                            use_best_model=True)

clf.fit(X, y, eval_set = (X_eval, y_eval), verbose=False, plot=True)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/ch_catboost.png" style="width:95.0%" />
</center>
<p>Salvar previs√µes:</p>
<pre class="python"><code>sub.loc[:, &#39;NU_NOTA_CH&#39;] = clf.predict(X_test)
# alunos que nao foram fazer a prova tiraram zero
sub.loc[test.TP_PRESENCA_CH!=1, &#39;NU_NOTA_CH&#39;] = 0</code></pre>
<p>Comparar distribui√ß√£o da target nos dados de treino com rela√ß√£o √†s previs√µes do modelo:</p>
<pre class="python"><code>sns.kdeplot(train.loc[:, target], shade=True, color=&#39;r&#39;, clip=[0,1000])
sns.kdeplot(sub.loc[:, target], shade=True, color=&#39;b&#39;, clip=[0,1000])
plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
plt.title(target)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/ch_pred.png" style="width:50.0%" />
</center>
</div>
<div id="ci√™ncias-da-natureza" class="section level3">
<h3>Ci√™ncias da Natureza</h3>
<p>Novas features desenvolvidas especificamente para este modelo:</p>
<pre class="python"><code>def fe_cn(df):
    df.loc[:, &#39;FE_RENDA&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000,
    &#39;C&#39;:1500, &#39;D&#39;:2000, &#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, 
    &#39;I&#39;:6000, &#39;J&#39;:7000, &#39;K&#39;:8000,&#39;L&#39;:9000, &#39;M&#39;:10000, &#39;N&#39;:12000, 
    &#39;O&#39;:15000, &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
    df.loc[:, &#39;FE_NU_IDADE*TP_ANO_CONCLUIU&#39;] = df.TP_ANO_CONCLUIU * df.NU_IDADE
    df.loc[:, &#39;FE_Q002+Q024&#39;] = df.loc[:, &#39;Q002&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2,
    &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}).astype(int) + 
    df.loc[:, &#39;Q024&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}).astype(int) 
    df.loc[:, &#39;FE_SCORE&#39;] = (1/df.TP_ANO_CONCLUIU) + np.sqrt(df.NU_IDADE) + np.where(df.TP_ESCOLA==3, 1, 0)
    
    df.loc[:, &#39;FE_UF_ESCOLA&#39;] = df.SG_UF_ESC.map({
      &#39;AM&#39;:&#39;Norte&#39;, &#39;RR&#39;:&#39;Norte&#39;, &#39;AP&#39;:&#39;Norte&#39;, &#39;PA&#39;:&#39;Norte&#39;, &#39;TO&#39;:&#39;Norte&#39;, &#39;RO&#39;:&#39;Norte&#39;, &#39;AC&#39;:&#39;Norte&#39;,
      &#39;MA&#39;:&#39;Nordeste&#39;, &#39;PI&#39;:&#39;Nordeste&#39;, &#39;CE&#39;:&#39;Nordeste&#39;, &#39;RN&#39;:&#39;Nordeste&#39;, &#39;PE&#39;:&#39;Nordeste&#39;, &#39;PB&#39;:&#39;Nordeste&#39;, &#39;SE&#39;:&#39;Nordeste&#39;, &#39;AL&#39;:&#39;Nordeste&#39;, &#39;BA&#39;:&#39;Nordeste&#39;,
      &#39;MT&#39;: &#39;CentroOeste&#39;, &#39;MS&#39;: &#39;CentroOeste&#39;, &#39;GO&#39;: &#39;CentroOeste&#39;,
      &#39;SP&#39;: &#39;Sudeste&#39;, &#39;RJ&#39;: &#39;Sudeste&#39;, &#39;ES&#39;: &#39;Sudeste&#39;, &#39;MG&#39;: &#39;Sudeste&#39;,
      &#39;PR&#39;: &#39;Sul&#39;, &#39;RS&#39;: &#39;Sul&#39;, &#39;SC&#39;: &#39;Sul&#39;}).astype(&#39;category&#39;)
    return df</code></pre>
<p>Definir finalidade de algumas colunas:</p>
<pre class="python"><code># colunas que serao dropadas
to_drop = [&#39;IN_PROVA_DEITADO&#39;,
           &#39;NU_INSCRICAO&#39;,
           &#39;CO_MUNICIPIO_ESC&#39;,
           &#39;CO_UF_NASCIMENTO&#39;,
           &#39;CO_UF_RESIDENCIA&#39;,
           &#39;CO_UF_ESC&#39;,
           &#39;CO_UF_PROVA&#39;,
           &#39;CO_MUNICIPIO_PROVA&#39;,
           &#39;CO_MUNICIPIO_RESIDENCIA&#39;,
          &#39;CO_MUNICIPIO_NASCIMENTO&#39;]

# definir target e presenca
target = &quot;NU_NOTA_CN&quot;
presenca = &quot;TP_PRESENCA_CN&quot;

# demais notas para dropar (menos mt)
notas = list(set(targets)-set([target, &#39;NU_NOTA_MT&#39;]))</code></pre>
<p>Pr√©-processamento nos dados de treino</p>
<pre class="python"><code>X = train.copy()
X = X.drop(to_drop, axis=1) 
X = X[X[presenca]==1]
X = X[~X[target].isnull()]

X = X.loc[:, ~X.columns.isin([target]+[presenca]+notas)]
X = fe_cn(X)
X = prep_data_questionarios(X)
X = fe_mun(X)
X = fe_questionario(X)
X = fe_in(X)
X = prep_co_escola(X)
X = fe_extra(X)

y = train.loc[(train[presenca]==1)&amp;(~train[target].isnull()), target].astype(np.float64)</code></pre>
<p>Pr√©-processamento nos dados de teste</p>
<pre class="python"><code>X_test = test.copy()
X_test = X_test.drop(to_drop, axis=1) 

X_test = X_test.loc[:, ~X_test.columns.isin([presenca])]
X_test = fe_cn(X_test)
X_test = prep_data_questionarios(X_test)
X_test = fe_mun(X_test)
X_test = fe_questionario(X_test)
X_test = fe_in(X_test)
X_test = prep_co_escola(X_test)
X_test = fe_extra(X_test)</code></pre>
<p>Feature engineering separada para evitar data leak:</p>
<pre class="python"><code># calcular estatisticas nos dados de treino
co_escola_renda_media = X.groupby(&#39;CO_ESCOLA&#39;).FE_RENDA.mean()
co_escola_idade_media = X.groupby(&#39;CO_ESCOLA&#39;).NU_IDADE.mean()
co_escola_nota_mt = X.groupby(&#39;CO_ESCOLA&#39;).NU_NOTA_MT.mean()
X = X.drop(&#39;NU_NOTA_MT&#39;, axis=1)

# instanciar objeto com as estatisticas por escola
co_escola_aux = pd.DataFrame({
    &#39;CO_ESCOLA&#39;: co_escola_renda_media.index,
    &#39;FE_ESCOLA_RENDA_MEDIA&#39;: co_escola_renda_media,
    &#39;FE_IDADE_MEDIA&#39;: co_escola_idade_media,
    &#39;FE_NOTA_MT&#39;: co_escola_nota_mt
}).reset_index(drop=True)

# Concatenar estatisticas nas bases de treino e teste
X = pd.merge(X, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)
X_test = pd.merge(X_test, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)

# Codigo da escola para categorico
X.loc[:, &#39;CO_ESCOLA&#39;] = X.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)
X_test.loc[:, &#39;CO_ESCOLA&#39;] = X_test.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)

# Features de contagem
X.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X_test.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X.NO_MUNICIPIO_RESIDENCIA.map({x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X_test.NO_MUNICIPIO_RESIDENCIA.map({ x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X.NO_MUNICIPIO_NASCIMENTO.map({ x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X_test.NO_MUNICIPIO_NASCIMENTO.map({x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})

X.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X_test.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})</code></pre>
<p>Ajustar modelo:</p>
<pre class="python"><code>%%time

cat_feat = X.columns[X.dtypes==&#39;category&#39;]
cat_indices = [X.columns.get_loc(x) for x in cat_feat]

for c in list(cat_feat):
    X.loc[:, c] = X.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)
    X_test.loc[:, c] = X_test.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)

X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.1, random_state=SEED)

clf = CatBoostRegressor(random_state=314,
                            cat_features=cat_indices,
                            verbose=0,
                            loss_function = &quot;RMSE&quot;,
                            od_type = &quot;Iter&quot;,
                            od_wait = 100,iterations=3000,
                            use_best_model=True)

clf.fit(X, y, eval_set = (X_eval, y_eval), verbose=False, plot=True)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/cn_catboost.png" style="width:95.0%" />
</center>
<p>Salvar previs√µes:</p>
<pre class="python"><code>sub.loc[:, &#39;NU_NOTA_CN&#39;] = clf.predict(X_test)
# alunos que nao foram fazer a prova tiraram zero
sub.loc[test.TP_PRESENCA_CN!=1, &#39;NU_NOTA_CN&#39;] = 0</code></pre>
<p>Comparar distribui√ß√£o da target nos dados de treino com rela√ß√£o √†s previs√µes do modelo:</p>
<pre class="python"><code>sns.kdeplot(train.loc[:, target], shade=True, color=&#39;r&#39;, clip=[0,1000])
sns.kdeplot(sub.loc[:, target], shade=True, color=&#39;b&#39;, clip=[0,1000])
plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
plt.title(target)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/cn_pred.png" style="width:50.0%" />
</center>
</div>
<div id="matem√°tica" class="section level3">
<h3>Matem√°tica</h3>
<p>Novas features desenvolvidas especificamente para este modelo:</p>
<pre class="python"><code>def fe_mt(df):
    
    df.loc[:, &#39;FE_RENDA&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000, &#39;C&#39;:1500, &#39;D&#39;:2000, &#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, &#39;I&#39;:6000, &#39;J&#39;:7000, &#39;K&#39;:8000,&#39;L&#39;:9000, &#39;M&#39;:10000, &#39;N&#39;:12000, &#39;O&#39;:15000, &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
    df.loc[:, &#39;FE_NU_IDADE*TP_ANO_CONCLUIU&#39;] = df.TP_ANO_CONCLUIU * df.NU_IDADE
    df.loc[:, &#39;FE_Q002+Q024&#39;] = df.loc[:, &#39;Q002&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}).astype(int) + df.loc[:, &#39;Q024&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}).astype(int) 
    df.loc[:, &#39;FE_SCORE&#39;] = (1/df.TP_ANO_CONCLUIU) + np.sqrt(df.NU_IDADE) + np.where(df.TP_ESCOLA==3, 1, 0)
    
    df.loc[:, &#39;FE_UF_ESCOLA&#39;] = df.SG_UF_ESC.map({&#39;AM&#39;:&#39;Norte&#39;, &#39;RR&#39;:&#39;Norte&#39;, &#39;AP&#39;:&#39;Norte&#39;, &#39;PA&#39;:&#39;Norte&#39;, &#39;TO&#39;:&#39;Norte&#39;, &#39;RO&#39;:&#39;Norte&#39;, &#39;AC&#39;:&#39;Norte&#39;,
                &#39;MA&#39;:&#39;Nordeste&#39;, &#39;PI&#39;:&#39;Nordeste&#39;, &#39;CE&#39;:&#39;Nordeste&#39;, &#39;RN&#39;:&#39;Nordeste&#39;, &#39;PE&#39;:&#39;Nordeste&#39;, &#39;PB&#39;:&#39;Nordeste&#39;, &#39;SE&#39;:&#39;Nordeste&#39;, &#39;AL&#39;:&#39;Nordeste&#39;, &#39;BA&#39;:&#39;Nordeste&#39;,
                &#39;MT&#39;: &#39;CentroOeste&#39;, &#39;MS&#39;: &#39;CentroOeste&#39;, &#39;GO&#39;: &#39;CentroOeste&#39;,
                &#39;SP&#39;: &#39;Sudeste&#39;, &#39;RJ&#39;: &#39;Sudeste&#39;, &#39;ES&#39;: &#39;Sudeste&#39;, &#39;MG&#39;: &#39;Sudeste&#39;,
                &#39;PR&#39;: &#39;Sul&#39;, &#39;RS&#39;: &#39;Sul&#39;, &#39;SC&#39;: &#39;Sul&#39;}).astype(&#39;category&#39;)
    
    
    return df</code></pre>
<p>Definir finalidade de algumas colunas:</p>
<pre class="python"><code># colunas que serao dropadas
to_drop = [&#39;IN_PROVA_DEITADO&#39;,
           &#39;NU_INSCRICAO&#39;,
           &#39;CO_MUNICIPIO_ESC&#39;,
           &#39;CO_UF_NASCIMENTO&#39;,
           &#39;CO_UF_RESIDENCIA&#39;,
           &#39;CO_UF_ESC&#39;,
           &#39;CO_UF_PROVA&#39;,
           &#39;CO_MUNICIPIO_PROVA&#39;,
           &#39;CO_MUNICIPIO_RESIDENCIA&#39;,
          &#39;CO_MUNICIPIO_NASCIMENTO&#39;]

# definir target e presenca
target = &quot;NU_NOTA_MT&quot;
presenca = &quot;TP_PRESENCA_MT&quot;

# demais notas para dropar (menos cn)
notas = list(set(targets)-set([target, &#39;NU_NOTA_CN&#39;]))</code></pre>
<p>Pr√©-processamento nos dados de treino</p>
<pre class="python"><code>X = train.copy()
X = X.drop(to_drop, axis=1) 
X = X[X[presenca]==1]
X = X[~X[target].isnull()]

X = X.loc[:, ~X.columns.isin([target]+[presenca]+notas)]
X = fe_mt(X)
X = prep_data_questionarios(X)
X = fe_mun(X)
#X = fe_questionario(X)
#X = fe_in(X)
X = prep_co_escola(X)
X = fe_extra(X)

y = train.loc[(train[presenca]==1)&amp;(~train[target].isnull()), target].astype(np.float64)</code></pre>
<p>Pr√©-processamento nos dados de teste</p>
<pre class="python"><code>X_test = test.copy()
X_test = X_test.drop(to_drop, axis=1) 

X_test = X_test.loc[:, ~X_test.columns.isin([presenca])]
X_test = fe_mt(X_test)
X_test = prep_data_questionarios(X_test)
X_test = fe_mun(X_test)
#X_test = fe_questionario(X_test)
#X_test = fe_in(X_test)
X_test = prep_co_escola(X_test)
X_test = fe_extra(X_test)</code></pre>
<p>Feature engineering separada para evitar data leak:</p>
<pre class="python"><code># calcular estatisticas nos dados de treino
co_escola_renda_media = X.groupby(&#39;CO_ESCOLA&#39;).FE_RENDA.mean()
co_escola_idade_media = X.groupby(&#39;CO_ESCOLA&#39;).NU_IDADE.mean()
co_escola_nota_cn = X.groupby(&#39;CO_ESCOLA&#39;).NU_NOTA_CN.mean()
X = X.drop(&#39;NU_NOTA_CN&#39;, axis=1)

# instanciar objeto com as estatisticas por escola
co_escola_aux = pd.DataFrame({
    &#39;CO_ESCOLA&#39;: co_escola_renda_media.index,
    &#39;FE_ESCOLA_RENDA_MEDIA&#39;: co_escola_renda_media,
    &#39;FE_IDADE_MEDIA&#39;: co_escola_idade_media,
    &#39;FE_NOTA_CN&#39;: co_escola_nota_cn
}).reset_index(drop=True)

# Concatenar estatisticas nas bases de treino e teste
X = pd.merge(X, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)
X_test = pd.merge(X_test, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)

# Codigo da escola para categorico
X.loc[:, &#39;CO_ESCOLA&#39;] = X.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)
X_test.loc[:, &#39;CO_ESCOLA&#39;] = X_test.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)

# Features de contagem
X.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X_test.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X.NO_MUNICIPIO_RESIDENCIA.map({x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X_test.NO_MUNICIPIO_RESIDENCIA.map({ x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X.NO_MUNICIPIO_NASCIMENTO.map({ x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X_test.NO_MUNICIPIO_NASCIMENTO.map({x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})

X.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X_test.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})</code></pre>
<p>Ajustar modelo:</p>
<pre class="python"><code>%%time

cat_feat = X.columns[X.dtypes==&#39;category&#39;]
cat_indices = [X.columns.get_loc(x) for x in cat_feat]

for c in list(cat_feat):
    X.loc[:, c] = X.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)
    X_test.loc[:, c] = X_test.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)

X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.1, random_state=SEED)
    
clf = CatBoostRegressor(random_state=314,
                            cat_features=cat_indices,
                            verbose=0,
                            loss_function = &quot;RMSE&quot;,
                            od_type = &quot;Iter&quot;,
                            od_wait = 100,iterations=3000,
                            use_best_model=True)

clf.fit(X, y, eval_set = (X_eval, y_eval), verbose=False, plot=True)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/mt_catboost.png" style="width:95.0%" />
</center>
<p>Salvar previs√µes:</p>
<pre class="python"><code>sub.loc[:, &#39;NU_NOTA_MT&#39;] = clf.predict(X_test)
# alunos que nao foram fazer a prova tiraram zero
sub.loc[test.TP_PRESENCA_CN!=1, &#39;NU_NOTA_MT&#39;] = 0</code></pre>
<p>Comparar distribui√ß√£o da target nos dados de treino com rela√ß√£o √†s previs√µes do modelo:</p>
<pre class="python"><code>sns.kdeplot(train.loc[:, target], shade=True, color=&#39;r&#39;, clip=[0,1000])
sns.kdeplot(sub.loc[:, target], shade=True, color=&#39;b&#39;, clip=[0,1000])
plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
plt.title(target)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/mt_pred.png" style="width:50.0%" />
</center>
</div>
<div id="reda√ß√£o" class="section level3">
<h3>Reda√ß√£o</h3>
<p>Novas features desenvolvidas especificamente para este modelo:</p>
<pre class="python"><code>def fe_rd(df):
    
    df.loc[:, &#39;FE_RENDA&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000, &#39;C&#39;:1500, &#39;D&#39;:2000, &#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, &#39;I&#39;:6000, &#39;J&#39;:7000, &#39;K&#39;:8000,&#39;L&#39;:9000, &#39;M&#39;:10000, &#39;N&#39;:12000, &#39;O&#39;:15000, &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
    df.loc[:, &#39;FE_NU_IDADE*TP_ANO_CONCLUIU&#39;] = df.TP_ANO_CONCLUIU * df.NU_IDADE
    df.loc[:, &#39;FE_Q002+Q024&#39;] = df.loc[:, &#39;Q002&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}).astype(int) + df.loc[:, &#39;Q024&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}).astype(int) 
    df.loc[:, &#39;FE_SCORE&#39;] = (1/df.TP_ANO_CONCLUIU) + np.sqrt(df.NU_IDADE) + np.where(df.TP_ESCOLA==3, 1, 0)
    
    df.loc[:, &#39;FE_RENDA_FAMILIA_+_IDADE&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;:5, &#39;G&#39;:6, &#39;H&#39;:7, &#39;I&#39;:8, &#39;J&#39;:9, &#39;K&#39;:10,&#39;L&#39;:11, &#39;M&#39;:12, &#39;N&#39;:13, &#39;O&#39;:14, &#39;P&#39;:15, &#39;Q&#39;:16}).astype(int) + df.NU_IDADE        
    df.loc[:, &#39;FE_RENDA_FAMILIA_+_ANO_CONCLUIU&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;:5, &#39;G&#39;:6, &#39;H&#39;:7, &#39;I&#39;:8, &#39;J&#39;:9, &#39;K&#39;:10,&#39;L&#39;:11, &#39;M&#39;:12, &#39;N&#39;:13, &#39;O&#39;:14, &#39;P&#39;:15, &#39;Q&#39;:16}).astype(int)+ df.TP_ANO_CONCLUIU  
    
    return df</code></pre>
<p>Definir finalidade de algumas colunas:</p>
<pre class="python"><code># colunas que serao dropadas
to_drop = [&#39;IN_PROVA_DEITADO&#39;,
           &#39;NU_INSCRICAO&#39;,
           &#39;CO_MUNICIPIO_ESC&#39;,
           &#39;CO_UF_NASCIMENTO&#39;,
           &#39;CO_UF_RESIDENCIA&#39;,
           &#39;CO_UF_ESC&#39;,
           &#39;CO_UF_PROVA&#39;,
           &#39;CO_MUNICIPIO_PROVA&#39;,
           &#39;CO_MUNICIPIO_RESIDENCIA&#39;,
          &#39;CO_MUNICIPIO_NASCIMENTO&#39;]

# definir target e presenca
target = &quot;NU_NOTA_REDACAO&quot;
presenca = &quot;TP_STATUS_REDACAO&quot;

# demais notas para dropar 
notas = list(set(targets)-set([target]))</code></pre>
<p>Pr√©-processamento nos dados de treino</p>
<pre class="python"><code>X = train.copy()
X = X.drop(to_drop, axis=1) 
X = X[X[presenca]==1]
X = X[~X[target].isnull()]


X = X.loc[:, ~X.columns.isin([target]+[presenca]+notas)]
X = fe_rd(X)
X = prep_data_questionarios(X)
X = fe_mun(X)
#X = fe_questionario(X)
X = fe_in(X)
X = prep_co_escola(X)
X = fe_extra(X)

y = train.loc[(train[presenca]==1)&amp;(~train[target].isnull()), target].astype(np.float64)</code></pre>
<p>Pr√©-processamento nos dados de teste</p>
<pre class="python"><code>X_test = test.copy()
X_test = X_test.drop(to_drop, axis=1) 

X_test = X_test.loc[:, ~X_test.columns.isin([presenca])]
X_test = fe_rd(X_test)
X_test = prep_data_questionarios(X_test)
X_test = fe_mun(X_test)
#X_test = fe_questionario(X_test)
X_test = fe_in(X_test)
X_test = prep_co_escola(X_test)
X_test = fe_extra(X_test)</code></pre>
<p>Feature engineering separada para evitar data leak:</p>
<pre class="python"><code># calcular estatisticas nos dados de treino
co_escola_renda_media = X.groupby(&#39;CO_ESCOLA&#39;).FE_RENDA.mean()
co_escola_idade_media = X.groupby(&#39;CO_ESCOLA&#39;).NU_IDADE.mean()

# instanciar objeto com as estatisticas por escola
co_escola_aux = pd.DataFrame({
    &#39;CO_ESCOLA&#39;: co_escola_renda_media.index,
    &#39;FE_ESCOLA_RENDA_MEDIA&#39;: co_escola_renda_media,
    &#39;FE_IDADE_MEDIA&#39;: co_escola_idade_media
}).reset_index(drop=True)

# Concatenar estatisticas nas bases de treino e teste
X = pd.merge(X, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)
X_test = pd.merge(X_test, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)

# Codigo da escola para categorico
X.loc[:, &#39;CO_ESCOLA&#39;] = X.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)
X_test.loc[:, &#39;CO_ESCOLA&#39;] = X_test.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)

# Features de contagem
X.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X_test.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X.NO_MUNICIPIO_RESIDENCIA.map({x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X_test.NO_MUNICIPIO_RESIDENCIA.map({ x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X.NO_MUNICIPIO_NASCIMENTO.map({ x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X_test.NO_MUNICIPIO_NASCIMENTO.map({x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})

X.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X_test.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})</code></pre>
<p>Ajustar modelo:</p>
<pre class="python"><code>%%time

cat_feat = X.columns[X.dtypes==&#39;category&#39;]
cat_indices = [X.columns.get_loc(x) for x in cat_feat]

for c in list(cat_feat):
    X.loc[:, c] = X.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)
    X_test.loc[:, c] = X_test.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)

X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.1, random_state=SEED)

clf = CatBoostRegressor(random_state=314,
                            cat_features=cat_indices,
                            verbose=0,
                            loss_function = &quot;RMSE&quot;,
                            od_type = &quot;Iter&quot;,
                            od_wait = 100,iterations=3000,
                            use_best_model=True)

clf.fit(X, y, eval_set = (X_eval, y_eval), verbose=False, plot=True)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/redacao_catboost.png" style="width:95.0%" />
</center>
<p>Salvar previs√µes:</p>
<pre class="python"><code>sub.loc[:, &#39;NU_NOTA_REDACAO&#39;] = clf.predict(X_test)
# alunos que nao foram fazer a prova tiraram zero
sub.loc[test.TP_STATUS_REDACAO!=1, &#39;NU_NOTA_REDACAO&#39;] = 0</code></pre>
<p>Comparar distribui√ß√£o da target nos dados de treino com rela√ß√£o √†s previs√µes do modelo:</p>
<pre class="python"><code>sns.kdeplot(train.loc[:, target], shade=True, color=&#39;r&#39;, clip=[0,1000])
sns.kdeplot(sub.loc[:, target], shade=True, color=&#39;b&#39;, clip=[0,1000])
plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
plt.title(target)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/redacao_pred.png" style="width:50.0%" />
</center>
</div>
</div>
</div>
<div id="submiss√£o" class="section level1">
<h1>Submiss√£o</h1>
<p>Veja a seguir como ficou a distribui√ß√£o das previs√µes comparada √† distribui√ß√£o da target nos dados de treino:</p>
<pre class="python"><code>plt.figure(figsize=(16, 5))

notas = [&#39;NU_NOTA_CH&#39;, &#39;NU_NOTA_CN&#39;, &#39;NU_NOTA_MT&#39;, &#39;NU_NOTA_LC&#39;, &#39;NU_NOTA_REDACAO&#39;]

for i in range(len(notas)):

    plt.subplot(1, 5, i+1)
    sns.kdeplot(train.loc[:, notas[i]], shade=True, color=&#39;r&#39;, clip=[0,1000])
    sns.kdeplot(sub.loc[:, notas[i]], shade=True, color=&#39;b&#39;, clip=[0,1000])
    plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
    plt.title(notas[i])
plt.tight_layout()
plt.show()</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/all_pred.png" style="width:95.0%" />
</center>
<p>Acredito que talvez um tuning do modelo poderia trazer mais qualidade √†s previs√µes mas com o tempo limitado n√£o pude investir muito nesta etapa.</p>
</div>
<div id="considera√ß√µes-finais" class="section level1">
<h1>Considera√ß√µes Finais</h1>
<p>Em resumo, essas foram as principais id√©ias para a solu√ß√£o da competi√ß√£o e acredito que um dos segredos era focar em feature engineering por 2 motivos:</p>
<ul>
<li>A base era muito grande e o processo de tuning seria muito custoso (a n√£o ser que tenha um √≥timo computador a disposi√ß√£o);</li>
<li>Os atributos n√£o eram an√¥nimos, o que d√° muita informa√ß√£o de contexto.</li>
</ul>
<p>Agrade√ßo aos organizadores e √† todos os participantes que tornaram esta competi√ß√£o t√£o divertida! Por mais competi√ß√µes como esta, que valorizam a comunidade brasileira de Data Science!</p>
<p>Espero que tenham gostado e at√© logo!</p>
<p>Abra√ßos!</p>
<p>Fellipe Gomes</p>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2022-04-20-solucao-final-education-quality-kaggle-competition/">Solu√ß√£o Final - ML Olympiad [2¬∫ lugar]</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">catboost</category>
      <category domain="tag">data-mining</category>
      <category domain="tag">data-science</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">machine-learning</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">pratica</category>
      <category domain="tag">r</category>
      <category domain="tag">regressao</category>
    </item>
    <item>
      <title>Solu√ß√£o Final - Porto Seguro Data Challenge [3¬∫ lugar]</title>
      <link>https://gomesfellipe.github.io/post/2021-11-01-solucao-final-porto-seguro-data-challenge/</link>
      <pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2021-11-01-solucao-final-porto-seguro-data-challenge/</guid>
      <description>Confira a estrat√©gia aplicada para a competi√ß√£o de machine learning do Porto Seguro hospedada no Kaggle</description>
      <content:encoded>&lt;![CDATA[
        

<div id="TOC">
<ul>
<li><a href="#introdu%C3%A7%C3%A3o" id="toc-introdu√ß√£o">Introdu√ß√£o</a></li>
<li><a href="#defini%C3%A7%C3%A3o-do-problema-de-neg%C3%B3cio" id="toc-defini√ß√£o-do-problema-de-neg√≥cio">Defini√ß√£o do problema de neg√≥cio</a></li>
<li><a href="#an%C3%A1lise-explorat%C3%B3ria-em-r" id="toc-an√°lise-explorat√≥ria-em-r">An√°lise Explorat√≥ria (em R)</a></li>
<li><a href="#machine-learning-em-python" id="toc-machine-learning-em-python">Machine Learning (em Python)</a>
<ul>
<li><a href="#importar-depend%C3%AAncias" id="toc-importar-depend√™ncias">Importar depend√™ncias</a></li>
<li><a href="#stage-0-feature-extraction-com-knn" id="toc-stage-0-feature-extraction-com-knn">Stage 0: Feature Extraction com KNN</a></li>
<li><a href="#stage-1-tuning-xgboost-com-optuna" id="toc-stage-1-tuning-xgboost-com-optuna">Stage 1: Tuning XGBoost com Optuna</a></li>
<li><a href="#stage-2-calcular-out-of-fold-shap-values" id="toc-stage-2-calcular-out-of-fold-shap-values">Stage 2: Calcular Out-Of-Fold SHAP values</a></li>
<li><a href="#stage-3-modelo-final-com-autogluon" id="toc-stage-3-modelo-final-com-autogluon">Stage 3: Modelo Final com AutoGluon</a></li>
</ul></li>
<li><a href="#conclus%C3%A3o" id="toc-conclus√£o">Conclus√£o</a></li>
<li><a href="#refer%C3%AAncias" id="toc-refer√™ncias">Refer√™ncias</a></li>
</ul>
</div>

<style>
.column {
float: left;
width: 50%;
padding: 10px;
}

.column4 {
float: left;
width: 20%;
padding: 10px;
}

.column8 {
float: left;
width: 80%;
padding: 10px;
}

.row:after {
content: "";
display: table;
clear: both;
}

.center {
display: flex;
justify-content: center;
align-items: center;
height: 200px;
}
</style>
<hr />
<div id="introdu√ß√£o" class="section level1">
<h1>Introdu√ß√£o</h1>
<div class="row">
<div class="column8">
<p>Em Agosto e 2021 a Porto Seguro lan√ßou um desafio no Kaggle que consistia em estimar a propens√£o de aquisi√ß√£o de novos produtos. Tratava-se de um problema de classifica√ß√£o e foi bem desafiador principalmente por 2 motivos:</p>
<ol style="list-style-type: decimal">
<li>Todas as features da base de ddos eram anonimas;</li>
<li>A m√©trica de avalia√ß√£o foi a F1 Score (sens√≠vel √† um ponto de corte)</li>
</ol>
</div>
<div class="column4">
<div class="float">
<img src="https://media.giphy.com/media/Ie2Hs3A0uJRtK/giphy.gif" alt="Via Giphy" />
<div class="figcaption"><a href="https://media.giphy.com/media/Ie2Hs3A0uJRtK/giphy.gif">Via Giphy</a></div>
</div>
</div>
</div>
<p>Depois de 2 longos meses e dezenas de notebooks desenvolvidos, muitas submiss√µes frustradas e muitas horas a menos de sono, cheguei em uma solu√ß√£o final que envole um <em>blending</em> de modelos e <em>pseudo-labels</em> e quando a competi√ß√£o acabou, percebi que uma solu√ß√£o mais simples de implementar teria um resultado privado ainda maior do que o notebook que selecionei. üòÖ</p>
<div class="w3-panel w3-sand w3-border">
<p>‚ö†Ô∏è Aten√ß√£o!</p>
<p>Neste post abordarei uma solu√ß√£o mais simples e eficiente mas caso tenha interesse em conferir a solu√ß√£o final completa (um grande frankstein), j√° est√° <a href="https://github.com/gomesfellipe/porto_seguro_data_challenge">publica la no github</a>.</p>
</div>
<p>Este notebook √© uma <a href="https://www.kaggle.com/gomes555/3st-place-simplified-solution-0-6967-private">reescritura do meu notebook publicado no Kaggle em linguagem Python</a>. Para quem acompanha meus posts de R pode achar meio estranho este notebook mas convido-o a tentar entender a solu√ß√£o pois foi desenvolvida pela perspectiva de um usu√°rio nativo de R.</p>
<p>Espero que gostem! ü§ò</p>
</div>
<div id="defini√ß√£o-do-problema-de-neg√≥cio" class="section level1">
<h1>Defini√ß√£o do problema de neg√≥cio</h1>
<p>Segundo a descri√ß√£o da competi√ß√£o:</p>
<blockquote>
<p>Voc√™ provavelmente j√° recebeu uma liga√ß√£o de telemarketing oferecendo um produto que voc√™ n√£o precisa. Essa situa√ß√£o de estresse √© minimizada quando voc√™ oferece um produto que o cliente realmente precisa. <br /><br /> Nessa competi√ß√£o voc√™ ser√° desafiado a construir um modelo que prediz a probabilidade de aquisi√ß√£o de um produto.</p>
</blockquote>
<p>Sobre a m√©trica de avalia√ß√£o:</p>
<p>O crit√©rio utilizado para defini√ß√£o da melhor solu√ß√£o ser√° o F1-Score, veja sua formula:</p>
<p><span class="math display">\[
F_1 = 2 \times \frac{precision \times recall}{precision + recall}  
\]</span></p>
<p>Note que tanto a <em>Precision</em> quanto a <em>Recall</em> precisam de um ponto de corte para obter as classes e por isso busquei otmizar as m√©tricas <em>ROC-AUC</em> e <em>Log Loss</em> para obter estimativas de probabilidades com qualidade para finalmente calcular os pontos de corte que maximizam a <em>F1</em>.</p>
</div>
<div id="an√°lise-explorat√≥ria-em-r" class="section level1">
<h1>An√°lise Explorat√≥ria (em R)</h1>
<p>Antes de partir para modelagem fiz uma an√°lise explorat√≥ria utilizando a linguagem R. Neste post tratarei de maneira bem breve e quem tiver interesse em conferir mais detalhes bem como os c√≥digos dos gr√°ficos basta acessar o <a href="https://www.kaggle.com/gomes555/porto-seguro-r-an-lise-explorat-ria-dos-dados">notebook que deixei aberto no Kaggle</a>.</p>
<p>Veja alguns gr√°ficos:</p>
<center>
<img src="/post/2021-11-01-solucao-final-porto-seguro-data-challenge/aed.png" style="width:90.0%" />
</center>
<!-- <div class="w3-panel w3-light-blue w3-border"> -->
<p><strong>üìå Interpreta√ß√£o:</strong> <br></p>
<ul>
<li><strong>Categ√≥ricas</strong>:
<div style="color: rgb(0, 0, 0);">
<ul>
<li>
<strong>Qualitativo nominal</strong>: Possuem muitas classes, poderia ser o nome do produto, regi√£o, um texto o que torna o desafio ainda maior para criar novas features;
</li>
<li>
<strong>Qualitativo ordinal</strong>: Basicamente deixei como veio pois j√° tava como numerico;
</li>
</ul>
</div></li>
<li><strong>Num√©ricas</strong>:
<div style="color: rgb(0, 0, 0);">
<ul>
<li>
<strong>Quantitativo continua</strong>: Todas est√£o normalizadas (0, 1), algumas s√£o bimodais, algumas assim√©tricas a direita (pode ser tempo ate alguma coisa);
</li>
<li>
<strong>Quantitativo discreto</strong>: Sem muito o que fazer, observa√ß√£o apenas a feature <code>var52</code> que parece idade
</li>
</ul>
</div></li>
<li><strong>Dados missing</strong>: Parece haver algum padr√£o na maneira como os dados missing ocorrem e tentei substituir os <code>-999</code> por <code>NaN</code>, imputar a m√©dia, a mediana e via outros modelos
<!-- </div> --></li>
</ul>
<p>N√£o achei que seria muito produtivo ficar adivinhando o que poderia ser cada feature pois praticamente todos as transforma√ß√µes e novas features que gerei n√£o superavam o resultado do modelo ajustado nos dados da maneira que vinham portanto procurei investir mais tempo na modelagem mesmo.</p>
</div>
<div id="machine-learning-em-python" class="section level1">
<h1>Machine Learning (em Python)</h1>
<p>Veja a estrat√©gia de modelagem de maneira visual:</p>
</br>
<center>
<img src="/post/2021-11-01-solucao-final-porto-seguro-data-challenge/final_pipeline.png" style="width:60.0%" />
</center>
<p></br></p>
<div id="importar-depend√™ncias" class="section level2">
<h2>Importar depend√™ncias</h2>
<p>Carregar pacotes do Python</p>
<pre class="python"><code># general packages
import pandas as pd
import numpy as np
import time
# knn features
from gokinjo import knn_kfold_extract
from gokinjo import knn_extract
# ml tools
from sklearn.model_selection import StratifiedKFold, KFold
from sklearn.metrics import f1_score, log_loss, roc_auc_score
# models
from catboost import CatBoostClassifier
from xgboost import XGBClassifier
# optimization
import optuna
# interpretable ml
import shap
# automl
from autogluon.tabular import TabularPredictor
# ignore specific warnings
import warnings
warnings.filterwarnings(&quot;ignore&quot;, message=&quot;ntree_limit is deprecated, use `iteration_range` or model slicing instead.&quot;)</code></pre>
<p>Definir fun√ß√µes auxiliares para calcular o ponto de corte que maximiza a F1:</p>
<pre class="python"><code>def get_threshold(y_true, y_pred):
    thresholds = np.arange(0.0, 1.0, 0.01)
    f1_scores = []
    for thresh in thresholds:
        f1_scores.append(
            f1_score(y_true, [1 if m&gt;thresh else 0 for m in y_pred]))
    f1s = np.array(f1_scores)
    return thresholds[f1s.argmax()]
    
def custom_f1(y_true, y_pred):
    max_f1_threshold =  get_threshold(y_true, y_pred)
    y_pred = np.where(y_pred&gt;max_f1_threshold, 1, 0)
    return f1_score(y_true, y_pred) </code></pre>
<p>Carregar <a href="https://www.kaggle.com/c/porto-seguro-data-challenge/data">dados da competi√ß√£o</a>:</p>
<pre class="python"><code># load data
train = pd.read_csv(&#39;../input/porto-seguro-data-challenge/train.csv&#39;).drop(&#39;id&#39;, axis=1)
test = pd.read_csv(&#39;../input/porto-seguro-data-challenge/test.csv&#39;).drop(&#39;id&#39;, axis=1)
sample_submission = pd.read_csv(&#39;../input/porto-seguro-data-challenge/submission_sample.csv&#39;)
meta = pd.read_csv(&#39;../input/porto-seguro-data-challenge/metadata.csv&#39;)

# get data types
cat_nom = [x for x in meta.iloc[1:-1, :].loc[(meta.iloc[:,1]==&quot;Qualitativo nominal&quot;)].iloc[:,0]] 
cat_ord = [x for x in meta.iloc[1:-1, :].loc[(meta.iloc[:,1]==&quot;Qualitativo ordinal&quot;)].iloc[:,0]] 
num_dis = [x for x in meta.iloc[1:-1, :].loc[(meta.iloc[:,1]==&quot;Quantitativo discreto&quot;)].iloc[:,0]] 
num_con = [x for x in meta.iloc[1:-1, :].loc[(meta.iloc[:,1]==&quot;Quantitativo continua&quot;)].iloc[:,0]] </code></pre>
</div>
<div id="stage-0-feature-extraction-com-knn" class="section level2">
<h2>Stage 0: Feature Extraction com KNN</h2>
<p>Esta t√©cnica gera <span class="math inline">\(k \times c\)</span> novas features, onde <span class="math inline">\(c\)</span> √© o n√∫mero de classes da target. As novas features s√£o calculadas a partir das dist√¢ncias entre as observa√ß√µes e seus k vizinhos mais pr√≥ximos dentro de cada classe;</p>
<p>O valor para os <span class="math inline">\(K\)</span> vizinhos mais pr√≥ximos selecionado foi <span class="math inline">\(K=1\)</span> e para isso utilizei a biblioteca
<a href="https://github.com/momijiame/gokinjo"><code>gokinjo</code></a> que foi <a href="https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335">inspirada nas id√©ias apresentadas na solu√ß√£o vencedora do Otto Group Product Classification Challenge.</a></p>
<pre class="python"><code># convert to numpy because gokinjo expects np arrays
X = train[cat_nom+cat_ord+num_dis+num_con].to_numpy()
y = train.y.to_numpy()
X_test = test[cat_nom+cat_ord+num_dis+num_con].to_numpy()

# extract on train data
KNN_feat_train = knn_kfold_extract(X, y, k=1, normalize=&#39;standard&#39;)
print(&quot;KNN features for training set, shape: &quot;, np.shape(KNN_feat_train))

# extract on test data
KNN_feat_test = knn_extract(X, y, X_test, k=1, normalize=&#39;standard&#39;)
print(&quot;KNN features for test set, shape: &quot;, np.shape(KNN_feat_test))

# convert to dataframe
knn_feat_train = pd.DataFrame(KNN_feat_train, columns=[&quot;knn&quot;+str(x) for x in range(knn_feat_train.shape[1])])
knn_feat_test = pd.DataFrame(KNN_feat_test, columns=[&quot;knn&quot;+str(x) for x in range(knn_feat_test.shape[1])])</code></pre>
<pre><code>## KNN features for training set, shape:  (14123, 2)
## KNN features for test set, shape:  (21183, 2)</code></pre>
</div>
<div id="stage-1-tuning-xgboost-com-optuna" class="section level2">
<h2>Stage 1: Tuning XGBoost com Optuna</h2>
<p>Testei e otimizei muitos modelos como XGBoost, NGBoost, LightGBM, CatBoost, TabNet, HistGradientBoosting e algumas DNNs e em todos os casos (exceto DNNs) utilizei o Optuna para a sele√ß√£o dos hiperpar√¢metros.</p>
<p>Tamb√©m inclui nas tentativas iniciais de otimiza√ß√£o alguns m√©todos de remostrarem como Random Under Sampling, Smote, Tomek, Adasyn dentre outros mas n√£o tive muito sucesso.. apenas a combina√ß√£o Tomek + CatBoost pareceu trazer algum ganho.</p>
<p>Claro que minhas tentativas n√£o foram exautivas e devido ao tempo limitado acabei selecionando o XGBoost que foi o que apresentou as melhores m√©tricas depois de otimizado e tamb√©m o CatBoost com alguns hiperpar√¢metros fixos para serem a base deste pipeline.</p>
<p>Principais Informa√ß√µes üìå :</p>
<ul>
<li>Nenhum pr√©-processamento;</li>
<li>KFold K=10;</li>
<li>Otimiza√ß√£o de hiperpar√¢metros com Optuna;</li>
<li>Loss do XGBoost: Log Loss;</li>
<li>Loss do Otimizador: Log Loss;</li>
<li>Sem resampling;</li>
<li>Previs√£o final com a probabilidade m√©dia de 10 seeds diferentes</li>
</ul>
<pre class="python"><code>X_test = test[cat_nom+cat_ord+num_dis+num_con]
X = train[cat_nom+cat_ord+num_dis+num_con]
y = train.y

K=10
SEED=314
kf = KFold(n_splits=K, random_state=SEED, shuffle=True)</code></pre>
<pre class="python"><code>fixed_params = {
    &#39;random_state&#39;: 9,
    &quot;objective&quot;: &quot;binary:logistic&quot;,
    &quot;eval_metric&quot;: &#39;logloss&#39;,
    &#39;use_label_encoder&#39;:False,
    &#39;n_estimators&#39;:10000,
}

def objective(trial):
    
    hyperparams = {
        &#39;clf&#39;:{
        &quot;booster&quot;: trial.suggest_categorical(&quot;booster&quot;, [&quot;gbtree&quot;]),
        &quot;lambda&quot;: trial.suggest_float(&quot;lambda&quot;, 1e-8, 5.0, log=True),
        &quot;alpha&quot;: trial.suggest_float(&quot;alpha&quot;, 1e-8, 5.0, log=True)
        }
    }
    
    if hyperparams[&#39;clf&#39;][&quot;booster&quot;] == &quot;gbtree&quot; or hyperparams[&#39;clf&#39;][&quot;booster&quot;] == &quot;dart&quot;:
        hyperparams[&#39;clf&#39;][&quot;max_depth&quot;] = trial.suggest_int(&quot;max_depth&quot;, 1, 9)
        hyperparams[&#39;clf&#39;][&quot;eta&quot;] = trial.suggest_float(&quot;eta&quot;, 0.01, 0.1, log=True)
        hyperparams[&#39;clf&#39;][&quot;gamma&quot;] = trial.suggest_float(&quot;gamma&quot;, 1e-8, 1.0, log=True)
        hyperparams[&#39;clf&#39;][&quot;grow_policy&quot;] = trial.suggest_categorical(&quot;grow_policy&quot;, [&quot;depthwise&quot;, &quot;lossguide&quot;])
        hyperparams[&#39;clf&#39;][&#39;min_child_weight&#39;] = trial.suggest_int(&#39;min_child_weight&#39;, 5, 20)
        hyperparams[&#39;clf&#39;][&quot;subsample&quot;] = trial.suggest_float(&quot;subsample&quot;, 0.03, 1)
        hyperparams[&#39;clf&#39;][&quot;colsample_bytree&quot;] = trial.suggest_float(&quot;colsample_bytree&quot;, 0.03, 1)
        hyperparams[&#39;clf&#39;][&#39;max_delta_step&#39;] = trial.suggest_float(&#39;max_delta_step&#39;, 0, 10)
        
    if hyperparams[&#39;clf&#39;][&quot;booster&quot;] == &quot;dart&quot;:
        hyperparams[&#39;clf&#39;][&quot;sample_type&quot;] = trial.suggest_categorical(&quot;sample_type&quot;, [&quot;uniform&quot;, &quot;weighted&quot;])
        hyperparams[&#39;clf&#39;][&quot;normalize_type&quot;] = trial.suggest_categorical(&quot;normalize_type&quot;, [&quot;tree&quot;, &quot;forest&quot;])
        hyperparams[&#39;clf&#39;][&quot;rate_drop&quot;] = trial.suggest_float(&quot;rate_drop&quot;, 1e-8, 1.0, log=True)
        hyperparams[&#39;clf&#39;][&quot;skip_drop&quot;] = trial.suggest_float(&quot;skip_drop&quot;, 1e-8, 1.0, log=True)
    
    params = dict(**fixed_params, **hyperparams[&#39;clf&#39;])
    xgb_oof = np.zeros(X.shape[0])

    for fold, (train_idx, val_idx) in enumerate(kf.split(X=X, y=y)):
        X_train = X.iloc[train_idx]
        y_train = y.iloc[train_idx]
        X_val = X.iloc[val_idx]
        y_val = y.iloc[val_idx]
        
        model = XGBClassifier(**params)
        
        model.fit(X_train, y_train,
                  eval_set=[(X_val, y_val)],
                  early_stopping_rounds=150,
                  verbose=False)
    
        xgb_oof[val_idx] = model.predict_proba(X_val)[:,1]

        del model

    return log_loss(y, xgb_oof)</code></pre>
<p>Como no Kaggle existe o limite de aproximadamente 8h para executar um notebook, coloquei um limite de 7.5 horas para a busca de hiperpar√¢metros:</p>
<pre class="python"><code>study_xgb = optuna.create_study(direction=&#39;minimize&#39;)

study_xgb.optimize(objective, 
               timeout=60*60*7.5, 
               gc_after_trial=True)</code></pre>
<p>Resultados da busca:</p>
<pre class="python"><code>print(&#39;-&gt; Number of finished trials: &#39;, len(study_xgb.trials))
print(&#39;-&gt; Best trial:&#39;)
trial = study_xgb.best_trial
print(&#39;\tValue: {}&#39;.format(trial.value))
print(&#39;-&gt; Params: &#39;)
trial.params</code></pre>
<pre><code>## -&gt; Number of finished trials:  197
## -&gt; Best trial:
## 	Value: 0.3028443879614926
## -&gt; Params: 
## {&#39;booster&#39;: &#39;gbtree&#39;,
##  &#39;lambda&#39;: 9.012384508756378e-07,
##  &#39;alpha&#39;: 0.7472040331088792,
##  &#39;max_depth&#39;: 5,
##  &#39;eta&#39;: 0.01507605562231303,
##  &#39;gamma&#39;: 1.0214961302342215e-08,
##  &#39;grow_policy&#39;: &#39;lossguide&#39;,
##  &#39;min_child_weight&#39;: 5,
##  &#39;subsample&#39;: 0.9331005225916879,
##  &#39;colsample_bytree&#39;: 0.25392142363325004,
##  &#39;max_delta_step&#39;: 5.685109389498008}</code></pre>
<p>Acompanhar o hist√≥rico de cada etapa da otimiza√ß√£o:</p>
<pre class="python"><code>plot_optimization_history(study_xgb)</code></pre>
<center>
<img src="/post/2021-11-01-solucao-final-porto-seguro-data-challenge/optimization_hist.png" style="width:90.0%" />
</center>
<p>Avaliar as combina√ß√µes de hiperpar√¢metros mais bem sucedidas:</p>
<pre class="python"><code>optuna.visualization.plot_parallel_coordinate(study_xgb)</code></pre>
<center>
<img src="/post/2021-11-01-solucao-final-porto-seguro-data-challenge/parallel_plot.png" style="width:90.0%" />
</center>
<p>Quais hiperpar√¢metros tiveram mais impacto na modelagem:</p>
<pre class="python"><code>plot_param_importances(study_xgb)</code></pre>
<center>
<img src="/post/2021-11-01-solucao-final-porto-seguro-data-challenge/param_imp.png" style="width:90.0%" />
</center>
<p>Ap√≥s as 7.5 horas de busca, a melhor combina√ß√£o encontrada para o XGBoost foi a seguinte:</p>
<pre class="python"><code># After 7.5 hours...
study_xgb = {&#39;booster&#39;: &#39;gbtree&#39;,
 &#39;lambda&#39;: 9.012384508756378e-07,
 &#39;alpha&#39;: 0.7472040331088792,
 &#39;max_depth&#39;: 5,
 &#39;eta&#39;: 0.01507605562231303,
 &#39;gamma&#39;: 1.0214961302342215e-08,
 &#39;grow_policy&#39;: &#39;lossguide&#39;,
 &#39;min_child_weight&#39;: 5,
 &#39;subsample&#39;: 0.9331005225916879,
 &#39;colsample_bytree&#39;: 0.25392142363325004,
 &#39;max_delta_step&#39;: 5.685109389498008}</code></pre>
<p>Preparar lista de hiperpar√¢metros do XGBoost:</p>
<pre class="python"><code>final_params_xgb = dict()
final_params_xgb[&#39;clf&#39;]=dict(**fixed_params, **study_xgb)</code></pre>
</div>
<div id="stage-2-calcular-out-of-fold-shap-values" class="section level2">
<h2>Stage 2: Calcular Out-Of-Fold SHAP values</h2>
<p>Ap√≥s obter a melhor combina√ß√£o de hiperpar√¢metros para o XGBoost e encontrar resultados formid√°veis com o CatBoost modificando apenas alguns hiperpar√¢metros, resolvi tentar utilizar a informa√ß√£o adquirida pelo <em>SHAP values</em> desses modelos como entrada para novos modelos.</p>
<p>Algumas vantagens de se usar o shap values como um m√©todo de encoder dos dados, <a href="https://www.kaggle.com/pavelvod/gbm-supervised-pretraining">segundo este notebook publicado no Kaggle</a> (muito interessante por sinal):</p>
<ul>
<li>Normaliza os dados;</li>
<li>Mais ou menos Linearizado pois as <em>features</em> s√£o transformadas em suas import√¢ncias;</li>
<li>Recursos categ√≥ricos codificados de maneira mais inteligente (A codifica√ß√£o n√£o √© linear e depende de outros recursos da amostra);</li>
<li>Tratamento mais inteligente para valores <em>missing</em>.</li>
</ul>
<p>Para evitar <em>data leak</em>, o <em>SHAP values</em> foi calculado em cima dos dados <em>out-of-fold</em> para os dados de treino e a m√©dia da previs√£o de todos os <em>fold</em> nos dados de teste.</p>
<p>Definir estrat√©gia de valida√ß√£o cruzada:</p>
<pre class="python"><code>X_test = test[cat_nom+cat_ord+num_dis+num_con]
X = train[cat_nom+cat_ord+num_dis+num_con]
y = train.y

K=15 # number of bins with Sturge‚Äôs rule
SEED=123
kf = StratifiedKFold(n_splits=K, random_state=SEED, shuffle=True)</code></pre>
<div id="xgboost" class="section level3">
<h3>XGBoost</h3>
<p>Obter <em>out-of-fold</em> SHAP do modelo XGBoost tunado:</p>
<pre class="python"><code>shap1_oof = np.zeros((X.shape[0], X.shape[1]))
shap1_test = np.zeros((X_test.shape[0], X_test.shape[1]))
model_shap1_oof = np.zeros(X.shape[0])

for fold, (train_idx, val_idx) in enumerate(kf.split(X=X, y=y)):
    print(f&quot;‚ûú FOLD :{fold}&quot;)
    X_train = X.iloc[train_idx]
    y_train = y.iloc[train_idx]
    X_val = X.iloc[val_idx]
    y_val = y.iloc[val_idx]
    
    start = time.time()
    
    model = XGBClassifier(**final_params_xgb[&#39;clf&#39;])
    
    model.fit(X_train, y_train,
              eval_set=[(X_val, y_val)],
              early_stopping_rounds=150,
              verbose=False)
    
    model_shap1_oof[val_idx] += model.predict_proba(X_val)[:,1]
    
    print(&quot;Final F1     :&quot;, custom_f1(y_val, model_shap1_oof[val_idx]))
    print(&quot;Final AUC    :&quot;, roc_auc_score(y_val, model_shap1_oof[val_idx]))
    print(&quot;Final LogLoss:&quot;, log_loss(y_val, model_shap1_oof[val_idx]))

    explainer = shap.TreeExplainer(model)
    shap1_oof[val_idx] = explainer.shap_values(X_val)
    shap1_test += explainer.shap_values(X_test) / K

    print(f&quot;elapsed: {time.time()-start:.2f} sec\n&quot;)
    
shap1_oof = pd.DataFrame(shap1_oof, columns = [x+&quot;_shap1&quot; for x in X.columns])
shap1_test = pd.DataFrame(shap1_test, columns = [x+&quot;_shap1&quot; for x in X_test.columns])

print(&quot;Final F1     :&quot;, custom_f1(y, model_shap1_oof))
print(&quot;Final AUC    :&quot;, roc_auc_score(y, model_shap1_oof))
print(&quot;Final LogLoss:&quot;, log_loss(y, model_shap1_oof))</code></pre>
<pre><code>## ‚ûú FOLD :0
## Final F1     : 0.7032967032967034
## Final AUC    : 0.902330627099664
## Final LogLoss: 0.2953604946129216
## elapsed: 62.58 sec
## 
## ‚ûú FOLD :1
## Final F1     : 0.6193853427895981
## Final AUC    : 0.8613101903695408
## Final LogLoss: 0.34227429854659686
## elapsed: 45.96 sec
## 
## ‚ûú FOLD :2
## Final F1     : 0.6793478260869567
## Final AUC    : 0.8945898656215007
## Final LogLoss: 0.3085819148842589
## elapsed: 58.84 sec
## 
## ‚ûú FOLD :3
## Final F1     : 0.7073791348600509
## Final AUC    : 0.9058020716685331
## Final LogLoss: 0.2881665477053405
## elapsed: 62.24 sec
## 
## ‚ûú FOLD :4
## Final F1     : 0.7239583333333334
## Final AUC    : 0.9053121500559911
## Final LogLoss: 0.29320601468396107
## elapsed: 93.74 sec
## 
## ‚ûú FOLD :5
## Final F1     : 0.7009803921568627
## Final AUC    : 0.9076567749160134
## Final LogLoss: 0.2872539995859452
## elapsed: 73.34 sec
## 
## ‚ûú FOLD :6
## Final F1     : 0.6736292428198434
## Final AUC    : 0.8822788353863381
## Final LogLoss: 0.320014158050091
## elapsed: 55.16 sec
## 
## ‚ûú FOLD :7
## Final F1     : 0.7135416666666666
## Final AUC    : 0.9016657334826428
## Final LogLoss: 0.29617989833438774
## elapsed: 74.49 sec
## 
## ‚ûú FOLD :8
## Final F1     : 0.7135135135135134
## Final AUC    : 0.8893825776158104
## Final LogLoss: 0.29351621553572266
## elapsed: 93.71 sec
## 
## ‚ûú FOLD :9
## Final F1     : 0.7391304347826086
## Final AUC    : 0.9064054944284814
## Final LogLoss: 0.28033187155768635
## elapsed: 95.65 sec
## 
## ‚ûú FOLD :10
## Final F1     : 0.684863523573201
## Final AUC    : 0.9031046324199313
## Final LogLoss: 0.29823173886367804
## elapsed: 64.70 sec
## 
## ‚ûú FOLD :11
## Final F1     : 0.704225352112676
## Final AUC    : 0.8882052000840984
## Final LogLoss: 0.30525241732057884
## elapsed: 50.06 sec
## 
## ‚ûú FOLD :12
## Final F1     : 0.6666666666666666
## Final AUC    : 0.8905529469479291
## Final LogLoss: 0.313654842143217
## elapsed: 78.45 sec
## 
## ‚ûú FOLD :13
## Final F1     : 0.6500000000000001
## Final AUC    : 0.8745111780783517
## Final LogLoss: 0.3300786509821235
## elapsed: 59.54 sec
## 
## ‚ûú FOLD :14
## Final F1     : 0.7135416666666666
## Final AUC    : 0.9063284042329526
## Final LogLoss: 0.29314716930177404
## elapsed: 70.28 sec
## 
## Final F1     : 0.6822461331540014
## Final AUC    : 0.8945288307257988
## Final LogLoss: 0.30301717097927483</code></pre>
</div>
<div id="catboost" class="section level3">
<h3>CatBoost</h3>
<p>Obter <em>out-of-fold</em> SHAP do modelo CatBoost + features extrat√≠das via KNN:</p>
<pre class="python"><code>X = pd.concat([X, knn_feat_train], axis=1)
X_test = pd.concat([X_test, knn_feat_test], axis=1)</code></pre>
<pre class="python"><code>shap2_oof = np.zeros((X.shape[0], X.shape[1]))
shap2_test = np.zeros((X_test.shape[0], X_test.shape[1]))
model_shap2_oof = np.zeros(X.shape[0])

for fold, (train_idx, val_idx) in enumerate(kf.split(X=X, y=y)):
    print(f&quot;‚ûú FOLD :{fold}&quot;)
    X_train = X.iloc[train_idx]
    y_train = y.iloc[train_idx]
    X_val = X.iloc[val_idx]
    y_val = y.iloc[val_idx]
    
    start = time.time()
    
    model = CatBoostClassifier(random_seed=SEED,
                               verbose = 0,
                               n_estimators=10000,
                               loss_function= &#39;Logloss&#39;,
                               use_best_model=True,
                               eval_metric= &#39;Logloss&#39;)
    
    model.fit(X_train, y_train, 
              eval_set = [(X_val,y_val)], 
              early_stopping_rounds = 100,
              verbose = False)
    
    model_shap2_oof[val_idx] += model.predict_proba(X_val)[:,1]
    
    print(&quot;Final F1     :&quot;, custom_f1(y_val, model_shap2_oof[val_idx]))
    print(&quot;Final AUC    :&quot;, roc_auc_score(y_val, model_shap2_oof[val_idx]))
    print(&quot;Final LogLoss:&quot;, log_loss(y_val, model_shap2_oof[val_idx]))

    explainer = shap.TreeExplainer(model)
    shap2_oof[val_idx] = explainer.shap_values(X_val)
    shap2_test += explainer.shap_values(X_test) / K

    print(f&quot;elapsed: {time.time()-start:.2f} sec\n&quot;)
    
shap2_oof = pd.DataFrame(shap2_oof, columns = [x+&quot;_shap&quot; for x in X.columns])
shap2_test = pd.DataFrame(shap2_test, columns = [x+&quot;_shap&quot; for x in X_test.columns])

print(&quot;Final F1     :&quot;, custom_f1(y, model_shap2_oof))
print(&quot;Final AUC    :&quot;, roc_auc_score(y, model_shap2_oof))
print(&quot;Final LogLoss:&quot;, log_loss(y, model_shap2_oof))</code></pre>
<pre><code>## ‚ûú FOLD :0
## Final F1     : 0.6972010178117048
## Final AUC    : 0.8954157334826428
## Final LogLoss: 0.29952314366911725
## elapsed: 22.84 sec
## 
## ‚ûú FOLD :1
## Final F1     : 0.6348448687350835
## Final AUC    : 0.8628429451287795
## Final LogLoss: 0.3407490151943705
## elapsed: 12.59 sec
## 
## ‚ûú FOLD :2
## Final F1     : 0.6809651474530831
## Final AUC    : 0.8949538073908175
## Final LogLoss: 0.3066089330852162
## elapsed: 18.03 sec
## 
## ‚ûú FOLD :3
## Final F1     : 0.702247191011236
## Final AUC    : 0.9107992721164613
## Final LogLoss: 0.2877216893570601
## elapsed: 15.66 sec
## 
## ‚ûú FOLD :4
## Final F1     : 0.7131367292225201
## Final AUC    : 0.9018687010078387
## Final LogLoss: 0.2976481761596595
## elapsed: 29.35 sec
## 
## ‚ûú FOLD :5
## Final F1     : 0.7055837563451777
## Final AUC    : 0.909231522956327
## Final LogLoss: 0.28834373773423566
## elapsed: 15.35 sec
## 
## ‚ûú FOLD :6
## Final F1     : 0.6631578947368421
## Final AUC    : 0.8796402575587906
## Final LogLoss: 0.32303153676573987
## elapsed: 19.13 sec
## 
## ‚ûú FOLD :7
## Final F1     : 0.6997389033942559
## Final AUC    : 0.901637737961926
## Final LogLoss: 0.2985978485411335
## elapsed: 23.30 sec
## 
## ‚ûú FOLD :8
## Final F1     : 0.6965699208443271
## Final AUC    : 0.8825565912117177
## Final LogLoss: 0.3009859242847037
## elapsed: 20.19 sec
## 
## ‚ûú FOLD :9
## Final F1     : 0.7435897435897436
## Final AUC    : 0.9042469689536757
## Final LogLoss: 0.28276851015512977
## elapsed: 24.39 sec
## 
## ‚ûú FOLD :10
## Final F1     : 0.6767676767676767
## Final AUC    : 0.902712173242694
## Final LogLoss: 0.29999812838692497
## elapsed: 16.14 sec
## 
## ‚ûú FOLD :11
## Final F1     : 0.7013698630136986
## Final AUC    : 0.8865022075828719
## Final LogLoss: 0.3081393413008847
## elapsed: 13.50 sec
## 
## ‚ûú FOLD :12
## Final F1     : 0.6630434782608696
## Final AUC    : 0.8920456934613498
## Final LogLoss: 0.31338640296724246
## elapsed: 24.48 sec
## 
## ‚ûú FOLD :13
## Final F1     : 0.6485148514851485
## Final AUC    : 0.8689887167986544
## Final LogLoss: 0.3369797070301582
## elapsed: 17.17 sec
## 
## ‚ûú FOLD :14
## Final F1     : 0.7108753315649867
## Final AUC    : 0.8994743850304856
## Final LogLoss: 0.301420230674656
## elapsed: 16.51 sec
## 
## Final F1     : 0.6823234134098244
## Final AUC    : 0.892656043550729
## Final LogLoss: 0.305726567456891</code></pre>
<pre class="python"><code>train = pd.concat([train, shap1_oof], axis=1)
test = pd.concat([test, shap1_test], axis=1)

train = pd.concat([train, shap2_oof], axis=1)
test = pd.concat([test, shap2_test], axis=1)</code></pre>
</div>
</div>
<div id="stage-3-modelo-final-com-autogluon" class="section level2">
<h2>Stage 3: Modelo Final com AutoGluon</h2>
<p>AutoGluon √© um <a href="https://github.com/awslabs/autogluon">AutoML desenvolvido pela Amazon</a> muito f√°cil de utilizar (no melhor estilo <code>sklearn</code> com m√©todos <code>.fit()</code> e <code>.predict()</code>).</p>
<p>Principais Informa√ß√µes üìå :</p>
<ul>
<li>Inputs: Dataset original + knn features + Shapt values do XGBoost tunado e do CatBoost;</li>
<li>Loss do XGBoost: Log Loss;</li>
<li>Loss do CatBoost: AUC;</li>
<li>Loss do AutoGluon: Log Loss;</li>
<li>Tempo de processamento: 7h30m</li>
</ul>
<div class="w3-panel w3-pale-green w3-border">
<p><strong>üí° Insight</strong> <br></p>
<p>Um recurso muito √∫til do AutoGluon √© poder acessar as previs√µes out-of-folds, o que facilita no c√°lculo do <em>threshold</em> que maximiza a <em>F1 Score</em>.</p>
</div>
<pre class="python"><code>predictor = TabularPredictor(label=&quot;y&quot;,
                             problem_type=&#39;binary&#39;,
                             eval_metric=&quot;log_loss&quot;,
                             path=&#39;./AutoGlon/&#39;,
                             verbosity=1)

predictor.fit(train, presets=&#39;best_quality&#39;, time_limit=60*60*7.5) 

results = predictor.fit_summary()</code></pre>
<pre><code>## *** Summary of fit() ***
## Estimated performance of each model:
##                       model  score_val  pred_time_val      fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order
## 0       WeightedEnsemble_L2  -0.299310      30.410467   8888.826963                0.001654           2.456810            2       True         14
## 1           CatBoost_BAG_L1  -0.301038       3.051793   2376.887100                3.051793        2376.887100            1       True          7
## 2       WeightedEnsemble_L3  -0.301722     194.034947  22907.669139                0.001541           2.008858            3       True         26
## 3         LightGBMXT_BAG_L2  -0.302135     131.534432  17201.299530                1.378576         389.400378            2       True         15
## 4         LightGBMXT_BAG_L1  -0.302562       3.570399    969.385833                3.570399         969.385833            1       True          3
## 5           CatBoost_BAG_L2  -0.302646     131.912474  17619.939451                1.756617         808.040299            2       True         19
## 6           LightGBM_BAG_L2  -0.303002     131.422007  17281.518763                1.266150         469.619612            2       True         16
## 7           LightGBM_BAG_L1  -0.303264       2.964433   1038.037160                2.964433        1038.037160            1       True          4
## 8            XGBoost_BAG_L1  -0.303471       4.475003   2036.551052                4.475003        2036.551052            1       True         11
## 9    NeuralNetFastAI_BAG_L1  -0.304455      19.917584   3434.894841               19.917584        3434.894841            1       True         10
## 10           XGBoost_BAG_L2  -0.304499     132.757505  17834.135370                2.601648        1022.236218            2       True         23
## 11   NeuralNetFastAI_BAG_L2  -0.306339     142.018741  18777.287244               11.862885        1965.388093            2       True         22
## 12     LightGBMLarge_BAG_L2  -0.306606     131.701429  18260.504603                1.545573        1448.605452            2       True         25
## 13    NeuralNetMXNet_BAG_L2  -0.308237     177.769179  19273.211899               47.613322        2461.312748            2       True         24
## 14     LightGBMLarge_BAG_L1  -0.309686       3.042399   2629.185346                3.042399        2629.185346            1       True         13
## 15    ExtraTreesEntr_BAG_L2  -0.314045     132.017535  16815.886061                1.861679           3.986910            2       True         21
## 16  RandomForestEntr_BAG_L2  -0.314454     132.061970  16843.769642                1.906114          31.870490            2       True         18
## 17    ExtraTreesGini_BAG_L2  -0.314960     132.123651  16816.087081                1.967794           4.187930            2       True         20
## 18    NeuralNetMXNet_BAG_L1  -0.317156      81.677096   4258.886806               81.677096        4258.886806            1       True         12
## 19  RandomForestGini_BAG_L2  -0.321702     132.035970  16835.326491                1.880114          23.427339            2       True         17
## 20    ExtraTreesEntr_BAG_L1  -0.323283       1.794093      4.051307                1.794093           4.051307            1       True          9
## 21  RandomForestEntr_BAG_L1  -0.324296       1.966043     33.380685                1.966043          33.380685            1       True          6
## 22    ExtraTreesGini_BAG_L1  -0.325897       1.796291      3.748723                1.796291           3.748723            1       True          8
## 23  RandomForestGini_BAG_L1  -0.328218       1.778995     22.705248                1.778995          22.705248            1       True          5
## 24    KNeighborsDist_BAG_L1  -1.070156       2.010938      2.075571                2.010938           2.075571            1       True          2
## 25    KNeighborsUnif_BAG_L1  -1.071373       2.110790      2.109480                2.110790           2.109480            1       True          1
## Number of models trained: 26
## Types of models trained:
## {&#39;StackerEnsembleModel_RF&#39;, &#39;StackerEnsembleModel_NNFastAiTabular&#39;, &#39;WeightedEnsembleModel&#39;, &#39;StackerEnsembleModel_XGBoost&#39;, &#39;StackerEnsembleModel_CatBoost&#39;, &#39;StackerEnsembleModel_KNN&#39;, &#39;StackerEnsembleModel_LGB&#39;, &#39;StackerEnsembleModel_XT&#39;, &#39;StackerEnsembleModel_TabularNeuralNet&#39;}
## Bagging used: True  (with 10 folds)
## Multi-layer stack-ensembling used: True  (with 3 levels)
## Feature Metadata (Processed):
## (raw dtype, special dtypes):
## (&#39;float&#39;, [])     : 152 | [&#39;var55&#39;, &#39;var56&#39;, &#39;var57&#39;, &#39;var58&#39;, &#39;var59&#39;, ...]
## (&#39;int&#39;, [])       :  48 | [&#39;var1&#39;, &#39;var2&#39;, &#39;var3&#39;, &#39;var4&#39;, &#39;var5&#39;, ...]
## (&#39;int&#39;, [&#39;bool&#39;]) :   6 | [&#39;var27&#39;, &#39;var31&#39;, &#39;var44&#39;, &#39;var49&#39;, &#39;var50&#39;, ...]
## Plot summary of models saved to file: ./AutoGlon/SummaryOfModels.html
## *** End of fit() summary ***</code></pre>
<p>Nota: Os resultados podem variar devido √† natureza estoc√°stica do algoritmo ou procedimento de avalia√ß√£o.</p>
<pre class="python"><code># get final predictions
y_oof = predictor.get_oof_pred_proba().iloc[:,1]
y_pred = predictor.predict_proba(test).iloc[:,1]</code></pre>
<pre class="python"><code>final_threshold = get_threshold(train.y, y_oof)
final_threshold</code></pre>
<pre><code>## 0.31</code></pre>
<pre class="python"><code>print(&quot;Final F1     :&quot;, custom_f1(y, y_oof))
print(&quot;Final AUC    :&quot;, roc_auc_score(y, y_oof))
print(&quot;Final LogLoss:&quot;, log_loss(y, y_oof))</code></pre>
<pre><code>## Final F1     : 0.6846193682030037
## Final AUC    : 0.8961328807692966
## Final LogLoss: 0.2993098559321765</code></pre>
<p>Ap√≥s submiss√£o:</p>
<center>
<img src="/post/2021-11-01-solucao-final-porto-seguro-data-challenge/final_sub.png" style="width:90.0%" />
</center>
</div>
</div>
<div id="conclus√£o" class="section level1">
<h1>Conclus√£o</h1>
<p>Gostaria de agradecer imensamente ao time do Porto Seguro pela iniciativa, pois esse tipo de competi√ß√£o (t√£o detalhada e desafiadora) n√£o tem sido muito comum no Brasil e √© muito importante para fomentar a comunidade brasileira de ci√™ncia de dados!</p>
<p>Sabemos que o ‚Äúmundo real‚Äù √© diferente do mundo das competi√ß√µes (onde buscamos o melhor score a todo custo) por√©m, na minha vis√£o, n√£o deixa de ser um √≥timo exerc√≠cio para treinar o racioc√≠nio anal√≠tico.. al√©m de ser muito empolgante e divertido!</p>
<p>Tive o enorme prazer de trocar id√©ias e conhecer pessoas fora da curva bem como me tornar f√£ de alguns competidores! A cada semana q passava o n√≠vel estava cada vez mais alto!</p>
<p>Com certeza este pipeline poderia ser muito melhor, sinto que poderia ter gasto mais tempo com <em>feature engineering</em> e tido mais paciencia com alguns modelos. Tentei fazer o melhor que pude com o tempo dispon√≠vel e me sinto muito grato pela experi√™ncia de apresentar os resultados e aprender bastante com a solu√ß√£o dos top colocados.</p>
<p>N√£o acaba por aqui! Agora √© hora de voltar aos estudos, continuar praticando com as <a href="https://www.kaggle.com/c/tabular-playground-series-nov-2021/overview">TPS‚Äôs do Kaggle</a> e, quem sabe, ir melhor na pr√≥xima!</p>
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias</h1>
<ul>
<li><a href="https://github.com/momijiame/gokinjo" class="uri">https://github.com/momijiame/gokinjo</a></li>
<li><a href="https://www.kaggle.com/melanie7744/tps6-boost-your-score-with-knn-features" class="uri">https://www.kaggle.com/melanie7744/tps6-boost-your-score-with-knn-features</a></li>
<li><a href="https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335" class="uri">https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335</a></li>
<li><a href="https://www.kaggle.com/pavelvod/gbm-supervised-pretraining" class="uri">https://www.kaggle.com/pavelvod/gbm-supervised-pretraining</a></li>
</ul>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2021-11-01-solucao-final-porto-seguro-data-challenge/">Solu√ß√£o Final - Porto Seguro Data Challenge [3¬∫ lugar]</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">catboost</category>
      <category domain="tag">data-science</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">knn</category>
      <category domain="tag">machine-learning</category>
      <category domain="tag">optuna</category>
      <category domain="tag">pratica</category>
      <category domain="tag">python</category>
      <category domain="tag">shap</category>
      <category domain="tag">threshold-movel</category>
    </item>
    <item>
      <title>Otimizando pipelines que envolvem dados desbalanceados</title>
      <link>https://gomesfellipe.github.io/post/2021-06-28-imbalanced-workflowsets/</link>
      <pubDate>Mon, 28 Jun 2021 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2021-06-28-imbalanced-workflowsets/</guid>
      <description>Utilizaremos o framework tidymodels para machine learning em R com o aux√≠lio do pacote workflowsets para otimizar pipelines de dados desbalanceados</description>
      <content:encoded>&lt;![CDATA[
        

<div id="TOC">
<ul>
<li><a href="#o-problema-envolvendo-dados-desbalanceados" id="toc-o-problema-envolvendo-dados-desbalanceados">O problema envolvendo dados desbalanceados</a></li>
<li><a href="#objetivo" id="toc-objetivo">Objetivo</a></li>
<li><a href="#depend%C3%AAncias" id="toc-depend√™ncias">Depend√™ncias</a></li>
<li><a href="#preparar-dados" id="toc-preparar-dados">Preparar dados</a></li>
<li><a href="#breve-an%C3%A1lise-explorat%C3%B3ria" id="toc-breve-an√°lise-explorat√≥ria">Breve an√°lise explorat√≥ria</a></li>
<li><a href="#modelagem" id="toc-modelagem">Modelagem</a>
<ul>
<li><a href="#baselines" id="toc-baselines">Baselines</a></li>
<li><a href="#preparar-pipeline-de-dados-com-workflowsets" id="toc-preparar-pipeline-de-dados-com-workflowsets">Preparar Pipeline de dados com <code>workflowsets</code></a></li>
<li><a href="#benchmark" id="toc-benchmark">Benchmark</a></li>
</ul></li>
<li><a href="#conclus%C3%A3o" id="toc-conclus√£o">Conclus√£o</a></li>
<li><a href="#refer%C3%AAncias" id="toc-refer√™ncias">Refer√™ncias</a></li>
</ul>
</div>

<style>
.column {
float: left;
width: 50%;
padding: 10px;
}

.column4 {
float: left;
width: 33%;
padding: 10px;
}

.column8 {
float: left;
width: 66%;
padding: 10px;
}

.row:after {
content: "";
display: table;
clear: both;
}

.center {
display: flex;
justify-content: center;
align-items: center;
height: 200px;
}
</style>
<div id="o-problema-envolvendo-dados-desbalanceados" class="section level1">
<h1>O problema envolvendo dados desbalanceados</h1>
<p>A tarefa de classifica√ß√£o com dados desbalanceados √© muito comum na vida real podendo variar desde um leve vi√©s at√© um enorme desequil√≠brio na distribui√ß√£o da classe de interesse. Problemas mais comuns envolvem:</p>
<ul>
<li>Detec√ß√£o de fraude;</li>
<li>Previs√£o de inadimpl√™ncia;</li>
<li>Identificador de <em>spam</em>;</li>
<li>Busca por anomalias/outliers;</li>
<li>Detec√ß√£o de poss√≠veis roubos/furtos/vulnerabilidades;</li>
<li>Previs√£o de <em>churn</em>;</li>
<li>etc</li>
</ul>
<div class="row">
<div class="column8">
<p>Este tipo de tarefa representa um enorme desafio para modelagem preditiva pois a maioria dos algoritmos de machine learning foram projetados sob suposi√ß√£o de haver um n√∫mero igual de exemplos para cada classe de interesse.</p>
<p>E isso √© um grande problema pois normalmente estamos interessados em prever a classe minorit√°ria e para isso √© preciso tomar uma s√©rie de decis√µes, como por exemplo: m√©trica utilizada, m√©todo para valida√ß√£o cruzada, ado√ß√£o (ou n√£o) do uso de m√©todos de reamostragem, quais algoritmos utilizar, qual ser√° o threshold, etc</p>
</div>
<div class="column4">
<p></br>
<img src="https://media.giphy.com/media/JPV8lNtI59zaWyL4pf/giphy.gif" alt="Via Giphy" /></p>
</div>
</div>
<p>Lidar com dados desbalanceados √© um assunto longo portanto tentarei dar mais aten√ß√£o apenas em um <em>hack</em> para encontrar a melhor forma de se aplicar o balanceamento dos dados. N√£o pretendo me aprofundar na teoria envolvida na escolha das m√©tricas neste post, caso o leitor deseje se aprofundar sobre a teoria envolvida com classifica√ß√£o que envolve dados desbalanceados, sugiro a leitura do livro: <a href="https://machinelearningmastery.com/imbalanced-classification-with-python/">Imbalanced Classification with Python - Choose Better Metrics, Balance Skewed Classes and Apply Cost-Sensitive Learning</a> e consultar os links de refer√™ncia no final do post).</p>
</div>
<div id="objetivo" class="section level1">
<h1>Objetivo</h1>
<p>Utilizaremos neste post o pacote <code>workflowsets</code> a fim de otimizar o pipeline de reamostragem da base para lidar com o desbalanceamento dos dados.</p>
<p>Para efeitos de compara√ß√£o, utilizarei como refer√™ncia o (excelente) <a href="https://juliasilge.com/blog/sliced-aircraft/">post escrito recentemente pela Julia Silge</a> em seu blog que tamb√©m aborda o problema de dados desbalanceados utilizando um conjunto de dados de uma <a href="https://www.kaggle.com/c/sliced-s01e02-xunyc5">competi√ß√£o do Kaggle</a>. Utilizarei a mesma configura√ß√£o de pr√©-processamento adotado em seu post para que a compara√ß√£o seja justa.</p>
<p>Portanto, nosso objetivo de modelagem ser√° prever se uma colis√£o com animais selvagens resultou em danos a aeronave.</p>
<div class="w3-panel w3-pale-green w3-border">
<p>‚ö†Ô∏è Este dataset √© rico em possibilidades para diferentes tipos de pr√© processamentos e por isso convido o leitor a analis√°-lo com maior profundidade e tamb√©m a compartilhar seus resultados!</p>
</div>
</div>
<div id="depend√™ncias" class="section level1">
<h1>Depend√™ncias</h1>
<p>Primeiro vamos carregar as bibliotecas necess√°rias e algumas fun√ß√µes desenvolvidas para o post</p>
<pre class="r"><code>library(tidyverse)    # ds toolkit
library(tidymodels)   # ml toolkit
library(baguette)     # bag_tree
library(themis)       # imbalanced
library(workflowsets) # opt pipelines
library(patchwork)    # arrange plots 

doParallel::registerDoParallel()
theme_set(theme_bw())</code></pre>
<details>
<summary>
(<em>Clique aqui para ver as fun√ß√µes</em> <code>print_table</code> <em>e</em> <code>conf_mat_plot</code> <em>importadas</em>)
</summary>
<pre class="r"><code># Para o print de tabelas
print_table &lt;- function(x, round=0, cv=F, wf=F, bm=F, ...){ 
  
  if(round&gt;0) x &lt;- x %&gt;% mutate_if(is.numeric, ~round(.x, round))
  
  if(cv==T){
    columns_spec = list(
      .metric = reactable::colDef(minWidth = 75),
      .estimator = reactable::colDef(minWidth = 70),
      .config = reactable::colDef(minWidth = 120)
    )
  } else if(wf==T){
    columns_spec = list(
      wflow_id = reactable::colDef(minWidth = 100),
      .metric = reactable::colDef(minWidth = 100),
      preprocessor = reactable::colDef(minWidth = 110),
      rank = reactable::colDef(minWidth = 50),
      n = reactable::colDef(minWidth = 50)
    )
  }else if (bm==T){
    columns_spec = list(
      wflow_id = reactable::colDef(minWidth = 130),
      model = reactable::colDef(minWidth = 80)
    )
  }else{
    columns_spec = NULL
  }
  
  reactable::reactable(x, striped = T, bordered = T,
                       highlight = T, pagination = F, resizable = T, 
                       columns = columns_spec, ...)
  
}

# Para plot da matriz de confusao e distribuicoes de probabilidade
conf_mat_plot &lt;- function(x, null_model = FALSE){
  p1 &lt;- 
    x %&gt;%
    select(.pred_class, damaged) %&gt;%
    table() %&gt;% 
    conf_mat() %&gt;% 
    autoplot(type = &quot;heatmap&quot;)+
    labs(title = &quot;Matriz de confus√£o&quot;)
  
  p2 &lt;- 
    x  %&gt;%
    ggplot() +
    geom_density(aes(x = .pred_damage, fill = damaged), 
                 alpha = 0.5)+
    labs(title = &quot;Distribui√ß√µes de probabilidade previstas&quot;,
         subtitle = &quot;por classe&quot;)+ 
    scale_x_continuous(limits = 0:1)+
    scale_fill_brewer(palette=&quot;Set1&quot;)
  
  p1 | p2
} </code></pre>
</details>
<p>¬†</p>
<p>Em seguida vamos importar os dados provenientes da competi√ß√£o Inclass do Kaggle <a href="https://www.kaggle.com/c/sliced-s01e02-xunyc5">SLICED s01e02 - Predict whether an aircraft strike with wildlife causes damage</a>. Para mais informa√ß√µes consulte a <a href="https://www.kaggle.com/c/sliced-s01e02-xunyc5/data">documenta√ß√£o e dicion√°rio dos dados</a>.</p>
<pre class="r"><code>df &lt;- read_csv(&quot;train.csv&quot;)</code></pre>
<p>Note que carregamos apenas os dados de treino pois os dados de teste n√£o possuem a target.</p>
</div>
<div id="preparar-dados" class="section level1">
<h1>Preparar dados</h1>
<p>Tratar a vari√°vel target <code>damaged</code> e avaliar sua distribui√ß√£o:</p>
<pre class="r"><code>df &lt;- df %&gt;% 
  mutate(damaged = if_else(damaged==1, &quot;damage&quot;, &quot;not_damage&quot;) %&gt;% 
           factor(levels = c(&quot;damage&quot;, &quot;not_damage&quot;)))</code></pre>
<details>
<summary>
(<em>Clique aqui para ver o c√≥digo do gr√°fico abaixo</em>)
</summary>
<pre class="r"><code>p1 &lt;- df %&gt;% 
  count(damaged) %&gt;% 
  ggplot(aes(x=rev(damaged), y=n, fill=damaged))+
  geom_bar(stat = &quot;identity&quot;)+
  scale_fill_brewer(palette=&quot;Set1&quot;)+
  theme(legend.position = &quot;bottom&quot;)+
  labs(y=&quot;N√∫mero de inst√¢ncias&quot;, x = &quot;&quot;)

p2 &lt;- df %&gt;% 
  count(damaged) %&gt;% 
  arrange(desc(damaged)) %&gt;%
  mutate(prop = n / sum(n)) %&gt;%
  mutate(ypos = cumsum(prop)- 0.5*prop )%&gt;% 
  ggplot(aes(x=&quot;&quot;, y=prop, fill=damaged)) +
  geom_bar(stat=&quot;identity&quot;, width=1) +
  coord_polar(&quot;y&quot;, start=0) +
  theme_void() + 
  theme(legend.position=&quot;none&quot;) +
  geom_text(aes(y = ypos,
                label = paste(scales::comma(n, big.mark = &quot;.&quot;),
                              scales::comma(n/sum(n), big.mark = &quot;.&quot;, 
                                            suffix = &quot;%&quot; ),sep = &quot;\n&quot;)
                
  ), 
  color = &quot;white&quot;, size=6) +
  scale_fill_brewer(palette=&quot;Set1&quot;)</code></pre>
</details>
<p>¬†</p>
<pre class="r"><code>p1 + p2 </code></pre>
<center>
<img src="/post/2021-06-28-imbalanced-workflowsets/unnamed-chunk-6-1.png" style="width:80.0%" />
</center>
<p>Veja que estamos diante de um problema que existem aproximadamente 9 casos de dano para cada 100 eventos observados.</p>
</div>
<div id="breve-an√°lise-explorat√≥ria" class="section level1">
<h1>Breve an√°lise explorat√≥ria</h1>
<p>Vamos iniciar a explorat√≥ria com uma avalia√ß√£o geral dos dados brutos</p>
<pre class="r"><code>DataExplorer::plot_intro(df, ggtheme = theme_bw(), 
                         theme_config = list(legend.position = &quot;bottom&quot;))</code></pre>
<center>
<img src="/post/2021-06-28-imbalanced-workflowsets/unnamed-chunk-7-1.png" style="width:80.0%" />
</center>
<p>Primeira informa√ß√£o que chama aten√ß√£o √© que quase 1/4 desses dados √© faltante. Vamos olhar a estrutura dessa base de maneira mais aprofundada:</p>
<pre class="r"><code>df %&gt;% 
  sample_frac(0.01) %&gt;% 
  visdat::vis_dat()</code></pre>
<center>
<img src="/post/2021-06-28-imbalanced-workflowsets/unnamed-chunk-8-1.png" style="width:80.0%" />
</center>
<p>Parece existir algum padr√£o nos dados faltantes (que coocorrem em diveros atributos). Al√©m disso algumas colunas est√£o quase inteiramente vazias e ser√£o descartadas no processo de modelagem.</p>
<p>Uma vis√£o geral das classes das features categ√≥ricas:</p>
<pre class="r"><code>df %&gt;%
  select(-damaged, -id)%&gt;%
  mutate_all(as.factor) %&gt;%
  inspectdf::inspect_cat() %&gt;% 
  inspectdf::show_plot()</code></pre>
<center>
<img src="/post/2021-06-28-imbalanced-workflowsets/unnamed-chunk-9-1.png" style="width:80.0%" />
</center>
<p>Algumas features possuem muitas classes e caso seja feita a transforma√ß√£o <em>one-hot-encoding</em> (estrat√©gia amplamente utilizada para lidar com features categ√≥ricas) sem algum cuidado, o desempenho da maioria dos modelos de machine learning pode ser prejudicado por tornar a base anal√≠tica muito esparsa.</p>
<p>Uma vis√£o geral das classes das features num√©ricas em rela√ß√£o a target:</p>
<pre class="r"><code>num_columns &lt;- c(df %&gt;% select_if(is.numeric) %&gt;% colnames(), &#39;damaged&#39;)
df%&gt;% 
  select_at(num_columns) %&gt;% 
  select(-id) %&gt;%
  gather(key, value, -damaged) %&gt;%
  ggplot(aes(y=damaged, x=value))+
  geom_boxplot()+
  facet_wrap(~key, ncol=5, scales = &quot;free_x&quot;)+
  labs(x = &quot;&quot;, y=&quot;&quot;)</code></pre>
<center>
<img src="/post/2021-06-28-imbalanced-workflowsets/unnamed-chunk-10-1.png" style="width:80.0%" />
</center>
<p>Parece que algumas features possuem comportamentos diferentes quando avaliados segundo a target. Al√©m disso √© poss√≠vel notar que as features <code>aircraft_mass</code>, <code>distance</code>, <code>engine4_position</code>, <code>engines</code>, <code>height</code> e <code>speed</code> apresentam outliers.</p>
</div>
<div id="modelagem" class="section level1">
<h1>Modelagem</h1>
<p>Finalmente chegamos a modelagem!</p>
<p>Primeiro vamos definir um esquema de reamostragem (com estratifica√ß√£o) que ser√° utilizado para avaliar os modelos e as m√©tricas de qualidade.</p>
<pre class="r"><code>set.seed(123)

bird_folds &lt;- vfold_cv(df, v = 5, strata = damaged)
bird_metrics &lt;- metric_set(mn_log_loss, accuracy, sensitivity, specificity)</code></pre>
<p>Nossos conjuntos de pipelines necessitar√£o de um pr√©-processador base que ser√° comum a todos como camada inicial. Para isso utilizaremos o mesmo definido no post de refer√™ncia.</p>
<pre class="r"><code>base_rec &lt;- recipe(damaged ~ ., data = df) %&gt;%
  step_select( damaged, flight_impact, precipitation,
               visibility, flight_phase, engines, incident_year,
               incident_month, species_id, engine_type,
               aircraft_model, species_quantity, height, speed) %&gt;% 
  step_novel(all_nominal_predictors()) %&gt;%
  step_other(all_nominal_predictors(), threshold = 0.01) %&gt;%
  step_unknown(all_nominal_predictors()) %&gt;%
  step_impute_median(all_numeric_predictors()) %&gt;%
  step_zv(all_predictors())</code></pre>
<div id="baselines" class="section level2">
<h2>Baselines</h2>
<p>Para efeitos de compara√ß√£o, vamos ajustar 2 modelos que ser√£o utilizados como baselines para saber se a complexidade que estamos adicionando no modelo est√° realmente trazendo algum ganho na performance do modelo. Os modelos ser√£o:</p>
<ul>
<li>Modelo nulo: um modelo que sempre prev√™ a classe majorit√°ria;</li>
<li>Modelo de base: <a href="https://bradleyboehmke.github.io/HOML/bagging.html">Bagged Decision Tree</a> sem adicionar pr√©-processamento para compensar o desequil√≠brio de classe.</li>
</ul>
<div id="modelo-nulo" class="section level3">
<h3>Modelo nulo</h3>
<p>Avaliando modelo nulo via valida√ß√£o cruzada:</p>
<pre class="r"><code>null_spec &lt;- null_model(mode = &quot;classification&quot;) %&gt;% 
  set_engine(&quot;parsnip&quot;)

null_wf &lt;-
  workflow() %&gt;%
  add_recipe(base_rec) %&gt;%
  add_model(null_spec)

null_rs &lt;-
  fit_resamples(
    object = null_wf,
    resamples = bird_folds,
    metrics = bird_metrics,
    control = control_resamples(save_pred = TRUE)
  ) 

collect_metrics(null_rs) %&gt;% print_table(round = 5, cv = T) </code></pre>
<p><img src="/post/2021-06-28-imbalanced-workflowsets/tab1.png" /></p>
<p>Qualquer modelo com desempenho pior do que este deve ser descartado. Vejamos a matriz de confus√£o:</p>
<pre class="r"><code>collect_predictions(null_rs) %&gt;% 
  conf_mat_plot()</code></pre>
<center>
<img src="/post/2021-06-28-imbalanced-workflowsets/unnamed-chunk-14-1.png" style="width:80.0%" />
</center>
</div>
<div id="modelo-de-base" class="section level3">
<h3>Modelo de base</h3>
<p>Agora vamos ajusta o modelo <em>Bagged Decision Tree</em> sem o pr√©-processamento para compensar o desequil√≠brio de classe:</p>
<pre class="r"><code>bag_spec &lt;-
  bag_tree(min_n = 10) %&gt;%
  set_engine(&quot;rpart&quot;, times = 25) %&gt;%
  set_mode(&quot;classification&quot;)

imb_wf &lt;-
  workflow() %&gt;%
  add_recipe(base_rec) %&gt;%
  add_model(bag_spec)

set.seed(123)
imb_rs &lt;-
  fit_resamples(
    imb_wf,
    resamples = bird_folds,
    metrics = bird_metrics,
    control = control_resamples(save_pred = TRUE)
  )

collect_metrics(imb_rs) %&gt;% print_table(round = 5, cv = T)</code></pre>
<p><img src="/post/2021-06-28-imbalanced-workflowsets/tab2.png" /></p>
<p>Apesar do elevado n√∫mero de falsos negativos, este modelo j√° esta com um desempenho razo√°vel em compara√ß√£ao ao modelo nulo e o n√∫mero de verdadeiros positivos j√° √© quase o dobro do n√∫mero de falsos positivos. Veja na matriz de confus√£o abaixo:</p>
<pre class="r"><code>collect_predictions(imb_rs) %&gt;% 
  conf_mat_plot()</code></pre>
<center>
<img src="/post/2021-06-28-imbalanced-workflowsets/unnamed-chunk-16-1.png" style="width:80.0%" />
</center>
</div>
</div>
<div id="preparar-pipeline-de-dados-com-workflowsets" class="section level2">
<h2>Preparar Pipeline de dados com <code>workflowsets</code></h2>
<p>A escolha do m√©todo de amostragem dos dados √© t√£o importante quanto a escolha do modelo preditivo que ser√° utilizado pois o desempenho pode ser enganosamente otimista visto que o algoritmo de bagging n√£o esta usando nenhuma estrat√©gia de subamostragem aleat√≥ria da classe majorit√°ria em cada amostra de bootstrap para equilibrar as duas classes.</p>
<p>Existem muitos m√©todos para amostragem de dados e n√£o h√° um m√©todo √∫nico que seja melhor em todos os problemas de classifica√ß√£o (assim como n√£o existe o ‚Äúmelhor modelo‚Äù) portanto, utilizaremos este pacote para testar diferentes m√©todos e tamb√©m tunar seus hiperpar√¢metros.</p>
<div id="oversampling" class="section level3">
<h3>Oversampling</h3>
<p>Estes m√©todos duplicam ou sintetizam novos dados da classe minorit√°ria. Deve ser usado com cautela pois na vida real pode gerar alguns dados que n√£o condizem com a relidade ou criar tantas inst√¢ncias que acaba consumindo muito mais tempo de processamento.</p>
<div id="random-oversampling" class="section level4">
<h4>Random Oversampling</h4>
<p>Este m√©todo simplesmente duplica aleat√≥riamente exemplos da classe minorit√°ria. Vamos tunar esta propor√ß√£o buscando n√∫meros reais no intervalo [0.5,1].</p>
<pre class="r"><code>rec_up &lt;- base_rec %&gt;% 
  step_upsample(damaged, over_ratio = tune())

params_up &lt;- rec_up %&gt;% 
  parameters() %&gt;% update(over_ratio = mixture(c(0.5, 1)))</code></pre>
</div>
<div id="smote---synthetic-minority-oversampling-technique" class="section level4">
<h4>SMOTE - Synthetic Minority Oversampling Technique</h4>
<p>O SMOTE funciona gerando novos dados sint√©tios baseados em exemplos selecionando que est√£o ‚Äúpr√≥ximos‚Äù. Vamos tunar tanto a propor√ß√£o de dados que ser√£o gerados quanto a quantidade de vizinhos selecionados, buscando n√∫meros reais e inteiros no intervalo [0.5,1] e [1, 10], respectivamente.</p>
<pre class="r"><code>rec_smote &lt;- base_rec %&gt;%
  step_dummy(all_nominal_predictors()) %&gt;%
  step_smote(damaged, over_ratio = tune(), 
             neighbors = tune())

params_smote &lt;- rec_smote %&gt;% 
  parameters() %&gt;% update(over_ratio = mixture(c(0.5, 1)),
                          neighbors = neighbors())</code></pre>
</div>
<div id="adasyn---adaptive-synthetic-sampling" class="section level4">
<h4>ADASYN - Adaptive Synthetic Sampling</h4>
<p>O ADASYN √© uma extens√£o do SMOTE que busca propor melhorias. Vamos tunar os mesmos par√¢metros definidos no SMOTE.</p>
<pre class="r"><code>rec_adasyn &lt;- base_rec %&gt;%
  step_dummy(all_nominal_predictors()) %&gt;%
  step_adasyn(damaged, 
              over_ratio = tune(), 
              neighbors = tune())

params_adasyn &lt;- rec_adasyn %&gt;% 
  parameters() %&gt;% update(over_ratio = mixture(c(0.5, 1)),
                          neighbors = neighbors())</code></pre>
</div>
</div>
<div id="undersampling" class="section level3">
<h3>Undersampling</h3>
<p>S√£o t√©cnicas que excluem ou selecionam um subconjunto de exemplos da classe majorit√°ria e existem dezenas (se n√£o centenas) desses m√©todos. Neste post utilizaremos s√≥ 3 mas existem outros implementados em outras bibliotecas (em R e em Python).</p>
<div id="random-undersampling" class="section level4">
<h4>Random Undersampling</h4>
<p>Este √© o m√©todo mais simples e envolve a exclus√£o aleat√≥ria de algumas inst√¢ncias da classe majorit√°ria. Vamos tunar esta propor√ß√£o de frequ√™ncias da minorit√°ria para a majorit√°ria.</p>
<pre class="r"><code>rec_down &lt;- base_rec %&gt;% 
  step_downsample(damaged, under_ratio = tune())

params_down &lt;- rec_down %&gt;% 
  parameters() %&gt;% update(under_ratio = deg_free())</code></pre>
</div>
<div id="near-miss-undersampling" class="section level4">
<h4>Near Miss Undersampling</h4>
<p>Este algoritmo se baseia em m√©todos de KNN selecionando exemplos da classe majorit√°ria que tem menor dist√¢ncia m√©dia dos k exemplos mais pr√≥ximos. Vamos tunar tanto a propor√ß√£o quanto o n√∫mero de vizinhos utilizados.</p>
<pre class="r"><code>rec_nearmiss &lt;- base_rec %&gt;%
  step_dummy(all_nominal_predictors()) %&gt;%
  step_nearmiss(damaged, 
                under_ratio = tune(), 
                neighbors = tune())

params_nearmiss &lt;- rec_nearmiss %&gt;% 
  parameters() %&gt;% update(under_ratio = deg_free(),
                          neighbors = neighbors())</code></pre>
</div>
<div id="tomek-links-undersampling" class="section level4">
<h4>Tomek Links Undersampling</h4>
<p>Este algoritmo que tenta excluir inst√¢ncias que sejam pr√≥ximas e que possuam classes diferentes, buscando diminuir a ambiguidade dos dados. N√£o vamos tunar nenhum hiperpar√¢metro aqui.</p>
<pre class="r"><code>rec_tomek &lt;- base_rec %&gt;%
  step_dummy(all_nominal_predictors()) %&gt;%
  step_tomek(damaged)</code></pre>
</div>
</div>
<div id="preparar-pipeline-de-dados" class="section level3">
<h3>Preparar pipeline de dados</h3>
<p>Agora que todos pipelines de dados candidatos est√£o definidos, vamos combinar tudo em um √∫nico objeto com <code>workflow_set</code>:</p>
<pre class="r"><code>chi_models &lt;- 
  workflow_set(
    preproc = list(upsample = rec_up,
                   smote = rec_smote,
                   adasyn = rec_adasyn,
                   downsample = rec_down,
                   nearmiss = rec_nearmiss,
                   tomek = rec_tomek),
    models = list(bag_spec = bag_spec),
    cross = TRUE
  )</code></pre>
<p>Utilizar a fun√ß√£o <code>option_add</code> para adicionar as informa√ß√µes dos intervalos definidos para cada hiperpar√¢metro:</p>
<pre class="r"><code>chi_models &lt;- chi_models %&gt;% 
  option_add(param_info = params_up, id = &quot;upsample_bag_spec&quot;)  %&gt;% 
  option_add(param_info = params_smote, id = &quot;smote_bag_spec&quot;) %&gt;% 
  option_add(param_info = params_adasyn, id = &quot;adasyn_bag_spec&quot;) %&gt;% 
  option_add(param_info = params_down, id = &quot;downsample_bag_spec&quot;) %&gt;% 
  option_add(param_info = params_nearmiss, id = &quot;nearmiss_bag_spec&quot;)</code></pre>
<p>Finalmente, vamos ajustar todos os modelos utilizando o m√©todo simples para fazer a busca dos melhores hiperpar√¢metros em grids de 20 valores aleat√≥rios e calcular os scores via valida√ß√£o cruzada (esta parte pode demorar bastante tempo):</p>
<pre class="r"><code>set.seed(123)
chi_models &lt;- 
  chi_models %&gt;% 
  workflow_map(&quot;tune_grid&quot;,
               resamples = bird_folds, 
               grid = 20, 
               metrics = bird_metrics,
               control = control_resamples(save_pred = TRUE),
               verbose = TRUE)</code></pre>
<p>Vejamos os resultados:</p>
<pre class="r"><code>rank_results(chi_models, rank_metric = &quot;mn_log_loss&quot;, select_best = TRUE) %&gt;% 
  select(-.config) %&gt;%
  mutate(wflow_id = str_remove(wflow_id, &quot;_bag_spec&quot;)) %&gt;% 
  print_table(round = 5, wf=T, height = 300, filterable = T)</code></pre>
<p>Matriz de confus√£o do modelo com menor <em>logloss</em>:</p>
<pre class="r"><code>collect_predictions(chi_models) %&gt;% 
  filter(wflow_id == &quot;tomek_bag_spec&quot;) %&gt;% 
  conf_mat_plot()</code></pre>
<center>
<img src="/post/2021-06-28-imbalanced-workflowsets/unnamed-chunk-29-1.png" style="width:80.0%" />
</center>
</div>
</div>
<div id="benchmark" class="section level2">
<h2>Benchmark</h2>
<p>Comparando os resultados dos modelos ajustados:</p>
<details>
<summary>
(<em>Clique aqui para ver o c√≥digo que cria o objeto</em> <code>benchmark</code>)
</summary>
<pre class="r"><code>benchmark &lt;- bind_rows(
  mutate(collect_metrics(null_rs), wflow_id = &quot;default&quot;, model = &quot;null_model&quot;) %&gt;% 
    select(.metric, mean, wflow_id, model) %&gt;% 
    spread(.metric, mean)
  ,
  mutate(collect_metrics(imb_rs), wflow_id = &quot;default&quot;, model = &quot;bag_tree&quot;) %&gt;% 
    select(.metric, mean, wflow_id, model) %&gt;% 
    spread(.metric, mean)
  ,
  rank_results(chi_models, rank_metric = &quot;mn_log_loss&quot;, select_best = TRUE) %&gt;% 
    filter(wflow_id==&quot;smote_bag_spec&quot;) %&gt;% 
    select(.metric, mean, wflow_id, model) %&gt;% 
    spread(.metric, mean)
  ,
  rank_results(chi_models, rank_metric = &quot;mn_log_loss&quot;, select_best = TRUE) %&gt;% 
    filter(rank==1) %&gt;% 
    select(.metric, mean, wflow_id, model) %&gt;% 
    spread(.metric, mean)
) </code></pre>
</details>
<pre class="r"><code>benchmark  %&gt;%
  print_table(round = 5, bm = T)</code></pre>
<p>Como no post da Julia, a logloss e a precis√£o dos modelos que utilizaram m√©todos de balanceamento dos dados pioraram em rela√ß√£o ao modelo de <em>Bagged Decision Tree</em> sem o uso desses pipelines. Apesar da piora em rela√ß√£o ao modelo de base nota-se que outros m√©todos como <em>Tomek Links</em> e <em>Adasyn</em> se sa√≠ram ligeiramente melhores do que o <em>Smote</em> (al√©m disso vimos que o <em>Smote</em> com sua configura√ß√£o <em>default</em> n√£o necessariamente produriz√° os melhores resultados).</p>
<p>Este tipo de performance √© muito comum e at√© esperado visto que estamos avaliando o modelo atrav√©s de uma √∫nica m√©trica (com os mesmos pontos de corte e com o mesmo algoritmo). Normalmente no mundo real monitoramos diversas m√©tricas e experimentamos mais configura√ß√µes de hiperpar√¢metros de diferentes modelos com diferentes pipelines.</p>
</div>
</div>
<div id="conclus√£o" class="section level1">
<h1>Conclus√£o</h1>
<p>Assim como n√£o existe melhor modelo, n√£o existe melhor t√©cnica de balanceamento de dados. Portanto, na busca de melhores resultados n√≥s podemos tentar otimizar qual abordagem ser√° uyilizada bem como seus hiperpar√¢metros (em conjunto com os hiperpar√¢metros dos modelos em quest√£o).</p>
<p>Esta abordagem em R √© nova para mim (estou mais acostumado a utilizar em Python com o m√©todo <code>sklearn.pipeline.Pipeline</code> em conjunto com a biblioteca <a href="https://pypi.org/project/imblearn/">imblearn</a>) ent√£o qualquer cr√≠tica e sugest√£o de melhoria ser√° muito bem vinda! Basta entrar em contato ou deixar aqui nos coment√°rios!</p>
<p>Bons estudos e espero que gostem! üöÄ</p>
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias</h1>
<ul>
<li><a href="https://www.tidyverse.org/blog/2021/03/workflowsets-0-0-1/" class="uri">https://www.tidyverse.org/blog/2021/03/workflowsets-0-0-1/</a></li>
<li><a href="https://www.kaggle.com/c/sliced-s01e02-xunyc5" class="uri">https://www.kaggle.com/c/sliced-s01e02-xunyc5</a></li>
<li><a href="https://juliasilge.com/blog/sliced-aircraft/" class="uri">https://juliasilge.com/blog/sliced-aircraft/</a></li>
<li><a href="https://topepo.github.io/caret/subsampling-for-class-imbalances.html" class="uri">https://topepo.github.io/caret/subsampling-for-class-imbalances.html</a></li>
<li><a href="https://machinelearningmastery.com/bagging-and-random-forest-for-imbalanced-classification/" class="uri">https://machinelearningmastery.com/bagging-and-random-forest-for-imbalanced-classification/</a></li>
<li><a href="https://machinelearningmastery.com/what-is-imbalanced-classification/" class="uri">https://machinelearningmastery.com/what-is-imbalanced-classification/</a></li>
<li><a href="https://machinelearningmastery.com/framework-for-imbalanced-classification-projects/" class="uri">https://machinelearningmastery.com/framework-for-imbalanced-classification-projects/</a></li>
<li><a href="https://machinelearningmastery.com/data-sampling-methods-for-imbalanced-classification/" class="uri">https://machinelearningmastery.com/data-sampling-methods-for-imbalanced-classification/</a></li>
</ul>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2021-06-28-imbalanced-workflowsets/">Otimizando pipelines que envolvem dados desbalanceados</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">imbalanced</category>
      <category domain="tag">imbalanced-data</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">machine-learning</category>
      <category domain="tag">pratica</category>
      <category domain="tag">r</category>
      <category domain="tag">random-forest</category>
      <category domain="tag">tidymodels</category>
      <category domain="tag">tidyverse</category>
      <category domain="tag">tunning</category>
    </item>
    <item>
      <title>Um ano de blog!</title>
      <link>https://gomesfellipe.github.io/post/2018-12-26-retrospectiva/retrospectiva/</link>
      <pubDate>Wed, 26 Dec 2018 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2018-12-26-retrospectiva/retrospectiva/</guid>
      <description>Este post traz uma retrospectiva do que aconteceu por aqui em 1 ano de blog, foi muito empolgante o t√©rmino do desafio que fiz a mim mesmo e a anima√ß√£o para novos desafios continua!</description>
      <content:encoded>&lt;![CDATA[
        


<style type="text/css">
.column-left{
  float: left;
  width: 33%;
  text-align: center;
}
.column-center{
  display: inline-block;
  width: 33%;
  text-align: center;
}
.column-right{
  float: right;
  width: 33%;
  text-align: center;
}

# left {
#   left:-8.33%;
#   text-align: left;
#   float: left;
#   width:50%;
#   z-index:-10;
# }
# 
# right {
#   left:31.25%;
#   top: 75px;
#   float: right;
#   text-align: right;
#   z-index:-10;
#   width:50%;
# }

</style>
<div id="at√©-que-um-dia.." class="section level2">
<h2>At√© que um dia..</h2>
<p>Agora em dezembro encerro um desafio pessoal de fazer pelo menos um post por m√™s durante o ano de 2018 e estou muito animado com o t√©rmino deste ciclo! Espero ter contribu√≠do um pouquinho com a comunidade de Estat√≠stica e Ci√™ncia de Dados que est√° maior a cada dia e cada vez mais importante.</p>
<p>A ideia de fazer um blog <strong>come√ßou</strong> quando me deparei que tinha muitos scripts e rotinas guardados de alguns estudos sobre programa√ß√£o e estat√≠stica tanto sobre os assuntos da faculdade quanto sobre estudos avulsos de programa√ß√£o em R. Guardar os scripts de maneira organizada para relembrar e tentar fixar melhor alguns conceitos j√° vistos pode ser uma boa id√©ia acelerando o processo de busca por solu√ß√µes para problemas computacionais do dia a dia.</p>
<p>Desde o in√≠cio da gradua√ß√£o tomei o h√°bito de guardar todos meus arquivos no Dropbox (que √© excelente ferramenta, inclusive porque √© poss√≠vel <a href="https://gomesfellipe.github.io/post/gerenciando-arquivos-do-dropbox-com-r/gerenciando-arquivos-do-dropbox-com-r/">integrar com o R/Shiny</a>). <strong>At√© que um dia</strong> me dei conta de que estava acumulando cada vez mais scripts e relat√≥rios em meu dropbox e al√©m de n√£o ser muito pr√°tico o acesso aos scripts, acaba n√£o sendo muito din√¢mico no compartilhamento com os membros da equipe, ent√£o passei a utilizar o <a href="https://github.com/gomesfellipe">Github</a> para hospedar meus c√≥digos, projetos, fun√ß√µes e estudos que poderiam tornar-se p√∫blicos e assim facilitar na hora de compartilhar algum c√≥digo ou receber algum feedback.</p>
<p>Al√©m do <a href="https://github.com/gomesfellipe">Github</a>, o <a href="%5BGithub%5D(https://github.com/gomesfellipe)">RPubs</a> tamb√©m foi √∫til para publica√ß√£o de alguns trabalhos/projetos durante a gradua√ß√£o e alguns <a href="https://www.kaggle.com/gomes555">testes e contribui√ß√µes nas competi√ß√µes e conjuntos de dados do Kaggle</a> (al√©m de bisbilhotar as resolu√ß√µes dos top players) para aprender de uma maneira diferente um pouquinho mais a cada dia. J√° no caso de trabalhos privados (solo ou em equipe), o <a href="https://bitbucket.org/">Bitbucket</a> tamb√©m tem se mostrado uma √≥tima ferramenta.</p>
<p>Passei a receber feedback sobre meus c√≥digos, sugest√µes de melhorias e isso me motivou a <strong>continuar</strong> e <strong>persistir</strong> nos posts independente de monografia, est√°gio, trabalho, ou qualquer outro compromisso super importante. Firmei esse desafio pessoal e abaixo ser√° poss√≠vel conferir um pouco de como foi minha evolu√ß√£o, algumas t√©cnicas e curiosidades que aprendi e achei que valia a pena compartilhar com a comunidade.</p>
<p><br></p>
<hr />
<div id="an√°lise-de-sobreviv√™ncia-com-dados-do-jogo-pubg-dispon√≠veis-no-kaggle" class="section level4 column-left">
<h4>An√°lise de sobreviv√™ncia com dados do jogo pubg dispon√≠veis no kaggle</h4>
<p><br></p>
<p><a href="https://gomesfellipe.github.io/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle/"><img src="/post/2018-12-26-retrospectiva/retrospectiva_files/nov.png" title="An√°lise de sobreviv√™ncia com dados do jogo pubg dispon√≠veis no kaggle" style="width:80.0%" alt="nov" /></a></p>
<p>Calcular a probabilidade de um indiv√≠duo sobreviver e tamb√©m encontrar fatores de risco de morte em um intervalo de tempo. Foram realizadas estat√≠sticas descritivas e o ajuste de modelos de sobreviv√™ncia como o de Kaplan-Meier e o Modelo de regress√£o de riscos proporcionais de Cox.</p>
</div>
<div id="seu-app-rstudio-e-shiny-server-na-nuvem-do-google" class="section level4 column-center">
<h4>Seu app, RStudio e Shiny Server na nuvem do Google</h4>
<p><a href="https://gomesfellipe.github.io/post/2018-10-27-server-cloud/server-cloud/"><img src="/post/2018-12-26-retrospectiva/retrospectiva_files/out.png" title="Seu app, RStudio e Shiny Server na nuvem do Google" style="width:80.0%" alt="out" /></a></p>
<p><br></p>
<p>Aprendemos como dar in√≠cio √† uma m√°quina virtual rodando Linux Ubuntu 16.04 no servidor do Google Cloud. Com nossa m√°quina virtual na configura√ß√£o desejada somos capazes de dar in√≠cio ao nosso pr√≥prio RStudio Server e tamb√©m nosso Shiny Server para facilitar a entrega de nossas aplica√ß√µes, como a que foi criada de exemplo no post para acompanhar as cota√ß√µes e algumas a√ß√µes de bolsas de valores americanas.</p>
</div>
<div id="com-que-frequ√™ncia-ocorrem-acidentes-na-ponte-rio-niteroi" class="section level4 column-right">
<h4>Com que frequ√™ncia ocorrem acidentes na ponte rio-niteroi?</h4>
<p><a href="https://gomesfellipe.github.io/post/2018-09-29-freq-acidente-ponte-rio-niteroi.md/freq-acidente-ponte-rio-niteroi/"><img src="/post/2018-12-26-retrospectiva/retrospectiva_files/set.jpg" title="Com que frequ√™ncia ocorrem acidentes na ponte rio-niteroi?" style="width:80.0%" alt="set" /></a></p>
<p><br></p>
<p>Neste post utilizamos os dados p√∫blicos fornecidos pela pol√≠cia rodovi√°ria federal para responder algumas perguntas relacionadas √† ocorr√™ncia de acidentes. Ser√° que o n√∫mero de acidentes vem diminuindo? Ser√° que a instala√ß√£o de c√¢meras de seguran√ßa reduziu o n√∫mero de acidentes? Uma breve an√°lise de car√°ter descritivo ir√° ajudar a entender o comportamento dos dados e permitir novas reflex√µes.</p>
</div>
<p><br></p>
<hr />
<div id="um-estudo-sobre-modelos-de-aprendizagem-baseados-em-√°rvores-com-desafio-do-kaggle" class="section level3 column-left">
<h3>Um estudo sobre modelos de aprendizagem baseados em √°rvores com desafio do kaggle</h3>
<p><br></p>
<p><a href="https://gomesfellipe.github.io/img/2018/08/img1.png"><img src="/post/2018-12-26-retrospectiva/retrospectiva_files/jul.png" title="Um estudo sobre modelos de aprendizagem baseados em √°rvores com desafio do kaggle" style="width:80.0%" /></a></p>
<p>Utilizamos a base de dados da competi√ß√£o do Kaggle <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/">House Prices: Advanced Regression Techniques</a> para colocar em pr√°tica alguns dos conhecimentos adquiridos estudando sobre algoritmos como √°rvore de decis√£o, random forest, gradient boost machine e por fim uma regress√£o linear para comparar os resultados obtidos</p>
</div>
<div id="ajustando-um-modelo-de-regress√£o-linear-bayesiano-do-zero" class="section level3 column-center">
<h3>Ajustando um modelo de regress√£o linear bayesiano do zero</h3>
<p><br></p>
<p><a href="https://gomesfellipe.github.io/img/2018/07/img1.png"><img src="/post/2018-12-26-retrospectiva/retrospectiva_files/jun.png" title="Ajustando um modelo de regress√£o linear bayesiano do zero" style="width:80.0%" /></a></p>
<p>Neste post s√£o realizados os c√°lculos das distribui√ß√µes condicionais completas a posteriori (DCCP) para cada par√¢metro do ajuste de um modelo de regress√£o linear bayesiano para poder implementar o algoritmo do amostrador de Gibbs em dados simulados (para conferir a qualidade do ajuste do modelo) e em dados reais. Este post √© o resumo de uma das aplica√ß√µes que fiz em um projeto de inicia√ß√£o √† pesquisa que deixei dispon√≠vel no Github</p>
</div>
<div id="brasil-x-argentina-tidytext-e-machine-learning" class="section level3 column-right">
<h3>Brasil x Argentina, tidytext e machine learning</h3>
<p><br></p>
<p><a href="https://gomesfellipe.github.io/img/2018/06/img.jpg"><img src="/post/2018-12-26-retrospectiva/retrospectiva_files/mai.jpg" title="Brasil x Argentina, tidytext e machine learning" style="width:80.0%" /></a></p>
<p>Nesta √©poca eu estava estudando o livro da Julia Silge depois de uma palestra que assisti no SER 2018 e aproveitando o embalo da copa do mundo resolvi conferir os dados do twitter disponibilizados atrav√©s da API do Twitter naquela √©poca para uma aplica√ß√£o de an√°lise de sentimentos utilizando a abordagem <code>tidytext</code>. Al√©m disso foram utilizados os modelos knn, random forest, naive bayes e um modelo de regress√£o log√≠stico para um estudo sobre classifica√ß√£o em an√°lises de texto com o pacote caret</p>
</div>
<p><br></p>
<hr />
<div id="aed-de-forma-r√°pida-e-um-pouco-de-machine-learning" class="section level3 column-left">
<h3>AED de forma r√°pida e um pouco de machine learning</h3>
<p><br></p>
<p><a href="https://gomesfellipe.github.io/img/2018/05/img1.png"><img src="/post/2018-12-26-retrospectiva/retrospectiva_files/abr.png" title="AED de forma r√°pida e um pouco de machine learning" style="width:80.0%" /></a></p>
<p>Este post veio para lembrar como a an√°lise explorat√≥ria em R n√£o precisa sempre ser uma tarefa longa e trabalhosa. Com o pacote SmartEAD √© poss√≠vel gerar muitos gr√°ficos com poucas linhas de c√≥digo. Ao longo do post algumas tarefas de machine learning s√£o executadas, como k-means (tarefa n√£o-supervisionada de agrupamento), um modelo de regress√£o linear com stepwise (tarefa supervisionado de regress√£o) e um modelo random forest (tarefa supervisionada de classifica√ß√£o)</p>
</div>
<div id="s√©ries-temporais-com-google-trends-e-r" class="section level3 column-center">
<h3>S√©ries temporais com google trends e R</h3>
<p><br></p>
<p><a href="https://gomesfellipe.github.io/img/2018/04/img1.png"><img src="/post/2018-12-26-retrospectiva/retrospectiva_files/abr2.png" title="S√©ries temporais com google trends e R" style="width:80.0%" /></a></p>
<p><br></p>
<p>Como √© f√°cil obter dados do Google Trends para analis√°-los no RStudio. Obtivemos algumas s√©ries e duas delas chamaram mais a aten√ß√£o: a popularidade do termo <code>Big Data</code> (que est√° aumentando) e a popularidade do termo <code>Estat√≠stica</code> que estava diminuindo. Realizamos a decomposi√ß√£o da s√©rie, realizamos alguns testes estat√≠sticos para detectar signific√¢ncia dos componentes das s√©ries e foram ajustados modelos de Holt Winters para tentar prever o comportamento destas s√©ries no pr√≥ximo ano.</p>
<p><br>
<br>
<br>
<br></p>
</div>
<div id="produzindo-e-formatando-um-documento-word-direto-em-r" class="section level3 column-right">
<h3>Produzindo e formatando um documento word direto em R</h3>
<p><br></p>
<p><a href="https://gomesfellipe.github.io/img/2018/03/Word-R.png"><img src="/post/2018-12-26-retrospectiva/retrospectiva_files/mar.png" title="Produzindo e formatando um documento word direto em R" style="width:80.0%" /></a></p>
<p><br></p>
<p>O bom de programar em R √© que n√£o precisamos nos limitar apenas √† ferramenta, podemos importar e exportar arquivos em diferentes formatos e entregas nossos resultados de muitas maneiras. Neste post √© mostrado como exportar seus relat√≥rios em Word utilizando um template.</p>
<p><br>
<br>
<br>
<br>
<br></p>
</div>
<p><br></p>
<hr />
<div id="o-que-s√£o-cheatsheets-gamifica√ß√£o-e-por-que-aprender-r-√©-t√£o-divertido" class="section level3 column-left">
<h3>O que s√£o cheatsheets, gamifica√ß√£o e por que aprender R √© t√£o divertido?</h3>
<p><a href="https://gomesfellipe.github.io/post/2018-02-17-cheatsheet-gamificacao-r/cheatsheet-gamificacao-r/"><img src="/post/2018-12-26-retrospectiva/retrospectiva_files/fev.png" title="O que s√£o cheatsheets, gamifica√ß√£o e por que aprender R √© t√£o divertido?" style="width:80.0%" /></a></p>
<p>A aprendizagem √© um processo que nunca acaba e neste post mostro algumas maneiras de estudar R e estat√≠stica se divertindo! Falamos o que s√£o as <a href="https://www.rstudio.com/resources/cheatsheets/">CheatSheets</a>, cursos da <a href="https://www.datacamp.com">DataCamp</a>, onde √© poss√≠vel ter cursos altamente pr√°ticos com os desenvolvedores dos principais pacotes utilizando em Ci√™ncia de Dados e falamos sobre a plataforma <a href="https://www.kaggle.com">Kaggle</a>, um verdadeiro playgroud para Cientistas de dados que podem disputar por posi√ß√µes em um rank mundial e compartilhar c√≥digos e an√°lises para ter o feedback de outros cientistas de dados ao redor do mundo</p>
</div>
<div id="carnaval-e-mapas-interativos-com-r" class="section level3 column-center">
<h3>Carnaval e mapas interativos com R</h3>
<p><br></p>
<p><a href="https://gomesfellipe.github.io/img/2018/02/carnaval2.png"><img src="/post/2018-12-26-retrospectiva/retrospectiva_files/fev2.png" title="Carnaval e mapas interativos com R" style="width:80.0%" /></a></p>
<p><br></p>
<p>Era fevereiro, v√©spera de carnaval e grande parte das pessoas no Rio de Janeiro gostariam de responder a seguinte pergunta: ‚ÄúOnde est√£o os blocos?‚Äù. Neste post constru√≠mos mapas interativos com o uso do pacote leaflet e em poucas linhas de c√≥digo foi poss√≠vel mapear os blocos ao redor do Rio de Janeiro facilitando assim a vida do foli√£o carioca</p>
<p><br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br></p>
</div>
<div id="o-paradoxo-dos-anivers√°rios-com-simula√ß√£o-e-probabilidade" class="section level3 column-right">
<h3>O paradoxo dos anivers√°rios com simula√ß√£o e probabilidade</h3>
<p><br></p>
<p><a href="https://gomesfellipe.github.io/img/2018/01/modelagem-probabilidade2.png"><img src="/post/2018-12-26-retrospectiva/retrospectiva_files/jan1.png" title="O paradoxo dos anivers√°rios com simula√ß√£o e probabilidade" style="width:80.0%" /></a></p>
<p><br></p>
<p>A probabilidade √© contra intuitiva. Existem diversos exemplos cl√°ssicos de probabilidade que nos mostram como pode ser enganosa nossa intui√ß√£o ao tratar com a incerteza. Neste post utilizamos de simula√ß√£o e de teoria das probabilidades para mostra como a probabilidade de duas pessoas fazerem anivers√°rio no mesmo dia em uma sala de aula aumenta muito r√°pido conforme aumentamos o n√∫mero de alunos nessa sala.</p>
<p><br></p>
</div>
<p><br></p>
<hr />
<div id="tabelas-incriveis-com-r" class="section level3 column-left">
<h3>Tabelas incriveis com R</h3>
<p><br></p>
<p><a href="https://gomesfellipe.github.io/post/2018-01-12-tabelas-incriveis-com-r/tabelas-incriveis-com-r/"><img src="/post/2018-12-26-retrospectiva/retrospectiva_files/jan2.jpg" title="Tabelas incriveis com R" style="width:80.0%" /></a></p>
<p><br></p>
<p>As sa√≠das do console do R podem n√£o ser a melhor forma de entregas suas valiosas an√°lises. Quando estamos numa situa√ß√£o de neg√≥cios ou consultoria a apresenta√ß√£o dos dados tamb√©m √© muito importante e o R n√£o fica para tr√°s nisso! Neste post vimos alguns pacotes como o DT, knitr, kableExtra, formattable, sparkline e rhandsontable podem tornar a apresenta√ß√£o de nossas tabelas muito mais elegantes</p>
<p><br></p>
</div>
<div id="an√°lise-multivariada-com-r" class="section level3 column-center">
<h3>An√°lise multivariada com R</h3>
<p><br></p>
<p><a href="https://gomesfellipe.github.io/post/2018-01-01-analise-multivariada-em-r/an%C3%A1lise-multivariada-em-r/"><img src="/post/2018-12-26-retrospectiva/retrospectiva_files/jan3.png" title="An√°lise multivariada com R" style="width:80.0%" /></a></p>
<p><br></p>
<p>Em tempos de Big Data esse campo da estat√≠stica torna-se cada vez mais importante. Saber lidar com grandes volumes e variedades de dados pode ser muito importante e neste post vimos um pouco sobre t√©cnicas de estat√≠stica multivariada como PCA, An√°lise Fatorial, Clusteriza√ß√£o com agrupamento hier√°rquico e com k-means</p>
<p><br></p>
</div>
<div id="pacotes-do-r-para-avaliar-o-ajuste-de-modelos" class="section level3 column-right">
<h3>Pacotes do R para avaliar o ajuste de modelos</h3>
<p><br></p>
<p><a href="https://gomesfellipe.github.io/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos/"><img src="/post/2018-12-26-retrospectiva/retrospectiva_files/dez.jpg" title="Pacotes do R para avaliar o ajuste de modelos" style="width:80.0%" /></a></p>
<p><br></p>
<p>O Modelo de regress√£o linear √© uma √≥tima ferramenta para qualquer estat√≠stico, ele √© altamente interpret√°vel e seu a avalia√ß√£o de seu desempenho pode ser feita de forma meticulosa. Neste post utilizamos os pacotes GGally e o ggfortify para fazer diagn√≥sticos bem elaborados sobre as premissas que sustentam o uso de tais modelos</p>
<p><br></p>
</div>
<p><br></p>
<hr />
</div>
<div id="al√©m-disso" class="section level2">
<h2>Al√©m disso‚Ä¶</h2>
<p>Al√©m dos posts acima, tamb√©m j√° passou por aqui:</p>
<div id="posts-mais-antigos" class="section level3">
<h3>Posts mais antigos <i class='fa fa-coffee'></i></h3>
<p>Al√©m dos posts acima que ficaram em destaque, por aqui tamb√©m foram postados:</p>
<ul>
<li><a href="https://gomesfellipe.github.io/post/2017-12-18-bayesiana-jags-mcmcplot/bayesiana-jags-mcmcplot/">AJUSTANDO MODELOS BAYESIANOS COM JAGS</a></li>
<li><a href="https://gomesfellipe.github.io/post/2017-12-17-string/string/">MANIPULA√á√ÉO DE STRINGS E TEXT MINING</a></li>
<li><a href="https://gomesfellipe.github.io/post/2017-12-14-criando-relat%C3%B3rios-com-template-tufle/criando-relat%C3%B3rios-com-template-tufle/">CRIANDO RELAT√ìRIOS COM TEMPLATE TUFLE</a></li>
<li><a href="https://gomesfellipe.github.io/post/2017-12-07-manipulando-dados-com-dplyr/manipulando-dados-com-dplyr/">MANIPULANDO DADOS COM DPLYR</a></li>
<li><a href="https://gomesfellipe.github.io/post/analisando-o-mercado-de-cryptomoedas-com-r/">ANALISANDO O MERCADO DE CRYPTOMOEDAS COM R</a></li>
<li><a href="https://gomesfellipe.github.io/post/gerenciando-arquivos-do-dropbox-com-r/gerenciando-arquivos-do-dropbox-com-r/">GERENCIANDO ARQUIVOS DO DROPBOX COM R</a></li>
<li><a href="https://gomesfellipe.github.io/post/tipos-de-relacoes-entre-variaveis/">TIPOS DE RELA√á√ïES ENTRE VARI√ÅVEIS</a></li>
<li><a href="https://gomesfellipe.github.io/post/tipos-de-correlacoes/">TIPOS DE CORRELACOES</a></li>
</ul>
</div>
<div id="aplicativos-shiny" class="section level3">
<h3>Aplicativos Shiny</h3>
<p>Esses s√£o os aplicativos para uso p√∫blico que desenvolvi (alguns ainda em faze de aprendizagem) que contam com os c√≥digos hospedados no Github, al√©m deles desenvolvi uma por√ß√£o de aplica√ß√µes que est√£o hospedadas no Bitbucket por√©m como s√£o privados n√£o poderei compartilha-las aqui:</p>
<ul>
<li><a href="https://github.com/gomesfellipe/dashboard-text-mining-1">Dashboard com textmining de m√≠dias sociais</a></li>
<li><a href="https://github.com/gomesfellipe/appwordcloud">App para cria√ß√£o de n√∫vens de palavras cusmomizadas</a></li>
<li><a href="https://github.com/gomesfellipe/app_acoes">App para acompanhar cota√ß√µes de a√ß√µes da bolsa de valores americana (e algumas a√ß√µes BR)</a></li>
<li><a href="https://github.com/gomesfellipe/appPCAkmeans">App para r√°pica aplica√ß√£o de componentes principais e do algoritmo de machine learning k-means</a></li>
</ul>
</div>
<div id="inicia√ß√£o-√†-pesquisa" class="section level3">
<h3>Inicia√ß√£o √† Pesquisa:</h3>
<p>Este foi um estudo sobre modelagem hier√°rquica sob a √≥tica da escola bayesiana para me preparar para escrever minha monografia, que contou com uma aplica√ß√£o deste modelo na vida real:</p>
<ul>
<li><a href="https://github.com/gomesfellipe/projeto_modelos_hierarquicos_bayesianos">Projeto de Inicia√ß√£o √† Pesquisa: Modelos Hier√°rquicos Bayesianos</a></li>
</ul>
</div>
<div id="writer-da-ensina.ai" class="section level3">
<h3>Writer da ENSINA.AI</h3>
<p>Esse ano tamb√©m recebi um convite inesperado de participar como escritor contribuidor da p√°gina <a href="https://medium.com/ensina-ai">ENSINA.AI</a> no medium, que tem como compromisso trazer ao p√∫blico: Tudo sobre Intelig√™ncia Artificial em Portugu√™s. Fiquei muito feliz com o convite pois j√° acompanhava a p√°gina e os posts sempre s√£o muito bons! Os posts que contribui foram:</p>
<ul>
<li><a href="https://medium.com/ensina-ai/com-que-frequ%C3%AAncia-ocorrem-acidentes-na-ponte-rio-niter%C3%B3i-58d7f779c6d0">Com que frequ√™ncia ocorrem acidentes na ponte Rio-Niter√≥i?</a></li>
<li><a href="https://medium.com/ensina-ai/seu-app-rstudio-e-shiny-na-nuvem-do-google-31753d71619">Seu app, RStudio e Shiny na nuvem do Google</a></li>
<li><a href="https://medium.com/ensina-ai/um-estudo-sobre-modelos-de-aprendizagem-baseados-em-%C3%A1rvores-com-desafio-do-kaggle-a73c32abc9f1">Um estudo sobre modelos de aprendizagem baseados em √°rvores com desafio do Kaggle</a></li>
<li><a href="https://medium.com/ensina-ai/ajustando-um-modelo-de-regress%C3%A3o-linear-bayesiano-do-zero-c2a8c2351202">Ajustando um modelo de regress√£o linear bayesiano do zero</a></li>
</ul>
</div>
<div id="extras" class="section level3">
<h3>Extras <i class='fa fa puzzle-piece'></i> :</h3>
<ul>
<li><a href="https://gomesfellipe.github.io/itemized/item2/">BANCOS DE DADOS P√öBLICOS - Apresentando alguns sites que disponibilizam dados p√∫blicos</a></li>
<li><a href="https://gomesfellipe.github.io/itemized/item1/">SOBRE OMBROS DE GIGANTES - Algumas frases de pensadores que me inspiram</a></li>
</ul>
<p><br></p>
<hr />
<p><br></p>
</div>
</div>
<div id="boas-festas" class="section level1">
<h1>Boas festas!</h1>
<p>Agora o ano est√° se encerrando e assim se encerra mais um ciclo, estou muito satisfeito pois aprendi muita coisa nova, interagi com muita gente inteligente e me diverti demais na produ√ß√£o deste blog.</p>
<p>Foram diversos desafios pois no come√ßo foi bem complicado entender como manipular <code>html</code>, <code>css</code> e controlar as vers√µes dos c√≥digos no <code>git</code> pois era tudo muito novo mas o desafio valeu a pena, hoje aprendi um pouquinho sobre essas e outras ferramentas que um estat√≠stico n√£o v√™ na gradua√ß√£o e foi bem proveitoso em meus trabalhos. Se consegui ajudar pelo menos uma pessoa j√° atingi minha meta, se ajudei duas pessoas ent√£o consegui dobrar essa meta!</p>
<div class="row">
<div class="col-sm-6">
<p><img vspace="20"></p>
<blockquote>
<p>Se souber exatamente o que fazer, qual a vantagem de faz√™-lo? - <em>Pablo Picasso</em></p>
</blockquote>
</div>
<!-- <div class="column-center"> -->
<!-- <img vspace="100"> -->
<!-- </div> -->
<div class="col-sm-6">
<p><img src="/post/2018-12-26-retrospectiva/retrospectiva_files/picasso-victorian-house.jpg" style="width:60.0%" /></p>
</div>
</div>
<hr />
<p>Agora no ano de 2019 ‚Äúfazer pelo menos um post por m√™s‚Äù n√£o estar√° em meus planos, mas j√° tenho diversas ideias de posts e continuarei escrevendo em um ritmo um pouco mais calmo pois os estudos est√£o cada vez mais dif√≠ceis!</p>
<p>Espero que tenham gostado do conte√∫do e obrigado por toda ajuda direta ou indiretamente de cada membro da comunidade que fez valer a pena cada minuto de esfor√ßo!</p>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2018-12-26-retrospectiva/retrospectiva/">Um ano de blog!</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">estatistica</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">machine-learning</category>
      <category domain="tag">retrospectiva</category>
      <category domain="tag">rmarkdown</category>
      <category domain="tag">rstudio</category>
      <category domain="tag">shiny</category>
    </item>
    <item>
      <title>An√°lise de sobreviv√™ncia com dados do jogo PUBG dispon√≠veis no Kaggle</title>
      <link>https://gomesfellipe.github.io/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle/</link>
      <pubDate>Wed, 28 Nov 2018 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle/</guid>
      <description>O que interefere na probabilidade de um indiv√≠duo sobreviver? Quais fatores apresentam efeito no risco de morte em um intervalo de tempo? Neste post buscaremos evid√™ncias estat√≠sticas para responder estas perguntas em dados abertos do PUBG hospedados no Kaggle</description>
      <content:encoded>&lt;![CDATA[
        


<div id="an√°lise-de-sobreviv√™ncia-e-pubg" class="section level1">
<h1>An√°lise de sobreviv√™ncia e PUBG</h1>
<p>An√°lise de sobreviv√™ncia √© um termo que se refere a situa√ß√µes m√©dicas e √© caracterizada pela sua vari√°vel resposta, que pode ser apresentada de tr√™s formas: probabilidade de sobreviv√™ncia, taxa de incid√™cia e taxa de incid√™ncia acumulada.</p>
<p>Na engenharia este termo tamb√©m √© conhecido como confiabilidade, no entanto, condi√ß√µes parecidas podem ocorrer em (inusitadas) outras √°reas.</p>
<p>PUBG √© um jogo online multiplayer de batalha em que 100 jogadores s√£o lan√ßados em uma ilha e tem como objetivo principal <strong>sobreviver</strong>, a √°rea de jogo diminui progressivamente, confinando os sobreviventes a um espa√ßo cada vez menor e for√ßando encontros e o vencedor √© o √∫ltimo jogador (ou time) a permanecer vivo.</p>
<p>Um √∫nico jogo dura aproximadamente de 30-35 minutos e neste tempo o jogador coleta itens (arma, cura, boost), abate outros jogadores, comete e leva dano de seus advers√°rios, pode dirigir ve√≠culos dentre outras a√ß√µes enquanto tentam sobrevier ao mesmo tempo.</p>
<p>Quest√µes que surgiram em mente ap√≥s um per√≠odo de estudos de an√°lise de sobreviv√™ncia e confiabilidade e ouvindo pessoas falarem sobre esta modalidade de jogo:</p>
<ul>
<li>O que interefere na probabilidade de um indiv√≠duo sobreviver?</li>
<li>O que tem efeito no risco de um jogador ser abatido em um intervalo de tempo?</li>
</ul>
<p>Faremos uma abordagem estat√≠stica aqui, ap√≥s uma breve an√°lise explorat√≥ria os dados ser√£o avaliados utilizando o modelo de Kaplan-Meier, que √© um estimador de forma n√£o param√©trica para a fun√ß√£o de sobreviv√™ncia e o modelo semiparam√©trico de regress√£o de riscos proporcionais de Cox.</p>
</div>
<div id="a-base-de-dados" class="section level1">
<h1>A Base de dados</h1>
<p>A base de dados utilizada foi obtida atrav√©s do Kaggle em ‚ÄúPUBG Match Deaths and Statistics‚Äù: <a href="https://www.kaggle.com/skihikingkevin/pubg-match-deaths" class="uri">https://www.kaggle.com/skihikingkevin/pubg-match-deaths</a> que conta com mais de 65 milh√µes de registros de mortes no jogo PlayerUnknown Battleground‚Äôs matches - PUBG.</p>
<p><a href="https://www.kaggle.com/gomes555/analise-de-sobrevivencia-km-e-cox/">Existe uma vers√£o deste post no kaggle</a> e al√©m desta base, existe uma competi√ß√£o em andamento que vai at√© o dia 30 de Janeiro no link:<a href="https://www.kaggle.com/c/pubg-finish-placement-prediction" class="uri">https://www.kaggle.com/c/pubg-finish-placement-prediction</a> que desafia os jogadores a prever o posicionamento do vencedor em percentil, onde 1 corresponde ao 1¬∫ lugar e 0 corresponde ao √∫ltimo lugar do jogo. Fiz uma participa√ß√£o com um <a href="https://www.kaggle.com/gomes555/xgboost-caret-for-fun">script testando os resultados do algor√≠tmo xgboost com caret</a> e tamb√©m testei uns <a href="https://www.kaggle.com/gomes555/tidyverse-machine-learning-for-fun">ajustes com random forest utilizando o tidyverse</a>. Esses scripts s√£o abertos e est√£o prontos para uso, <a href="https://www.kaggle.com/gomes555">n√£o me renderam a melhor posi√ß√£o</a> mas a intens√£o aqui √©, principalmente, aprender e testar os m√©todos pois S√£o muitas possibilidade para aprender e praticar. Voltando a base de dados:</p>
<p>Segundo a <a href="https://www.kaggle.com/skihikingkevin/pubg-match-deaths#aggregate.zip">descri√ß√£o da base no kaggle</a>:</p>
<p><code>agg_match_stats_x.csv</code> fornece informa√ß√µes de correspond√™ncia mais agregadas sobre os dados de mortes, como tamanho da fila, fpp/tpp, morte do jogador, etc.</p>
<p>As colunas s√£o as seguintes:</p>
<div class="col2">
<ul>
<li><code>match_id</code> : O id √∫nico de correspond√™ncia gerado por pubg.op.gg. √â poss√≠vel fazer uma jun√ß√£o disso com os dados das mortes para ver todas as informa√ß√µes</li>
<li><code>party_size</code> : o n√∫mero m√°ximo de jogadores por equipe. por exemplo, 2 implica que era um sistema de fila dupla</li>
<li><code>player_dist_ride</code> : unidades de distancia total (metros?) que o jogador percorreu em um ve√≠culo</li>
<li><code>player_dist_walk</code> : unidades de distancia total (metros?) percorrida pelo jogador a p√©</li>
<li><code>match_mode</code> : se o jogo foi jogado em primeira pessoa (fpp) ou em terceira pessoa (tpp)</li>
<li><code>team_placement</code> : a classifica√ß√£o final da equipe dentro da partida</li>
<li><code>player_dmg</code> : Total de pontos de vida que o jogador distribuiu</li>
<li><code>player_assists</code> : N√∫mero de assist√™ncias que o jogador marcou</li>
<li><code>game_size</code> : o n√∫mero total de equipes que estavam no jogo</li>
<li><code>player_dbno</code> : N√∫mero de knockdowns que o jogador marcou</li>
<li><code>player_kills</code> : N√∫mero de mortes que o jogador marcou</li>
<li><code>team_id</code> : o ID da equipe √† qual o jogador pertencia</li>
<li><code>date</code> : a data e a hora em que a partida ocorreu</li>
<li><code>player_name</code> : nome do jogador</li>
</ul>
<hr />
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/img.png" /></p>
</div>
<p>A rotinas abaixo carregam os pacotes, fun√ß√µes customizadas e salva em extens√£o <code>.rds</code>uma amostra da base de dados utilizadas ao longo do post:</p>
<pre class="r"><code># Carregar pacotes --------------------------------------------------------
packages &lt;- c(&quot;data.table&quot;, &quot;dplyr&quot;, &quot;purrr&quot;, &quot;survival&quot;  , &quot;survminer&quot;,
              &quot;ggfortify&quot;,&quot;GGally&quot;, &quot;ggplot2&quot;,&quot;moments&quot;, &quot;gridExtra&quot;,&quot;ggExtra&quot;,
              &quot;cowplot&quot;,&quot;lubridate&quot;, &quot;scales&quot;, &quot;knitr&quot;, &quot;kableExtra&quot;, &quot;grid&quot;,
              &quot;broom&quot;, &quot;formattable&quot;, &quot;grid&quot;)
purrr::walk(packages,library, character.only = TRUE, warn.conflicts = FALSE)
rm(packages)

# Funcoes customizadas do github ------------------------------------------
source(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/inicio_e_fim_da_base.R&quot;)
source(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/grafico_descritivo.R&quot;)
source(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/sumario_custom_num.R&quot;)

# Opcoes do documento -----------------------------------------------------
# options(scipen = 99999)

# Tema dos graficos -------------------------------------------------------
theme_set(theme_bw()+
            theme(axis.text.x = element_text(size=17),
                  axis.text.y = element_text(size=17),
                  axis.title.y = element_text(size=20), legend.position = &quot;bottom&quot;))

# Tema das tabelas kable --------------------------------------------------
kable2 &lt;- function(x,linhas=NULL,colunas=NULL, ...){
  k &lt;- 
    kable(x,digits = 4,...) %&gt;%
    kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) %&gt;%
    kable_styling(c(&quot;striped&quot;, &quot;bordered&quot;)) 
  
  if (!is.null(linhas)) {
    # destque na linha:
    k &lt;-  k %&gt;% row_spec(linhas, bold = T, color = &quot;white&quot;, background = &quot;#FFE8BD&quot;)
  }
  
  if (!is.null(colunas)) {
    # destque na colunas:
    k &lt;-  k %&gt;% column_spec(colunas,bold=T, color=&quot;white&quot;, background = &quot;#FFE8BD&quot;)
  }
  k %&gt;%
    scroll_box(width = &quot;850px&quot;)
}</code></pre>
<p>Em uma an√°lise de sobreviv√™ncia √© comum a presen√ßa de observa√ß√µes censuradas, (isto √©, quando ocorre a perda de informa√ß√£o decorrente de n√£o se ter observado a data de ocorr√™ncia do desfecho). No caso dessa base de dados n√£o existe uma vari√°vel que define a censura, pois apenas a morte do jogador √© registrada e √© poss√≠vel que se os jogadores se desconectarem do jogo mesmo que n√£o sejam mortos seja contado como morte de qualquer jeito. Os detalhes por tr√°s da aquisi√ß√£o de dados n√£o trazem essa informa√ß√£o portanto pode n√£o ser poss√≠vel distinguir a censura do desfecho e isso √© um detalhe relevante que deve ser levado em conta.</p>
<pre class="r"><code># Carregar base -----------------------------------------------------------
set.seed(2)   # reprodutivel
pubg_tpp1 &lt;-  # Informacoes dos criterios de selecao no corpo do texto
  map_df(paste0(&quot;agg_match_stats_&quot;,0:4,&quot;.csv&quot;), 
         ~ fread(.x, showProgress = T,
                 data.table = T)[match_mode == &quot;tpp&quot; &amp; party_size == 1 &amp; year(date) == 2018 &amp; player_dist_walk&gt;10 &amp; player_dmg != 0 ][, !c(&quot;match_mode&quot;,&quot;party_size&quot;,&quot;game_size&quot;,&quot;date&quot;, &quot;team_id&quot;,&quot;player_dbno&quot;, &quot;team_placement&quot;), with=FALSE][,player_survive_time := player_survive_time/60] %&gt;% 
           group_by(match_id) %&gt;%
           do(sample_n(.,1)) %&gt;% 
           ungroup() 
  )

# Salvar base coletada ----------------------------------------------------
saveRDS(pubg_tpp1,&quot;pubg_tpp1.rds&quot;)</code></pre>
<!-- <iframe src="https://giphy.com/embed/3oKIPmaM8aFolCcuI0" width="100%" height="270" frameBorder="0" class="giphy-embed" allowFullScreen></iframe> -->
<div class="col2">
<p>Descri√ß√£o da rotina acima e os crit√©rios para a sele√ß√£o da amostra:</p>
<ol style="list-style-type: decimal">
<li>percorre as 5 bases dispon√≠veis: <code>paste0("agg_match_stats_",0:4,".csv")</code></li>
<li>seleciona partidas em terceira pessoa: <code>match_mode == "tpp"</code></li>
<li>com tamanho da equipe = 1 (individual): <code>party_size == 1</code></li>
<li>do ano de 2018: <code>year(date) == 2018</code></li>
<li>andaram mais que 10 unidades de distancia (metros?): <code>player_dist_walk&gt;10</code></li>
<li>fizeram algum dano (evitar jogadores ausentes): <code>player_dmg != 0</code><br />
</li>
<li>remove colunas n√£o utilizadas na analise</li>
<li>converte do tempo para minutos: <code>player_survive_time := player_survive_time/60</code></li>
<li>agrupa por partida: <code>group_by(match_id)</code></li>
<li>seleciona um jogador de cada partida: <code>do(sample_n(.,1))</code></li>
</ol>
<iframe src="https://giphy.com/embed/g4OqNwXDrnfOcbaaUM" width="240" height="300" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
</div>
<p>Note que apenas um jogador de cada partida √© selecionado na inten√ß√£o de obter independ√™ncia entre observa√ß√µes, isso reduziu drasticamente seu tamanho. Agora que a base j√° foi importada e filtrada, faremos a leitura de 200 linhas aleat√≥rias com a finalidade de diminuir o tempo computacional das opera√ß√µes realizadas em seguida.</p>
<pre class="r"><code>set.seed(1)
pubg_tpp1 &lt;- readRDS(&quot;pubg_tpp1.rds&quot;) %&gt;% sample_n(200)%&gt;% 
  select(-one_of(c(&quot;match_id&quot;, &quot;player_name&quot;)))</code></pre>
<p>Veja a seguir de forma visual como as vari√°veis num√©ricas se correlacionam:</p>
<pre class="r"><code>pubg_tpp1 %&gt;% 
  rev %&gt;% 
  grafico_descritivo()</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-4-1.png" style="width:80.0%" /></p>
</center>
<div id="vari√°vel-resposta" class="section level3">
<h3>Vari√°vel resposta</h3>
<p>Vejamos o que acontece ao analisar o tempo de sobreviv√™ncia de cada jogador</p>
<iframe src="https://giphy.com/embed/3oKIP5KxPss1gjwpG0" width="100%" height="270" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
<p>A seguir, a distribui√ß√£o da vari√°vel resposta <code>player_survive_time</code> :</p>
<pre class="r"><code>plot_grid(pubg_tpp1 %&gt;% 
            ggplot(aes(x=player_survive_time))+
            geom_histogram(aes(y = ..density..), bins = 30, fill=&quot;white&quot;, color=&quot;black&quot;)+
            geom_density(alpha=.2, fill=&quot;white&quot;)+
            scale_x_continuous(labels = scales::comma, limits = c(0,40), breaks = seq(0,40,5))+
            labs(x=&quot;&quot;,y=&quot;&quot;, title = &quot;Tempo de sobreviv√™ncia dos jogadores selecionados&quot;)
          ,
          pubg_tpp1 %&gt;% 
            ggplot(aes(x=&quot; &quot;, y=player_survive_time))+
            geom_boxplot()+
            labs(x=&quot;&quot;)+
            coord_flip()
          ,
          ncol = 1, nrow = 2, align = &quot;v&quot;, rel_heights = c(3,1))</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-6-1.png" style="width:80.0%" /></p>
</center>
<p>Note que possue uma <a href="https://binged.it/2BAYX3s">assimetria positiva</a></p>
</div>
<div id="data-wrangling" class="section level3">
<h3>Data Wrangling</h3>
<p>Primeiramente, vejamos as vari√°veis se relacionam entre si e com a vari√°vel resposta com os coeficientes de correla√ß√£o de Pearson:</p>
<pre class="r"><code># Correlations
pubg_tpp1 %&gt;% 
  select_if(is.numeric) %&gt;% 
  cor() %&gt;% 
  corrplot::corrplot(method = &quot;number&quot;,type = &quot;upper&quot;,diag = F, order = &quot;hclust&quot;,number.cex = 0.7, title = &quot;Correlation correlated numerics&quot;, mar=c(0,0,1,0))</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-7-1.png" style="width:80.0%" /></p>
</center>
<p>√â poss√≠vel notar que apenas a vari√°vel <code>player_assists</code> n√£o correlaciona-se com a vari√°vel resposta nem com as demais vari√°veis e <code>player_dmg</code> e <code>player_kills</code> s√£o fortemente correlacionadas, isso indica que pode ser interessante remover uma delas ou juntar toda essa informa√ß√£o em uma √∫nica vari√°vel, veremos‚Ä¶</p>
<p>Al√©m disso nota-se que a dist√¢ncia percorrida a p√© √© fortemente correlacionada com a vari√°vel resposta enquanto que a dist√¢ncia de quem andou de carro n√£o √© t√£o correlacionada. Uma transforma√ß√£o na vari√°vel <code>player_dist_ride</code> para uma dummy <code>drive</code> indicando se o indiv√≠duo dirigiu ou n√£o pode representar melhor esta informa√ß√£o.</p>
<p>Vejamos algumas caracter√≠sticas peculiares:</p>
<pre class="r"><code>pubg_tpp1 %&gt;% 
  select(player_kills, player_dist_ride, player_assists) %&gt;% 
  map_dfr(~quantile(.x,  probs = seq(0,1,0.25)) %&gt;% round(2)) %&gt;% 
  t  %&gt;% tidy() %&gt;% 
  `colnames&lt;-`(c(&quot;vari√°vel&quot;,percent(seq(0,1,0.25)))) %&gt;% 
  kable2()</code></pre>
<p>Praticamente metade da amostra n√£o registrou abates nem possui marca√ß√£o de <code>player_dist_ride</code>. Como a vari√°vel <code>player_dmg</code> apresentou correla√ß√£o com a vari√°vel resposta <code>player_survive_time</code>, vamos fazer algumas transforma√ß√µes:</p>
<ol style="list-style-type: decimal">
<li>Criar uma vari√°vel dummy <code>drive</code> se jogador usou carro</li>
<li>Somar a <code>player_dist_ride</code> e <code>player_dist_walk</code> em uma √∫nica vari√°vel: <code>player_dist</code></li>
<li>Juntar <code>player_kills</code>, <code>player_dmg</code> e <code>player_assists</code> em uma √∫nica vari√°vel: <code>player_performance</code></li>
</ol>
<div id="player-performance" class="section level4">
<h4>Player performance</h4>
<p>Como criar a vari√°vel <code>player_performance</code>?</p>
<iframe src="https://giphy.com/embed/xT9IgnOQS8e8uKkflK" width="100%" height="270" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
<p>Tentei inventar uma metodologia e com certeza devem existir maneiras mais eficientes de se fazer isso, por√©m, deixa eu explicar o que eu pensei, considere a formula:</p>
<p><span class="math display">\[
Playerperformance = log(WPlayerDmg + WPlayerAssists + WPlayerKills)
\]</span></p>
<p>onde:</p>
<p><span class="math display">\[
WPlayerKills = log(PlayerKills+0.5)\\
WPlayerDmg = log(PlayerDmg)\\
WPlayerAssists = PlayerAssists
\]</span></p>
<p>Note que:</p>
<ul>
<li><span class="math inline">\(WPlayerAssists\)</span>: N√£o √© feita qualquer transforma√ß√£o;</li>
<li><span class="math inline">\(WPlayerDmg\)</span>: A distribui√ß√£o fica ‚Äúquase sim√©trica‚Äù ap√≥s a transforma√ß√£o log;</li>
<li><span class="math inline">\(WPlayerKills\)</span>: adiciona-se 0.5 para poder tirar o log pois podem existir zeros nessa vari√°vel e al√©m disso, quem n√£o marcou abate ser√° penalizado com <span class="math inline">\(-1\)</span> na soma final do score: <code>player_performance</code>.</li>
</ul>
<p>Veja a seguir de forma visual a distribui√ß√£o das vari√°veis que far√£o parte da vari√°vel <code>player_performance</code> na parte de cima e na parte inferior o que acontece ap√≥s sua soma, gerando a nova vari√°vel <code>player_performance</code> :</p>
<pre class="r"><code>performance &lt;- tibble(w_player_kills = log(pubg_tpp1$player_kills+0.5),
                      w_player_dmg = log(pubg_tpp1$player_dmg),
                      w_player_assists = pubg_tpp1$player_assists) %&gt;% 
  mutate(player_performance = log(w_player_dmg + w_player_assists + w_player_kills))

grid.arrange(
  performance %&gt;% 
    select(-player_performance) %&gt;% 
    tidyr::gather() %&gt;% 
    ggplot(aes(x=value))+
    geom_histogram(aes(y = ..density..), bins = 30, fill=&quot;white&quot;, color=&quot;black&quot;)+
    geom_density(alpha=.2, fill=&quot;white&quot;)+
    scale_x_continuous(labels = scales::comma, limits = c(-1.5,8), breaks = seq(-1,8,1))+
    labs(x=&quot;&quot;, y=&quot;&quot;)+
    theme(axis.title.x=element_blank(),
          axis.text.x=element_blank(),
          axis.ticks.x=element_blank())+
    facet_wrap(~key, scales = &quot;free&quot;)
  ,
  performance %&gt;% 
    select(player_performance) %&gt;% 
    tidyr::gather() %&gt;% 
    ggplot(aes(x=value))+
    geom_histogram(aes(y = ..density..), fill=&quot;white&quot;, color=&quot;black&quot;,bins = 15)+
    geom_density(alpha=.2, fill=&quot;white&quot;)+
    scale_x_continuous(limits = c(-1.,2.5), breaks = seq(-1,3,0.5))+
    labs(x=&quot;&quot;, y=&quot;&quot;, title = &quot;performance&quot;),
  ncol=1
)</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-9-1.png" style="width:80.0%" /></p>
</center>
</div>
<div id="transforma√ß√µes-na-base" class="section level4">
<h4>Transforma√ß√µes na base</h4>
<p>A seguir faremos as mudan√ßas diretamente no dataset que estamos trabalhando:</p>
<pre class="r"><code>pubg_tpp1 &lt;- 
  pubg_tpp1 %&gt;% 
  mutate(player_dist = log(player_dist_ride + player_dist_walk)) %&gt;%  
  mutate(player_assists_d = if_else(player_assists ==0, 0, 1)) %&gt;% 
  mutate(player_performance = performance$player_performance )%&gt;% 
  mutate(drive = ifelse(player_dist_ride==0, &quot;no&quot;, &quot;yes&quot;) %&gt;% as.factor()) %&gt;% 
  mutate(player_kills_d = ifelse(player_kills==0, &quot;no&quot;, &quot;yes&quot;) %&gt;% as.factor()) </code></pre>
<p>A manipula√ß√£o acima cria as seguintes vari√°veis:</p>
<ol style="list-style-type: decimal">
<li><code>player_dist</code> como o log da soma de <code>player_dist_ride</code> e <code>player_dist_walk</code></li>
<li><code>player_assists_d</code> como uma dummy: 1 se o jogador deu assist√™ncia; 0 c.c.</li>
<li><code>player_performaec</code> como a combina√ß√£o de <code>player_dmg</code>, <code>player_assists</code> e <code>player_kills</code></li>
<li><code>drive</code> como uma dummy: 1 se o jogador dirigiu; 0 c.c.</li>
<li><code>player_kills_d</code> como uma dummy: 1 se jogador matou algu√©m; 0 c.c.</li>
</ol>
<p>Vejamos como ocorre a distribui√ß√£o das vari√°veis num√©ricas ap√≥s as transforma√ß√µes:</p>
<pre class="r"><code>g1 &lt;- 
  pubg_tpp1 %&gt;% 
  # select_if(~ !length(table(.x))==2 &amp; is.numeric(.x)) %&gt;% colnames() %&gt;% 
  select(player_survive_time,player_performance,player_dist) %&gt;% colnames() %&gt;% 
  map2(c(&quot;Densidade&quot;, &quot;&quot;, &quot;&quot;),
       ~ plot_grid(
         pubg_tpp1 %&gt;% 
           ggplot(aes_string(x=.x)) + 
           geom_histogram(aes(y=..density..),colour=&quot;black&quot;, fill=&quot;white&quot;, bins = 15) +
           geom_density(alpha=.2, fill=&quot;lightgrey&quot;) +
           scale_x_continuous()+
           ggtitle(.x)+
           labs(x=&quot;&quot;, y=.y)+
           theme(axis.title.x=element_blank(),
                 axis.text.x=element_blank(),
                 axis.ticks.x=element_blank())
         ,
         pubg_tpp1 %&gt;% 
           ggplot(aes_string(, y=.x))+
           geom_boxplot(aes(x=&quot; &quot;))+
           labs(x=&quot;&quot;, y=&quot;&quot;)+
           coord_flip()+
           theme(axis.title.x=element_blank(),
                 axis.text.x=element_blank(),
                 axis.ticks.x=element_blank()),
         
         ncol = 1, nrow = 2, align = &quot;v&quot;, rel_heights = c(3,1)
       )
  )

dat &lt;- 
  pubg_tpp1 %&gt;% 
  select_if(~.x %&gt;% table %&gt;% length == 2) %&gt;% 
  mutate_at(2,~if_else(.x==0, &quot;no&quot;, &quot;yes&quot;)) %&gt;% 
  .[,-1]

g2 &lt;- map2(colnames(dat),
           c( &quot;Porcentagem&quot;, &quot;&quot;,&quot;&quot;),
           ~ dat[,.x] %&gt;% 
             tidyr::gather() %&gt;% 
             group_by(key, value) %&gt;% 
             summarise(n = n()) %&gt;% 
             mutate(prop = n/sum(n)) %&gt;% 
             ggplot(aes(x = key, y = prop,fill = value)) + 
             geom_bar(position = &quot;fill&quot;,stat = &quot;identity&quot;, alpha=0.7) +
             scale_y_continuous(labels = percent_format())+
             labs(x=&quot;&quot;, y = .y)+
             scale_fill_manual(values = c(&quot;grey&quot;, &quot;#FCC14B&quot;), name = &quot;Legenda:&quot;)
)

grid.arrange(g1[[1]], g1[[2]], g1[[3]],g2[[1]], g2[[2]], g2[[3]], ncol=3, heights=c(3/5, 2/5))</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-11-1.png" style="width:80.0%" /></p>
</center>
<!-- A distribui√ß√£o dos dados ordenada pela vari√°vel resposta `player_survive_time` : -->
<!-- ```{r} -->
<!-- # Sorted -->
<!-- pubg_tpp1 %>%  -->
<!--   select(player_survive_time, everything()) %>%  -->
<!--   mutate_if(~length(unique(.x))==2, as.factor) %>%  -->
<!--   tabplot::tableplot(sortCol = player_survive_time,decreasing = T) -->
<!-- ``` -->
<p>Apos a transforma√ß√£o a distribui√ß√£o e demais informa√ß√µes dos dados, vejamos novamente a distribui√ß√£o das vari√°veis da amostra com os gr√°ficos de dispers√£o, densidade e correla√ß√µes levando em conta se dirigiu ou n√£o:</p>
<pre class="r"><code>grafico_descritivo(x = pubg_tpp1,
                   colNames = c(&#39;player_survive_time&#39;, &quot;player_performance&quot;, &#39;player_dist&#39;,
                                &#39;player_assists_d&#39;,&quot;player_kills_d&quot;, &#39;drive&#39;),
                   color=&#39;drive&#39;,
                   colors = c(&quot;grey&quot;, &quot;#FCC14B&quot;))</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-13-1.png" style="width:80.0%" /></p>
</center>
<p>O fato do jogador ter dirigido ou n√£o exibiu padr√µes interessantes, pode ser que seja significante no ajuste do modelo final.</p>
</div>
</div>
</div>
<div id="an√°lise-de-sobrevivencia" class="section level1">
<h1>An√°lise de sobrevivencia</h1>
<p>O passo inicial de qualquer an√°lise estat√≠stica consiste em uma descri√ß√£o dos dados e o principal componente da an√°lise descritiva envolvendo dados de tempo de vida √© a fun√ß√£o de sobreviv√™ncia: <span class="math inline">\(S(t) = P(T&gt;t)\)</span>, que determina a probabilidade de um indiv√≠duo sobreviver por mais do que um determinado tempo <span class="math inline">\(t\)</span>, ou por no m√≠nimo um tempo igual a <span class="math inline">\(t\)</span>.</p>
<p>A descri√ß√£o dos dados j√° foi realizada, agora faremos a descri√ß√£o envolvendo a fun√ß√£o de sobreviv√™ncia.</p>
<iframe src="https://giphy.com/embed/xT0xeMrCEGPiU5uw0w" width="100%" height="266" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
<div id="kaplan-meier" class="section level2">
<h2>Kaplan-Meier</h2>
<p>Para isso existem algumas alternativas como o estimador de Kaplan-Meier, que utiliza os conceitos de independ√™ncia e de probabilidade condicional para deduzir a probabilidade de sobreviver at√© o tempo <span class="math inline">\(t\)</span>.</p>
<p>Veja a seguir s√£o ajustados os modelos univariados de Kaplan-Meier para cada uma das coivar√°veis da amostra:</p>
<pre class="r"><code>surv &lt;- Surv(pubg_tpp1$player_survive_time)
resultado_km &lt;-
  list(geral            = survfit(surv ~ 1 ,data = pubg_tpp1),
       player_assists_d = survfit(surv ~ player_assists_d ,data = pubg_tpp1),
       drive            = survfit(surv ~ drive,data = pubg_tpp1 ),
       player_kills_d   = survfit(surv ~ player_kills_d,data = pubg_tpp1))</code></pre>
<p>Veja os resultados da fun√ß√£o de sobreviv√™ncia sem levar em considera√ß√£o nenhuma das coivar√°veis:</p>
<pre class="r"><code>surv_summary(resultado_km[[1]], pubg_tpp1) %&gt;% .[1:5,-ncol(.)] %&gt;% cbind(variable = &quot;Geral&quot;) %&gt;% 
  select(variable, everything())%&gt;%
  kable2()</code></pre>
<p>A fun√ß√£o <code>surv_summary()</code> retorna um quadro de dados com as seguintes colunas:</p>
<ul>
<li>time: o tempo em que a curva tem um passo.</li>
<li>n.risk: o n√∫mero de sujeitos em risco em t.</li>
<li>n.evento: o n√∫mero de eventos que ocorrem no tempo t.</li>
<li>n.censor: n√∫mero de eventos censurados.</li>
<li>surv: estimativa da probabilidade de sobreviv√™ncia.</li>
<li>std.err: erro padr√£o de sobreviv√™ncia.</li>
<li>superior: extremidade superior do intervalo de confian√ßa</li>
<li>inferior: extremidade inferior do intervalo de confian√ßa</li>
<li>estratos: indica a estratifica√ß√£o da estimativa de curvas. Os n√≠veis de estratos (um fator) s√£o os r√≥tulos das curvas (se houver).</li>
</ul>
<div id="log-rank" class="section level3">
<h3>Log-rank</h3>
<p>Al√©m da an√°lise visual das estimativas √© importante comparar as curvas de sobreviv√™ncia com testes de hip√≥teses para obter-se signific√¢ncia estat√≠stica para nossas afirma√ß√µes.</p>
<p>O teste log rank √© um teste n√£o param√©trico, que n√£o faz suposi√ß√µes sobre as distribui√ß√µes de sobreviv√™ncia. Essencialmente, o teste log rank compara o n√∫mero observado de eventos em cada grupo com o que seria esperado se a hip√≥tese nula fosse verdadeira. Considere ent√£o <span class="math inline">\(H_0: S_1(t)=S_2(t)\)</span> para todo <span class="math inline">\(t\)</span> no per√≠odo de acompanhamento (ou seja, se as curvas de sobreviv√™ncia fossem id√™nticas). A estat√≠stica utilizada no teste √© um <span class="math inline">\(T\)</span> com distribui√ß√£o aproximadamente <span class="math inline">\(\chi^2\)</span> com 1 grau de liberdade.</p>
<p>O objeto criado abaixo guarda o valor p para o teste de log-rank de cada em cada um dos modelos:</p>
<pre class="r"><code>resultado_log_rank &lt;- 
  c(geral = &quot;&quot;,
    player_assists_d=round(1-pchisq(survdiff(surv~player_assists_d,data = pubg_tpp1)$chisq,1),5),
    drive=round(1-pchisq(survdiff(surv~drive,data=pubg_tpp1)$chisq,1),5),
    player_kills_d=round(1-pchisq(survdiff(surv~player_kills_d,data=pubg_tpp1)$chisq,1),5)
  )</code></pre>
<p>Os gr√°ficos gerados a partir dos modelos ajustados acima bem como o resultado dos testes de log-rank s√£o exibidos na imagem a seguir:</p>
<pre class="r"><code>survplot &lt;- map2(resultado_km,
                 case_when(resultado_log_rank == &#39;0&#39; ~ &quot;log-rank: \n p &lt; 0,00001&quot;,
                           resultado_log_rank == &quot;&quot; ~ &quot;log-rank n√£o se aplica&quot;,
                           resultado_log_rank != &#39;0&#39; | resultado_log_rank != &#39;&#39; ~ 
                             paste0(&quot;log-rank: \n p =&quot;,as.numeric(resultado_log_rank))),
                 ~ autoplot(.x)+
                   ggtitle(stringr::str_remove_all(names(.x$strata)[1],&quot;(=no|=yes)&quot;))+
                   annotate(&quot;label&quot;,y = 0.20, x = 5,
                            label = .y,
                            size = 4, colour = &quot;red&quot;,hjust=0.1)+ 
                   scale_fill_manual(values = c(&quot;grey&quot;, &quot;#FCC14B&quot;))+
                   scale_color_manual(values = c(&quot;grey&quot;, &quot;#FCC14B&quot;))+
                   theme(legend.position = c(0.85,0.7))+
                   scale_x_continuous(limits = c(0,30), breaks = seq(0,30,5))
                 
)
grid.arrange(survplot[[1]], survplot[[2]] ,survplot[[3]], survplot[[4]], ncol=2)</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-17-1.png" style="width:80.0%" /></p>
</center>
<p>O eixo horizontal (eixo x) representa o tempo em minutos, e o eixo vertical (eixo y) mostra a probabilidade de sobreviv√™ncia ou a propor√ß√£o de jogadores que sobrevivem. As linhas representam curvas de sobreviv√™ncia dos dois grupos.</p>
<p>Uma queda vertical nas curvas indica um evento. No tempo zero, a probabilidade de sobreviv√™ncia √© de 1,0 (ou 100% dos jogadores vivos).</p>
<p>Interpreta√ß√£o: Pelo gr√°fico, aparentemente n√£o existe diferen√ßa no tempo de sobreviv√™ncia com estratifica√ß√£o dos dados de acordo com quem deu assist√™ncia ou n√£o, j√° para o teste que compara igualdade de fun√ß√µes de sobreviv√™ncia das demais vari√°veis, existem evidencias estat√≠sticas para rejeitar a hip√≥tese de que n√£o h√° diferen√ßa na sobrevida entre os dois grupos</p>
</div>
</div>
<div id="fun√ß√£o-de-risco-hazard-ou-taxa-de-falha" class="section level2">
<h2>Fun√ß√£o de risco (hazard) ou taxa de falha</h2>
<p>Fun√ß√£o de risco (hazard) ou taxa de falha √© o risco ‚Äúinstant√¢neo‚Äù denotada por <span class="math inline">\(\lambda(t)\)</span> √© uma taxa, n√£o uma probabilidade e pode assumir qualquer valor real maior que zero.</p>
<p>No exemplo representa a taxa de incid√™ncia ou risco acumulado para um indiv√≠duo morrer at√© o momento <span class="math inline">\(t\)</span>, dado que sobreviveu at√© este momento. √â muito informativa quando comparada com a fun√ß√£o de sobreviv√™ncia pois diferentes <span class="math inline">\(S(t)\)</span> podem ter formas semelhantes, enquanto que respectivas <span class="math inline">\(\lambda(t)\)</span> podem diferir drasticamente.</p>
<pre class="r"><code>survplot &lt;-
  map(resultado_km  ,
      ~ ggsurvplot(.x, conf.int = TRUE, 
                   palette = c(&quot;grey&quot;, &quot;#FCC14B&quot;),
                   risk.table = F,break.time.by = 5,
                   fun = &quot;cumhaz&quot;,title = stringr::str_remove_all(names(.x$strata)[1],&quot;(=no|=yes)&quot;))
  )
arrange_ggsurvplots(survplot, print = TRUE,
                    ncol = 2, nrow = 2)</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-18-1.png" style="width:80.0%" /></p>
</center>
<p>O risco cumulativo <span class="math inline">\(H( t)\)</span> pode ser interpretado como a for√ßa cumulativa da mortalidade.
Em outras palavras, corresponde ao n√∫mero de eventos que seriam esperados para cada indiv√≠duo
pelo tempo t se o evento fosse um processo repetitivo.</p>
</div>
<div id="modelo-de-cox" class="section level2">
<h2>Modelo de cox</h2>
<p>√â caracterizado pela presen√ßa dos coeficientes <span class="math inline">\(\beta\)</span>s que medem os efeitos (semelhantes √† an√°lise de regress√£o log√≠stica m√∫ltipla e linear m√∫ltipla) das vari√°veis explicativas sobre a fun√ß√£o de risco. Em um modelo de regress√£o de riscos proporcionais de Cox, a medida do efeito √© a <em>taxa de risco</em>, que √© o risco de falha, dado que o participante sobreviveu at√© um tempo espec√≠fico.</p>
<p>Algumas das suposi√ß√µes para o correto uso do modelo de regress√£o de riscos proporcionais de Co incluem:</p>
<ul>
<li>independ√™ncia dos tempos de sobreviv√™ncia entre indiv√≠duos distintos na amostra,</li>
<li>rela√ß√£o multiplicativa entre os preditores e o risco,</li>
<li>uma taxa de risco constante ao longo do tempo.</li>
</ul>
<p>O modelo de riscos proporcionais de Cox √© chamado de modelo semi-param√©trico , porque n√£o h√° suposi√ß√µes sobre o formato da fun√ß√£o de risco de linha de base. No entanto, existem outras suposi√ß√µes, como observado acima.</p>
<p>√â poss√≠vel utilizar as estat√≠sticas de Wald, da raz√£o de verossimilhan√ßa e escore para fazer infer√™ncias sobre os par√¢metros do modelo</p>
<p>Veja a seguir a signific√¢ncia dos coeficiente estimado em modelos univariados para cada vari√°vel candidata ao modelo:</p>
<pre class="r"><code># Modelos univariados
covariates    &lt;- c(&quot;player_kills&quot;,&quot;player_dist_ride&quot;,&quot;player_performance&quot;,
                   &quot;player_dist_walk&quot;,&quot;player_dmg&quot;, &quot;player_dist&quot;, 
                   &quot;player_assists_d&quot;,&quot;drive&quot;, &quot;player_kills_d&quot;)
univ_formulas &lt;- map(covariates,~ as.formula(paste(&#39;Surv(player_survive_time) ~&#39;, .x)))
univ_models   &lt;- map( univ_formulas, ~coxph(.x, data = pubg_tpp1))

# estrair resultados 
map2_df(univ_models,
        covariates,
        function(x,y){ 
          x                = summary(x)
          p.value          = signif(x$wald[&quot;pvalue&quot;], digits=2)
          wald.test        = signif(x$wald[&quot;test&quot;], digits=2)
          beta             = signif(x$coef[1], digits=2);#coeficient beta
          HR               = signif(x$coef[2], digits=2);#exp(beta)
          HR.confint.lower = signif(x$conf.int[,&quot;lower .95&quot;], 2)
          HR.confint.upper = signif(x$conf.int[,&quot;upper .95&quot;],2)
          HR               = paste0(HR, &quot; (&quot;, HR.confint.lower, &quot;-&quot;, HR.confint.upper, &quot;)&quot;)
          res              = tibble(y,beta, HR, wald.test, p.value)
          colnames(res)    = c(&quot;covariates&quot;,&quot;beta&quot;, &quot;HR (95% CI for HR)&quot;, &quot;wald.test&quot;, &quot;p.value&quot;)
          res
        }) %&gt;% 
  kable2(linhas = 7)</code></pre>
<p>Modelo de Cox usando uma vari√°vel categ√≥rica retorna uma raz√£o de risco, que, acima de 1 indica uma covari√°vel que est√° positivamente associada √† probabilidade do evento e, portanto, negativamente associada ao tempo de sobrevida. O oposto vale para HR menor que um e HR = 1 indica que a covari√°vel n√£o tem efeito.</p>
<pre class="r"><code>final_model  &lt;- 
  coxph(Surv(player_survive_time) ~ player_performance+player_dist+drive,
        data = pubg_tpp1,x=T,method=&quot;breslow&quot;)

summary(final_model)</code></pre>
<pre><code>## Call:
## coxph(formula = Surv(player_survive_time) ~ player_performance + 
##     player_dist + drive, data = pubg_tpp1, x = T, method = &quot;breslow&quot;)
## 
##   n= 200, number of events= 200 
## 
##                       coef exp(coef) se(coef)       z Pr(&gt;|z|)    
## player_performance -0.7469    0.4738   0.1787  -4.179 2.92e-05 ***
## player_dist        -1.7599    0.1721   0.1150 -15.307  &lt; 2e-16 ***
## driveyes            0.8832    2.4186   0.2091   4.225 2.39e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
##                    exp(coef) exp(-coef) lower .95 upper .95
## player_performance    0.4738     2.1105    0.3338    0.6726
## player_dist           0.1721     5.8117    0.1374    0.2156
## driveyes              2.4186     0.4135    1.6055    3.6434
## 
## Concordance= 0.883  (se = 0.008 )
## Likelihood ratio test= 362.7  on 3 df,   p=&lt;2e-16
## Wald test            = 274  on 3 df,   p=&lt;2e-16
## Score (logrank) test = 407.8  on 3 df,   p=&lt;2e-16</code></pre>
<p>No modelo ajustado note-se que existe uma associa√ß√£o negativa entre <code>player_performance</code> e mortalidade e entre <code>player_dist</code> e mortalidade (ou seja, o risco de morte diminui para jogadores que percorrem maiores dist√¢ncias e possuem melhor performance).</p>
<p>As estimativas dos par√¢metros representam o aumento no log esperado do risco relativo para cada aumento de uma unidade no preditor, mantendo os outros preditores constantes.</p>
<p>Para interpretabilidade, calcularemos as taxas de risco exponenciando das estimativas dos par√¢metros. Para a <code>player_performance</code>, <span class="math inline">\(exp(-0.7469196)= 0.4738239\)</span>. Isso implica que diminui para <span class="math inline">\(47.38\)</span> do valor original do risco esperado em rela√ß√£o a um aumento de uma unidade na performance, mantendo as demais vari√°veis constantes. A interpreta√ß√£o de <code>player_dist</code> em escala logar√≠timica √© feita de maneira semelhante.`</p>
<p>J√° para os jogadores onde <code>drive</code> = 1 (que dirigiram durante a partida) existe uma rela√ß√£o positiva, como <span class="math inline">\(exp(0.8831835)= 2.4185871\)</span>. O risco esperado corresponde √† <span class="math inline">\(2.4185871\)</span> do valor original nos que dirigiram em compara√ß√£o aos que n√£o dirigiram, mantendo as demais vari√°veis constantes.</p>
<pre class="r"><code>map2_df(1:3,final_model$coefficients %&gt;% names(),~
          tibble(
            variable = .y,
            beta             = signif(summary(final_model)$coef[.x,1], digits=2), #coeficient beta
            HR               = signif(summary(final_model)$coef[.x,2], digits=2), #exp(beta)
            HR.confint.lower = signif(summary(final_model)$conf.int[.x,&quot;lower .95&quot;], 2),
            HR.confint.upper = signif(summary(final_model)$conf.int[.x,&quot;upper .95&quot;],2)) %&gt;% 
          mutate(HR= paste0(HR, &quot; (&quot;, HR.confint.lower, &quot;-&quot;, HR.confint.upper, &quot;)&quot;)
          )
) %&gt;% kable2()</code></pre>
<p>Em suma:</p>
<ul>
<li>HR = 1: sem efeito</li>
<li>HR &lt;1: Redu√ß√£o do risco</li>
<li>HR&gt; 1: aumento do risco</li>
</ul>
<iframe src="https://giphy.com/embed/2Us3iTghyffcfeI35h" width="100%" height="200" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
<div id="res√≠duos-de-martingal-e-deviance" class="section level3">
<h3>Res√≠duos de Martingal e Deviance</h3>
<p>Como foi visto, o modelo de regress√£o de riscos proporcionais de Cox faz diversas suposi√ß√µes que precisam ser conferidas ap√≥s o ajuste do modelo para chegar a qualidade de seus resultados pois um modelo mais ajustado pode trazer resultados enganosos e que n√£o fa√ßam sentido algum</p>
<iframe src="https://giphy.com/embed/l0CLSXnSgbYma8EOA" width="100%" height="269" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
<p>Gr√°ficos dos res√≠duos Martingal ou deviance contra os tempos fornecem
uma forma de verificar a adequa√ß√£o do modelo ajustado, bem como
ajudar na detec√ß√£o de observa√ß√µes at√≠picas.</p>
<p><strong>Deviance</strong></p>
<p>Esses res√≠duos, que s√£o uma tentativa de tornar os res√≠duos
Martingal mais sim√©tricos em torno do zero, facilitam, em geral,
a detec√ß√£o de pontos at√≠picos (outliers).
Se o modelo for apropriado, esses res√≠duos devem apresentar um
comportamento aleat√≥rio em torno de zero.</p>
<p><strong>Martingal</strong></p>
<p>Esses res√≠duos s√£o vistos como uma estimativa do numero de falhas em excesso
observada nos dados mas n√£o predito pelo modelo. Os mesmos s√£o usados, em geral,
para examinar a melhor forma funcional (linear, quadr√°tica, etc.)
para uma dada covariavel em um modelo de regress√£o assumido para os dados do estudo.</p>
<pre class="r"><code>res &lt;- 
  tibble(residuo_deviance = resid(final_model,type=&quot;deviance&quot;) ,
         residuo_martingal = resid(final_model,type=&quot;martingal&quot;),
         linear_predictors = final_model$linear.predictors)

# Graficos:
grid.arrange(
  ggplot(res, aes(x=linear_predictors, y=residuo_martingal))+ geom_point()+geom_hline(yintercept=0, color=&#39;coral&#39;)+ylab(&quot;Res√≠duos Martingual&quot;),
  ggplot(res, aes(x=linear_predictors, y=residuo_deviance))+ geom_point()+geom_hline(yintercept=0, color=&#39;coral&#39;)+ylab(&quot;Deviance&quot;),
  ncol=2
)</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-22-1.png" style="width:80.0%" /></p>
</center>
<p>Interpreta√ß√£o:</p>
<ul>
<li><strong>Martingal</strong>: Parecido com deviance mais acentuado;</li>
<li><strong>Deviance</strong>: Modelo n√£o eh tao ruim assim, se fosse um modelo linear talvez dever√≠amos tomar cuidado.</li>
</ul>
<div id="residuos-de-schoenfeld" class="section level4">
<h4>Residuos de Schoenfeld</h4>
<p>Em princ√≠pio, os res√≠duos de Schoenfeld s√£o independentes do tempo.
Um gr√°fico que mostra um padr√£o n√£o aleat√≥rio contra o tempo √©
evid√™ncia de viola√ß√£o da suposi√ß√£o de hip√≥tese.</p>
<p>Para testar a suposi√ß√£o de riscos proporcionais:</p>
<pre class="r"><code>final_model %&gt;% cox.zph %&gt;% ggcoxzph</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-23-1.png" style="width:80.0%" /></p>
</center>
<p>A partir da inspe√ß√£o gr√°fica, n√£o h√° padr√£o com o tempo.
A suposi√ß√£o de riscos proporcionais parece ser suportada
pelas covari√°veis</p>
</div>
</div>
</div>
<div id="considera√ß√µes-finais" class="section level2">
<h2>Considera√ß√µes finais</h2>
<iframe src="https://giphy.com/embed/ZacieLN2WI2AedWrz9" width="100%" height="216" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
<p>Como era de se esperar, o risco de ser abatido diminui para jogadores que possuem melhor performance e tamb√©m para os jogadores que percorrem maiores dist√¢ncias (o que mostra que ficar parado no jogo em uma zona pode n√£o ser a melhor ideia, j√° √© quanto mais se movimenta maior a quantidade de itens que podem ser coletados).</p>
<p>Interessante notar que a curva de <strong>sobreviv√™ncia</strong> para os jogadores que dirigiram apresenta resultado oposto ao <strong>risco</strong> esperado nos que dirigiram, isso ocorre pois esses dois modelos calculam medidas diferentes.</p>
</div>
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias</h1>
<ul>
<li>Carvalho,M.A., Andreozzi,V.L., Codec¬∏o,C.T., Campos,D.P., Barbosa,M.T.S., Shimakura,S.E., An√°lise de sobreviv√™ncia: Teoria e aplica√ß√µes em sa√∫de, Segunda Edi√ß√£o, Editora FIOCRUZ, Rio de Janeiro, 2011.</li>
<li>Colosimo,E.A., Giolo,S.R., An√°lise de sobreviv√™ncia aplicada, ABE-Projeto Fisher, S√£o Paulo, 2010</li>
<li>Lewis,E.E., Introduction to reliability engineering, John Wiley, New York, 1987</li>
<li><a href="http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Survival/BS704_Survival6.html" class="uri">http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Survival/BS704_Survival6.html</a></li>
<li><a href="http://www.sthda.com/english/wiki/cox-model-assumptions" class="uri">http://www.sthda.com/english/wiki/cox-model-assumptions</a></li>
</ul>
<p>Cuiriosidades / Leituras futuras:</p>
<ul>
<li>Evaluating Random Forests for Survival Analysis Using Prediction Error Curves: <a href="https://www.jstatsoft.org/article/view/v050i11" class="uri">https://www.jstatsoft.org/article/view/v050i11</a></li>
<li>randomForestSRC: <a href="https://cran.r-project.org/web/packages/randomForestSRC/index.html" class="uri">https://cran.r-project.org/web/packages/randomForestSRC/index.html</a></li>
<li>WTTE-RNN - Less hacky churn prediction: <a href="https://ragulpr.github.io/2016/12/22/WTTE-RNN-Hackless-churn-modeling/" class="uri">https://ragulpr.github.io/2016/12/22/WTTE-RNN-Hackless-churn-modeling/</a></li>
<li>Weibull Time To Event Recurrent Neural Network: <a href="https://github.com/ragulpr/wtte-rnn/" class="uri">https://github.com/ragulpr/wtte-rnn/</a></li>
<li>Neural Networks as Statistical Methods in Survival Analysis: <a href="https://www.stats.ox.ac.uk/pub/bdr/NNSM.pdf" class="uri">https://www.stats.ox.ac.uk/pub/bdr/NNSM.pdf</a></li>
<li>Continuous and Discrete Time Survival Analysis: Neural Network
Approaches: <a href="http://pcwww.liv.ac.uk/~afgt/eleuteri_lyon07.pdf" class="uri">http://pcwww.liv.ac.uk/~afgt/eleuteri_lyon07.pdf</a></li>
<li>Cox Proportional Hazards Model - h2O Documentation: <a href="http://s3.amazonaws.com/h2o-release/h2o/master/1579/docs-website/datascience/coxph.html" class="uri">http://s3.amazonaws.com/h2o-release/h2o/master/1579/docs-website/datascience/coxph.html</a></li>
<li>Introduction to H2OCoxPH: <a href="https://www.slideshare.net/0xdata/introduction-to-h2ocoxph" class="uri">https://www.slideshare.net/0xdata/introduction-to-h2ocoxph</a></li>
</ul>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle/">An√°lise de sobreviv√™ncia com dados do jogo PUBG dispon√≠veis no Kaggle</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">analise-de-sobrevivencia</category>
      <category domain="tag">data-mining</category>
      <category domain="tag">estatistica</category>
      <category domain="tag">gamificacao</category>
      <category domain="tag">gamification</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">modelagem-estatistica</category>
      <category domain="tag">r</category>
      <category domain="tag">survivor</category>
    </item>
    <item>
      <title>Um estudo sobre modelos de aprendizagem baseados em √°rvores com desafio do Kaggle</title>
      <link>https://gomesfellipe.github.io/post/2018-08-31-modelos-em-arvore/modelos-em-arvore/</link>
      <pubDate>Fri, 31 Aug 2018 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2018-08-31-modelos-em-arvore/modelos-em-arvore/</guid>
      <description>Um estudo aplicado de modelos de aprendizagem baseados em √°rvores utilizando a base de dados do Kaggle para prever o pre√ßo final de casas residenciais em Ames, Iowa, utilizando uma variedade de aspectos</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="kaggle" class="section level1">
<h1>Kaggle</h1>
<p>Segundo o <a href="https://en.wikipedia.org/wiki/Kaggle">Wikip√©dia</a>: ‚ÄúKaggle √© a maior comunidade mundial de cientistas de dados e machine learning.‚Äù Aprendo muito estudando as resolu√ß√µes de alguns competidores pois l√° √© poss√≠vel conferir tanto as metodologias utilizadas pelos competidores quando os c√≥digos e √© not√°vel o cuidado dos participantes para que seja poss√≠vel a reprodutibilidade dos resultados, o que pode impulsionar o aprendizado.</p>
<p>O Kaggle trabalha com a ideia de <a href="https://en.wikipedia.org/wiki/Gamification">gamifica√ß√£o</a>, que √© um assunto do qual j√° escrevi em um post sobre <a href="https://gomesfellipe.github.io/post/2018-02-17-cheatsheet-gamificacao-r/cheatsheet-gamificacao-r/">gamifica√ß√£o e porque aprender R √© t√£o divertido</a> e gosto deste conceito de se criar jogos para motivar e engajar as pessoas em atividades profissionais e a ideia de se estar em um jogo possibilita doses de motiva√ß√£o especialmente a quem gosta de competir.</p>
<p>A plataforma √© focada em competi√ß√µes que envolvem modelagem preditiva, que julgam apenas o seu desempenho preditivo, embora a inteligibilidade n√£o deixe de ser importante. Neste post farei tamb√©m a modelagem descritiva com modelos de aprendizagem baseados em √°rvores, na qual o principal objetivo ser√° obter informa√ß√µes sobre os dados para o ajuste dos modelos preditivos que iremos submeter √† competi√ß√£o do Kaggle <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/">House Prices: Advanced Regression Techniques</a>.</p>
<p>A diferen√ßa entre modelos preditivos e descritivos n√£o √© t√£o rigorosa assim pois algumas das t√©cnicas podem ser utilizadas para ambos e geralmente um modelo pode servir para ambos os prop√≥sitos (mesmo que de de forma insuficiente).</p>
<p>Al√©m dos modelos de machine learning baseados em √°rvores, tamb√©m ser√° ajustado um modelo de regress√£o linear multivariado para compararmos os resultados dos ajustes e submeter nossas previs√µes no site do <a href="https://kaggle.com">kaggle</a>.</p>
<p>Os pacotes que ser√£o utilizados ser√£o os seguintes:</p>
<pre class="r"><code>library(purrr)       # Programacao funciona
library(broom)       # Arrumar outputs
library(dplyr)       # Manipulacao de dados
library(magrittr)    # pipes
library(funModeling) # df_status()
library(plyr)        # revalue()
library(gridExtra)   # Juntar ggplots
library(reshape)     # funcao melt()
library(rpart)       # Arvore de Decisoes
library(rpart.plot)  # Plot da Arvore de Decisoes
library(data.table)  # aux na manipulacao do heatmap
library(readr)       # Leitura da base de dados
library(stringr)     # Manipulacao de strings
library(ggplot2)     # Graficos elegantes
library(caret)       # Machine Learning 
library(GGally)      # up ggplot
library(ggfortify)   # autoplot()</code></pre>
<div id="base-de-dados" class="section level2">
<h2>Base de dados</h2>
<p>A base de dados deste post vem de uma competi√ß√£o √≥tima para estudantes de ci√™ncia de dados de dados com alguma experi√™ncia com R ou Python e no√ß√µes b√°sicas de machine learning e estat√≠stica.</p>
<p>Pode ser √∫til para aqueles que desejam expandir seu conjunto de habilidades em uma tarefa de regress√£o, quando a vari√°vel <span class="math inline">\(y\)</span> que desejamos estimar √© do tipo num√©rico (cont√≠nuo ou discreto).</p>
<p>Trata-se do <a href="https://ww2.amstat.org/publications/jse/v19n3/decock.pdf">conjunto de dados Ames Housing</a> que foi compilado por Dean De Cock para uso em educa√ß√£o de ci√™ncia de dados.</p>
<pre class="r"><code>train &lt;- read_csv(&quot;train.csv&quot;)
test  &lt;- read_csv(&quot;test.csv&quot;)
full  &lt;- bind_rows(train, test)

id    &lt;- test$Id
full %&lt;&gt;% select(-Id)</code></pre>
<div id="descri√ß√£o-da-competi√ß√£o" class="section level3">
<h3>Descri√ß√£o da Competi√ß√£o</h3>
<p>Traduzido do site oficial do kaggle:</p>
<p>"Pe√ßa a um comprador que descreva a casa dos seus sonhos, e eles provavelmente n√£o come√ßar√£o com a altura do teto do por√£o ou a proximidade de uma ferrovia leste-oeste. Mas o conjunto de dados desta competi√ß√£o de playground prova que muito mais influencia as negocia√ß√µes de pre√ßo do que o n√∫mero de quartos ou uma cerca branca.</p>
<p>Com 79 vari√°veis explicativas descrevendo (quase) todos os aspectos de casas residenciais em Ames, Iowa, esta competi√ß√£o desafia voc√™ a prever o pre√ßo final de cada casa."</p>
<p>Portanto, primeiramente vamos entender o comportamento da vari√°vel resposta, depois buscar quais dessas 79 vari√°veis explicativas s√£o mais importantes para representar a varia√ß√£o do pre√ßo de venda das casas atrav√©s dos m√©todos baseados em √°rvores e por fim ajustar os modelos propostos e submeter nossas estimativas no site!</p>
</div>
</div>
</div>
<div id="an√°lise-explorat√≥ria-dos-dados" class="section level1">
<h1>An√°lise explorat√≥ria dos dados</h1>
<p>Antes de pensar em ajustar algum modelo √© extremamente necess√°rio entender como se comportam os dados, portanto, tanto a vari√°vel resposta quanto as vari√°veis explicativas ser√£o avaliadas.</p>
<div id="vari√°vel-resposta" class="section level2">
<h2>Vari√°vel resposta:</h2>
<p><code>SalePrice</code> - o pre√ßo de venda da propriedade em d√≥lares. Essa √© a vari√°vel de destino que estamos tentando prever.</p>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Note que a distribui√ß√£o dos dados referentes ao pre√ßo de venda se distribui de maneira assim√©trica e n√£o possuem evid√™ncias de normalidade dos dados. Apesar dos m√©todos baseados em √°rvore se tratarem de t√©cnicas n√£o param√©tricas essa transforma√ß√£o ser√° feita pois ao final deste post desejo comparar os resultados com um modelo de regress√£o linear m√∫ltipla.</p>
</div>
</div>
<div id="√°rvore-de-decis√£o" class="section level1">
<h1>√Årvore de decis√£o</h1>
<p>Uma t√©cnica muito popular que √© mais comumente usada para resolver tarefas de classifica√ß√£o de dados por√©m a √°rvore conhecida como <a href="https://tinyurl.com/ybhlsgom">CART (Classification and Regression Trees)(Breiman, 1986)</a> lida com todos os tipos de atributos (incluindo atributos num√©ricos que s√£o tratados a partir da cria√ß√£o de intervalos). Para seu ajuste √© poss√≠vel realizar podas e produzir √°rvores bin√°rias.</p>
<p>A constru√ß√£o da √°rvore √© realizada por meio do algoritmo que iterativamente analisa os atributos descritivos de um conjunto de dados previamente rotulado. Sua popularidade como apoio para a tomada de decis√£o se deve principalmente ao fato da f√°cil visualiza√ß√£o do conhecimento gerado e o f√°cil entendimento.</p>
<p>Outra caracter√≠stica legal da √°rvore de decis√µes √© que ela permite ajustar um modelo sem um pr√©-processamento detalhado, pois √© f√°cil de ajustar, aceita valores faltantes e √© de f√°cil interpreta√ß√£o, veja:</p>
<pre class="r"><code>library(rpart)

control &lt;- rpart.control(minsplit =10, # o n√∫mero m√≠nimo de observa√ß√µes em um n√≥
                         cp = 0.006    # parametro de complexidade q controla o tamanho da arvore
)
rpartFit &lt;- rpart(exp(SalePrice) ~ . , train, method = &quot;anova&quot;, control = control) 

rpart.plot::rpart.plot(rpartFit,cex = 0.6)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-5-1.png" width="1200" /></p>
<p>No topo, vemos o primeiro n√≥ com 100% das observa√ß√µes, que representa o total da base (100%). Em seguida, vemos que a primeira vari√°vel que determina o pre√ßo de venda das casas <code>SalePrice</code> √© a vari√°vel <code>OverallQual</code>. As casas que apresentaram <code>OverallQual</code> &lt; 7.5 ocorrem em maior propor√ß√£o do que as que tiveram <code>OverallQual</code>&gt;7.5. A interpreta√ß√£o pode continuar dessa forma recursivamente.</p>
<p>√â poss√≠vel notar que as vari√°veis <code>OverallQual</code>,<code>Neighborhood</code>,<code>1stFlrSF</code>,<code>2ndFlrSF</code>,<code>GrLivArea</code>, <code>BsmtFinSF1</code> foram as que melhor representaram os dados de acordo com os par√¢metros que determinamos para ajustar esta √°rvore, vejamos com mais detalhes se existe rela√ß√£o linear e intensidade e dire√ß√£o dessa rela√ß√£o com o <a href="https://pt.wikipedia.org/wiki/Coeficiente_de_correla%C3%A7%C3%A3o_de_Pearson">coeficiente de correla√ß√£o de Pearson</a> entre estas vari√°veis dois a dois e em rela√ß√£o √† vari√°vel resposta:</p>
<pre class="r"><code>devtools::source_url(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/correlations_for_ggpairs.R&quot;)

train %&gt;% 
  select(SalePrice,OverallQual,`1stFlrSF`,`2ndFlrSF`,GrLivArea,BsmtFinSF1) %&gt;% 
  ggpairs(lower = list(continuous = my_fn))+
  theme_bw()</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Com esta figura temos muitas informa√ß√µes, destaca-se que todas essas vari√°veis possuem algum tipo de rela√ß√£o linear com a vari√°vel resposta, a menor correla√ß√£o observada foi com o <code>BsmtFinSF1</code> e a vari√°vel que apresentou a maior correla√ß√£o foi a <code>OverallQual</code>. Aten√ß√£o para a correla√ß√£o entre <code>SalePrice</code> e <code>OverallQual</code>, pois <code>Overallqual</code> parece ser uma vari√°vel ordinal e uma outra medida de correla√ß√£o que melhor representaria esta rela√ß√£o √© o <a href="https://pt.wikipedia.org/wiki/Coeficiente_de_correla%C3%A7%C3%A3o_de_postos_de_Spearman">coeficiente de correla√ß√£o de Spearman</a>, veja:</p>
<pre class="r"><code>cor(full$SalePrice, full$OverallQual, method = &quot;spearman&quot;, use = &quot;complete.obs&quot;)</code></pre>
<pre><code>## [1] 0.8098286</code></pre>
<p>Um pouco diferente do resultado da correla√ß√£o de Pearson pois avalia rela√ß√µes lineares, j√° a correla√ß√£o de Spearman avalia rela√ß√µes mon√≥tonas, sejam elas lineares ou n√£o.</p>
<div id="an√°lise-explorat√≥ria-e-input-de-nas" class="section level2 tabset">
<h2>An√°lise explorat√≥ria e input de <code>NA</code>s</h2>
<p>Arrumar a base de dados √© uma tarefa longa e que geralmente consome grande parte no tempo em um projeto de ci√™ncia de dados. N√£o adianta usar o algor√≠timo mais poderoso de machine learning se a base de dados n√£o estiver arrumada de maneira que possibilite a an√°lise dos dados.</p>
<p>Para obter informa√ß√µes da amostra, confira no <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data">link do dataset da competi√ß√£o no Kaggle</a>. Na p√°gina √© poss√≠vel conferir <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/download/data_description.txt">a descri√ß√£o da amostra</a> e nela nota-se que alguns dos valores faltantes possuem significado, ent√£o √© necess√°rio rotul√°-los para que o R possa interpretar estes valores da maneira correta.</p>
<div id="status-da-amostra" class="section level3">
<h3>Status da amostra</h3>
<p>Conferindo o status da amostra com a fun√ß√£o <code>df_status()</code> do pacote <a href="https://cran.r-project.org/web/packages/funModeling/index.html"><code>funModeling</code></a>:</p>
<pre class="r"><code>full %&gt;% 
  df_status(print_results = F) %&gt;% 
  as_tibble() %&gt;%
  arrange(-p_na, -p_zeros)</code></pre>
<pre><code>## # A tibble: 80 x 9
##    variable     q_zeros p_zeros  q_na  p_na q_inf p_inf type      unique
##    &lt;chr&gt;          &lt;int&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;int&gt;
##  1 PoolQC             0       0  2909 99.7      0     0 character      3
##  2 MiscFeature        0       0  2814 96.4      0     0 character      4
##  3 Alley              0       0  2721 93.2      0     0 character      2
##  4 Fence              0       0  2348 80.4      0     0 character      4
##  5 SalePrice          0       0  1459 50.0      0     0 numeric      663
##  6 FireplaceQu        0       0  1420 48.6      0     0 character      5
##  7 LotFrontage        0       0   486 16.6      0     0 numeric      128
##  8 GarageYrBlt        0       0   159  5.45     0     0 numeric      103
##  9 GarageFinish       0       0   159  5.45     0     0 character      3
## 10 GarageQual         0       0   159  5.45     0     0 character      5
## # ‚Ä¶ with 70 more rows</code></pre>
<p>Note que as vari√°veis problem√°ticas foram ordenadas de forma decrescente (maior n√∫mero de dados faltantes e zeros) vamos tratar uma de cada vez partindo da vari√°vel mais cr√≠tica</p>
</div>
<div id="pool" class="section level3">
<h3>Pool</h3>
<ul>
<li><code>PoolQC</code> √© a vari√°vel que possui mais <code>NA</code> e a descri√ß√£o da base informa que:</li>
</ul>
<p><code>PoolQC</code>: qualidade da piscina</p>
<ul>
<li>Ex Excelente</li>
<li>Gd Good</li>
<li>TA M√©dia / T√≠pica</li>
<li>Fa Pequena</li>
<li>NA sem piscina</li>
</ul>
<p>√â poss√≠vel observar que se trata de uma vari√°vel ordinal, portanto vamos criar uma vari√°vel auxiliar (pois esta descri√ß√£o se repete em outras vari√°veis):</p>
<pre class="r"><code># Criando vari√°vel auxilar ordinal
Qualidade &lt;- c(&#39;None&#39; = 0, &#39;Po&#39; = 1, &#39;Fa&#39; = 2, &#39;TA&#39; = 3, &#39;Gd&#39; = 4, &#39;Ex&#39; = 5)

full %&lt;&gt;%
  mutate(PoolQC =  ifelse(PoolQC %&gt;% is.na, &quot;None&quot;, PoolQC) %&gt;% as.factor() ) %&gt;% 
  mutate(PoolQC = as.integer(revalue(PoolQC, Qualidade)))</code></pre>
<p>Al√©m disso, existe outra vari√°vel relacionada √† piscina, veja:</p>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Pool&quot;)]) %&gt;% 
  table </code></pre>
<pre><code>##         PoolQC
## PoolArea    1    2    3    4
##      0      0    0    0 2906
##      144    1    0    0    0
##      228    1    0    0    0
##      368    0    0    0    1
##      444    0    0    0    1
##      480    0    0    1    0
##      512    1    0    0    0
##      519    0    1    0    0
##      555    1    0    0    0
##      561    0    0    0    1
##      576    0    0    1    0
##      648    0    1    0    0
##      738    0    0    1    0
##      800    0    0    1    0</code></pre>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Pool&quot;)]) %&gt;%
  map(~sum(is.na(.x)))</code></pre>
<pre><code>## $PoolArea
## [1] 0
## 
## $PoolQC
## [1] 0</code></pre>
<pre class="r"><code># Arrumando inconsist√´ncias:
full %&lt;&gt;% 
  mutate(PoolQC = ifelse(PoolQC == 0 &amp; PoolArea !=0, 2, PoolQC))

# Arrumando inconsist√´ncias:
full %&lt;&gt;% 
  mutate(Pool = ifelse(PoolQC == 0 &amp; PoolArea ==0, &quot;no&quot;, &quot;yes&quot;))</code></pre>
</div>
<div id="misc" class="section level3">
<h3>Misc</h3>
<p>Se referem aos recursos diversos</p>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Misc&quot;)],
         SalePrice
  ) %&gt;%
  map(~sum(is.na(.x)))</code></pre>
<pre><code>## $MiscFeature
## [1] 2814
## 
## $MiscVal
## [1] 0
## 
## $SalePrice
## [1] 1459</code></pre>
<p><code>MiscFeature</code>: recurso diverso n√£o coberto em outras categorias</p>
<ul>
<li>Elevador elev</li>
<li>Gar2 2nd Garage (se n√£o for descrito na se√ß√£o de garagem)</li>
<li>Othr Outro</li>
<li>Galp√£o derramado (mais de 100 SF)</li>
<li>TenC Campo de t√©nis</li>
<li>NA Nenhum</li>
</ul>
<p>Desta vez n√£o se trata de uma vari√°vel ordinal, vejamos:</p>
<pre class="r"><code>full %&lt;&gt;%
  mutate(MiscFeature =  if_else(MiscFeature %&gt;% is.na, &quot;None&quot;, MiscFeature) %&gt;% as.factor) 

# Breve resumo:
g1 &lt;- 
  full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Misc&quot;)], SalePrice) %&gt;% 
  ggplot(aes(y=MiscVal,x= reorder(MiscFeature, -MiscVal,FUN = median) ,fill=MiscFeature))+
  geom_boxplot()+
  theme_bw() +
  scale_fill_viridis_d() +
  labs(x = &quot;Recurso Diverso&quot;)

g2 &lt;- 
  full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Misc&quot;)], SalePrice) %&gt;% 
  ggplot(aes(y=SalePrice,x= reorder(MiscFeature, -MiscVal,FUN = median) ,fill=MiscFeature))+
  geom_boxplot()+
  theme_bw() +
  scale_fill_viridis_d() +
  labs(x = &quot;Pre√ßo de Venda&quot;)

grid.arrange(g1, g2)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<pre class="r"><code>rm(g1,g2)</code></pre>
<p>Al√©m disso, <code>MiscVal</code>: Valor do recurso variado</p>
</div>
<div id="alley" class="section level3">
<h3>Alley</h3>
<p><code>Alley</code>: Tipo de acesso ao beco para a propriedade</p>
<ul>
<li>Grvl Cascalho</li>
<li>Pave pavimentado</li>
<li>NA Nenhum acesso de beco</li>
</ul>
<p>Basta realizar o input:</p>
<pre class="r"><code>full %&lt;&gt;% 
  mutate(Alley = Alley %&gt;% str_replace_na(&quot;None&quot;)) %&gt;% 
  mutate(Alley = as.factor(Alley))</code></pre>
<pre class="r"><code>full[!is.na(full$SalePrice),] %&gt;% 
  select(Alley, SalePrice) %&gt;% 
  ggplot(aes(y=SalePrice,x= reorder(Alley, -SalePrice,FUN = median) ,fill=Alley))+
  geom_boxplot()+
  theme_bw() +
  scale_fill_viridis_d() +
  labs(x = &quot;tipo de Acesso&quot;)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
</div>
<div id="fence" class="section level3">
<h3>Fence</h3>
<p><code>Fence</code>: qualidade da cerca</p>
<ul>
<li>GdPrv Boa privacidade</li>
<li>MnPrv minima privacidade</li>
<li>GdWo boa madeira</li>
<li>MnWw M√≠nima Madeira / Fio</li>
<li>NA Sem cerca</li>
</ul>
<p>Input ser√° da seguinte forma:</p>
<pre class="r"><code>full %&lt;&gt;% 
  mutate(Fence = Fence %&gt;% str_replace_na(&quot;None&quot;))</code></pre>
<pre class="r"><code>full[1:nrow(train),] %&gt;% 
  select(Fence, SalePrice) %&gt;% 
  ggplot(aes(y=SalePrice,x= reorder(Fence, -SalePrice, median) ,fill=Fence))+
  geom_boxplot()+
  theme_bw() +
  scale_fill_viridis_d() +
  labs(x = &quot;tipo de Acesso&quot;)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<pre class="r"><code>full %&lt;&gt;% mutate(Fence = as.factor(Fence))</code></pre>
<p>Aparentemente n√£o parece existir uma rela√ß√£o ordinal sobre o tipo de cerca quanto ao pre;o de venda da casa, portanto foi convertida para fator</p>
</div>
<div id="fireplace" class="section level3">
<h3>FirePlace</h3>
<p>Vari√°veis relacionadas com lareira. Segundo a descri√ß√£o, temos:</p>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Fireplace&quot;)], SalePrice)</code></pre>
<pre><code>## # A tibble: 2,919 x 3
##    Fireplaces FireplaceQu SalePrice
##         &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;
##  1          0 &lt;NA&gt;             12.2
##  2          1 TA               12.1
##  3          1 TA               12.3
##  4          1 Gd               11.8
##  5          1 TA               12.4
##  6          0 &lt;NA&gt;             11.9
##  7          1 Gd               12.6
##  8          2 TA               12.2
##  9          2 TA               11.8
## 10          2 TA               11.7
## # ‚Ä¶ with 2,909 more rows</code></pre>
<p><code>Fireplaces</code>: Numero de lareiras</p>
<p><code>FireplaceQu</code>: Qualidade da lareira</p>
<ul>
<li>Ex Excellente - Excepcional Lareira de Alvenaria</li>
<li>Gd Boa - Lareira de alvenaria no n√≠vel principal</li>
<li>TA M√©dia - lareira pr√©-fabricada na sala principal ou Lareira de alvenaria no por√£o</li>
<li>Fa Pequena - Lareira pr√©-fabricada no por√£o</li>
<li>Po Pobre - Fog√£o Ben Franklin</li>
<li>NA sem lareira</li>
</ul>
<p>Nota-se que se trata de uma vari√°vel ordinal de acordo com a qualidade, portanto:</p>
<pre class="r"><code>full %&lt;&gt;% 
  mutate(FireplaceQu =  if_else(FireplaceQu %&gt;% is.na, &quot;None&quot;, FireplaceQu) ) %&gt;% 
  mutate(FireplaceQu = as.integer(revalue(FireplaceQu, Qualidade)))</code></pre>
<p>Conferindo se existem inconsist√™ncias:</p>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Fireplace&quot;)]) %&gt;% 
  table </code></pre>
<pre><code>##           FireplaceQu
## Fireplaces    0    1    2    3    4    5
##          0 1420    0    0    0    0    0
##          1    0   46   63  495  627   37
##          2    0    0   10   92  112    5
##          3    0    0    1    4    5    1
##          4    0    0    0    1    0    0</code></pre>
</div>
<div id="lot" class="section level3">
<h3>Lot</h3>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Lot&quot;)], SalePrice) %&gt;% 
  map_dbl(~sum(is.na(.x)))</code></pre>
<pre><code>## LotFrontage     LotArea    LotShape   LotConfig   SalePrice 
##         486           0           0           0        1459</code></pre>
<p>Segundo a descri√ß√£o:</p>
<p><code>LotFrontage</code>: Ruas linearmente conectadas √† propriedade</p>
<p><code>LotArea</code> : Tamanho do lote em p√©s quadrados</p>
<p><code>LotShape</code>: forma geral da propriedade</p>
<ul>
<li>Regue Regular<br />
</li>
<li>IR1 ligeiramente irregular</li>
<li>IR2 moderadamente irregular</li>
<li>IR3 Irregular</li>
</ul>
<p><code>LotConfig</code>: configura√ß√£o de lote</p>
<ul>
<li>Inside Lote muito para dentro</li>
<li>Corner Canto de esquina</li>
<li>CulDSac Cul-de-sac</li>
<li>FR2 Frente em 2 lados da propriedade</li>
<li>FR3 Frente em 3 lados da propriedade</li>
</ul>
<p>Input para o <code>LotFrontage</code> ser√° feito considerando a configura√ß√£o do lote, veja:</p>
<pre class="r"><code>inputsLot &lt;- full %&gt;% 
  select(LotFrontage, LotConfig) %&gt;% 
  group_by(LotConfig) %&gt;%
  dplyr::summarise(Media = mean(LotFrontage, na.rm = T),
            Mediana = median(LotFrontage, na.rm = T))

full$LotFrontage[is.na(full$LotFrontage) &amp; full$LotConfig == inputsLot$LotConfig[1]] &lt;- inputsLot$Mediana[1] 
full$LotFrontage[is.na(full$LotFrontage) &amp; full$LotConfig == inputsLot$LotConfig[2]] &lt;- inputsLot$Mediana[2] 
full$LotFrontage[is.na(full$LotFrontage) &amp; full$LotConfig == inputsLot$LotConfig[3]] &lt;- inputsLot$Mediana[3] 
full$LotFrontage[is.na(full$LotFrontage) &amp; full$LotConfig == inputsLot$LotConfig[4]] &lt;- inputsLot$Mediana[4] 
full$LotFrontage[is.na(full$LotFrontage) &amp; full$LotConfig == inputsLot$LotConfig[5]] &lt;- inputsLot$Mediana[5] </code></pre>
<p>Arrumando vari√°veis nominais e ordinais:</p>
<pre class="r"><code>full %&lt;&gt;% 
  mutate(LotShape = as.integer(revalue(full$LotShape, c(&#39;IR3&#39;=0, &#39;IR2&#39;=1, &#39;IR1&#39;=2, &#39;Reg&#39;=3))))</code></pre>
</div>
<div id="garages" class="section level3">
<h3>Garages</h3>
<p>Vari√°veis relacionadas, segundo a descri√ß√£o, temos:</p>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Garage&quot;)], SalePrice) %&gt;% 
  map_dbl(~sum(is.na(.x)))</code></pre>
<pre><code>##   GarageType  GarageYrBlt GarageFinish   GarageCars   GarageArea   GarageQual 
##          157          159          159            1            1          159 
##   GarageCond    SalePrice 
##          159         1459</code></pre>
<p><code>GarageType</code>: localiza√ß√£o da garagem</p>
<ul>
<li>2Types Mais de um tipo de garagem</li>
<li>Attchd anexa a casa</li>
<li>Basement tipo porao</li>
<li>BuiltIn (garagem parte da casa - normalmente tem sala acima da garagem)</li>
<li>CarPort Porta do carro</li>
<li>Detchd nao anexa a casa</li>
<li>NA Sem Garagem</li>
</ul>
<p><code>GarageYrBlt</code>: garagem do ano foi constru√≠da</p>
<p><code>GarageFinish</code>: acabamento interior da garagem</p>
<ul>
<li>Fin Finished</li>
<li>RFn √Åspero Finalizado<br />
</li>
<li>Unf inacabado</li>
<li>NA Sem Garagem</li>
</ul>
<p><code>GarageCars</code>: Tamanho da garagem na capacidade do carro</p>
<p><code>GarageArea</code>: Tamanho da garagem em p√©s quadrados</p>
<p><code>GarageQual</code>: GarageQuality</p>
<ul>
<li>Ex Excelente</li>
<li>Gd Good</li>
<li>TA T√≠pico / M√©dio</li>
<li>FA Justo</li>
<li>Po Poor</li>
<li>NA Sem Garagem</li>
</ul>
<p><code>GarageCond</code>: condi√ß√£o de garagem</p>
<ul>
<li>Ex Excelente</li>
<li>Gd Good</li>
<li>TA T√≠pico / M√©dio</li>
<li>Fa Justo</li>
<li>Po Poor</li>
<li>NA Sem Garagem</li>
</ul>
<pre class="r"><code>full %&lt;&gt;% 
  mutate(GarageType   =  if_else(GarageType %&gt;% is.na, &quot;None&quot;, GarageType) ) %&gt;% 
  mutate(GarageYrBlt  = if_else(GarageYrBlt %&gt;% is.na,YearBuilt, GarageYrBlt) ) %&gt;% 
  mutate(GarageFinish =  if_else(GarageFinish %&gt;% is.na, &quot;None&quot;, GarageFinish) ) %&gt;% 
  mutate(GarageFinish = as.integer(revalue(GarageFinish, c(&#39;None&#39;=0, &#39;Unf&#39;=1, &#39;RFn&#39;=2, &#39;Fin&#39;=3)))) %&gt;% 
  mutate(GarageCars   = ifelse(GarageCars %&gt;% is.na, 0, GarageCars) ) %&gt;% 
  mutate(GarageArea   = ifelse(GarageArea %&gt;% is.na, 0, GarageArea)) %&gt;% 
  mutate(GarageQual   = if_else(GarageQual %&gt;% is.na, &quot;None&quot;, GarageQual)) %&gt;% 
  mutate(GarageQual   = as.integer(revalue(GarageQual, Qualidade))) %&gt;% 
  mutate(GarageCond   = if_else(GarageCond %&gt;% is.na, &quot;None&quot;, GarageCond)) %&gt;% 
  mutate(GarageCond   = as.integer(revalue(GarageCond, Qualidade))) 
  
table(full$GarageCond)</code></pre>
<pre><code>## 
##    0    1    2    3    4    5 
##  159   14   74 2654   15    3</code></pre>
</div>
<div id="bsmt" class="section level3">
<h3>Bsmt</h3>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Bsmt&quot;)], SalePrice) %&gt;% 
  map_dbl(~sum(is.na(.x)))</code></pre>
<pre><code>##     BsmtQual     BsmtCond BsmtExposure BsmtFinType1   BsmtFinSF1 BsmtFinType2 
##           81           82           82           79            1           80 
##   BsmtFinSF2    BsmtUnfSF  TotalBsmtSF BsmtFullBath BsmtHalfBath    SalePrice 
##            1            1            1            2            2         1459</code></pre>
<p><code>BsmtQual</code>: Avalia a altura do por√£o</p>
<ul>
<li>Ex Excelente (100+ polegadas)<br />
</li>
<li>Gd Bom (90-99 polegadas)</li>
<li>TA T√≠pica (80-89 polegadas)</li>
<li>Fa Justo (70-79 polegadas)</li>
<li>Po Pobre (&lt;70 polegadas</li>
<li>NA Sem Por√£o</li>
</ul>
<p><code>BsmtCond</code>: Avalia o estado geral do por√£o</p>
<ul>
<li>Ex Excelente</li>
<li>Gd Bom</li>
<li>TA T√≠pica - umidade ligeira permitida</li>
<li>Fa Razo√°vel - umidade ou alguma rachadura ou sedimenta√ß√£o</li>
<li>Po Insuficiente - Craqueamento severo, sedimenta√ß√£o ou umidade</li>
<li>NA Sem Por√£o</li>
</ul>
<p><code>BsmtExposure</code>: Refere-se a paralisa√ß√µes ou paredes no n√≠vel do jardim</p>
<ul>
<li>Gd Good Exposi√ß√£o</li>
<li>Av M√©dia Exposi√ß√£o (n√≠veis divididos ou foyers normalmente pontua√ß√£o m√©dia ou acima)<br />
</li>
<li>Mn Exposi√ß√£o M√≠nima</li>
<li>No N√£o Exposi√ß√£o</li>
<li>NA Sem por√£o</li>
</ul>
<p><code>BsmtFinType1</code>: Avalia√ß√£o da √°rea acabada do por√£o</p>
<ul>
<li>GLQ Bons Viver</li>
<li>ALQ M√©dia Living Quarters</li>
<li>BLQ Abaixo da m√©dia Living Quarters<br />
</li>
<li>Rec M√©dia Rec Room</li>
<li>LwQ Baixa Qualidade</li>
<li>Unf unfinshed</li>
<li>NA nenhum por√£o</li>
</ul>
<p><code>BsmtFinSF1</code>: pes quadrados do tipo 1 terminado</p>
<p><code>BsmtFinType2</code>: Avalia√ß√£o do por√£o √°rea terminado (se v√°rios tipos)</p>
<ul>
<li>GLQ Bons aposentos</li>
<li>ALQ Medianos</li>
<li>BLQ abaixo da media</li>
<li>Rec Aposentos m√©dia qualidade</li>
<li>LwQ Baixa Qualidade</li>
<li>Unf</li>
<li>N√£o Sem Por√£o</li>
</ul>
<p><code>BsmtFinSF2</code>: P√©s quadrados acabados do Tipo 2</p>
<p><code>BsmtUnfSF</code>: P√©s quadrados inacabados da √°rea do por√£o</p>
<p><code>TotalBsmtSF</code>: Total p√©s quadrados da √°rea do por√£o</p>
<p>Input das vari√°veis n√£o num√©ricas com <code>None</code> e convertendo para ordinal as vari√°veis com rela√ß√£o de ordem. Para os faltantes das vari√°veis num√©ricas foram imputados o valor 0 (zeros).</p>
<pre class="r"><code># Categ√≥ricos:
full[,names(full)[names(full) %&gt;% str_detect(&quot;Bsmt&quot;)]] &lt;- 
  full[,names(full)[names(full) %&gt;% str_detect(&quot;Bsmt&quot;)]] %&gt;%
  select(names(full)[names(full) %&gt;% str_detect(&quot;Bsmt&quot;)]) %&gt;%
  mutate_if( ~ !is.numeric(.x) , ~ ifelse(is.na(.x), &quot;None&quot;, .x)) %&gt;% 
  mutate(BsmtQual = as.integer(revalue(BsmtQual, Qualidade))) %&gt;% 
  mutate(BsmtCond = as.integer(revalue(BsmtCond, Qualidade))) %&gt;% 
  mutate(BsmtExposure = as.integer(revalue(BsmtExposure, c(&#39;None&#39;=0, &#39;No&#39;=1, &#39;Mn&#39;=2, &#39;Av&#39;=3, &#39;Gd&#39;=4)))) %&gt;% 
  mutate(BsmtFinType1 = as.integer(revalue(BsmtFinType1,c(&#39;None&#39;=0, &#39;Unf&#39;=1, &#39;LwQ&#39;=2, &#39;Rec&#39;=3, &#39;BLQ&#39;=4, &#39;ALQ&#39;=5, &#39;GLQ&#39;=6)))) 

# Num√©ricos:
full[,names(full)[names(full) %&gt;% str_detect(&quot;Bsmt&quot;)]] &lt;- 
  full[,names(full)[names(full) %&gt;% str_detect(&quot;Bsmt&quot;)]] %&gt;%
  select(names(full)[names(full) %&gt;% str_detect(&quot;Bsmt&quot;)]) %&gt;%
  mutate_if( ~ is.numeric(.x) , ~ ifelse(is.na(.x), 0, .x))</code></pre>
</div>
<div id="masvnr" class="section level3">
<h3>MasVnr</h3>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;MasVnr&quot;)], SalePrice) %&gt;% 
  map_dbl(~sum(is.na(.x)))</code></pre>
<pre><code>## MasVnrType MasVnrArea  SalePrice 
##         24         23       1459</code></pre>
<p><code>MasVnrType</code>: Alvenaria tipo de verniz</p>
<ul>
<li>BrkCmn Brick Common</li>
<li>BrkFace Face de tijolos</li>
<li>CBlock Bloco cinza</li>
<li>None Nenhum</li>
<li>Stone Pedra</li>
</ul>
<p><code>MasVnrArea</code>: √Årea de folheado de alvenaria em p√©s quadrados</p>
<pre class="r"><code>full %&lt;&gt;% 
  mutate(MasVnrType = if_else(is.na(MasVnrType), &quot;None&quot;, MasVnrType)) %&gt;% 
  mutate(MasVnrType = as.integer(revalue(MasVnrType, c(&#39;None&#39;=0, &#39;BrkCmn&#39;=0, &#39;BrkFace&#39;=1, &#39;Stone&#39;=2)))) %&gt;% 
  mutate(MasVnrArea = if_else(is.na(MasVnrArea), 0, 1))</code></pre>
</div>
<div id="vari√°veis-restantes-com-poucos-na" class="section level3">
<h3>Vari√°veis restantes com poucos <code>NA</code></h3>
<p>A estrat√©gia adotada para imputar estes dados ser√° tomada de maneira arbitr√°ria. Os valores faltantes ser√£o preenchidos com o valor comum mais frequente daquela vari√°vel. As vari√°veis que restam s√£o:</p>
<pre class="r"><code>full %&gt;% 
  df_status(print_results = F) %&gt;% 
  as_tibble() %&gt;%
  arrange(-p_na, -p_zeros)</code></pre>
<pre><code>## # A tibble: 81 x 9
##    variable    q_zeros p_zeros  q_na  p_na q_inf p_inf type      unique
##    &lt;chr&gt;         &lt;int&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;int&gt;
##  1 SalePrice         0     0    1459 50.0      0     0 numeric      663
##  2 MSZoning          0     0       4  0.14     0     0 character      5
##  3 Utilities         0     0       2  0.07     0     0 character      2
##  4 Functional        0     0       2  0.07     0     0 character      7
##  5 Exterior1st       0     0       1  0.03     0     0 character     15
##  6 Exterior2nd       0     0       1  0.03     0     0 character     16
##  7 Electrical        0     0       1  0.03     0     0 character      5
##  8 KitchenQual       0     0       1  0.03     0     0 character      4
##  9 SaleType          0     0       1  0.03     0     0 character      9
## 10 PoolArea       2906    99.6     0  0        0     0 numeric       14
## # ‚Ä¶ with 71 more rows</code></pre>
<p>Vejamos:</p>
<p><code>MSZoning</code>: Identifica a classifica√ß√£o geral de zoneamento da venda.</p>
<ul>
<li>Ser√° convertida para fator, vari√°vel nominal</li>
</ul>
<p><code>KitchenQual</code>: Qualidade da cozinha</p>
<ul>
<li>Ser√° convertida para ordinal</li>
</ul>
<p><code>Utilities</code>: Tipo de utilidade dispon√≠vel</p>
<ul>
<li>Ser√° removida</li>
</ul>
<p><code>Functional</code>: Funcionalidade dom√©stica</p>
<ul>
<li>Ser√° considerada como ordinal</li>
</ul>
<p><code>Exterior1st</code>: revestimento Exterior em casa</p>
<ul>
<li>Convertida para fator, vari√°vel nominal</li>
</ul>
<p><code>Electrical</code>: Sistema el√©trico</p>
<ul>
<li>Convertida para fator, vari√°vel nominal</li>
</ul>
<p><code>SaleType</code>: Tipo de venda</p>
<ul>
<li>Convertida para fator, vari√°vel nominal</li>
</ul>
<pre class="r"><code>full &lt;- full %&gt;% 
  mutate(MSZoning    = ifelse(is.na(MSZoning),
                            full$MSZoning %&gt;% table %&gt;% sort %&gt;% names %&gt;% last, MSZoning)) %&gt;% 
  mutate(MSZoning    = as.factor(MSZoning)) %&gt;% 
  mutate(KitchenQual = ifelse(is.na(KitchenQual),
                            full$KitchenQual %&gt;% 
                              table %&gt;% sort %&gt;% names %&gt;% last, KitchenQual)) %&gt;% 
  mutate(KitchenQual = as.integer(revalue(as.character(full$KitchenQual), Qualidade))) %&gt;% 
  select(-Utilities) %&gt;% 
  mutate(Exterior1st = ifelse(is.na(Exterior1st),
                            full$Exterior1st %&gt;% table %&gt;% sort %&gt;% names %&gt;% last, Exterior1st)) %&gt;% 
  mutate(Exterior1st = as.factor(Exterior1st)) %&gt;% 
  mutate(Exterior2nd = ifelse(is.na(Exterior2nd),
                            full$Exterior2nd %&gt;% table %&gt;% sort %&gt;% names %&gt;% last, Exterior2nd)) %&gt;% 
  mutate(Exterior2nd = as.factor(Exterior2nd)) %&gt;% 
  mutate(Electrical  = ifelse(is.na(Electrical),
                            full$Electrical %&gt;% table %&gt;% sort %&gt;% names %&gt;% last, Electrical)) %&gt;% 
  mutate(Electrical  = as.factor(Electrical)) %&gt;% 
  mutate(SaleType    = ifelse(is.na(SaleType ),
                            full$SaleType  %&gt;% table %&gt;% sort %&gt;% names %&gt;% last, SaleType )) %&gt;% 
  mutate(SaleType    = as.factor(SaleType )) 


full[is.na(full$Functional),&quot;Functional&quot;] &lt;- full$Functional %&gt;% table %&gt;% sort %&gt;% names %&gt;% last
full$Functional = as.integer(revalue(full$Functional, c(&#39;Sal&#39;=0, &#39;Sev&#39;=1, &#39;Maj2&#39;=2, &#39;Maj1&#39;=3, &#39;Mod&#39;=4, &#39;Min2&#39;=5, &#39;Min1&#39;=6, &#39;Typ&#39;=7)))
full[is.na(full$KitchenQual),&quot;KitchenQual&quot;] &lt;- full$KitchenQual %&gt;% table %&gt;% sort %&gt;% names %&gt;% last %&gt;% as.numeric()
full$KitchenQual = as.integer(revalue(as.character(full$KitchenQual), Qualidade))
# full[is.na(full$Electrical),&quot;Electrical&quot;] &lt;- 3

to_remove &lt;- full %&gt;% map(~table(.x) %&gt;% length()) %&gt;% .[.== 1] %&gt;% names()
full &lt;- full %&gt;% select(-one_of(to_remove))</code></pre>
<p>Status da base no momento:</p>
<pre class="r"><code>full %&gt;% 
  df_status(print_results = F) %&gt;% 
  as_tibble() %&gt;%
  arrange(-p_na,-p_zeros, type)</code></pre>
<pre><code>## # A tibble: 79 x 9
##    variable      q_zeros p_zeros  q_na  p_na q_inf p_inf type    unique
##    &lt;chr&gt;           &lt;int&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt;
##  1 SalePrice           0     0    1459  50.0     0     0 numeric    663
##  2 PoolArea         2906    99.6     0   0       0     0 numeric     14
##  3 3SsnPorch        2882    98.7     0   0       0     0 numeric     31
##  4 LowQualFinSF     2879    98.6     0   0       0     0 numeric     36
##  5 MiscVal          2816    96.5     0   0       0     0 numeric     38
##  6 BsmtHalfBath     2744    94       0   0       0     0 numeric      3
##  7 ScreenPorch      2663    91.2     0   0       0     0 numeric    121
##  8 BsmtFinSF2       2572    88.1     0   0       0     0 numeric    272
##  9 EnclosedPorch    2460    84.3     0   0       0     0 numeric    183
## 10 HalfBath         1834    62.8     0   0       0     0 numeric      3
## # ‚Ä¶ with 69 more rows</code></pre>
<p>Transformando o <code>character</code> para <code>factor</code>:</p>
<pre class="r"><code>full %&lt;&gt;% mutate_if(is.character, as.factor)</code></pre>
<p>Transformando novamente nossa base de treino e de teste:</p>
<pre class="r"><code>train &lt;- full[1:nrow(train),] %&gt;% as.data.frame() 
test  &lt;- full[(nrow(train)+1):nrow(full),] %&gt;% select(-SalePrice) %&gt;% as.data.frame()

# # Input Missing
# train_miss_model = preProcess(train, &quot;knnImpute&quot;)
# train = predict(train_miss_model, train)
# test = predict(train_miss_model, test)
# 
# train$SalePrice &lt;- y</code></pre>
</div>
</div>
</div>
<div id="machine-learning-com-algor√≠tmos-de-aprendizagem-baseados-em-√°rvores" class="section level1">
<h1>Machine Learning com algor√≠tmos de aprendizagem baseados em √°rvores</h1>
<p>Os m√©todos baseados em √°rvores fornecem modelos preditivos de alta precis√£o, estabilidade e facilidade de interpreta√ß√£o. Ao contr√°rio dos modelos lineares, eles s√£o capazes de lidar bem com rela√ß√µes n√£o-lineares al√©m de poderem ser adaptados para resolver tanto problemas de classifica√ß√£o quanto problemas de regress√£o.</p>
<p>Algoritmos como √°rvores de decis√£o, random forest e ‚Äúgradient boosting‚Äù est√£o sendo muito usados em todos os tipos de problemas de data science e √© not√°vel o uso desses algor√≠timos para resolver os desafios do <a href="https://www.kaggle.com/">Kaggle</a>. Para resolver este problema utilizaremos estes tr√™s algoritmos e ao final, pegando carona na sele√ß√£o de vari√°veis para os algoritmos de √°rvore, ser√° ajustado um modelo de regress√£o linear para compararmos e conferirmos a signific√¢ncia estat√≠stica de cada uma das vari√°veis.</p>
<div id="varimp-com-random-forest" class="section level2">
<h2>VarImp com Random Forest</h2>
<p>Um dos benef√≠cios da floresta aleat√≥ria √© o poder de lidar com grande conjunto de dados com maior dimensionalidade e identificar as vari√°veis a import√¢ncia das vari√°veis, que pode ser uma caracter√≠stica muito √∫til por√©m deve ser feita com cautela.</p>
<p>Veja uma reflex√£o (traduzida) da <a href="https://www.stat.berkeley.edu/~breiman/RandomForests/reg_philosophy.htm">nota de Leo Breiman (Universidade da Calif√≥rnia em Berkeley)</a></p>
<blockquote>
<p>‚ÄúUma nota filos√≥fica: RF √© um exemplo de uma ferramenta que √© √∫til para fazer an√°lises de dados cient√≠ficos; Mas os algoritmos mais inteligentes n√£o substituem a intelig√™ncia humana e o conhecimento dos dados do problema; Pegue a sa√≠da de florestas aleat√≥rias n√£o como verdade absoluta, mas como suposi√ß√µes geradas por um computador inteligente que podem ser √∫teis para levar a uma compreens√£o mais profunda do problema.‚Äù</p>
</blockquote>
<p>O ajuste da √°rvore ser√° feito com o pacote <code>caret</code> e o estudo de estimativas de erro foi definido como o <a href="https://en.wikipedia.org/wiki/Out-of-bag_error">Out of bag</a> que remove a necessidade de um conjunto de teste pois √© o erro m√©dio de previs√£o em cada amostra de treinamento <span class="math inline">\(x_i\)</span> , usando apenas as √°rvores que n√£o tinham <span class="math inline">\(x_i\)</span> em sua amostra de <a href="https://www.ime.usp.br/~chang/home/mae5704/aula-bootstrap.pdf">bootstrap</a>.</p>
<pre class="r"><code>set.seed(1)
control &lt;- trainControl(method = &quot;oob&quot;,verboseIter = F)

rfFit1 &lt;- train(SalePrice ~. ,
      data=train,
      method=&quot;rf&quot;,
      metric = &quot;Rsquared&quot;,
      trControl = control,
      preProcess = c(&quot;knnImpute&quot;)
      )

randomForest::varImpPlot(rfFit1$finalModel)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<pre class="r"><code>rfFit1$finalModel$importance %&gt;% 
  as.data.frame %&gt;%
  mutate(row = rownames(.)) %&gt;% 
  arrange(desc(IncNodePurity)) %&gt;% 
  as_tibble()</code></pre>
<pre><code>## # A tibble: 217 x 2
##    IncNodePurity row        
##            &lt;dbl&gt; &lt;chr&gt;      
##  1         77.9  OverallQual
##  2         35.0  GrLivArea  
##  3         14.8  YearBuilt  
##  4         11.5  KitchenQual
##  5          9.75 TotalBsmtSF
##  6          9.29 GarageCars 
##  7          6.74 `1stFlrSF` 
##  8          6.33 GarageArea 
##  9          5.02 ExterQualTA
## 10          4.04 BsmtFinSF1 
## # ‚Ä¶ with 207 more rows</code></pre>
<p>Ap√≥s inspecionar a import√¢ncia das vari√°veis vamos selecionar as seguintes vari√°veis:</p>
<pre class="r"><code>full %&lt;&gt;% 
  select(
    SalePrice  , Neighborhood, OverallQual , GrLivArea   , YearBuilt   ,  KitchenQual, 
    GarageCars ,  GarageArea , `1stFlrSF`  , ExterQual   , BsmtFinSF1  , FireplaceQu, 
    BsmtQual   , `2ndFlrSF`  , CentralAir  , GarageFinish, YearRemodAdd, FullBath, 
    GarageYrBlt, Fireplaces  , LotFrontage , BsmtUnfSF   , TotalBsmtSF , BsmtFinType1,
    OpenPorchSF, GarageType  , BsmtExposure, OverallCond , TotalBsmtSF , LotArea
  )</code></pre>
<p>Portanto, vamos definir novamente o conjunto de dados de treino e de teste:</p>
<pre class="r"><code>train &lt;- full[1:nrow(train),] %&gt;% as.data.frame()
test  &lt;- full[(nrow(train)+1):nrow(full),-1] %&gt;% as.data.frame()</code></pre>
</div>
<div id="vari√°veis-num√©ricas" class="section level2">
<h2>Vari√°veis num√©ricas</h2>
<p>Ap√≥s a sele√ß√£o dessas vari√°veis, vamos entender como elas est√£o correlacionadas dois a dois com o <a href="https://pt.wikipedia.org/wiki/Coeficiente_de_correla%C3%A7%C3%A3o_de_Pearson">coeficiente de correla√ß√£o de pearson</a>, exibindo a matrix em um <a href="https://en.wikipedia.org/wiki/Heat_map">Heatmap</a> (ou mapa de calor ), que √© uma representa√ß√£o gr√°fica de dados em que os valores individuais contidos em uma matriz representados como cores.</p>
<pre class="r"><code>cormat &lt;- 
  full %&gt;% 
  select(SalePrice, everything()) %&gt;% 
  select_if(is.numeric) %&gt;% 
  as.data.frame() %&gt;% 
  cor(use = &quot;na.or.complete&quot;) %&gt;% 
  melt

cormat %&gt;%   
  ggplot( aes(reorder(Var1,value), reorder(Var2,value), fill=value))+
  geom_tile(color=&quot;white&quot;)+
  scale_fill_gradient2(low=&quot;blue&quot;, high=&quot;red&quot;, mid=&quot;white&quot;, midpoint=0, limit=c(-1,1), space=&quot;Lab&quot;, name=&quot;Pearson\nCorrelation&quot;)+
  theme_bw()+
  theme(axis.text.x=element_text(angle=45, vjust=1, size=10, hjust=1))+
  coord_fixed()+
  labs(x=&quot;&quot;,y=&quot;&quot;)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-37-1.png" width="1152" /></p>
<p>√â poss√≠vel notar que existem vari√°veis explicativas correlacionadas o que indica que a presen√ßa de algumas vari√°veis pode possivelmente interferir no ajuste final do modelo linear multivariado.</p>
</div>
<div id="vari√°veis-categ√≥ricas" class="section level2">
<h2>Vari√°veis categ√≥ricas</h2>
<p>J√° a rela√ß√£o das var√°veis categ√≥ricas n√£o podem ser calculada com o coeficiente de correla√ß√£o calculado anteriormente, para avaliar como elas est√£o associadas ser√° calculado a medida de associa√ß√£o <a href="https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V">V de Cram√©r</a>. Novamente a matrix dos resultados ser√£o novamente apresentados em um <a href="https://en.wikipedia.org/wiki/Heat_map">Heatmap</a> (ou mapa de calor ) que foi inspirado <a href="http://analysingstuffs.xyz/2017/12/01/visualizing-the-correlations-between-categorical-variables-with-r-a-cramers-v-heatmap/">neste post</a> (neste post tamb√©m √© apresentada uma fun√ß√£o para o c√°lculo da matrix, adaptei de forma que se tornasse mais geral e disponibilizei no meu github <a href="https://github.com/gomesfellipe/functions/blob/master/interaction_all.R">neste link</a>).</p>
<pre class="r"><code># Carrega funcao que calcula o V de Cramer:
devtools::source_url(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/cv_test.R&quot;)
# Carrega a funcao que realiza as intera√ß√µes dos calculos dois a dois:
devtools::source_url(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/interaction_all.R&quot;)</code></pre>
<p>Veja:</p>
<pre class="r"><code>cvmat &lt;- 
train %&gt;%
  select_if(~!is.numeric(.x)) %&gt;% 
  as.data.table() %&gt;%
  interaction_all(cv_test) %&gt;% 
  as_tibble() 

cvmat %&gt;% 
  ggplot( aes(variable_x, variable_y, fill=v_cramer))+
  geom_tile(color=&quot;white&quot;)+
  scale_fill_gradient2(low=&quot;blue&quot;, high=&quot;red&quot;, mid=&quot;white&quot;, midpoint=0, limit=c(-1,1), space=&quot;Lab&quot;, name=&quot;Cramer&#39;s V&quot;)+
  theme_bw()+
  theme(axis.text.x=element_text(angle=45, vjust=1, size=10, hjust=1))+
  coord_fixed()+
  labs(x=&quot;&quot;,y=&quot;&quot;)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
</div>
</div>
<div id="ajustando-modelos" class="section level1">
<h1>Ajustando modelos</h1>
<div id="arvore-de-decisao" class="section level2">
<h2>Arvore de decisao</h2>
<p>O modelo de √°rvore de decis√£o j√° foi comentado e deixei algumas refer√™ncias ao final do post portanto vejamos a seguir o ajusto no R. Segundo a <a href="https://cran.r-project.org/web/packages/rpart/rpart.pdf">documenta√ß√£o</a>:</p>
<p><code>cp</code>: par√¢metro de complexidade. No nosso caso isso significa que o <a href="https://pt.wikipedia.org/wiki/R%C2%B2"><span class="math inline">\(R^2\)</span></a> total deve aumentar em cp em cada etapa. O principal papel desse par√¢metro √© economizar tempo de computa√ß√£o removendo as divis√µes que obviamente n√£o valem a pena. Essencialmente, informamos ao programa que qualquer divis√£o que n√£o melhore o ajuste por <code>cp</code> provavelmente ser√° eliminada por <a href="https://pt.wikipedia.org/wiki/Valida%C3%A7%C3%A3o_cruzada">valida√ß√£o cruzada</a>, e que, portanto, o programa n√£o precisa busc√°-la.</p>
<p>Para pesquisa de grade existem duas maneiras de ajustar um algoritmo no pacote <code>caret</code>: permitir que o sistema fa√ßa isso automaticamente ou especificar o <code>tuneGride</code> manualmente onde cada par√¢metro do algoritmo pode ser especificado como um vetor de valores poss√≠veis. Confira o ajuste manual em R:</p>
<pre class="r"><code>set.seed(1)

control &lt;- trainControl(method = &quot;cv&quot;, number = 5,verboseIter = F)

tunegrid &lt;- expand.grid(cp=seq(0.001, 0.01, 0.001))

rpartFit2 &lt;- 
  train(y=train$SalePrice, x=train[,-1],
        method=&quot;rpart&quot;,
        trControl=control,
        tuneGrid=tunegrid,
        metric = &quot;Rsquared&quot;
  )
rpartFit2</code></pre>
<pre><code>## CART 
## 
## 1460 samples
##   28 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 1169, 1169, 1168, 1168, 1166 
## Resampling results across tuning parameters:
## 
##   cp     RMSE       Rsquared   MAE      
##   0.001  0.1918932  0.7757730  0.1386651
##   0.002  0.1943654  0.7690391  0.1410967
##   0.003  0.2016485  0.7513005  0.1457213
##   0.004  0.2029596  0.7462748  0.1457752
##   0.005  0.2098812  0.7279462  0.1534384
##   0.006  0.2090073  0.7291130  0.1539830
##   0.007  0.2110066  0.7227211  0.1544402
##   0.008  0.2120734  0.7198280  0.1555415
##   0.009  0.2142488  0.7143975  0.1570535
##   0.010  0.2148236  0.7126454  0.1575360
## 
## Rsquared was used to select the optimal model using the largest value.
## The final value used for the model was cp = 0.001.</code></pre>
<p>Podemos conferir os resultados novamente de maneira visual:</p>
<pre class="r"><code>rpart.plot(rpartFit2$finalModel, cex = 0.5)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-41-1.png" width="1200" /></p>
<p>Gerando arquivo para submiss√£o no kaggle:</p>
<pre class="r"><code>id %&gt;% cbind(predict(rpartFit2, test) %&gt;% exp) %&gt;% 
  `colnames&lt;-`(c(&quot;Id&quot;, &quot;SalePrice&quot;)) %&gt;%
  write.csv(&quot;rpartFit2.csv&quot;,row.names = F)</code></pre>
</div>
<div id="bagging" class="section level2">
<h2>Bagging</h2>
<p><a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating">‚ÄúBagging‚Äù</a> √© usado quando desejamos reduzir a varia√ß√£o de uma √°rvore de decis√£o. Ela combina o resultado de v√°rios modelos onde todas as vari√°veis s√£o considerados para divis√£o um n√≥. Em R:</p>
<pre class="r"><code>set.seed(1)

control &lt;- trainControl(method = &quot;cv&quot;, number = 5,verboseIter = F)

treebagFit &lt;- train(y=train$SalePrice, 
                    x=train[,-1], 
                    method = &quot;treebag&quot;,
                    metric = &quot;Rsquared&quot;,
                    trControl=control
)
treebagFit</code></pre>
<pre><code>## Bagged CART 
## 
## 1460 samples
##   28 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 1169, 1169, 1168, 1168, 1166 
## Resampling results:
## 
##   RMSE       Rsquared   MAE      
##   0.1831872  0.7946059  0.1288626</code></pre>
<p>Note que o <span class="math inline">\(R^2\)</span> aumentou e o <a href="https://en.wikipedia.org/wiki/Root-mean-square_deviation"><span class="math inline">\(RMSE\)</span></a> diminuiu ap√≥s o uso desta t√©cnica.</p>
<p>Resultados para enviar para o Kaggle:</p>
<pre class="r"><code>id %&gt;% cbind(predict(treebagFit, test)%&gt;% exp) %&gt;% 
  `colnames&lt;-`(c(&quot;Id&quot;, &quot;SalePrice&quot;)) %&gt;%
  write.csv(&quot;treebagFit.csv&quot;,row.names = F)</code></pre>
</div>
<div id="random-forest" class="section level2">
<h2>Random Forest</h2>
<p>A principal diferen√ßa entre ‚Äúbagging‚Äù e o algoritmo Random Forest √© que em <code>randomForest</code>, apenas um subconjunto de caracter√≠sticas √© selecionado aleatoriamente em cada divis√£o em uma √°rvore de decis√£o enquanto que no bagging todos os recursos s√£o usados.</p>
<p>Para pesquisa de grade especificaremos um vetor com os poss√≠veis valores, <a href="https://cran.r-project.org/web/packages/randomForest/randomForest.pdf">pois o default adotado para o par√¢metro</a> <code>mtry</code> √© <code>mtry</code> = p/3 (N√∫mero de vari√°veis amostradas aleatoriamente como candidatos em cada divis√£o), onde p √© o n√∫mero de vari√°veis e pode ser que o modelo se ajuste melhor aos dados ao utilizar outro valor.</p>
<p>Veja:</p>
<pre class="r"><code>set.seed(1)

tunegrid &lt;- expand.grid(mtry = seq(4, ncol(train) * 0.8, 2))

control &lt;- trainControl(method = &quot;cv&quot;, number = 5,verboseIter = F)

rfFit &lt;- train(SalePrice ~. ,
               data=train,
               method=&quot;rf&quot;,
               metric = &quot;Rsquared&quot;,
               tuneGrid=tunegrid,
               trControl=control
)
rfFit</code></pre>
<pre><code>## Random Forest 
## 
## 1460 samples
##   28 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 1169, 1169, 1168, 1168, 1166 
## Resampling results across tuning parameters:
## 
##   mtry  RMSE       Rsquared   MAE       
##    4    0.1455656  0.8781772  0.09755474
##    6    0.1417368  0.8817193  0.09435674
##    8    0.1405084  0.8826370  0.09350712
##   10    0.1395367  0.8834153  0.09290816
##   12    0.1385338  0.8845102  0.09181049
##   14    0.1386865  0.8840165  0.09223527
##   16    0.1381776  0.8846283  0.09155563
##   18    0.1384532  0.8837305  0.09222536
##   20    0.1380863  0.8840803  0.09173754
##   22    0.1383788  0.8835938  0.09189772
## 
## Rsquared was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 16.</code></pre>
<p>Note que o <span class="math inline">\(R^2\)</span> aumentou e o <span class="math inline">\(RMSE\)</span> apresentou resultados ainda mais satisfat√≥rios.</p>
<p>Veja visualmente a import√¢ncia de ada vari√°vel:</p>
<pre class="r"><code>randomForest::varImpPlot(rfFit$finalModel)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
<p>Resultados para enviar para o Kaggle:</p>
<pre class="r"><code>id %&gt;% cbind(predict(rfFit, test) %&gt;% exp) %&gt;% 
  `colnames&lt;-`(c(&quot;Id&quot;, &quot;SalePrice&quot;)) %&gt;%
  write.csv(&quot;rfFit.csv&quot;,row.names = F) </code></pre>
</div>
<div id="gbm" class="section level2">
<h2>GBM</h2>
<p>Diferentemente do ‚Äúbagging‚Äù, o ‚Äúboosting‚Äù √© uma t√©cnica de ensemble (conjunto) na qual os preditores n√£o s√£o feitos independentemente, mas sequencialmente. Na imagem a seguir √© poss√≠vel ver uma representa√ß√£o visual dessa diferen√ßa:</p>
<p><img src="https://cdn-images-1.medium.com/max/1600/1*PaXJ8HCYE9r2MgiZ32TQ2A.png" /></p>
<p>A imagem foi obtida <a href="https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d">neste artigo: Gradient Boosting from scratch</a>, recomendo a leitura pois da uma boa intui√ß√£o de como o algoritmo funciona.</p>
<p>Para a pesquisa de grade vamos permitir que o sistema fa√ßa isso automaticamente configurando apenas o <code>tuneLength</code> para indicar o n√∫mero de valores diferentes para cada par√¢metro do algoritmo.</p>
<pre class="r"><code>set.seed(1)

control &lt;- trainControl(method = &quot;cv&quot;, number = 5,verboseIter = F)

gbmFit &lt;- train(SalePrice~.,data=train,
                method = &quot;gbm&quot;,
                trControl=control,
                tuneLength=5,
                metric = &quot;Rsquared&quot;,
                verbose = FALSE
)
gbmFit</code></pre>
<pre><code>## Stochastic Gradient Boosting 
## 
## 1460 samples
##   28 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 1169, 1169, 1168, 1168, 1166 
## Resampling results across tuning parameters:
## 
##   interaction.depth  n.trees  RMSE       Rsquared   MAE       
##   1                   50      0.1736970  0.8346902  0.12145158
##   1                  100      0.1474386  0.8663694  0.10371271
##   1                  150      0.1400060  0.8775141  0.09804851
##   1                  200      0.1381902  0.8803999  0.09607709
##   1                  250      0.1375854  0.8817130  0.09502881
##   2                   50      0.1511051  0.8640075  0.10557294
##   2                  100      0.1379357  0.8815852  0.09546142
##   2                  150      0.1360260  0.8846503  0.09326628
##   2                  200      0.1355702  0.8852090  0.09248558
##   2                  250      0.1362827  0.8841734  0.09254710
##   3                   50      0.1434808  0.8743589  0.09910961
##   3                  100      0.1363881  0.8838715  0.09355652
##   3                  150      0.1346606  0.8868808  0.09163759
##   3                  200      0.1339427  0.8880370  0.09062153
##   3                  250      0.1336666  0.8886732  0.08979366
##   4                   50      0.1376575  0.8824442  0.09516571
##   4                  100      0.1334392  0.8884173  0.09192150
##   4                  150      0.1330866  0.8890336  0.09156893
##   4                  200      0.1334706  0.8886198  0.09096598
##   4                  250      0.1335809  0.8884950  0.09101981
##   5                   50      0.1384852  0.8813449  0.09535954
##   5                  100      0.1350803  0.8863344  0.09231165
##   5                  150      0.1340246  0.8878172  0.09112111
##   5                  200      0.1342892  0.8874590  0.09088714
##   5                  250      0.1349331  0.8867525  0.09104875
## 
## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1
## 
## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10
## Rsquared was used to select the optimal model using the largest value.
## The final values used for the model were n.trees = 150, interaction.depth =
##  4, shrinkage = 0.1 and n.minobsinnode = 10.</code></pre>
<p>Note que este foi o modelo que apresentou os melhores resultados quanto s√≥ <span class="math inline">\(R^2\)</span> e ao <span class="math inline">\(RMSE\)</span> em compara√ß√£o com os outros modelos.</p>
<p>Submiss√£o para Kaggle:</p>
<pre class="r"><code>id %&gt;% cbind(predict(gbmFit, test) %&gt;% exp) %&gt;%
  `colnames&lt;-`(c(&quot;Id&quot;, &quot;SalePrice&quot;)) %&gt;%
  write.csv(&quot;gbmFit.csv&quot;, row.names = F)</code></pre>
</div>
<div id="regress√£o-linear" class="section level2">
<h2>Regress√£o Linear</h2>
<p>Por fim faremos o ajuste de um modelo de regress√£o linear multivariado utilizando o pacote caret.</p>
<p>Utilizaremos valida√ß√£o cruzada separando nossa amostra em 5 e utilizaremos o m√©todo <code>lmStepAIC</code> que realiza a sele√ß√£o do modelo escalonado pelo crit√©rio de informa√ß√£o de Akaike - <a href="https://en.wikipedia.org/wiki/Akaike_information_criterion">AIC</a>.</p>
<pre class="r"><code>set.seed(1)

control &lt;- trainControl(method = &quot;cv&quot;, number = 5,verboseIter = F)

lmFit &lt;- train(SalePrice~.,data=train,
               method = &quot;lmStepAIC&quot;,
               trControl=control,
               metric = &quot;Rsquared&quot;,trace=F
)
lmFit</code></pre>
<pre><code>## Linear Regression with Stepwise Selection 
## 
## 1460 samples
##   28 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 1169, 1169, 1168, 1168, 1166 
## Resampling results:
## 
##   RMSE      Rsquared   MAE       
##   0.147716  0.8632513  0.09574552</code></pre>
<p>Note que o ajuste do modelo se apresenta de maneira satisfat√≥ria com <span class="math inline">\(R^2\)</span> e <span class="math inline">\(RMSE\)</span> semelhantes aos modelos de <code>bagging</code> e <code>boosting</code> e al√©m disso, diferente dos modelos baseados em √°rvore, com este ajuste √© poss√≠vel notar a signific√¢ncia estat√≠stica de cada par√¢metro ajustado, o que possibilita tanto o uso tanto como modelo preditivo quanto como modelo descritivo. Veja:</p>
<pre class="r"><code>ggcoef(
  lmFit$finalModel,                      #O modelo a ser conferido
  vline_color = &quot;red&quot;,          #Reta em zero  
  errorbar_color = &quot;blue&quot;,      #Cor da barra de erros
  errorbar_height = .25,
  shape = 18,                   #Altera o formato dos pontos centrais
  size=2,                      #Altera o tamanho do ponto
  color=&quot;black&quot;,
  exclude_intercept = TRUE,                #Altera a cor do ponto
  mapping = aes(x = estimate, y = term, size = p.value))+
  scale_size_continuous(trans = &quot;reverse&quot;)+ #Essa linha faz com que inverta o tamanho
  theme_bw()</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-51-1.png" width="672" /></p>
<p>Note que o intercepto <span class="math inline">\(\beta_0\)</span> foi retirado da imagem pois √© muito superior aos demais coeficientes. Note tamb√©m que <span class="math inline">\(\beta_i\)</span> informa qu√£o sens√≠vel √© <span class="math inline">\(y\)</span>, no caso <code>log(SalePrice)</code> √†s varia√ß√µes de cara umas das <span class="math inline">\(x_{i,j}\)</span> vari√°veis explicativas. Mais concretamente, se <span class="math inline">\(x_{i,j}\)</span> aumenta em uma unidade, o valor de <span class="math inline">\(y\)</span> varia em <span class="math inline">\(\beta_1\)</span> unidades.</p>
<p>Uma r√°pida <a href="http://www.portalaction.com.br/analise-de-regressao/analise-dos-residuos">An√°lise dos Res√≠duos</a>:</p>
<pre class="r"><code>lmFit$finalModel %&gt;% 
  autoplot(which = 1:2) + 
  theme_bw()</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-52-1.png" width="1500" /></p>
<p>√â poss√≠vel notar que parece haver alguns outliers em ambas as figuras. Na primeira √© poss√≠vel notar uma nuvem de pontos aleat√≥rios em torno de zero por√©m na segunda figura nota-se que alguns valores n√£o est√£o de acordo com os quantils te√≥ricos de uma distribui√ß√£o normal, o que pode prejudicar nossa interpreta√ß√£o dos coeficientes do modelo. Vamos encerrar o modelo por aqui mesmo e ver como ele se sai na competi√ß√£o do Kaggle, preparando a submiss√£o:</p>
<pre class="r"><code>id %&gt;% cbind(predict(lmFit, test) %&gt;% exp ) %&gt;% 
  `colnames&lt;-`(c(&quot;Id&quot;, &quot;SalePrice&quot;)) %&gt;%
  write.csv(&quot;lmFit.csv&quot;,row.names = F)</code></pre>
<p>O score obtido com esta submiss√£o no Kaggle foi muito pr√≥ximo dos modelos baseados e √°rvore e o tempo computacional para este ajuste foi bem menor.</p>
</div>
<div id="comparando-ajustes" class="section level2">
<h2>Comparando ajustes</h2>
<p>Vejamos a seguir uma compara√ß√£o entre estes modelos com as fun√ß√µes fornecidas pelo pacote `caret:.</p>
<pre class="r"><code>resamps &lt;- resamples(list(rpart = rpartFit2,
                          treebag = treebagFit,
                          rf = rfFit,
                          gbm = gbmFit,
                          lm = lmFit 
                          )) 
bwplot(resamps)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-54-1.png" width="672" /></p>
<p>Com este gr√°fico √© poss√≠vel notar que o modelo de regress√£o linear m√∫ltipla apresentou resultados semelhantes aos de bagging e boosting.</p>
<p>√â importante frisar que a maneira como as vari√°veis foram selecionadas para o modelo de regress√£o linear m√∫ltipla atrav√©s da import√¢ncia das vari√°veis obtida com o modelo randomForest n√£o √© um padr√£o e existem diversos outros modos estat√≠sticos de se de determinar a signific√¢ncia e a rela√ß√£o das vari√°veis para o modelo.</p>
<p>Um poss√≠vel problema neste m√©todo √© que n√£o detecta a multicolinearidade, que ocorre quando as vari√°veis explicativas est√£o fortemente correlacionadas entre si e a an√°lise de regress√£o linear pode ficar confusa e desprovida de significado, pois h√° dificuldade em distinguir o efeito de uma ou outra vari√°vel explicativa sobre a vari√°vel resposta <span class="math inline">\(Y\)</span> devido √† vari√¢ncias muito elevadas ou sinais inconsistentes.</p>
<p>Essa proposta de aprender se divertindo e de maneira produtiva me deixa muito empolgado, espero que tenham se divertido como eu me diverti fazendo este post!</p>
</div>
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias:</h1>
<ul>
<li><a href="https://www.datacamp.com/courses/machine-learning-with-tree-based-models-in-r">DataCamp Course:Machine Learning with Tree-Based Models in R</a></li>
<li><a href="https://tinyurl.com/y796aa4t">Data Science <em>for</em> Business</a></li>
<li><a href="https://lethalbrains.com/learn-ml-algorithms-by-coding-decision-trees-439ac503c9a4">Learn ML Algorithms by coding: Decision Trees</a></li>
<li><a href="https://www.datacamp.com/community/tutorials/decision-trees-R">DataCamp Tutorials: Decision Trees in R</a></li>
<li><a href="https://topepo.github.io/caret/">The caret Package - Max Kuhn</a></li>
<li><a href="https://www.vooo.pro/insights/um-tutorial-completo-sobre-a-modelagem-baseada-em-tree-arvore-do-zero-em-r-python/">Um tutorial completo sobre modelagem baseada em √°rvores de decis√£o (c√≥digos R e Python)</a></li>
<li><a href="https://machinelearningmastery.com/tuning-machine-learning-models-using-the-caret-r-package/">Tuning Machine Learning Models Using the Caret R Package</a></li>
<li><a href="https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d">Gradient Boosting from scratch</a></li>
<li><a href="https://machinelearningmastery.com/tune-machine-learning-algorithms-in-r/">Tune Machine Learning Algorithms in R (random forest case study)</a></li>
<li><a href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_manual.htm">Random Forests - Leo Breiman and Adele Cutler</a></li>
<li><a href="https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf">An Introduction to Recursive Partitioning Using the RPART Routines - CRAN</a></li>
</ul>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2018-08-31-modelos-em-arvore/modelos-em-arvore/">Um estudo sobre modelos de aprendizagem baseados em √°rvores com desafio do Kaggle</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Analise Explorat√≥ria</category>
      <category>Aprendizado Supervisionado</category>
      <category>Data mining</category>
      <category>Estatistica</category>
      <category>Machine Learning</category>
      <category>Pr√°tica</category>
      <category>Probabilidade</category>
      <category>R</category>
      <category>modelo baseado em arvores</category>
      <category>kaggle</category>
      <category>Regress√£o</category>
      <category domain="tag">Data Mining</category>
      <category domain="tag">Estatistica</category>
      <category domain="tag">gomesfellipe</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">Correlacoes</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">modelagem estatistica</category>
      <category domain="tag">Pr√°tica</category>
      <category domain="tag">R</category>
      <category domain="tag">regression</category>
      <category domain="tag">caret</category>
      <category domain="tag">xgboost</category>
      <category domain="tag">random forest</category>
      <category domain="tag">decisiontree</category>
    </item>
    <item>
      <title>O que s√£o CheatSheets, gamifica√ß√£o e por que aprender R √© t√£o divertido?</title>
      <link>https://gomesfellipe.github.io/post/2018-02-17-cheatsheet-gamificacao-r/cheatsheet-gamificacao-r/</link>
      <pubDate>Sat, 17 Feb 2018 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2018-02-17-cheatsheet-gamificacao-r/cheatsheet-gamificacao-r/</guid>
      <description>Voc√™ costuma ler o manual de instru√ß√µes? Veja como equipes t√™m trabalhado para contribuir e facilitar o aprendizado da linguagem R ampliando a intersec√ß√£o entre a curiosidade de nossa infancia e o amadurecimento. Programar se torna uma tarefa divertida e pr√°tica mas sem abandonar o manual de instru√ß√µes escrito por quem sabe do que esta falando!</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="voc√™-costuma-ler-o-manual" class="section level1">
<h1>Voc√™ costuma ler o manual?</h1>
<p>Quando eramos crian√ßas geralmente n√£o tinhamos o costume de ler o manual das coisas n√£o √© mesmo? Particularmente eu sempre gostei de aprender como as coisas funcionavam diretamente com a pr√°tica para poder us√°-las depois. Adorava buscar entender como as coisas se encaixavam ao montar os brinquedinhos do kinder-ovo sem ler as instru√ß√µes ou criar diferentes combina√ß√µes com lego customizados, por exemplo. Acredito que isso seja da natureza de toda crian√ßa!</p>
<div class="col2">
<p>Acontece que com o passar dos anos vamos adquirindo conhecimento e come√ßamos a perceber que quanto mais aprendemos maior a quantidade de coisas novas que ainda temos a aprender.</p>
<p>Especialmente ao ler noticias do tipo: <a href="http://www.jornalciencia.com/einstein-estava-certo-cientistas-detectam-ondas-gravitacionais-comprovando-a-teoria-da-relatividade-geral/">‚ÄúEinstein estava certo a cem anos atr√°s!‚Äù</a> depois de ele j√° ter tomado nota das ondas gravitacionais a tantos anos.. Um dever de casa que durou 100 anos foi deixado por um g√™nio e isso serve para nos lembra como somos ‚Äúpequenos‚Äù, o quanto √© importante seguir boas refer√™ncias nos apoiando em ombros de gigantes para enxergar mais longe!</p>
<p>(Quem ai aos 22 anos, desenvolveu o c√°lculo infinitesimal, as bases da teoria das cores, contribuiu com o estudo da √≥tica, formulou conceitos sobre as leis do movimento planet√°rio e virou lenda com um famoso incidente da ma√ß√£ que levou a formular a teoria da gravidade? Ser√° que <a href="https://www.oficinadanet.com.br/post/15839-isaac-newton-o-maior-genio-de-todos-os-tempos">Isaac Newton</a> conhece alguem?)</p>
<p><img src="http://i.giphy.com/IZ4EXtpPkamXe.gif" /></p>
</div>
<p>Isso faz pensar em como √© importante ouvir (ou ler) quem entende do assunto para darmos nossos pr√≥ximos passos</p>
</div>
<div id="mas-o-r-tem-manual" class="section level1">
<h1>Mas o R tem manual?</h1>
<p>Seja estudando estat√≠stica, programa√ß√£o em R, qualquer outra mat√©ria ou mesmo configurando seu rel√≥gio, montando um daqueles m√≥veis complicados ou que seja montando um avi√£o! Nem tudo precisa ser um quebra cabe√ßa, n√£o importa o qu√£o √°vido por saber, consultar o manual (ou um livro se torna uma tarefa fundamental para darmos o pr√≥ximo passo!</p>
<p>A medida que vamos avan√ßando no aprendendizado da linguagem R, mais consultas ao <a href="https://www.r-project.org/help.html">‚ÄúHelp‚Äù</a> v√£o sendo realizadas. Isso ocorre tamb√©m quando avan√ßamos no estudo de qualquer √°rea, acaba sendo natural elevar o n√∫mero de consultas ao ‚Äúmanual‚Äù.</p>
<p>Acontece que nem sempre encontramos explica√ß√µes detalhadas ou suficientes no Help para solucionar nossos problemas e em busca de mais detalhes e referencias quase sempre podemos encontrar <a href="http://r-pkgs.had.co.nz/vignettes.html">vignettes</a> (que s√£o guias longos para os pacotes, geralmente com exemplos reprodut√≠veis e algumas dicas para o uso) em uma breve pesquisa escrevendo: ‚ÄúNome do pacote‚Äù + ‚ÄúCRAN‚Äù na busca do Google e geralmente logo no inicio j√° existe uma ou mais refer√™ncias al√©m do Help do RStudio no <a href="https://cran.r-project.org/">CRAN</a> disponibilizado pelos desenvolvedores dos pacotes.</p>
</div>
<div id="aprendendo-com-a-pr√°tica" class="section level1">
<h1>Aprendendo com a pr√°tica</h1>
<p>Algo muito legal disponibilizado pela RStudio dentre os <a href="https://www.rstudio.com/resources">recursos em seu site oficial</a> s√£o as <a href="https://www.rstudio.com/resources/cheatsheets/">CheatSheets</a> e a <a href="https://www.rstudio.com/online-learning/">aprendizagem online</a> onde √© apresentada a <a href="https://www.datacamp.com/">DataCamp</a> que tornam f√°cil aprender e usar alguns dos pacotes mais utilizados unindo a ideia da ‚Äúconsulta ao manual‚Äù e a ideia de se aprender na pr√°tica.</p>
<p>Al√©m desses recursos ainda existe <a href="https://www.kaggle.com/">kaggle</a> que funciona quase como uma plataforma de ‚Äújogos com a ci√™ncia de dados‚Äù onde competidores comparam resultados dos ajustes de seus modelos, an√°lises descrtias e relat√≥rios valendo premios em dinheiro!</p>
<p>Divertido como quando eramos crian√ßas!</p>
<p>Veja a seguir uma breve explica√ß√£o e refer√™ncias para acessar esses recursos</p>
<div id="cheatsheets" class="section level2">
<h2>Cheatsheets</h2>
<p>Ao longo do tempo novas cheatsheets v√£o sendo adicionadas e todas elas est√£o dispon√≠veis para download, al√©m disso qualquer pessoa que quiser contribuir com a comunidade √© <a href="https://www.rstudio.com/resources/cheatsheets/how-to-contribute-a-cheatsheet/">convidado pelos desenvolvedores</a> a enviar suas pr√≥prias CheatSheets!</p>
<p>Todas as CheatSheets apresentadas abaixo est√£o dispon√≠veis no <a href="https://www.rstudio.com/resources/cheatsheets/">reposit√≥rio de CheatSheets</a> :</p>
<div id="exemplos-do-b√°sico" class="section level3">
<h3>Exemplos do b√°sico:</h3>
<p>Informa√ß√µes b√°sicas e fundamentais para o uso da IDE do RStudio e o uso da linguagem podem ser encontrados nessas CheatSheets:</p>
<div class="row">
<div class="col-sm-4">
<div class="figure">
<img src="https://image.slidesharecdn.com/rstudio-ide-cheatsheet-170605180146/95/rstudio-idecheatsheet-1-638.jpg?cb=1496686097" alt="" />
<p class="caption">RStudio IDE</p>
</div>
</div>
<div class="col-sm-4">
<div class="figure">
<img src="https://s2.studylib.es/store/data/008818835_1-c476932320d9a4cd7e891da23012aaa1.png" alt="" />
<p class="caption">Basics R</p>
</div>
</div>
<div class="col-sm-4">
<div class="figure">
<img src="https://i.pinimg.com/originals/d7/34/13/d734131229a11252c34f954df1fbd511.png" alt="" />
<p class="caption">Advanced R</p>
</div>
</div>
</div>
</div>
<div id="exemplos-de-recursos" class="section level3">
<h3>Exemplos de recursos:</h3>
<p>Existem tamb√©m algumas CheatSheets para auxiliar no uso dos recursos oferecidos pela RStudio como por exemplo a para RMarkdown e Shinny:</p>
<div class="row">
<div class="col-sm-4">
<div class="figure">
<img src="https://d33wubrfki0l68.cloudfront.net/65dffd1bdcaa0025006262164d98e8068e8b4387/c3895/wp-content/uploads/2018/08/rmarkdown-2.0.png" alt="" />
<p class="caption">RMarkdown</p>
</div>
</div>
<div class="col-sm-4">
<div class="figure">
<img src="https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference-guide.png" alt="" />
<p class="caption">Guia de Referencia para RMarkdown</p>
</div>
</div>
<div class="col-sm-4">
<div class="figure">
<img src="https://mcmoutletonline.com/pics/shiny-cheat-sheet.png" alt="" />
<p class="caption">Shiny</p>
</div>
</div>
</div>
</div>
<div id="exemplos-de-utilidades" class="section level3">
<h3>Exemplos de utilidades</h3>
<p>Al√©m das funcionalidades e recursos b√°sicos dispon√≠veis pela equipe da RStudio ainda contamos com uma enorme quantidade de pacotes que est√£o sendo desenvolvidos a todo momento com a finalidade de melhorar o desempenho de nossos programas e projetos, a seguir alguns exemplos de CheatSheets de pacotes que s√£o bastante √∫teis no dia a dia do programador estat√≠stico:</p>
<div id="exemplos-de-fornecidos-pela-rstudio" class="section level4">
<h4>Exemplos de fornecidos pela RStudio</h4>
<div class="row">
<div class="col-sm-4">
<div class="figure">
<img src="https://d33wubrfki0l68.cloudfront.net/ed91b01c08afed41e2df36b805e32c2c46e48857/21514/wp-content/uploads/2018/08/strings.png" alt="" />
<p class="caption">stringr - Para facilitar a manipula√ß√£o de strings</p>
</div>
</div>
<div class="col-sm-4">
<div class="figure">
<img src="https://d33wubrfki0l68.cloudfront.net/3ae77f446a7470730f3dbb7b6489525494ac8bd5/57024/wp-content/uploads/2018/08/purrr.png" alt="" />
<p class="caption">purrr - Pacote com ferramentas de programa√ß√£o funcional</p>
</div>
</div>
<div class="col-sm-4">
<div class="figure">
<img src="https://d33wubrfki0l68.cloudfront.net/db69c3d03699d395475d2ac14d64f611054fa9a4/e98f3/wp-content/uploads/2018/08/data-transformation.png" alt="" />
<p class="caption">dplyr - Para facilidade e velocidade na manipula√ß√£o de dados</p>
</div>
</div>
</div>
<p></br></p>
<div class="row">
<div class="col-sm-4">
<div class="figure">
<img src="https://d33wubrfki0l68.cloudfront.net/21d683072b0c21cbd9b41fc0e37a587ad26b9525/cbf41/wp-content/uploads/2018/08/data-visualization-2.1.png" alt="" />
<p class="caption">ggplot2 - Para apresenta√ß√µes visuais elegantes e pr√°ticas</p>
</div>
</div>
<div class="col-sm-4">
<div class="figure">
<img src="https://miro.medium.com/max/4582/1*W08jooOkrVu7iok96jsJpA.jpeg" alt="" />
<p class="caption">readr - Para facilitar a tarefa de importar arquivos</p>
</div>
</div>
<div class="col-sm-4">
<div class="figure">
<img src="https://image.slidesharecdn.com/devtools-cheatsheet-170605180143/95/devtools-cheatsheet-1-638.jpg?cb=1496685956" alt="" />
<p class="caption">devtools - Possibilitando o usu√°rio criar seus pr√≥prios pacotes</p>
</div>
</div>
</div>
<p>Dentre muitos outros dispon√≠veis no <a href="https://www.rstudio.com/resources/cheatsheets/">link para galeria de CheatSheets</a></p>
</div>
<div id="exemplos-fornecidos-por-contribuidores" class="section level4">
<h4>Exemplos fornecidos por contribuidores</h4>
<p>Como mostrado anteriormente, o pacote <a href="https://cran.r-project.org/web/packages/devtools/index.html"><code>devtools</code></a> possibilita que qualquer usu√°rio crie e disponibilize seus pr√≥prios pacotes, ent√£o al√©m dos pacotes a comunidade tamb√©m contribuiu com diversas cheatsheets, veja algumas delas:</p>
<div class="row">
<div class="col-sm-4">
<div class="figure">
<img src="https://d33wubrfki0l68.cloudfront.net/ad16acdb544c1a9ca00c7dd175312a52f45e8979/7e9a2/wp-content/uploads/2015/01/caret-cheatsheet.png" alt="" />
<p class="caption">caret - Pacote muito famoso quando o assunto √© Machine Learning</p>
</div>
</div>
<div class="col-sm-4">
<div class="figure">
<img src="https://d33wubrfki0l68.cloudfront.net/1267e8f809560bdbad86d15763be08302a471fb5/138a1/wp-content/uploads/2015/01/leaflet-cheatsheet-1.png" alt="" />
<p class="caption">leaflet - Para criar mapas interativos com facilidade</p>
</div>
</div>
<div class="col-sm-4">
<div class="figure">
<img src="https://wch.github.io/latexsheet/latexsheet-0.png" alt="" />
<p class="caption"><span class="math inline">\(\LaTeX\)</span> - Linguagem muito √∫til e f√°cil para escrever muitos tipos de documentos</p>
</div>
</div>
</div>
<p>Como podemos ver s√£o muitas op√ß√µes para consulta, ao encontrar o que nos torna mais confort√°vel enquanto aprendemos a linguagem torna-se poss√≠vel dar passos mais largos</p>
</div>
</div>
</div>
</div>
<div id="aprendizagem-online-e-a-gamifica√ß√£o" class="section level1">
<h1>Aprendizagem Online e a gamifica√ß√£o</h1>
<p>A <a href="https://en.wikipedia.org/wiki/Gamification">gamifica√ß√£o</a> √© um conceito que vem sendo introduzido ap√≥s a introdu√ß√£o da tecnologia na hist√≥ria. A id√©ia de se criar jogos para motivar e engajar as pessoas em atividades profissionais e a id√©ia de se estar em um jogo possibilita doses de motiva√ß√£o especialmente a quem gosta de competir.</p>
<div id="datacamp" class="section level2">
<h2>DataCamp</h2>
<center>
<img src="https://nhorton.people.amherst.edu/rstudio/datacamp.png" />
</center>
<p>No site oficial da <a href="https://www.rstudio.com/">RStudio</a> encontramos al√©m das cheatsheets, dentre seus recursos existe a op√ß√£o de <a href="https://www.rstudio.com/online-learning/">aprendizagem online</a> onde √© apresentada a <a href="https://www.datacamp.com/">DataCamp</a> que √© o primeiro e um dos mais importantes l√≠deres em divulgar e ensinar Data Science, oferecendo treinamento baseado em habilidades, inova√ß√£o t√©cnica pioneira e cursos oferecidos pelos melhores educadores do mundo em data science!(Descri√ß√£o deles no <a href="https://www.datacamp.com/about">site</a>)</p>
<p>Apesar do site ser pago, existem diversas op√ß√µes de cursos gratuitos para quem esta come√ßando e √© muito simples e f√°cil aprender pelo site. Ao cumprir com exerc√≠cios e terminar os cursos o usu√°rio ganha xp (pontos de experiencia) que registram sua evolu√ß√£o. Pode ser um √≥timo investimento para aprender com os melhores do mundo e o legal de tudo isso √© que nunca fica f√°cil!</p>
</div>
<div id="kaggle" class="section level2">
<h2>Kaggle</h2>
<center>
<div style="width:300px; height=200px">
<p><img src="https://upload.wikimedia.org/wikipedia/commons/7/7c/Kaggle_logo.png" /></p>
</div>
</center>
<p>O <a href="https://www.kaggle.com/">kaggle</a> √© um playground para cientistas de dados, nele existem diversas modalidades e dentre elas uma das das que eu acho mais interessante s√£o as competi√ß√µes de machine learning onde pessoas e empresas interessadas em adquirir ou at√© comparar seus resultados com os modelos feitos por uma enorme comunidade programando em diferentes linguagens est√£o testando para saber quem treina o modelo mais preciso e √© poss√≠vel acompanhar o c√≥digo e o racioc√≠nio de cientistas de dados do mundo inteiro de maneira muito simples!</p>
<p>Tamb√©m existe a recompensa com pontos de experiencia para passar de n√≠vel, congratula√ß√£o com medalhas dentre outras recopensas semelhantes ao dos games.</p>
<p>Existem os famosos <a href="https://www.kaggle.com/datasets">datasets</a> que s√£o os bancos de dados fornecidos pelos usu√°rios da plataforma e os <a href="https://www.kaggle.com/kernels">kernels</a> √© um √≥timo lugar para compartilhar seu trabalho e debater sobre resultados de projetos e id√©ias de aplica√ß√µes de outras pessoas</p>
<p>Ao contr√°rio da DataCamp, o Kaggle √© gratuito e existem competi√ß√µes pagando at√© $100.000,00 para a pesosa (ou equipe) que apresentar os resultados mais satisfat√≥rios!!</p>
</div>
</div>
<div id="para-refletir.." class="section level1">
<h1>Para refletir..</h1>
<div class="col2">
<center>
<div style="width:150px; height=300px">
<p><img src="https://cdn.pensador.com/img/authors/co/nf/confucio-2-l.jpg" /></p>
</div>
</center>
<blockquote>
<p>‚ÄúH√° tr√™s m√©todos de ganhar sabedoria: primeiro, por reflex√£o, que √© o mais nobre; segundo, por imita√ß√£o, que √© o mais f√°cil; e o terceiro, por experi√™ncia, que √© o mais amargo.‚Äù
- Conf√∫cio</p>
</blockquote>
</div>
<p>Essa passagem de Conf√∫cio deixa claro que n√£o existe s√≥ um jeito de se aprender, com um mix de maneiras de se obter conhecimento fica um pouco menos dif√≠cil encontrar a sabedoria: estudar, exercitar e praticar se torna divertido quando se trata de programar em R!</p>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2018-02-17-cheatsheet-gamificacao-r/cheatsheet-gamificacao-r/">O que s√£o CheatSheets, gamifica√ß√£o e por que aprender R √© t√£o divertido?</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Estatistica</category>
      <category>Pr√°tica</category>
      <category>R</category>
      <category>Teoria</category>
      <category domain="tag">Estatistica</category>
      <category domain="tag">Pr√°tica</category>
      <category domain="tag">R</category>
      <category domain="tag">RStudio</category>
      <category domain="tag">cheatsheets</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">datacamp</category>
      <category domain="tag">gomesfellipe</category>
      <category domain="tag">gamification</category>
      <category domain="tag">gamificacao</category>
    </item>
  </channel>
</rss>