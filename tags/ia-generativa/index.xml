&lt;?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>ia-generativa on Fellipe Gomes - Data Science Blog</title>
    <link>https://gomesfellipe.github.io/tags/ia-generativa/</link>
    <description>√öltimos posts sobre Data Science, Machine Learning e R</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>pt-br</language>
    <managingEditor>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</managingEditor>
    <webMaster>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</webMaster>
    <lastBuildDate>Fri, 27 Sep 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://gomesfellipe.github.io/tags/ia-generativa/" rel="self" type="application/rss+xml" />
    <item>
      <title>Extra√ß√£o de informa√ß√µes de imagens com IA Generativa</title>
      <link>https://gomesfellipe.github.io/post/2024-09-27-image-text-to-text/</link>
      <pubDate>Fri, 27 Sep 2024 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2024-09-27-image-text-to-text/</guid>
      <description>Neste post, exploraremos como utilizar o modelo Llava para gerar r√≥tulos descritivos de imagens, usando dados do conjunto COCO-2017.</description>
      <content:encoded>&lt;![CDATA[
        


<div id="caso-de-uso-de-ia-generativa-extra√ß√£o-de-informa√ß√µes-de-imagens-com-o-modelo-llava" class="section level1">
<h1>Caso de Uso de IA Generativa: Extra√ß√£o de Informa√ß√µes de Imagens com o Modelo Llava</h1>
<p>GenAI refere-se a modelos de intelig√™ncia artificial capazes de gerar conte√∫do novo e criativo a partir de dados de entrada. Seu uso est√° revolucionando a maneira como processamos dados n√£o estruturados, como imagens, √°udios, textos, v√≠deos, etc. Trabalhar com modelos pr√©-treinados (i.e., que j√° foram treinados com grandes conjuntos de dados) e adapt√°-los para necessidades espec√≠ficas tem sido um divisor de √°guas.</p>
<p>Neste post, vamos explorar a utiliza√ß√£o do modelo Llava (Large Language and Vision Assistant) para extrair r√≥tulos descritivos de imagens e tamb√©m discutir como comparar a qualidade das previs√µes geradas com m√©tricas espec√≠ficas para avaliar a performance desse tipo de modelo.</p>
<div id="por-que-o-modelo-llava" class="section level2">
<h2>Por que o Modelo Llava?</h2>
<p>O modelo <a href="https://llava-vl.github.io/">Llava</a> √© uma alternativa de c√≥digo aberto ao <a href="https://chat-gpt-5.ai/capabilities-of-gpt-4v/">GPT-4 Vision</a> da OpenAI (que se destaca neste dom√≠nio, mas sua aplica√ß√£o √© restrita devido sua natureza propriet√°ria e comercial) que foi treinado em grandes conjuntos de dados multimodais, sendo capaz de compreender e gerar descri√ß√µes textuais para imagens.</p>
<p>Essa capacidade de ‚Äúconversar com imagens‚Äù tendo o mesmo ‚Äúpoder‚Äù de um LLM, possibilita seu uso em muitas solu√ß√µes desenvolvidas por cientistas de dados no mundo real, como:</p>
<ol style="list-style-type: decimal">
<li><strong>Classifica√ß√£o de produtos em e-commerce</strong>: gera√ß√£o de descri√ß√µes detalhadas de roupas, acess√≥rios, eletr√¥nicos, etc.</li>
<li><strong>Detec√ß√£o de defeitos em linhas de produ√ß√£o</strong>: identifica√ß√£o de falhas em produtos para automa√ß√£o e controle de qualidade.</li>
<li><strong>Diagn√≥stico m√©dico por imagens</strong>: auxiliar na detec√ß√£o precoce de doen√ßas a partir de descri√ß√µes detalhadas de imagens m√©dicas.</li>
<li><strong>Reconhecimento de placas de carros</strong>: transcri√ß√£o autom√°tica de textos de placas e caracter√≠sticas de ve√≠culos.</li>
<li><strong>Identifica√ß√£o de sinais de tr√¢nsito</strong>: aplica√ß√£o em ve√≠culos aut√¥nomos para navega√ß√£o e identifica√ß√£o de sinais.</li>
<li><strong>An√°lise de alimentos para calcular nutri√ß√£o</strong>: extra√ß√£o autom√°tica de informa√ß√µes nutricionais de fotos ou r√≥tulos de alimentos.</li>
<li><strong>Identifica√ß√£o de animais em c√¢meras de vida selvagem</strong>: gerar descri√ß√µes detalhadas de animais detectados, ajudando pesquisadores a automatizar o monitoramento da vida selvagem.</li>
<li><strong>Detec√ß√£o de aglomera√ß√µes em eventos</strong>: analisar imagens de c√¢meras de seguran√ßa para identificar a presen√ßa de grandes grupos de pessoas em eventos ou lugares p√∫blicos, √∫til em gest√£o de multid√µes ou para quest√µes de seguran√ßa.</li>
</ol>
</div>
<div id="dataset-coco-2017" class="section level2">
<h2>Dataset COCO-2017</h2>
<p>O <a href="https://cocodataset.org/">COCO</a> (Common Objects in Context) √© um dataset amplamente utilizado em vis√£o computacional. Ele √© um dos maiores conjuntos de imagens do dia a dia com objetos em diferentes contextos, com anota√ß√µes detalhadas fornecidas por humanos como tags, caixa delimitadora, pol√≠gono que segmenta a imagem detectando objetos bem como sua descri√ß√£o. Isso o torna ideal para testar o desempenho desse tipo de modelo para gera√ß√£o de legendas.</p>
<center>
<div style="display: flex; width: 100%;">
<div style="width: 50%;">
<p><img src="/post/2024-09-27-image-text-to-text/coco1.png" alt="Imagem 2" style="width: 100%;"></p>
</div>
<div style="width: 50%;">
<p><img src="/post/2024-09-27-image-text-to-text/coco2.png" alt="Imagem 2" style="width: 100%;"></p>
</div>
</div>
<center>
<small>
Imagem do COCO Dataset com e sem anota√ß√£o obtida na <a href="https://cocodataset.org/#explore">se√ß√£o explorat√≥ria</a> das imagens
</small>
</center>
</center>
</div>
</div>
<div id="preparando-o-ambiente" class="section level1">
<h1>Preparando o Ambiente</h1>
<p>Utilizei o ambiente do Kaggle para desenvolvimento deste notebook, que disponibiliza a utiliza√ß√£o de GPUs. Atrav√©s do Hardware Accelerator utilizaremos a <a href="https://www.kaggle.com/docs/efficient-gpu-usage">NVIDIA TESLA P100 GPU</a>.</p>
<details>
<summary>
<em>Expandir c√≥digo</em>
</summary>
<pre><code>%%capture
!pip -qqq install bitsandbytes accelerate rouge-score pycocoevalcap bert_score
!pip install -U nltk

import os
import re
import json
import pandas as pd
import numpy as np
from tqdm import tqdm

import seaborn as sns
import matplotlib.pyplot as plt

from PIL import Image
import requests
from io import BytesIO
from IPython.display import HTML
import base64

import torch
from transformers import pipeline, AutoProcessor, BitsAndBytesConfig

from nltk.translate.bleu_score import sentence_bleu
from rouge_score import rouge_scorer
from bert_score import score as bert_score
from nltk.translate.meteor_score import meteor_score

from transformers import logging
import warnings

logging.set_verbosity_error()
warnings.filterwarnings(&quot;ignore&quot;, &quot;use_inf_as_na&quot;)</code></pre>
</details>
<p><br></p>
</div>
<div id="carregar-dados" class="section level1">
<h1>Carregar dados</h1>
<p>Por fins de praticidade para este post, selecionei uma amostra de 10 imagens aleat√≥rias do dataset COCO - (Common Objects in Context) no site <a href="https://cocodataset.org" class="uri">https://cocodataset.org</a> (onde √© poss√≠vel ter uma descri√ß√£o detalhada do conjunto de dados, incluindo seu <a href="https://arxiv.org/abs/1405.0312">paper</a> para aprofundamento), para avaliar o desempenho do modelo.</p>
<details>
<summary>
<em>Expandir c√≥digo</em>
</summary>
<pre class="python"><code>df_sample = pd.DataFrame({
  &#39;coco_url&#39;: [
    &#39;http://images.cocodataset.org/train2017/000000058822.jpg&#39;,
    &#39;http://images.cocodataset.org/train2017/000000530396.jpg&#39;,
    &#39;http://images.cocodataset.org/train2017/000000097916.jpg&#39;,
    &#39;http://images.cocodataset.org/train2017/000000418492.jpg&#39;,
    &#39;http://images.cocodataset.org/train2017/000000022304.jpg&#39;,
    &#39;http://images.cocodataset.org/train2017/000000295999.jpg&#39;,
    &#39;http://images.cocodataset.org/train2017/000000406616.jpg&#39;,
    &#39;http://images.cocodataset.org/train2017/000000370926.jpg&#39;,
    &#39;http://images.cocodataset.org/train2017/000000005612.jpg&#39;,
    &#39;http://images.cocodataset.org/train2017/000000146436.jpg&#39;
  ],
  &#39;caption&#39;: [
    &#39;A laptop sitting on a desk with a cell phone and mouse.&#39;,
    &#39;A black bear walking through the grass field.&#39;,
    &#39;a person who is surfing in the ocean.&#39;,
    &#39;A young boy standing on a sandy beach holding a flag.&#39;,
    &#39;A man surfing on a wave in the ocean.&#39;,
    &#39;A herd of cows, grazing in a field.&#39;,
    &#39;There is a cutting board and knife with chopped apples and carrots.&#39;,
    &#39;A long yellow school bus is parked on a city street.\n&#39;,
    &#39;A black and white horse standing in the middle of a field.&#39;,
    &#39;A man in a red jacket looking at his phone.&#39;
    ]})
    
# Fun√ß√£o para verificar se o caminho √© uma URL
def is_url(path):
    return path.startswith(&#39;http://&#39;) or path.startswith(&#39;https://&#39;)

# Fun√ß√£o simplificada para gerar o thumbnail e convert√™-lo em base64 diretamente
def process_image(path):
    try:
        if is_url(path):
            # Se for uma URL, baixar a imagem
            response = requests.get(path)
            response.raise_for_status()  # Verifica se houve algum erro no download
            image = Image.open(BytesIO(response.content))  # Abrir a imagem do conte√∫do da resposta
        else:
            # Se for um caminho local, abrir a imagem diretamente
            image = Image.open(path)
        
        # Criar uma miniatura da imagem (thumbnail) com tamanho m√°ximo de 150x150
        image.thumbnail((150, 150), Image.LANCZOS)
        
        # Salvar a imagem em um buffer de mem√≥ria e convert√™-la para base64
        with BytesIO() as buffer:
            image.save(buffer, &#39;jpeg&#39;)
            image_base64 = base64.b64encode(buffer.getvalue()).decode()
        
        # Retornar a string HTML com a imagem embutida no formato base64
        return f&#39;&lt;img src=&quot;data:image/jpeg;base64,{image_base64}&quot;&gt;&#39;
    
    except Exception as e:
        # Em caso de erro, retornar uma string vazia ou uma mensagem de erro
        return f&quot;&lt;p&gt;Erro ao carregar imagem: {e}&lt;/p&gt;&quot;

# Aplicar o processamento de imagens diretamente no DataFrame
df_sample[&#39;image&#39;] = df_sample[&#39;coco_url&#39;].map(process_image)  # Pode ser URL ou caminho local

# Exibir as legendas e imagens formatadas em HTML
HTML(df_sample[[&#39;image&#39;, &#39;coco_url&#39;, &#39;caption&#39;]].head().to_html(escape=False))</code></pre>
</details>
<p><br></p>
<p><img src="/post/2024-09-27-image-text-to-text/df1.png" style="width: 100%;"></p>
<p>Caso voc√™ precise de mais imagens para testar, tamb√©m √© poss√≠vel encontrar uma <a href="https://www.kaggle.com/datasets/awsaf49/coco-2017-dataset/data">vers√£o disponibilizada no Kaggle</a> .</p>
</div>
<div id="carregar-modelo" class="section level1">
<h1>Carregar modelo</h1>
<p>Utilizaremos uma vers√£o de 7 bilh√µes de par√¢metros do modelo <a href="https://huggingface.co/llava-hf/llava-1.5-7b-hf">‚ÄúLLaVA 1.5‚Äù</a> (Language and Vision Assistant), dispon√≠vel no HuggingFace (Uma plataforma onde a comunidade de Machine Learning colabora com modelos, dados e aplica√ß√µes) treinada para tarefas de gera√ß√£o de texto a partir de imagens.</p>
<pre class="python"><code>%%time

model_id = &quot;llava-hf/llava-1.5-7b-hf&quot;

# Configura√ß√£o de quantiza√ß√£o do modelo, que permite reduzir o uso de mem√≥ria sem 
# comprometer muito a precis√£o. Aqui estamos configurando para usar quantiza√ß√£o em 4 bits.
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,  
    bnb_4bit_use_double_quant=True,  
    bnb_4bit_quant_type=&quot;nf4&quot;,  
    bnb_4bit_compute_dtype=torch.bfloat16  
)

# Cria√ß√£o de um pipeline de processamento de imagens para gera√ß√£o de texto
# O pipeline √© configurado para a tarefa &quot;image-to-text&quot;
pipe = pipeline(
    &quot;image-to-text&quot;, 
    model=model_id, 
    model_kwargs={
        &quot;quantization_config&quot;: quantization_config,
        &quot;low_cpu_mem_usage&quot;: True
    }
)

# Carregar o processador associado respons√°vel por pr√©-processar
# as imagens de entrada e preparar os dados para serem inseridos no modelo
processor = AutoProcessor.from_pretrained(model_id)</code></pre>
<pre><code>CPU times: user 28.7 s, sys: 28.1 s, total: 56.8 s
Wall time: 6min 26s</code></pre>
<div class="w3-panel w3-pale-blue w3-border">
<p>¬† <strong>üìå Nota:</strong> A quantiza√ß√£o √© uma t√©cnica para reduzir o tamanho do modelo, perdendo um pouco de performance para otimizar o desempenho e rodar em m√°quinas com mem√≥ria limitada.</p>
</div>
</div>
<div id="prompt-engineering" class="section level1">
<h1>Prompt Engineering</h1>
<p>Uma ampla variedade de <a href="https://www.promptingguide.ai/pt">t√©cnicas</a> poderiam ser aplicadas para desenvolver <a href="https://python.langchain.com/docs/how_to/multimodal_prompts/">prompts</a> mais eficazes (inclusive com <a href="https://python.langchain.com/docs/introduction/">LangChain</a>, como fiz no <a href="https://gomesfellipe.github.io/post/2024-05-26-detec-o-de-linguagem-t-xica-com-o-llm-gemma-e-langchain/">√∫ltimo post</a>) ou especializar o modelo com ajuste fino visando obter resultados otimizados. No entanto, como este n√£o √© o foco do post, usarei um prompt simples e direto para estabelecer um baseline para avaliar as capacidades do modelo com o m√≠nimo de esfor√ßo.</p>
<pre class="python"><code># Cada valor em &quot;content&quot; tem que ser uma lista de dicion√°rio com os tipos (&quot;text&quot;, &quot;image&quot;) 
conversation = [
    {
      &quot;role&quot;: &quot;user&quot;,
      &quot;content&quot;: [
          {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this image in a few words:&quot;},
          {&quot;type&quot;: &quot;image&quot;},
        ]
    },
]

# Formata a conversa (que pode incluir texto e imagens) no formato correto que o modelo entende.
prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)</code></pre>
<p>O <a href="https://colab.research.google.com/drive/1qsl6cd2c8gGtEW1xV5io7S8NHh-Cp1TV?usp=sharing#scrollTo=JvvtplWDRvfu">prompt</a> deve ser especificado no seguinte formato:</p>
<pre><code>USER: &lt;image&gt;
&lt;prompt&gt;
ASSISTANT:</code></pre>
</div>
<div id="infer√™ncia" class="section level1">
<h1>Infer√™ncia</h1>
<p>Com o modelo devidamente configurado e o prompt ajustado, estamos prontos para executar o pipeline de infer√™ncia. A vantagem de utilizar <a href="https://huggingface.co/docs/transformers/main_classes/pipelines#multimodal">pipelines</a> √© que eles abstraem boa parte da codifica√ß√£o complexa, proporcionando uma interface simples e eficiente. Essa API vers√°til √© dedicada a v√°rias tarefas, como NER (Reconhecimento de Entidades), An√°lise de Sentimentos, Extra√ß√£o de Features e Question Answering.</p>
<pre class="python"><code>for i in tqdm(range(df_sample.shape[0])):
    
    # preparar objetos do loop
    coco_url = df_sample.iloc[i][&#39;coco_url&#39;]
    caption = df_sample.iloc[i][&#39;caption&#39;]
    index = df_sample.iloc[i].name
    
    # Obter imagem
    response = requests.get(coco_url)
    image = Image.open(BytesIO(response.content))
    
    # Realizar a infer√™ncia usando o pipeline e o prompt gerado
    outputs = pipe(image, prompt=prompt, generate_kwargs={&quot;max_new_tokens&quot;: 32})
    
    # Processar o texto gerado para extrair a parte relevante
    result = outputs[0][&#39;generated_text&#39;].split(&#39;ASSISTANT:&#39;, 1)[1].strip()
    
    # Adicionar o resultado da infer√™ncia √† nova coluna &#39;llm&#39; do DataFrame
    df_sample.loc[index, &#39;llm&#39;] = result</code></pre>
<pre><code>100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:41&lt;00:00,  4.20s/it]</code></pre>
<p>Ap√≥s a execu√ß√£o do modelo, veja como ficaram os resultados:</p>
<details>
<summary>
<em>Expandir c√≥digo</em>
</summary>
<pre class="python"><code># Fun√ß√£o para destacar as palavras
def highlight_diff(caption, llm):
    # Divide as frases em palavras
    caption_words = caption.replace(&quot;.&quot;, &quot;&quot;).split()
    llm_words = llm.replace(&quot;.&quot;, &quot;&quot;).split()
    
    # Converte as palavras em conjuntos para encontrar a interse√ß√£o
    caption_set = set(caption_words)
    llm_set = set(llm_words)
    
    # Calcula as palavras que n√£o est√£o na interse√ß√£o
    caption_highlighted = &quot; &quot;.join([f&#39;&lt;span style=&quot;color:red&quot;&gt;{word}&lt;/span&gt;&#39; if word not in llm_set else word for word in caption_words])
    llm_highlighted = &quot; &quot;.join([f&#39;&lt;span style=&quot;color:red&quot;&gt;{word}&lt;/span&gt;&#39; if word not in caption_set else word for word in llm_words])
    
    return caption_highlighted, llm_highlighted

# Aplica a fun√ß√£o a cada linha do DataFrame e cria novas colunas
df_sample[&#39;highlighted_caption&#39;], df_sample[&#39;highlighted_llm&#39;] = zip(*df_sample.apply(lambda row: highlight_diff(row[&#39;caption&#39;], row[&#39;llm&#39;]), axis=1))

# Exibir o DataFrame formatado com HTML
HTML(df_sample[[&#39;image&#39;, &#39;highlighted_caption&#39;, &#39;highlighted_llm&#39;]].to_html(escape=False))</code></pre>
</details>
<p><br></p>
<p><img src="/post/2024-09-27-image-text-to-text/df2.png" alt="extra√ß√£o de r√≥tulos descritivos de imagens com Llava" style="width: 100%;"></p>
<p>Destaquei em vermelho as palavras que diferem entre a legenda original do dataset e a previs√£o gerada pelo nosso modelo de linguagem.</p>
<div class="w3-panel w3-pale-yellow w3-border">
<p>¬† üí≠ Apesar de algumas diferen√ßas sutis entre as duas vers√µes, como ‚Äòlooking at his phone‚Äô e ‚Äòlooking at his <span style="color:red;">cell</span> phone‚Äô, a ideia principal permanece bastante coerente com o que vemos nas imagens. Em alguns casos, como no item 3, a descri√ß√£o gerada pelo modelo, ‚Äòholding a <span style="color:red;">kite</span>‚Äô, parece at√© mais apropriada do que a fornecida pelo dataset, ‚Äòholding a <span style="color:red;">flag</span>‚Äô.</p>
</div>
<p>Agora, o pr√≥ximo passo ser√° quantificar essas diferen√ßas de maneira num√©rica.</p>
</div>
<div id="avaliar-modelo" class="section level1">
<h1>Avaliar modelo</h1>
<p>Para medir a precis√£o das legendas geradas, aplicaremos quatro m√©tricas amplamente usadas:</p>
<ul>
<li><strong><a href="https://aclanthology.org/P02-1040.pdf">BLEU</a> (Bilingual Evaluation Understudy Score)</strong>: Amplamente utilizada para medir a qualidade de tradu√ß√µes autom√°ticas, mede a <strong>sobreposi√ß√£o de n-gramas</strong> entre a tradu√ß√£o gerada por um modelo e as tradu√ß√µes de refer√™ncia, atribuindo uma pontua√ß√£o que varia de 0 a 1 (aplica tamb√©m um fator de penaliza√ß√£o para evitar que tradu√ß√µes curtas sejam favorecidas);</li>
<li><strong><a href="https://aclanthology.org/W04-1013.pdf">ROUGE-L</a> (Recall-Oriented Understudy for Gisting Evaluation)</strong>: Muito utilizado em tarefa de sumariza√ß√£o de textos, considera a sequ√™ncia mais longa de palavras que aparecem em ambas as refer√™ncias e previs√µes, medindo a capacidade de preservar a <strong>ordem das palavras</strong>;</li>
<li><strong><a href="https://www.cs.cmu.edu/~alavie/METEOR/">METEOR</a> (Metric for Evaluation of Translation with Explicit ORdering)</strong>: Baseada na m√©dia harm√¥nica da precis√£o e recall de n-gramas, com recall ponderado mais alto do que a precis√£o. Essa m√©trica METEOR foi projetada para corrigir alguns dos problemas (como encontrar sin√¥nimos) nas m√©tricas BLEU e ROGUE;</li>
<li><strong><a href="https://huggingface.co/spaces/evaluate-metric/bertscore">BERTScore</a></strong>: Usa embeddings (representa√ß√µes sem√¢nticas) obtidas a partir do modelo BERT para comparar a similaridade sem√¢ntica entre as descri√ß√µes geradas e as de refer√™ncia.</li>
</ul>
<details>
<summary>
<em>Expandir c√≥digo</em>
</summary>
<pre class="python"><code># Fun√ß√µes para calcular as m√©tricas
def calcular_bleu(referencias, previsao):
    return sentence_bleu([referencias.split(&quot; &quot;)], previsao.split(&quot; &quot;),weights = [1])

def calcular_rouge(referencias, previsao):
    scorer = rouge_scorer.RougeScorer([&#39;rougeL&#39;], use_stemmer=True)
    return scorer.score(referencias, previsao)[&#39;rougeL&#39;].fmeasure

def calcular_meteor(referencias, previsao):
    return meteor_score([referencias.split(&quot; &quot;)], previsao.split(&quot; &quot;))

def calcular_bertscore(referencias, previsao):
    P, R, F1 = bert_score([previsao], [referencias], lang=&quot;en&quot;, verbose=True)
    return F1.mean().item()</code></pre>
</details>
<p><br></p>
<pre class="python"><code>%%capture

# Avaliar as amostras no DataFrame
resultados = []
for i, row in df_sample.iterrows():
    
    referencias = row[&#39;caption&#39;].replace(&quot;.&quot;, &quot;&quot;)
    previsao = row[&#39;llm&#39;].replace(&quot;.&quot;, &quot;&quot;)
    
    bleu = calcular_bleu(referencias, previsao)
    rouge = calcular_rouge(referencias, previsao)
    meteor = calcular_meteor(referencias, previsao)
    bert = calcular_bertscore(referencias, previsao)
    
    resultados.append([referencias, previsao, bleu, rouge, meteor, bert])

# Converter os resultados para um DataFrame
df_resultados = pd.DataFrame(resultados, columns=[&#39;caption&#39;, &#39;llm&#39;, &#39;BLEU&#39;, &#39;ROUGE&#39;, &#39;METEOR&#39;, &#39;BERTScore&#39;])</code></pre>
<p>Vejamos os resultados:</p>
<details>
<summary>
<em>Expandir c√≥digo</em>
</summary>
<pre class="python"><code># Configurar o tema do Seaborn
sns.set_theme(style=&quot;white&quot;, rc={&quot;axes.facecolor&quot;: (0, 0, 0, 0)})

# Reformatar o DataFrame para o formato long
df_long = df_resultados[[&#39;BLEU&#39;, &#39;ROUGE&#39;, &#39;METEOR&#39;, &#39;BERTScore&#39;]].melt(var_name=&quot;M√©trica&quot;, value_name=&quot;Valor&quot;)

# Calcular a m√©dia de cada m√©trica
mean_values = df_long.groupby(&#39;M√©trica&#39;)[&#39;Valor&#39;].mean().reset_index()

# Inicializar o objeto FacetGrid
pal = sns.cubehelix_palette(len(df_long[&#39;M√©trica&#39;].unique()), rot=-.25, light=.7)
g = sns.FacetGrid(df_long, row=&quot;M√©trica&quot;, hue=&quot;M√©trica&quot;, aspect=6, height=1.5, palette=pal)

# Desenhar as densidades
g.map(sns.kdeplot, &quot;Valor&quot;, 
      bw_adjust=.5, clip_on=False, 
      fill=True, alpha=1, linewidth=1.5)
g.map(sns.kdeplot, &quot;Valor&quot;, clip_on=False, color=&quot;w&quot;, lw=2, bw_adjust=.5)

# Adicionar linha de refer√™ncia
g.refline(y=0, linewidth=2, linestyle=&quot;-&quot;, color=None, clip_on=False)

# Fun√ß√£o para rotular o gr√°fico
def label(x, color, label):
    ax = plt.gca()
    # Localizar a m√©dia correspondente √† m√©trica
    mean_value = mean_values[mean_values[&#39;M√©trica&#39;] == label][&#39;Valor&#39;].values[0]
    ax.text(0, .4, f&quot;{label} (M√©dia: {mean_value:.2f})&quot;, fontweight=&quot;bold&quot;, color=color,
            ha=&quot;left&quot;, va=&quot;center&quot;, transform=ax.transAxes, fontsize=20)

g.map(label, &quot;Valor&quot;)

# Ajustar espa√ßamento entre subplots manualmente
g.figure.subplots_adjust(hspace=0.2)

# Remover detalhes desnecess√°rios dos eixos
g.set_titles(&quot;&quot;)
g.set(yticks=[], ylabel=&quot;&quot;)
g.despine(bottom=True, left=True)

# Configurar o eixo x
g.set(xlim=(0.4, 1), xticks=np.arange(0.4, 1.05, 0.1))  # Limites e ticks do eixo x

# Remover r√≥tulos do eixo x em cada subplot
for ax in g.axes.flat:
    ax.set_xlabel(&quot;&quot;)  # Remover r√≥tulo do eixo x
    ax.tick_params(axis=&#39;x&#39;, labelsize=16)  # Aumentar o tamanho da fonte dos ticks do eixo x

# Exibir o gr√°fico
plt.show()</code></pre>
</details>
<p><br></p>
<center>
<img src="/post/2024-09-27-image-text-to-text/metrics.png" alt="metricas da extra√ß√£o de r√≥tulos descritivos de imagens com Llava" style="width: 80%;">
</center>
<div class="w3-panel w3-pale-yellow w3-border">
<p>¬† <strong>üìå Insights ao Avaliar as M√©tricas do Modelo: </strong></p>
<ul>
<li><p>As m√©tricas baseadas em <strong>n-grams e na correspond√™ncia de palavras</strong> mostraram desempenho <strong>subestimado</strong>. Embora o modelo tenha apresentado algumas varia√ß√µes na escolha das palavras, as frases geradas mantiveram um sentido geral muito semelhante ao que √© retratado nas imagens.</p></li>
<li><p>Por outro lado, a m√©trica baseada em <strong>embeddings</strong>, que avalia o significado <strong>sem√¢ntico</strong> das frases, apresentou resultados <strong>significativamente superiores</strong>. Essa abordagem se mostrou mais congruente em avaliar a similaridade das descri√ß√µes geradas e a descri√ß√£o informada do conte√∫do visual das imagens.</p></li>
<li><p>√â importante ressaltar que nosso <strong>prompt</strong> foi mantido na forma <strong>mais simples poss√≠vel</strong> e que o conjunto de dados abrange um <strong>escopo bastante amplo</strong>. Com isso, acredito que o modelo ainda tem muito potencial para oferecer resultados ainda mais robustos, sem a necessidade de ajustes finos, em tarefas mais espec√≠ficas.</p></li>
</ul>
</div>
</div>
<div id="conclus√£o" class="section level1">
<h1>Conclus√£o</h1>
<p>O uso da GenAI com o modelo Llava oferece uma solu√ß√£o eficiente para a extra√ß√£o de features de imagens em Python, possibilitando a cria√ß√£o de descri√ß√µes ricas e detalhadas. Ao comparar a qualidade das sa√≠das com m√©tricas como BLEU, podemos garantir que o modelo esteja oferecendo resultados satisfat√≥rios para as necessidades do projeto.</p>
<p>Se voc√™ deseja automatizar processos de an√°lise de imagens, explorar a cria√ß√£o de modelos customizados ou otimizar a organiza√ß√£o de dados visuais, a utiliza√ß√£o de GenAI com modelos como o Llava pode ser um divisor de √°guas em seus projetos.</p>
<p>Se este conte√∫do foi √∫til, continue acompanhando o blog para mais tutoriais sobre intelig√™ncia artificial e Python!</p>
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias</h1>
<ul>
<li><a href="https://huggingface.co/llava-hf/llava-1.5-7b-hf" class="uri">https://huggingface.co/llava-hf/llava-1.5-7b-hf</a></li>
<li><a href="https://github.com/haotian-liu/LLaVA" class="uri">https://github.com/haotian-liu/LLaVA</a></li>
<li><a href="https://colab.research.google.com/drive/1qsl6cd2c8gGtEW1xV5io7S8NHh-Cp1TV?usp=sharing#scrollTo=6Bx8iu9jOssW" class="uri">https://colab.research.google.com/drive/1qsl6cd2c8gGtEW1xV5io7S8NHh-Cp1TV?usp=sharing#scrollTo=6Bx8iu9jOssW</a></li>
<li><a href="https://cocodataset.org/#explore" class="uri">https://cocodataset.org/#explore</a></li>
<li><a href="https://www.kaggle.com/datasets/awsaf49/coco-2017-dataset/" class="uri">https://www.kaggle.com/datasets/awsaf49/coco-2017-dataset/</a></li>
</ul>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2024-09-27-image-text-to-text/">Extra√ß√£o de informa√ß√µes de imagens com IA Generativa</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">chatgpt</category>
      <category domain="tag">data-science</category>
      <category domain="tag">genai</category>
      <category domain="tag">ia-generativa</category>
      <category domain="tag">inteligencia-artificial</category>
      <category domain="tag">llama</category>
      <category domain="tag">llama2</category>
      <category domain="tag">llava</category>
      <category domain="tag">llm</category>
      <category domain="tag">lmm</category>
    </item>
  </channel>
</rss>