&lt;?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>optuna on Fellipe Gomes - Data Science Blog</title>
    <link>https://gomesfellipe.github.io/tags/optuna/</link>
    <description>√öltimos posts sobre Data Science, Machine Learning e R</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>pt-br</language>
    <managingEditor>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</managingEditor>
    <webMaster>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</webMaster>
    <lastBuildDate>Mon, 01 Nov 2021 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://gomesfellipe.github.io/tags/optuna/" rel="self" type="application/rss+xml" />
    <item>
      <title>Solu√ß√£o Final - Porto Seguro Data Challenge [3¬∫ lugar]</title>
      <link>https://gomesfellipe.github.io/post/2021-11-01-solucao-final-porto-seguro-data-challenge/</link>
      <pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2021-11-01-solucao-final-porto-seguro-data-challenge/</guid>
      <description>Confira a estrat√©gia aplicada para a competi√ß√£o de machine learning do Porto Seguro hospedada no Kaggle</description>
      <content:encoded>&lt;![CDATA[
        

<div id="TOC">
<ul>
<li><a href="#introdu%C3%A7%C3%A3o" id="toc-introdu√ß√£o">Introdu√ß√£o</a></li>
<li><a href="#defini%C3%A7%C3%A3o-do-problema-de-neg%C3%B3cio" id="toc-defini√ß√£o-do-problema-de-neg√≥cio">Defini√ß√£o do problema de neg√≥cio</a></li>
<li><a href="#an%C3%A1lise-explorat%C3%B3ria-em-r" id="toc-an√°lise-explorat√≥ria-em-r">An√°lise Explorat√≥ria (em R)</a></li>
<li><a href="#machine-learning-em-python" id="toc-machine-learning-em-python">Machine Learning (em Python)</a>
<ul>
<li><a href="#importar-depend%C3%AAncias" id="toc-importar-depend√™ncias">Importar depend√™ncias</a></li>
<li><a href="#stage-0-feature-extraction-com-knn" id="toc-stage-0-feature-extraction-com-knn">Stage 0: Feature Extraction com KNN</a></li>
<li><a href="#stage-1-tuning-xgboost-com-optuna" id="toc-stage-1-tuning-xgboost-com-optuna">Stage 1: Tuning XGBoost com Optuna</a></li>
<li><a href="#stage-2-calcular-out-of-fold-shap-values" id="toc-stage-2-calcular-out-of-fold-shap-values">Stage 2: Calcular Out-Of-Fold SHAP values</a></li>
<li><a href="#stage-3-modelo-final-com-autogluon" id="toc-stage-3-modelo-final-com-autogluon">Stage 3: Modelo Final com AutoGluon</a></li>
</ul></li>
<li><a href="#conclus%C3%A3o" id="toc-conclus√£o">Conclus√£o</a></li>
<li><a href="#refer%C3%AAncias" id="toc-refer√™ncias">Refer√™ncias</a></li>
</ul>
</div>

<style>
.column {
float: left;
width: 50%;
padding: 10px;
}

.column4 {
float: left;
width: 20%;
padding: 10px;
}

.column8 {
float: left;
width: 80%;
padding: 10px;
}

.row:after {
content: "";
display: table;
clear: both;
}

.center {
display: flex;
justify-content: center;
align-items: center;
height: 200px;
}
</style>
<hr />
<div id="introdu√ß√£o" class="section level1">
<h1>Introdu√ß√£o</h1>
<div class="row">
<div class="column8">
<p>Em Agosto e 2021 a Porto Seguro lan√ßou um desafio no Kaggle que consistia em estimar a propens√£o de aquisi√ß√£o de novos produtos. Tratava-se de um problema de classifica√ß√£o e foi bem desafiador principalmente por 2 motivos:</p>
<ol style="list-style-type: decimal">
<li>Todas as features da base de ddos eram anonimas;</li>
<li>A m√©trica de avalia√ß√£o foi a F1 Score (sens√≠vel √† um ponto de corte)</li>
</ol>
</div>
<div class="column4">
<div class="float">
<img src="https://media.giphy.com/media/Ie2Hs3A0uJRtK/giphy.gif" alt="Via Giphy" />
<div class="figcaption"><a href="https://media.giphy.com/media/Ie2Hs3A0uJRtK/giphy.gif">Via Giphy</a></div>
</div>
</div>
</div>
<p>Depois de 2 longos meses e dezenas de notebooks desenvolvidos, muitas submiss√µes frustradas e muitas horas a menos de sono, cheguei em uma solu√ß√£o final que envole um <em>blending</em> de modelos e <em>pseudo-labels</em> e quando a competi√ß√£o acabou, percebi que uma solu√ß√£o mais simples de implementar teria um resultado privado ainda maior do que o notebook que selecionei. üòÖ</p>
<div class="w3-panel w3-sand w3-border">
<p>‚ö†Ô∏è Aten√ß√£o!</p>
<p>Neste post abordarei uma solu√ß√£o mais simples e eficiente mas caso tenha interesse em conferir a solu√ß√£o final completa (um grande frankstein), j√° est√° <a href="https://github.com/gomesfellipe/porto_seguro_data_challenge">publica la no github</a>.</p>
</div>
<p>Este notebook √© uma <a href="https://www.kaggle.com/gomes555/3st-place-simplified-solution-0-6967-private">reescritura do meu notebook publicado no Kaggle em linguagem Python</a>. Para quem acompanha meus posts de R pode achar meio estranho este notebook mas convido-o a tentar entender a solu√ß√£o pois foi desenvolvida pela perspectiva de um usu√°rio nativo de R.</p>
<p>Espero que gostem! ü§ò</p>
</div>
<div id="defini√ß√£o-do-problema-de-neg√≥cio" class="section level1">
<h1>Defini√ß√£o do problema de neg√≥cio</h1>
<p>Segundo a descri√ß√£o da competi√ß√£o:</p>
<blockquote>
<p>Voc√™ provavelmente j√° recebeu uma liga√ß√£o de telemarketing oferecendo um produto que voc√™ n√£o precisa. Essa situa√ß√£o de estresse √© minimizada quando voc√™ oferece um produto que o cliente realmente precisa. <br /><br /> Nessa competi√ß√£o voc√™ ser√° desafiado a construir um modelo que prediz a probabilidade de aquisi√ß√£o de um produto.</p>
</blockquote>
<p>Sobre a m√©trica de avalia√ß√£o:</p>
<p>O crit√©rio utilizado para defini√ß√£o da melhor solu√ß√£o ser√° o F1-Score, veja sua formula:</p>
<p><span class="math display">\[
F_1 = 2 \times \frac{precision \times recall}{precision + recall}  
\]</span></p>
<p>Note que tanto a <em>Precision</em> quanto a <em>Recall</em> precisam de um ponto de corte para obter as classes e por isso busquei otmizar as m√©tricas <em>ROC-AUC</em> e <em>Log Loss</em> para obter estimativas de probabilidades com qualidade para finalmente calcular os pontos de corte que maximizam a <em>F1</em>.</p>
</div>
<div id="an√°lise-explorat√≥ria-em-r" class="section level1">
<h1>An√°lise Explorat√≥ria (em R)</h1>
<p>Antes de partir para modelagem fiz uma an√°lise explorat√≥ria utilizando a linguagem R. Neste post tratarei de maneira bem breve e quem tiver interesse em conferir mais detalhes bem como os c√≥digos dos gr√°ficos basta acessar o <a href="https://www.kaggle.com/gomes555/porto-seguro-r-an-lise-explorat-ria-dos-dados">notebook que deixei aberto no Kaggle</a>.</p>
<p>Veja alguns gr√°ficos:</p>
<center>
<img src="/post/2021-11-01-solucao-final-porto-seguro-data-challenge/aed.png" style="width:90.0%" />
</center>
<!-- <div class="w3-panel w3-light-blue w3-border"> -->
<p><strong>üìå Interpreta√ß√£o:</strong> <br></p>
<ul>
<li><strong>Categ√≥ricas</strong>:
<div style="color: rgb(0, 0, 0);">
<ul>
<li>
<strong>Qualitativo nominal</strong>: Possuem muitas classes, poderia ser o nome do produto, regi√£o, um texto o que torna o desafio ainda maior para criar novas features;
</li>
<li>
<strong>Qualitativo ordinal</strong>: Basicamente deixei como veio pois j√° tava como numerico;
</li>
</ul>
</div></li>
<li><strong>Num√©ricas</strong>:
<div style="color: rgb(0, 0, 0);">
<ul>
<li>
<strong>Quantitativo continua</strong>: Todas est√£o normalizadas (0, 1), algumas s√£o bimodais, algumas assim√©tricas a direita (pode ser tempo ate alguma coisa);
</li>
<li>
<strong>Quantitativo discreto</strong>: Sem muito o que fazer, observa√ß√£o apenas a feature <code>var52</code> que parece idade
</li>
</ul>
</div></li>
<li><strong>Dados missing</strong>: Parece haver algum padr√£o na maneira como os dados missing ocorrem e tentei substituir os <code>-999</code> por <code>NaN</code>, imputar a m√©dia, a mediana e via outros modelos
<!-- </div> --></li>
</ul>
<p>N√£o achei que seria muito produtivo ficar adivinhando o que poderia ser cada feature pois praticamente todos as transforma√ß√µes e novas features que gerei n√£o superavam o resultado do modelo ajustado nos dados da maneira que vinham portanto procurei investir mais tempo na modelagem mesmo.</p>
</div>
<div id="machine-learning-em-python" class="section level1">
<h1>Machine Learning (em Python)</h1>
<p>Veja a estrat√©gia de modelagem de maneira visual:</p>
</br>
<center>
<img src="/post/2021-11-01-solucao-final-porto-seguro-data-challenge/final_pipeline.png" style="width:60.0%" />
</center>
<p></br></p>
<div id="importar-depend√™ncias" class="section level2">
<h2>Importar depend√™ncias</h2>
<p>Carregar pacotes do Python</p>
<pre class="python"><code># general packages
import pandas as pd
import numpy as np
import time
# knn features
from gokinjo import knn_kfold_extract
from gokinjo import knn_extract
# ml tools
from sklearn.model_selection import StratifiedKFold, KFold
from sklearn.metrics import f1_score, log_loss, roc_auc_score
# models
from catboost import CatBoostClassifier
from xgboost import XGBClassifier
# optimization
import optuna
# interpretable ml
import shap
# automl
from autogluon.tabular import TabularPredictor
# ignore specific warnings
import warnings
warnings.filterwarnings(&quot;ignore&quot;, message=&quot;ntree_limit is deprecated, use `iteration_range` or model slicing instead.&quot;)</code></pre>
<p>Definir fun√ß√µes auxiliares para calcular o ponto de corte que maximiza a F1:</p>
<pre class="python"><code>def get_threshold(y_true, y_pred):
    thresholds = np.arange(0.0, 1.0, 0.01)
    f1_scores = []
    for thresh in thresholds:
        f1_scores.append(
            f1_score(y_true, [1 if m&gt;thresh else 0 for m in y_pred]))
    f1s = np.array(f1_scores)
    return thresholds[f1s.argmax()]
    
def custom_f1(y_true, y_pred):
    max_f1_threshold =  get_threshold(y_true, y_pred)
    y_pred = np.where(y_pred&gt;max_f1_threshold, 1, 0)
    return f1_score(y_true, y_pred) </code></pre>
<p>Carregar <a href="https://www.kaggle.com/c/porto-seguro-data-challenge/data">dados da competi√ß√£o</a>:</p>
<pre class="python"><code># load data
train = pd.read_csv(&#39;../input/porto-seguro-data-challenge/train.csv&#39;).drop(&#39;id&#39;, axis=1)
test = pd.read_csv(&#39;../input/porto-seguro-data-challenge/test.csv&#39;).drop(&#39;id&#39;, axis=1)
sample_submission = pd.read_csv(&#39;../input/porto-seguro-data-challenge/submission_sample.csv&#39;)
meta = pd.read_csv(&#39;../input/porto-seguro-data-challenge/metadata.csv&#39;)

# get data types
cat_nom = [x for x in meta.iloc[1:-1, :].loc[(meta.iloc[:,1]==&quot;Qualitativo nominal&quot;)].iloc[:,0]] 
cat_ord = [x for x in meta.iloc[1:-1, :].loc[(meta.iloc[:,1]==&quot;Qualitativo ordinal&quot;)].iloc[:,0]] 
num_dis = [x for x in meta.iloc[1:-1, :].loc[(meta.iloc[:,1]==&quot;Quantitativo discreto&quot;)].iloc[:,0]] 
num_con = [x for x in meta.iloc[1:-1, :].loc[(meta.iloc[:,1]==&quot;Quantitativo continua&quot;)].iloc[:,0]] </code></pre>
</div>
<div id="stage-0-feature-extraction-com-knn" class="section level2">
<h2>Stage 0: Feature Extraction com KNN</h2>
<p>Esta t√©cnica gera <span class="math inline">\(k \times c\)</span> novas features, onde <span class="math inline">\(c\)</span> √© o n√∫mero de classes da target. As novas features s√£o calculadas a partir das dist√¢ncias entre as observa√ß√µes e seus k vizinhos mais pr√≥ximos dentro de cada classe;</p>
<p>O valor para os <span class="math inline">\(K\)</span> vizinhos mais pr√≥ximos selecionado foi <span class="math inline">\(K=1\)</span> e para isso utilizei a biblioteca
<a href="https://github.com/momijiame/gokinjo"><code>gokinjo</code></a> que foi <a href="https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335">inspirada nas id√©ias apresentadas na solu√ß√£o vencedora do Otto Group Product Classification Challenge.</a></p>
<pre class="python"><code># convert to numpy because gokinjo expects np arrays
X = train[cat_nom+cat_ord+num_dis+num_con].to_numpy()
y = train.y.to_numpy()
X_test = test[cat_nom+cat_ord+num_dis+num_con].to_numpy()

# extract on train data
KNN_feat_train = knn_kfold_extract(X, y, k=1, normalize=&#39;standard&#39;)
print(&quot;KNN features for training set, shape: &quot;, np.shape(KNN_feat_train))

# extract on test data
KNN_feat_test = knn_extract(X, y, X_test, k=1, normalize=&#39;standard&#39;)
print(&quot;KNN features for test set, shape: &quot;, np.shape(KNN_feat_test))

# convert to dataframe
knn_feat_train = pd.DataFrame(KNN_feat_train, columns=[&quot;knn&quot;+str(x) for x in range(knn_feat_train.shape[1])])
knn_feat_test = pd.DataFrame(KNN_feat_test, columns=[&quot;knn&quot;+str(x) for x in range(knn_feat_test.shape[1])])</code></pre>
<pre><code>## KNN features for training set, shape:  (14123, 2)
## KNN features for test set, shape:  (21183, 2)</code></pre>
</div>
<div id="stage-1-tuning-xgboost-com-optuna" class="section level2">
<h2>Stage 1: Tuning XGBoost com Optuna</h2>
<p>Testei e otimizei muitos modelos como XGBoost, NGBoost, LightGBM, CatBoost, TabNet, HistGradientBoosting e algumas DNNs e em todos os casos (exceto DNNs) utilizei o Optuna para a sele√ß√£o dos hiperpar√¢metros.</p>
<p>Tamb√©m inclui nas tentativas iniciais de otimiza√ß√£o alguns m√©todos de remostrarem como Random Under Sampling, Smote, Tomek, Adasyn dentre outros mas n√£o tive muito sucesso.. apenas a combina√ß√£o Tomek + CatBoost pareceu trazer algum ganho.</p>
<p>Claro que minhas tentativas n√£o foram exautivas e devido ao tempo limitado acabei selecionando o XGBoost que foi o que apresentou as melhores m√©tricas depois de otimizado e tamb√©m o CatBoost com alguns hiperpar√¢metros fixos para serem a base deste pipeline.</p>
<p>Principais Informa√ß√µes üìå :</p>
<ul>
<li>Nenhum pr√©-processamento;</li>
<li>KFold K=10;</li>
<li>Otimiza√ß√£o de hiperpar√¢metros com Optuna;</li>
<li>Loss do XGBoost: Log Loss;</li>
<li>Loss do Otimizador: Log Loss;</li>
<li>Sem resampling;</li>
<li>Previs√£o final com a probabilidade m√©dia de 10 seeds diferentes</li>
</ul>
<pre class="python"><code>X_test = test[cat_nom+cat_ord+num_dis+num_con]
X = train[cat_nom+cat_ord+num_dis+num_con]
y = train.y

K=10
SEED=314
kf = KFold(n_splits=K, random_state=SEED, shuffle=True)</code></pre>
<pre class="python"><code>fixed_params = {
    &#39;random_state&#39;: 9,
    &quot;objective&quot;: &quot;binary:logistic&quot;,
    &quot;eval_metric&quot;: &#39;logloss&#39;,
    &#39;use_label_encoder&#39;:False,
    &#39;n_estimators&#39;:10000,
}

def objective(trial):
    
    hyperparams = {
        &#39;clf&#39;:{
        &quot;booster&quot;: trial.suggest_categorical(&quot;booster&quot;, [&quot;gbtree&quot;]),
        &quot;lambda&quot;: trial.suggest_float(&quot;lambda&quot;, 1e-8, 5.0, log=True),
        &quot;alpha&quot;: trial.suggest_float(&quot;alpha&quot;, 1e-8, 5.0, log=True)
        }
    }
    
    if hyperparams[&#39;clf&#39;][&quot;booster&quot;] == &quot;gbtree&quot; or hyperparams[&#39;clf&#39;][&quot;booster&quot;] == &quot;dart&quot;:
        hyperparams[&#39;clf&#39;][&quot;max_depth&quot;] = trial.suggest_int(&quot;max_depth&quot;, 1, 9)
        hyperparams[&#39;clf&#39;][&quot;eta&quot;] = trial.suggest_float(&quot;eta&quot;, 0.01, 0.1, log=True)
        hyperparams[&#39;clf&#39;][&quot;gamma&quot;] = trial.suggest_float(&quot;gamma&quot;, 1e-8, 1.0, log=True)
        hyperparams[&#39;clf&#39;][&quot;grow_policy&quot;] = trial.suggest_categorical(&quot;grow_policy&quot;, [&quot;depthwise&quot;, &quot;lossguide&quot;])
        hyperparams[&#39;clf&#39;][&#39;min_child_weight&#39;] = trial.suggest_int(&#39;min_child_weight&#39;, 5, 20)
        hyperparams[&#39;clf&#39;][&quot;subsample&quot;] = trial.suggest_float(&quot;subsample&quot;, 0.03, 1)
        hyperparams[&#39;clf&#39;][&quot;colsample_bytree&quot;] = trial.suggest_float(&quot;colsample_bytree&quot;, 0.03, 1)
        hyperparams[&#39;clf&#39;][&#39;max_delta_step&#39;] = trial.suggest_float(&#39;max_delta_step&#39;, 0, 10)
        
    if hyperparams[&#39;clf&#39;][&quot;booster&quot;] == &quot;dart&quot;:
        hyperparams[&#39;clf&#39;][&quot;sample_type&quot;] = trial.suggest_categorical(&quot;sample_type&quot;, [&quot;uniform&quot;, &quot;weighted&quot;])
        hyperparams[&#39;clf&#39;][&quot;normalize_type&quot;] = trial.suggest_categorical(&quot;normalize_type&quot;, [&quot;tree&quot;, &quot;forest&quot;])
        hyperparams[&#39;clf&#39;][&quot;rate_drop&quot;] = trial.suggest_float(&quot;rate_drop&quot;, 1e-8, 1.0, log=True)
        hyperparams[&#39;clf&#39;][&quot;skip_drop&quot;] = trial.suggest_float(&quot;skip_drop&quot;, 1e-8, 1.0, log=True)
    
    params = dict(**fixed_params, **hyperparams[&#39;clf&#39;])
    xgb_oof = np.zeros(X.shape[0])

    for fold, (train_idx, val_idx) in enumerate(kf.split(X=X, y=y)):
        X_train = X.iloc[train_idx]
        y_train = y.iloc[train_idx]
        X_val = X.iloc[val_idx]
        y_val = y.iloc[val_idx]
        
        model = XGBClassifier(**params)
        
        model.fit(X_train, y_train,
                  eval_set=[(X_val, y_val)],
                  early_stopping_rounds=150,
                  verbose=False)
    
        xgb_oof[val_idx] = model.predict_proba(X_val)[:,1]

        del model

    return log_loss(y, xgb_oof)</code></pre>
<p>Como no Kaggle existe o limite de aproximadamente 8h para executar um notebook, coloquei um limite de 7.5 horas para a busca de hiperpar√¢metros:</p>
<pre class="python"><code>study_xgb = optuna.create_study(direction=&#39;minimize&#39;)

study_xgb.optimize(objective, 
               timeout=60*60*7.5, 
               gc_after_trial=True)</code></pre>
<p>Resultados da busca:</p>
<pre class="python"><code>print(&#39;-&gt; Number of finished trials: &#39;, len(study_xgb.trials))
print(&#39;-&gt; Best trial:&#39;)
trial = study_xgb.best_trial
print(&#39;\tValue: {}&#39;.format(trial.value))
print(&#39;-&gt; Params: &#39;)
trial.params</code></pre>
<pre><code>## -&gt; Number of finished trials:  197
## -&gt; Best trial:
## 	Value: 0.3028443879614926
## -&gt; Params: 
## {&#39;booster&#39;: &#39;gbtree&#39;,
##  &#39;lambda&#39;: 9.012384508756378e-07,
##  &#39;alpha&#39;: 0.7472040331088792,
##  &#39;max_depth&#39;: 5,
##  &#39;eta&#39;: 0.01507605562231303,
##  &#39;gamma&#39;: 1.0214961302342215e-08,
##  &#39;grow_policy&#39;: &#39;lossguide&#39;,
##  &#39;min_child_weight&#39;: 5,
##  &#39;subsample&#39;: 0.9331005225916879,
##  &#39;colsample_bytree&#39;: 0.25392142363325004,
##  &#39;max_delta_step&#39;: 5.685109389498008}</code></pre>
<p>Acompanhar o hist√≥rico de cada etapa da otimiza√ß√£o:</p>
<pre class="python"><code>plot_optimization_history(study_xgb)</code></pre>
<center>
<img src="/post/2021-11-01-solucao-final-porto-seguro-data-challenge/optimization_hist.png" style="width:90.0%" />
</center>
<p>Avaliar as combina√ß√µes de hiperpar√¢metros mais bem sucedidas:</p>
<pre class="python"><code>optuna.visualization.plot_parallel_coordinate(study_xgb)</code></pre>
<center>
<img src="/post/2021-11-01-solucao-final-porto-seguro-data-challenge/parallel_plot.png" style="width:90.0%" />
</center>
<p>Quais hiperpar√¢metros tiveram mais impacto na modelagem:</p>
<pre class="python"><code>plot_param_importances(study_xgb)</code></pre>
<center>
<img src="/post/2021-11-01-solucao-final-porto-seguro-data-challenge/param_imp.png" style="width:90.0%" />
</center>
<p>Ap√≥s as 7.5 horas de busca, a melhor combina√ß√£o encontrada para o XGBoost foi a seguinte:</p>
<pre class="python"><code># After 7.5 hours...
study_xgb = {&#39;booster&#39;: &#39;gbtree&#39;,
 &#39;lambda&#39;: 9.012384508756378e-07,
 &#39;alpha&#39;: 0.7472040331088792,
 &#39;max_depth&#39;: 5,
 &#39;eta&#39;: 0.01507605562231303,
 &#39;gamma&#39;: 1.0214961302342215e-08,
 &#39;grow_policy&#39;: &#39;lossguide&#39;,
 &#39;min_child_weight&#39;: 5,
 &#39;subsample&#39;: 0.9331005225916879,
 &#39;colsample_bytree&#39;: 0.25392142363325004,
 &#39;max_delta_step&#39;: 5.685109389498008}</code></pre>
<p>Preparar lista de hiperpar√¢metros do XGBoost:</p>
<pre class="python"><code>final_params_xgb = dict()
final_params_xgb[&#39;clf&#39;]=dict(**fixed_params, **study_xgb)</code></pre>
</div>
<div id="stage-2-calcular-out-of-fold-shap-values" class="section level2">
<h2>Stage 2: Calcular Out-Of-Fold SHAP values</h2>
<p>Ap√≥s obter a melhor combina√ß√£o de hiperpar√¢metros para o XGBoost e encontrar resultados formid√°veis com o CatBoost modificando apenas alguns hiperpar√¢metros, resolvi tentar utilizar a informa√ß√£o adquirida pelo <em>SHAP values</em> desses modelos como entrada para novos modelos.</p>
<p>Algumas vantagens de se usar o shap values como um m√©todo de encoder dos dados, <a href="https://www.kaggle.com/pavelvod/gbm-supervised-pretraining">segundo este notebook publicado no Kaggle</a> (muito interessante por sinal):</p>
<ul>
<li>Normaliza os dados;</li>
<li>Mais ou menos Linearizado pois as <em>features</em> s√£o transformadas em suas import√¢ncias;</li>
<li>Recursos categ√≥ricos codificados de maneira mais inteligente (A codifica√ß√£o n√£o √© linear e depende de outros recursos da amostra);</li>
<li>Tratamento mais inteligente para valores <em>missing</em>.</li>
</ul>
<p>Para evitar <em>data leak</em>, o <em>SHAP values</em> foi calculado em cima dos dados <em>out-of-fold</em> para os dados de treino e a m√©dia da previs√£o de todos os <em>fold</em> nos dados de teste.</p>
<p>Definir estrat√©gia de valida√ß√£o cruzada:</p>
<pre class="python"><code>X_test = test[cat_nom+cat_ord+num_dis+num_con]
X = train[cat_nom+cat_ord+num_dis+num_con]
y = train.y

K=15 # number of bins with Sturge‚Äôs rule
SEED=123
kf = StratifiedKFold(n_splits=K, random_state=SEED, shuffle=True)</code></pre>
<div id="xgboost" class="section level3">
<h3>XGBoost</h3>
<p>Obter <em>out-of-fold</em> SHAP do modelo XGBoost tunado:</p>
<pre class="python"><code>shap1_oof = np.zeros((X.shape[0], X.shape[1]))
shap1_test = np.zeros((X_test.shape[0], X_test.shape[1]))
model_shap1_oof = np.zeros(X.shape[0])

for fold, (train_idx, val_idx) in enumerate(kf.split(X=X, y=y)):
    print(f&quot;‚ûú FOLD :{fold}&quot;)
    X_train = X.iloc[train_idx]
    y_train = y.iloc[train_idx]
    X_val = X.iloc[val_idx]
    y_val = y.iloc[val_idx]
    
    start = time.time()
    
    model = XGBClassifier(**final_params_xgb[&#39;clf&#39;])
    
    model.fit(X_train, y_train,
              eval_set=[(X_val, y_val)],
              early_stopping_rounds=150,
              verbose=False)
    
    model_shap1_oof[val_idx] += model.predict_proba(X_val)[:,1]
    
    print(&quot;Final F1     :&quot;, custom_f1(y_val, model_shap1_oof[val_idx]))
    print(&quot;Final AUC    :&quot;, roc_auc_score(y_val, model_shap1_oof[val_idx]))
    print(&quot;Final LogLoss:&quot;, log_loss(y_val, model_shap1_oof[val_idx]))

    explainer = shap.TreeExplainer(model)
    shap1_oof[val_idx] = explainer.shap_values(X_val)
    shap1_test += explainer.shap_values(X_test) / K

    print(f&quot;elapsed: {time.time()-start:.2f} sec\n&quot;)
    
shap1_oof = pd.DataFrame(shap1_oof, columns = [x+&quot;_shap1&quot; for x in X.columns])
shap1_test = pd.DataFrame(shap1_test, columns = [x+&quot;_shap1&quot; for x in X_test.columns])

print(&quot;Final F1     :&quot;, custom_f1(y, model_shap1_oof))
print(&quot;Final AUC    :&quot;, roc_auc_score(y, model_shap1_oof))
print(&quot;Final LogLoss:&quot;, log_loss(y, model_shap1_oof))</code></pre>
<pre><code>## ‚ûú FOLD :0
## Final F1     : 0.7032967032967034
## Final AUC    : 0.902330627099664
## Final LogLoss: 0.2953604946129216
## elapsed: 62.58 sec
## 
## ‚ûú FOLD :1
## Final F1     : 0.6193853427895981
## Final AUC    : 0.8613101903695408
## Final LogLoss: 0.34227429854659686
## elapsed: 45.96 sec
## 
## ‚ûú FOLD :2
## Final F1     : 0.6793478260869567
## Final AUC    : 0.8945898656215007
## Final LogLoss: 0.3085819148842589
## elapsed: 58.84 sec
## 
## ‚ûú FOLD :3
## Final F1     : 0.7073791348600509
## Final AUC    : 0.9058020716685331
## Final LogLoss: 0.2881665477053405
## elapsed: 62.24 sec
## 
## ‚ûú FOLD :4
## Final F1     : 0.7239583333333334
## Final AUC    : 0.9053121500559911
## Final LogLoss: 0.29320601468396107
## elapsed: 93.74 sec
## 
## ‚ûú FOLD :5
## Final F1     : 0.7009803921568627
## Final AUC    : 0.9076567749160134
## Final LogLoss: 0.2872539995859452
## elapsed: 73.34 sec
## 
## ‚ûú FOLD :6
## Final F1     : 0.6736292428198434
## Final AUC    : 0.8822788353863381
## Final LogLoss: 0.320014158050091
## elapsed: 55.16 sec
## 
## ‚ûú FOLD :7
## Final F1     : 0.7135416666666666
## Final AUC    : 0.9016657334826428
## Final LogLoss: 0.29617989833438774
## elapsed: 74.49 sec
## 
## ‚ûú FOLD :8
## Final F1     : 0.7135135135135134
## Final AUC    : 0.8893825776158104
## Final LogLoss: 0.29351621553572266
## elapsed: 93.71 sec
## 
## ‚ûú FOLD :9
## Final F1     : 0.7391304347826086
## Final AUC    : 0.9064054944284814
## Final LogLoss: 0.28033187155768635
## elapsed: 95.65 sec
## 
## ‚ûú FOLD :10
## Final F1     : 0.684863523573201
## Final AUC    : 0.9031046324199313
## Final LogLoss: 0.29823173886367804
## elapsed: 64.70 sec
## 
## ‚ûú FOLD :11
## Final F1     : 0.704225352112676
## Final AUC    : 0.8882052000840984
## Final LogLoss: 0.30525241732057884
## elapsed: 50.06 sec
## 
## ‚ûú FOLD :12
## Final F1     : 0.6666666666666666
## Final AUC    : 0.8905529469479291
## Final LogLoss: 0.313654842143217
## elapsed: 78.45 sec
## 
## ‚ûú FOLD :13
## Final F1     : 0.6500000000000001
## Final AUC    : 0.8745111780783517
## Final LogLoss: 0.3300786509821235
## elapsed: 59.54 sec
## 
## ‚ûú FOLD :14
## Final F1     : 0.7135416666666666
## Final AUC    : 0.9063284042329526
## Final LogLoss: 0.29314716930177404
## elapsed: 70.28 sec
## 
## Final F1     : 0.6822461331540014
## Final AUC    : 0.8945288307257988
## Final LogLoss: 0.30301717097927483</code></pre>
</div>
<div id="catboost" class="section level3">
<h3>CatBoost</h3>
<p>Obter <em>out-of-fold</em> SHAP do modelo CatBoost + features extrat√≠das via KNN:</p>
<pre class="python"><code>X = pd.concat([X, knn_feat_train], axis=1)
X_test = pd.concat([X_test, knn_feat_test], axis=1)</code></pre>
<pre class="python"><code>shap2_oof = np.zeros((X.shape[0], X.shape[1]))
shap2_test = np.zeros((X_test.shape[0], X_test.shape[1]))
model_shap2_oof = np.zeros(X.shape[0])

for fold, (train_idx, val_idx) in enumerate(kf.split(X=X, y=y)):
    print(f&quot;‚ûú FOLD :{fold}&quot;)
    X_train = X.iloc[train_idx]
    y_train = y.iloc[train_idx]
    X_val = X.iloc[val_idx]
    y_val = y.iloc[val_idx]
    
    start = time.time()
    
    model = CatBoostClassifier(random_seed=SEED,
                               verbose = 0,
                               n_estimators=10000,
                               loss_function= &#39;Logloss&#39;,
                               use_best_model=True,
                               eval_metric= &#39;Logloss&#39;)
    
    model.fit(X_train, y_train, 
              eval_set = [(X_val,y_val)], 
              early_stopping_rounds = 100,
              verbose = False)
    
    model_shap2_oof[val_idx] += model.predict_proba(X_val)[:,1]
    
    print(&quot;Final F1     :&quot;, custom_f1(y_val, model_shap2_oof[val_idx]))
    print(&quot;Final AUC    :&quot;, roc_auc_score(y_val, model_shap2_oof[val_idx]))
    print(&quot;Final LogLoss:&quot;, log_loss(y_val, model_shap2_oof[val_idx]))

    explainer = shap.TreeExplainer(model)
    shap2_oof[val_idx] = explainer.shap_values(X_val)
    shap2_test += explainer.shap_values(X_test) / K

    print(f&quot;elapsed: {time.time()-start:.2f} sec\n&quot;)
    
shap2_oof = pd.DataFrame(shap2_oof, columns = [x+&quot;_shap&quot; for x in X.columns])
shap2_test = pd.DataFrame(shap2_test, columns = [x+&quot;_shap&quot; for x in X_test.columns])

print(&quot;Final F1     :&quot;, custom_f1(y, model_shap2_oof))
print(&quot;Final AUC    :&quot;, roc_auc_score(y, model_shap2_oof))
print(&quot;Final LogLoss:&quot;, log_loss(y, model_shap2_oof))</code></pre>
<pre><code>## ‚ûú FOLD :0
## Final F1     : 0.6972010178117048
## Final AUC    : 0.8954157334826428
## Final LogLoss: 0.29952314366911725
## elapsed: 22.84 sec
## 
## ‚ûú FOLD :1
## Final F1     : 0.6348448687350835
## Final AUC    : 0.8628429451287795
## Final LogLoss: 0.3407490151943705
## elapsed: 12.59 sec
## 
## ‚ûú FOLD :2
## Final F1     : 0.6809651474530831
## Final AUC    : 0.8949538073908175
## Final LogLoss: 0.3066089330852162
## elapsed: 18.03 sec
## 
## ‚ûú FOLD :3
## Final F1     : 0.702247191011236
## Final AUC    : 0.9107992721164613
## Final LogLoss: 0.2877216893570601
## elapsed: 15.66 sec
## 
## ‚ûú FOLD :4
## Final F1     : 0.7131367292225201
## Final AUC    : 0.9018687010078387
## Final LogLoss: 0.2976481761596595
## elapsed: 29.35 sec
## 
## ‚ûú FOLD :5
## Final F1     : 0.7055837563451777
## Final AUC    : 0.909231522956327
## Final LogLoss: 0.28834373773423566
## elapsed: 15.35 sec
## 
## ‚ûú FOLD :6
## Final F1     : 0.6631578947368421
## Final AUC    : 0.8796402575587906
## Final LogLoss: 0.32303153676573987
## elapsed: 19.13 sec
## 
## ‚ûú FOLD :7
## Final F1     : 0.6997389033942559
## Final AUC    : 0.901637737961926
## Final LogLoss: 0.2985978485411335
## elapsed: 23.30 sec
## 
## ‚ûú FOLD :8
## Final F1     : 0.6965699208443271
## Final AUC    : 0.8825565912117177
## Final LogLoss: 0.3009859242847037
## elapsed: 20.19 sec
## 
## ‚ûú FOLD :9
## Final F1     : 0.7435897435897436
## Final AUC    : 0.9042469689536757
## Final LogLoss: 0.28276851015512977
## elapsed: 24.39 sec
## 
## ‚ûú FOLD :10
## Final F1     : 0.6767676767676767
## Final AUC    : 0.902712173242694
## Final LogLoss: 0.29999812838692497
## elapsed: 16.14 sec
## 
## ‚ûú FOLD :11
## Final F1     : 0.7013698630136986
## Final AUC    : 0.8865022075828719
## Final LogLoss: 0.3081393413008847
## elapsed: 13.50 sec
## 
## ‚ûú FOLD :12
## Final F1     : 0.6630434782608696
## Final AUC    : 0.8920456934613498
## Final LogLoss: 0.31338640296724246
## elapsed: 24.48 sec
## 
## ‚ûú FOLD :13
## Final F1     : 0.6485148514851485
## Final AUC    : 0.8689887167986544
## Final LogLoss: 0.3369797070301582
## elapsed: 17.17 sec
## 
## ‚ûú FOLD :14
## Final F1     : 0.7108753315649867
## Final AUC    : 0.8994743850304856
## Final LogLoss: 0.301420230674656
## elapsed: 16.51 sec
## 
## Final F1     : 0.6823234134098244
## Final AUC    : 0.892656043550729
## Final LogLoss: 0.305726567456891</code></pre>
<pre class="python"><code>train = pd.concat([train, shap1_oof], axis=1)
test = pd.concat([test, shap1_test], axis=1)

train = pd.concat([train, shap2_oof], axis=1)
test = pd.concat([test, shap2_test], axis=1)</code></pre>
</div>
</div>
<div id="stage-3-modelo-final-com-autogluon" class="section level2">
<h2>Stage 3: Modelo Final com AutoGluon</h2>
<p>AutoGluon √© um <a href="https://github.com/awslabs/autogluon">AutoML desenvolvido pela Amazon</a> muito f√°cil de utilizar (no melhor estilo <code>sklearn</code> com m√©todos <code>.fit()</code> e <code>.predict()</code>).</p>
<p>Principais Informa√ß√µes üìå :</p>
<ul>
<li>Inputs: Dataset original + knn features + Shapt values do XGBoost tunado e do CatBoost;</li>
<li>Loss do XGBoost: Log Loss;</li>
<li>Loss do CatBoost: AUC;</li>
<li>Loss do AutoGluon: Log Loss;</li>
<li>Tempo de processamento: 7h30m</li>
</ul>
<div class="w3-panel w3-pale-green w3-border">
<p><strong>üí° Insight</strong> <br></p>
<p>Um recurso muito √∫til do AutoGluon √© poder acessar as previs√µes out-of-folds, o que facilita no c√°lculo do <em>threshold</em> que maximiza a <em>F1 Score</em>.</p>
</div>
<pre class="python"><code>predictor = TabularPredictor(label=&quot;y&quot;,
                             problem_type=&#39;binary&#39;,
                             eval_metric=&quot;log_loss&quot;,
                             path=&#39;./AutoGlon/&#39;,
                             verbosity=1)

predictor.fit(train, presets=&#39;best_quality&#39;, time_limit=60*60*7.5) 

results = predictor.fit_summary()</code></pre>
<pre><code>## *** Summary of fit() ***
## Estimated performance of each model:
##                       model  score_val  pred_time_val      fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order
## 0       WeightedEnsemble_L2  -0.299310      30.410467   8888.826963                0.001654           2.456810            2       True         14
## 1           CatBoost_BAG_L1  -0.301038       3.051793   2376.887100                3.051793        2376.887100            1       True          7
## 2       WeightedEnsemble_L3  -0.301722     194.034947  22907.669139                0.001541           2.008858            3       True         26
## 3         LightGBMXT_BAG_L2  -0.302135     131.534432  17201.299530                1.378576         389.400378            2       True         15
## 4         LightGBMXT_BAG_L1  -0.302562       3.570399    969.385833                3.570399         969.385833            1       True          3
## 5           CatBoost_BAG_L2  -0.302646     131.912474  17619.939451                1.756617         808.040299            2       True         19
## 6           LightGBM_BAG_L2  -0.303002     131.422007  17281.518763                1.266150         469.619612            2       True         16
## 7           LightGBM_BAG_L1  -0.303264       2.964433   1038.037160                2.964433        1038.037160            1       True          4
## 8            XGBoost_BAG_L1  -0.303471       4.475003   2036.551052                4.475003        2036.551052            1       True         11
## 9    NeuralNetFastAI_BAG_L1  -0.304455      19.917584   3434.894841               19.917584        3434.894841            1       True         10
## 10           XGBoost_BAG_L2  -0.304499     132.757505  17834.135370                2.601648        1022.236218            2       True         23
## 11   NeuralNetFastAI_BAG_L2  -0.306339     142.018741  18777.287244               11.862885        1965.388093            2       True         22
## 12     LightGBMLarge_BAG_L2  -0.306606     131.701429  18260.504603                1.545573        1448.605452            2       True         25
## 13    NeuralNetMXNet_BAG_L2  -0.308237     177.769179  19273.211899               47.613322        2461.312748            2       True         24
## 14     LightGBMLarge_BAG_L1  -0.309686       3.042399   2629.185346                3.042399        2629.185346            1       True         13
## 15    ExtraTreesEntr_BAG_L2  -0.314045     132.017535  16815.886061                1.861679           3.986910            2       True         21
## 16  RandomForestEntr_BAG_L2  -0.314454     132.061970  16843.769642                1.906114          31.870490            2       True         18
## 17    ExtraTreesGini_BAG_L2  -0.314960     132.123651  16816.087081                1.967794           4.187930            2       True         20
## 18    NeuralNetMXNet_BAG_L1  -0.317156      81.677096   4258.886806               81.677096        4258.886806            1       True         12
## 19  RandomForestGini_BAG_L2  -0.321702     132.035970  16835.326491                1.880114          23.427339            2       True         17
## 20    ExtraTreesEntr_BAG_L1  -0.323283       1.794093      4.051307                1.794093           4.051307            1       True          9
## 21  RandomForestEntr_BAG_L1  -0.324296       1.966043     33.380685                1.966043          33.380685            1       True          6
## 22    ExtraTreesGini_BAG_L1  -0.325897       1.796291      3.748723                1.796291           3.748723            1       True          8
## 23  RandomForestGini_BAG_L1  -0.328218       1.778995     22.705248                1.778995          22.705248            1       True          5
## 24    KNeighborsDist_BAG_L1  -1.070156       2.010938      2.075571                2.010938           2.075571            1       True          2
## 25    KNeighborsUnif_BAG_L1  -1.071373       2.110790      2.109480                2.110790           2.109480            1       True          1
## Number of models trained: 26
## Types of models trained:
## {&#39;StackerEnsembleModel_RF&#39;, &#39;StackerEnsembleModel_NNFastAiTabular&#39;, &#39;WeightedEnsembleModel&#39;, &#39;StackerEnsembleModel_XGBoost&#39;, &#39;StackerEnsembleModel_CatBoost&#39;, &#39;StackerEnsembleModel_KNN&#39;, &#39;StackerEnsembleModel_LGB&#39;, &#39;StackerEnsembleModel_XT&#39;, &#39;StackerEnsembleModel_TabularNeuralNet&#39;}
## Bagging used: True  (with 10 folds)
## Multi-layer stack-ensembling used: True  (with 3 levels)
## Feature Metadata (Processed):
## (raw dtype, special dtypes):
## (&#39;float&#39;, [])     : 152 | [&#39;var55&#39;, &#39;var56&#39;, &#39;var57&#39;, &#39;var58&#39;, &#39;var59&#39;, ...]
## (&#39;int&#39;, [])       :  48 | [&#39;var1&#39;, &#39;var2&#39;, &#39;var3&#39;, &#39;var4&#39;, &#39;var5&#39;, ...]
## (&#39;int&#39;, [&#39;bool&#39;]) :   6 | [&#39;var27&#39;, &#39;var31&#39;, &#39;var44&#39;, &#39;var49&#39;, &#39;var50&#39;, ...]
## Plot summary of models saved to file: ./AutoGlon/SummaryOfModels.html
## *** End of fit() summary ***</code></pre>
<p>Nota: Os resultados podem variar devido √† natureza estoc√°stica do algoritmo ou procedimento de avalia√ß√£o.</p>
<pre class="python"><code># get final predictions
y_oof = predictor.get_oof_pred_proba().iloc[:,1]
y_pred = predictor.predict_proba(test).iloc[:,1]</code></pre>
<pre class="python"><code>final_threshold = get_threshold(train.y, y_oof)
final_threshold</code></pre>
<pre><code>## 0.31</code></pre>
<pre class="python"><code>print(&quot;Final F1     :&quot;, custom_f1(y, y_oof))
print(&quot;Final AUC    :&quot;, roc_auc_score(y, y_oof))
print(&quot;Final LogLoss:&quot;, log_loss(y, y_oof))</code></pre>
<pre><code>## Final F1     : 0.6846193682030037
## Final AUC    : 0.8961328807692966
## Final LogLoss: 0.2993098559321765</code></pre>
<p>Ap√≥s submiss√£o:</p>
<center>
<img src="/post/2021-11-01-solucao-final-porto-seguro-data-challenge/final_sub.png" style="width:90.0%" />
</center>
</div>
</div>
<div id="conclus√£o" class="section level1">
<h1>Conclus√£o</h1>
<p>Gostaria de agradecer imensamente ao time do Porto Seguro pela iniciativa, pois esse tipo de competi√ß√£o (t√£o detalhada e desafiadora) n√£o tem sido muito comum no Brasil e √© muito importante para fomentar a comunidade brasileira de ci√™ncia de dados!</p>
<p>Sabemos que o ‚Äúmundo real‚Äù √© diferente do mundo das competi√ß√µes (onde buscamos o melhor score a todo custo) por√©m, na minha vis√£o, n√£o deixa de ser um √≥timo exerc√≠cio para treinar o racioc√≠nio anal√≠tico.. al√©m de ser muito empolgante e divertido!</p>
<p>Tive o enorme prazer de trocar id√©ias e conhecer pessoas fora da curva bem como me tornar f√£ de alguns competidores! A cada semana q passava o n√≠vel estava cada vez mais alto!</p>
<p>Com certeza este pipeline poderia ser muito melhor, sinto que poderia ter gasto mais tempo com <em>feature engineering</em> e tido mais paciencia com alguns modelos. Tentei fazer o melhor que pude com o tempo dispon√≠vel e me sinto muito grato pela experi√™ncia de apresentar os resultados e aprender bastante com a solu√ß√£o dos top colocados.</p>
<p>N√£o acaba por aqui! Agora √© hora de voltar aos estudos, continuar praticando com as <a href="https://www.kaggle.com/c/tabular-playground-series-nov-2021/overview">TPS‚Äôs do Kaggle</a> e, quem sabe, ir melhor na pr√≥xima!</p>
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias</h1>
<ul>
<li><a href="https://github.com/momijiame/gokinjo" class="uri">https://github.com/momijiame/gokinjo</a></li>
<li><a href="https://www.kaggle.com/melanie7744/tps6-boost-your-score-with-knn-features" class="uri">https://www.kaggle.com/melanie7744/tps6-boost-your-score-with-knn-features</a></li>
<li><a href="https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335" class="uri">https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335</a></li>
<li><a href="https://www.kaggle.com/pavelvod/gbm-supervised-pretraining" class="uri">https://www.kaggle.com/pavelvod/gbm-supervised-pretraining</a></li>
</ul>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2021-11-01-solucao-final-porto-seguro-data-challenge/">Solu√ß√£o Final - Porto Seguro Data Challenge [3¬∫ lugar]</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">catboost</category>
      <category domain="tag">data-science</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">knn</category>
      <category domain="tag">machine-learning</category>
      <category domain="tag">optuna</category>
      <category domain="tag">pratica</category>
      <category domain="tag">python</category>
      <category domain="tag">shap</category>
      <category domain="tag">threshold-movel</category>
    </item>
  </channel>
</rss>