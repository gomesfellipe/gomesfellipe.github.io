&lt;?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Data Mining on Fellipe Gomes - Data Science Blog</title>
    <link>https://gomesfellipe.github.io/tags/data-mining/</link>
    <description>√öltimos posts sobre Data Science, Machine Learning e R</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>pt-br</language>
    <managingEditor>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</managingEditor>
    <webMaster>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</webMaster>
    <lastBuildDate>Tue, 30 May 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://gomesfellipe.github.io/tags/data-mining/" rel="self" type="application/rss+xml" />
    <item>
      <title>Solu√ß√£o Final - ML Olympiad [1¬∫ lugar]</title>
      <link>https://gomesfellipe.github.io/post/2023-05-30-solucao-final-ensure-healthy-lives-kaggle-competition/</link>
      <pubDate>Tue, 30 May 2023 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2023-05-30-solucao-final-ensure-healthy-lives-kaggle-competition/</guid>
      <description>Confira a estrat√©gia aplicada para esta competi√ß√£o</description>
      <content:encoded>&lt;![CDATA[
        
<link href="https://gomesfellipe.github.io/post/2023-05-30-solucao-final-ensure-healthy-lives-kaggle-competition/index_files/vembedr/css/vembedr.css" rel="stylesheet" />

<div id="TOC">
<ul>
<li><a href="#introdu%C3%A7%C3%A3o" id="toc-introdu√ß√£o">Introdu√ß√£o</a></li>
<li><a href="#defini%C3%A7%C3%A3o-do-problema-de-neg%C3%B3cio" id="toc-defini√ß√£o-do-problema-de-neg√≥cio">Defini√ß√£o do problema de neg√≥cio</a></li>
<li><a href="#solu%C3%A7%C3%B5es" id="toc-solu√ß√µes">Solu√ß√µes</a></li>
<li><a href="#estrat%C3%A9gia-anal%C3%ADtica" id="toc-estrat√©gia-anal√≠tica">Estrat√©gia anal√≠tica</a>
<ul>
<li><a href="#decis%C3%B5es-sobre-a-target" id="toc-decis√µes-sobre-a-target">Decis√µes sobre a target</a></li>
<li><a href="#processamento-dos-dados" id="toc-processamento-dos-dados">Processamento dos Dados</a></li>
<li><a href="#dados-externos" id="toc-dados-externos">Dados Externos</a></li>
<li><a href="#feature-engineering" id="toc-feature-engineering">Feature Engineering</a></li>
<li><a href="#modelos" id="toc-modelos">Modelos</a></li>
<li><a href="#ensemble" id="toc-ensemble">Ensemble</a></li>
<li><a href="#post-processing" id="toc-post-processing">Post Processing</a></li>
</ul></li>
<li><a href="#considera%C3%A7%C3%B5es-finais" id="toc-considera√ß√µes-finais">Considera√ß√µes Finais</a></li>
<li><a href="#sobre-o-autor" id="toc-sobre-o-autor">Sobre o Autor</a></li>
</ul>
</div>

<div id="introdu√ß√£o" class="section level1">
<h1>Introdu√ß√£o</h1>
<p>O <a href="https://www.meetup.com/TensorFlowSP/events/284607061/">TFUG - TensorFlow Users Group de S√£o Paulo</a> lan√ßou uma nova <a href="https://www.kaggle.com/competitions/ml-olympiad-ensure-healthy-lives">competi√ß√£o no Kaggle</a> onde o objetivo era desenvolver modelos para previs√£o de diagn√≥stico de s√≠ndromes respirat√≥rias, que √© um tema relacionado com um dos 17 t√≥picos de Desenvolvimento Sustent√°vel das Na√ß√µes Unidas - <em>Boa sa√∫de e bem-estar</em>.</p>
<p>Como um cientista de dados, acredito que seja muito importante continuarmos aprimorando nossas habilidades e conhecimentos. Competi√ß√µes como essa s√£o muito divertidas e possibilitam que testemos nossos limites em um ambiente competitivo e colaborativo, al√©m de ser uma grande oportunidade para nos desafiarmos e aprender uns com os outros.</p>
<p>Tive o enorme prazer de conquistar o primeiro lugar, dessa vez com meu grande amigo <a href="https://www.linkedin.com/in/kaike-wesley-reis">Kaike</a>, parceiro de competi√ß√µes de longa data que trouxe grande sinergia para a <a href="https://www.kaggle.com/code/gomes555/ml-olypiads-1-lugar-blending">solu√ß√£o final</a> com a contribui√ß√£o de seu modelo (compartilhado abertamente no Kaggle).</p>
<p>Aqui est√£o alguns dos pr√™mios recebidos:</p>
<center>
<img src="/post/2023-05-30-solucao-final-ensure-healthy-lives-kaggle-competition/premio2.png" style="width:80.0%" />
</center>
<p>Como nesta competi√ß√£o havia bastante trabalho a ser feito e tivemos apenas 1 m√™s para trabalhar na solu√ß√£o, foi preciso fazer uma boa gest√£o do c√≥digo e do tempo de desenvolvimento.</p>
</div>
<div id="defini√ß√£o-do-problema-de-neg√≥cio" class="section level1">
<h1>Defini√ß√£o do problema de neg√≥cio</h1>
<p>O objetivo desta competi√ß√£o consistiu em predizer qual o agente causador da s√≠ndrome respirat√≥ria aguda grave com base nos dados e sintomas dos pacientes.</p>
<p>Esta tarefa pode ser enquadrada como um problema supervisionado de classifica√ß√£o multinomial (com m√∫ltiplos outputs) na qual as previs√µes s√£o, de certa forma, dependentes da entrada umas das outras (o paciente s√≥ pode ter registrado uma das doen√ßas).</p>
<p>A valida√ß√£o da solu√ß√£o foi feita utilizando a m√©trica Macro (or Mean) F1-Score, que √© basicamente a m√©dia do F1 calculado sobre as previs√µes de cada nota.</p>
</div>
<div id="solu√ß√µes" class="section level1">
<h1>Solu√ß√µes</h1>
<p>Ambas solu√ß√µes (minha e do Kaike) foram compartilhadas no Kaggle:</p>
<ul>
<li><a href="https://www.kaggle.com/code/gomes555/ml-olympiad-1-lugar-catboost-pos-process">ML Olympiad - 1¬∫ Lugar - Catboost + Pos Process</a> (Fellipe)</li>
<li><a href="https://www.kaggle.com/code/kaikewreis/ml-olypiads-1-lugar-lightgbm-binary-ensemble">ML Olypiads - 1¬∫ Lugar - LightGBM Binary Ensemble</a> (Kaike)</li>
<li><a href="https://www.kaggle.com/code/gomes555/ml-olympiad-1-lugar-blending">ML Olympiad - 1¬∫ Lugar - Blending</a> (combina√ß√£o das solu√ß√µes em um emsemble)</li>
</ul>
<p>Disponibilizamos tamb√©m a solu√ß√£o em formato de v√≠deo, gravado em um meetup com dura√ß√£o de 1 hora e meia para o canal do <a href="https://www.youtube.com/@tensorflowugsp">TensorFlow UGSP</a> no Youtube no link: <a href="https://youtu.be/6HPJn38NF3w" class="uri">https://youtu.be/6HPJn38NF3w</a></p>
<center>
<div class="vembedr">
<div>
<iframe src="https://www.youtube.com/embed/6HPJn38NF3w" width="533" height="300" frameborder="0" allowfullscreen="" data-external="1"></iframe>
</div>
</div>
</center>
</div>
<div id="estrat√©gia-anal√≠tica" class="section level1">
<h1>Estrat√©gia anal√≠tica</h1>
<p>Nas se√ß√µes abaixo apresento o racional por tr√°s da minha solu√ß√£o, como chegamos nos 5 melhores modelos individuais (para cada doen√ßa respirat√≥ria) que utilizei em um ensemble para chegar ao primeiro lugar, bem como a estrat√©gia de p√≥s processamento que com que o score melhorasse significativamente.</p>
<div id="decis√µes-sobre-a-target" class="section level2">
<h2>Decis√µes sobre a target</h2>
<p>A primeira decis√£o importante era definir como enquadrar o problema; se utilizar√≠amos 1 modelo multiclasse ou diferentes modelos para cada classe.</p>
<p>Em todos os testes que fizemos, os modelos individuais superaram o F1-Score Macro de um modelo √∫nico. Como 3 das classes eram bastante desbalanceadas, acredito que modelos especializados nesses casos conseguiram captar melhor suas nuances.</p>
</div>
<div id="processamento-dos-dados" class="section level2">
<h2>Processamento dos Dados</h2>
<p>Como optamos por unificar os resultados apenas na reta final, meu pr√©-processamento foi muito diferente do feito pelo Kaike e isso foi fundamental para que as estimativas dos nossos modelos tivessem baixa correla√ß√£o. N√£o focarei aqui no meu pr√©-processamento, pois n√£o acho que foi o diferencial para atingir um score superior a 0.6 (quem tiver curiosidade est√° tudo bem documentado nos notebooks compartilhados).</p>
</div>
<div id="dados-externos" class="section level2">
<h2>Dados Externos</h2>
<p>O fato de n√£o termos as informa√ß√µes do ano em que esses dados foram coletados dificultou na busca de bases externas, pois indicadores socioecon√¥micos e de sa√∫de variam bastante ao longo do tempo.</p>
<p>Fizemos alguns testes utilizando o <a href="https://basedosdados.org/dataset/mundo-onu-adh">Atlas do Desenvolvimento Humano (ADH)</a>, mas n√£o tivemos muito sucesso, pois esses dados est√£o muito defasados (1991-2010). Tamb√©m tentamos acrescentar a informa√ß√£o de <a href="https://github.com/kelvins/Municipios-Brasileiros/">latitude e longitude de cada munic√≠pio</a>, mas isso n√£o trouxe uma melhora substancial no nosso score.</p>
</div>
<div id="feature-engineering" class="section level2">
<h2>Feature Engineering</h2>
<p>Outra etapa em que investimos bastante tempo foi para criar novas vari√°veis.</p>
<p>Novamente, nossa engenharia de recursos foi feita de maneira separada para que nossos modelos aprendessem aspectos diferentes dos dados. Abaixo, compartilho algumas das features que desenvolvi apenas para o meu modelo:</p>
<ul>
<li>Presen√ßa de sintomas relacionados √† Target;</li>
<li>Se tomografia era t√≠pica do COVID;</li>
<li>Intervalo de idade com mais casos;</li>
<li>Idade discretizada;</li>
<li>Diferen√ßa entre a semana de notifica√ß√£o e primeiros sintomas;</li>
<li>Novas features baseadas nas contagens de algumas features categ√≥ricas;</li>
<li>etc.</li>
</ul>
</div>
<div id="modelos" class="section level2">
<h2>Modelos</h2>
<p>Al√©m de pr√©-processamentos e feature engineering diferentes, tamb√©m utilizamos modelos e mecanismos de tunning diferentes, o que ajudou para que nossas estimativas tivessem baixa correla√ß√£o. Eu usei o Catboost como modelo final, j√° o Kaike optou por um LightGBM com tuning de hiperparametros.</p>
</div>
<div id="ensemble" class="section level2">
<h2>Ensemble</h2>
<p>Calculamos a m√©dia das probabilidades previstas de cada modelo para cada classe antes de selecionar a classe que tivesse a maior probabilidade.</p>
<p>Como nossas previs√µes tinham baixa correla√ß√£o, conseguimos ser bem sucedidos no ensemble combinando nossas submiss√µes com score ~0.6 alcan√ßando ~0.61 na tabela p√∫blica.</p>
</div>
<div id="post-processing" class="section level2">
<h2>Post Processing</h2>
<p>Acredito que o <strong>diferencial</strong> dessa competi√ß√£o estava no p√≥s processamento.</p>
<p>Quando avaliamos o score do modelo de cada classe, tamb√©m calculamos um threshold que maximizava os respectivos F1.</p>
<p>Observamos que nosso modelo para a classe 5 apresentava um F1 muito superior √†s demais classes com esse threshold otimizado, ent√£o fizemos o seguinte:</p>
<ol style="list-style-type: decimal">
<li>Calculamos as probabilidades individuais para cada classe;</li>
<li>Selecionamos a classe que tinha maior probabilidade estimada em cada inst√¢ncia;</li>
<li>Pegamos a classifica√ß√£o bin√°ria da classe 5 com o threshold otimizado e aplicamos a seguinte condi√ß√£o: Se o modelo da classe 5 estimou que y5[i]==1, ent√£o yfinal[i] √© 5, caso contr√°rio, use a classe de maior probabilidade entre as outras 4. (Em outras palavras: <code>np.where(y5_test_class==1, 5, sub.CLASSI_FIN)</code>)</li>
</ol>
</div>
</div>
<div id="considera√ß√µes-finais" class="section level1">
<h1>Considera√ß√µes Finais</h1>
<p>Foi uma competi√ß√£o muito interessante e desafiadora. Agrade√ßo imensamente ao <a href="https://www.meetup.com/TensorFlowSP/events/284607061/">TFUG</a> por organizar o evento e a todos os participantes que contribu√≠ram para o aprendizado coletivo.Foi uma √≥tima oportunidade de aprendizado e troca de experi√™ncias.</p>
<p>Espero que minha solu√ß√£o possa ser √∫til para outros projetos e desafios futuros.</p>
</div>
<div id="sobre-o-autor" class="section level1">
<h1>Sobre o Autor</h1>
<p>Me chamo Fellipe Gomes, sou cientista de dados e apaixonado por aprendizado de m√°quina. Compartilho meu conhecimento por meio de artigos, tutoriais e projetos de c√≥digo aberto. Se quiser saber mais sobre meu trabalho, sinta-se √† vontade para conferir meu <a href="https://www.linkedin.com/in/fellipe-gomes-06943264/">LinkedIn</a> e <a href="https://github.com/fellipe-gomes">GitHub</a>.</p>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2023-05-30-solucao-final-ensure-healthy-lives-kaggle-competition/">Solu√ß√£o Final - ML Olympiad [1¬∫ lugar]</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">catboost</category>
      <category domain="tag">classification</category>
      <category domain="tag">data-mining</category>
      <category domain="tag">data-science</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">machine-learning</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">pratica</category>
      <category domain="tag">r</category>
    </item>
    <item>
      <title>Solu√ß√£o Final - ML Olympiad [2¬∫ lugar]</title>
      <link>https://gomesfellipe.github.io/post/2022-04-20-solucao-final-education-quality-kaggle-competition/</link>
      <pubDate>Tue, 19 Apr 2022 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2022-04-20-solucao-final-education-quality-kaggle-competition/</guid>
      <description>Confira a estrat√©gia aplicada para esta competi√ß√£o</description>
      <content:encoded>&lt;![CDATA[
        

<div id="TOC">
<ul>
<li><a href="#introdu%C3%A7%C3%A3o" id="toc-introdu√ß√£o">Introdu√ß√£o</a></li>
<li><a href="#defini%C3%A7%C3%A3o-do-problema-de-neg%C3%B3cio" id="toc-defini√ß√£o-do-problema-de-neg√≥cio">Defini√ß√£o do problema de neg√≥cio</a></li>
<li><a href="#an%C3%A1lise-explorat%C3%B3ria-em-r" id="toc-an√°lise-explorat√≥ria-em-r">An√°lise Explorat√≥ria (em R)</a>
<ul>
<li><a href="#estrutura-da-base" id="toc-estrutura-da-base">Estrutura da base</a></li>
<li><a href="#ano-da-base-de-dados" id="toc-ano-da-base-de-dados">Ano da base de dados</a></li>
<li><a href="#target" id="toc-target">Target</a></li>
</ul></li>
<li><a href="#machine-learning-em-python" id="toc-machine-learning-em-python">Machine Learning (em Python)</a>
<ul>
<li><a href="#importar-dependencias" id="toc-importar-dependencias">Importar dependencias</a></li>
<li><a href="#carregar-dados" id="toc-carregar-dados">Carregar dados</a></li>
<li><a href="#modelagem" id="toc-modelagem">Modelagem</a></li>
</ul></li>
<li><a href="#submiss%C3%A3o" id="toc-submiss√£o">Submiss√£o</a></li>
<li><a href="#considera%C3%A7%C3%B5es-finais" id="toc-considera√ß√µes-finais">Considera√ß√µes Finais</a></li>
</ul>
</div>

<div id="introdu√ß√£o" class="section level1">
<h1>Introdu√ß√£o</h1>
<p>No final de Janeiro desde ano (2022) o <a href="https://www.meetup.com/TensorFlowSP/events/284607061/">TFUG - TensorFlow Users Group de S√£o Paulo</a> lan√ßou uma competi√ß√£o no Kaggle para prever as notas do enem que tem rela√ß√£o com um dos 17 t√≥picos de Desenvolvimento Sustent√°vel das Na√ß√µes Unidas - <em>Educa√ß√£o de Qualidade</em>.</p>
<p>Al√©m de divertido, o desafio foi repleto de possibilidades e bastante desafiador! Todos os competidores que trabalharam duro em pleno m√™s de carnaval est√£o de parab√©ns! üòÖ üòÇ</p>
<p>Aqui est√£o alguns dos pr√™mios recebidos:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/premio.png" style="width:80.0%" />
</center>
<p>Como nesta competi√ß√£o havia bastante trabalho a ser feito e tivemos apenas 1 m√™s para trabalhar na solu√ß√£o, foi preciso fazer uma boa gest√£o do c√≥digo e do tempo de desenvolvimento.</p>
<p>Nas se√ß√µes abaixo apresento o racional por tr√°s da minha solu√ß√£o bem como os 5 melhores modelos individuais (para cada nota) que utilizei em um ensemble para chegar ao segundo lugar.</p>
</div>
<div id="defini√ß√£o-do-problema-de-neg√≥cio" class="section level1">
<h1>Defini√ß√£o do problema de neg√≥cio</h1>
<p>O objetivo desta competi√ß√£o consistiu em prever as notas dos alunos(as) nas provas: Ci√™ncias da Natureza, Ci√™ncias Humanas, Linguagens e C√≥digos, Matem√°tica e Reda√ß√£o.</p>
<p>Apesar das notas serem calculadas de maneira independente, a partir de modelos de <a href="http://portal.mec.gov.br/ultimas-noticias/389-ensino-medio-2092297298/17319-teoria-de-resposta-ao-item-avalia-habilidade-e-minimiza-o-chute">TRI (Teoria de Resposta ao Item)</a> que levam em considera√ß√£o a performance em um caderno espec√≠fico e na dificuldade de cada quest√£o, o mesmo aluno realiza todas as provas em um curto per√≠odo de tempo.</p>
<p>Portanto, esta tarefa pode ser enquadrada como um problema supervisionado de regress√£o com m√∫ltiplos outputs na qual as previs√µes s√£o, de certa forma, dependentes da entrada umas das outras.</p>
<p>A valida√ß√£o da solu√ß√£o foi feita utilizando a m√©trica Mean Columnwise Root Mean Squared Error ‚Äì MCRMSE, que √© basicamente a m√©dia do RMSE calculado sobre as previs√µes de cada nota.</p>
</div>
<div id="an√°lise-explorat√≥ria-em-r" class="section level1">
<h1>An√°lise Explorat√≥ria (em R)</h1>
<p>Convido o leitor a conferir o <a href="https://gomesfellipe.github.io/post/2021-11-01-solucao-final-porto-seguro-data-challenge/">notebook publicado no Kaggle</a> com a an√°lise explorat√≥ria completa. Aqui irei trazer apenas alguns dos principais insights que encontrei durante a etapa de an√°lise explorat√≥ria.</p>
<div id="estrutura-da-base" class="section level2">
<h2>Estrutura da base</h2>
<p>Veja a seguir qual a estrutura geral da base de dados:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/02_df_status.png" style="width:95.0%" />
</center>
<p>√â not√≥rio que existem dados faltantes e que parece haver algum padr√£o. Vejamos com mais detalhse:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/03_missing.png" style="width:95.0%" />
</center>
<div class="w3-panel w3-pale-blue w3-border">
<p>¬† üí° Insights!</p>
<p>Existem dados <em>missing</em> nas 5 targets que queremos prever e note que existe uma rela√ß√£o tanto entre as provas de Matem√°tica e Ci√™ncias da Natuerza quanto nas de Ci√™ncias Humanas, Linguagens e C√≥digos e Reda√ß√£o, o que parece ocorrer devido a aus√™ncia do aluno incrito em comparecer a realiza√ß√£o da prova no respectivo dia.</p>
</div>
</div>
<div id="ano-da-base-de-dados" class="section level2">
<h2>Ano da base de dados</h2>
<p>Essa informa√ß√£o n√£o estava explicitamente dispon√≠vel, mas ap√≥s analisar a idade dos participantes em rela√ß√£o ao ano em que conclu√≠ram o ensino m√©dio, foi poss√≠vel identificar que tratavam-se dos dados de 2019, veja:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/05_ano_concluiu.png" style="width:95.0%" />
</center>
<p>Essa informa√ß√£o poderia ser √∫til na hora de buscar dados externos (permitido nesta competi√ß√£o).</p>
<div class="w3-panel w3-pale-blue w3-border">
<p>¬† üí° Insights!</p>
<p>‚Üí Aten√ß√£o aos outliers: √â no m√≠nimo estranho uma pessoa que formou em 2007 ter 17 anos;</p>
<p>‚Üí Como ningu√©m concluiu a escola no ano de 2019 e a m√©dia das idades vai diminuindo quanto mais pr√≥ximo de 2018, parece que estes dados s√£o de 2019. Essa inform√ß√£o poderia ser √∫til na hora de procurar por bases externas.</p>
</div>
</div>
<div id="target" class="section level2">
<h2>Target</h2>
<p>A primeira decis√£o importante era definir como enquadrar o problema; se seriam m√∫ltiplos modelos independentes ou modelos com sa√≠das dependentes.</p>
<p>Primeiramente vejamos como eram as distribui√ß√µes das notas por caderno:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/07_distribuicao_target.png" style="width:95.0%" />
</center>
<p>Ao olhar estas distribui√ß√µes foram surgindo v√°rias id√©ias! Cheguei at√© a tentar modelos estat√≠sticos GAM considerando a resposta como uma distribui√ß√£o Beta (transformando as targets no intervalo [0,1]) mas acabou n√£o apresentando bons resultados para a competi√ß√£o.. acho que seria necess√°rio um pouco mais de prepara√ß√£o nos dados.</p>
<p>Apesar das notas do enem serem calculadas via TRI (Teoria de Resposta ao Item) que considera as notas independentes, parece existir alguma correla√ß√£o entre as notas, veja:</p>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/08_correlacao_notas.png" style="width:95.0%" />
</center>
<p>As targets da nota de L√≠nguas e C√≥digos e Ci√™ncias Humanas pareciam possuir uma correla√ß√£o ‚Äúinteressante‚Äù, mas, ap√≥s testar modelos de m√∫ltiplas respostas dependentes para cada dia (com e sem a nota da reda√ß√£o), em nenhum de meus testes superou (de maneira consistente) o desempenho de modelos que considerassem as sa√≠das independentes. Portanto foquei em criar 5 modelos independentes.</p>
</div>
</div>
<div id="machine-learning-em-python" class="section level1">
<h1>Machine Learning (em Python)</h1>
<p>Toda a rotina de pr√©-processamento dos dados, feature engineering, modelagem, ensamble e p√≥s-processamento foi realizada utilizando a linguagem Python para cada uma das 5 notas. Trouxe apenas o modelo final neste post mas, para chegar at√© aqui foram necess√°rio muitos testes!</p>
<div id="importar-dependencias" class="section level2">
<h2>Importar dependencias</h2>
<p>Carregar pacotes Python:</p>
<pre class="python"><code># data prep
import numpy as np 
import pandas as pd 
# pre process
from sklearn.preprocessing import MinMaxScaler
# modeling
from sklearn.model_selection import train_test_split
from catboost import CatBoostRegressor
# plots
import seaborn as sns
import matplotlib.pyplot as plt</code></pre>
<p>Confira a baixo as fun√ß√µes desenvolvidas para a solu√ß√£o deste problema</p>
<details>
<summary>
(<em>Clique aqui para expandir as fun√ß√µes</em>)
</summary>
<pre class="python"><code>def prep_data_questionarios(df):
  &#39;&#39;&#39;
  Converte dados de questionario para ordinal
  &#39;&#39;&#39;
    # escolaridade pai
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}
    df.loc[:, &#39;Q001&#39;] = df.loc[:, &#39;Q001&#39;].map(to_map).astype(int)

    # escolaridade mae
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}
    df.loc[:, &#39;Q002&#39;] = df.loc[:, &#39;Q002&#39;].map(to_map).astype(int) 

    # ocupacao pai
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: -1}
    df.loc[:, &#39;Q003&#39;] = df.loc[:, &#39;Q003&#39;].map(to_map).astype(int) 

    # ocupacao mae
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: -1}
    df.loc[:, &#39;Q004&#39;] = df.loc[:, &#39;Q004&#39;].map(to_map).astype(int) 

    # renda da familia
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;:5, &#39;G&#39;:6, &#39;H&#39;:7, &#39;I&#39;:8,
              &#39;J&#39;:9, &#39;K&#39;:10,&#39;L&#39;:11, &#39;M&#39;:12, &#39;N&#39;:13, &#39;O&#39;:14, &#39;P&#39;:15, &#39;Q&#39;:16}
    df.loc[:, &#39;Q006&#39;] = df.loc[:, &#39;Q006&#39;].map(to_map).astype(int) 

    # empregado domestico
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3}
    df.loc[:, &#39;Q007&#39;] = df.loc[:, &#39;Q007&#39;].map(to_map).astype(int) 

    # banheiro
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q008&#39;] = df.loc[:, &#39;Q008&#39;].map(to_map).astype(int) 

    # qnt de quartos
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q009&#39;] = df.loc[:, &#39;Q009&#39;].map(to_map).astype(int) 

    # qnt de carros
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q010&#39;] = df.loc[:, &#39;Q010&#39;].map(to_map).astype(int) 

    # qnt de motocicleta
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q011&#39;] = df.loc[:, &#39;Q011&#39;].map(to_map).astype(int) 

    # qnt de geladeira
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q012&#39;] = df.loc[:, &#39;Q012&#39;].map(to_map).astype(int) 

    # qnt de freezer
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q013&#39;] = df.loc[:, &#39;Q013&#39;].map(to_map).astype(int) 

    # qnt de maquina de lavar roupa
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q014&#39;] = df.loc[:, &#39;Q014&#39;].map(to_map).astype(int) 

    # qnt de maquina de secar roupa
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q015&#39;] = df.loc[:, &#39;Q015&#39;].map(to_map).astype(int) 

    # qnt de microondas
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q016&#39;] = df.loc[:, &#39;Q016&#39;].map(to_map).astype(int) 

    # qnt de maquina de lavar louca
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q017&#39;] = df.loc[:, &#39;Q017&#39;].map(to_map).astype(int) 

    # tem aspirador de po
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1}
    df.loc[:, &#39;Q018&#39;] = df.loc[:, &#39;Q018&#39;].map(to_map).astype(int) 

    # qtd tv colorida
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q019&#39;] = df.loc[:, &#39;Q019&#39;].map(to_map).astype(int) 

    # tem dvd
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1}
    df.loc[:, &#39;Q020&#39;] = df.loc[:, &#39;Q020&#39;].map(to_map).astype(int) 

    # tem tv por assinatura
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1}
    df.loc[:, &#39;Q021&#39;] = df.loc[:, &#39;Q021&#39;].map(to_map).astype(int) 

    # qtd telefone celular
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q022&#39;] = df.loc[:, &#39;Q022&#39;].map(to_map).astype(int) 

    # qtd telefone fixo
    to_map = {&#39;A&#39;:0, &#39;B&#39;:1}
    df.loc[:, &#39;Q023&#39;] = df.loc[:, &#39;Q023&#39;].map(to_map).astype(int) 

    # qtd computador
    to_map =  {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}
    df.loc[:, &#39;Q024&#39;] = df.loc[:, &#39;Q024&#39;].map(to_map).astype(int) 

    # tem acesso a internet
    to_map =  {&#39;A&#39;:0, &#39;B&#39;:1}
    df.loc[:, &#39;Q025&#39;] = df.loc[:, &#39;Q025&#39;].map(to_map).astype(int) 
    
    return(df)
  
def fe_questionario(df):
  &#39;&#39;&#39;
  Gerar novas features artificiais baseadas nos dados de questionario
  &#39;&#39;&#39;
    df.loc[:, &quot;Q021+Q006&quot;] = df[&quot;Q021&quot;] + df[&quot;Q006&quot;]
    df.loc[:, &quot;Q018+Q006&quot;] = df[&quot;Q018&quot;] + df[&quot;Q006&quot;]
    df.loc[:, &quot;Q018+Q008&quot;] = df[&quot;Q018&quot;] + df[&quot;Q008&quot;]
    df.loc[:, &quot;Q010+Q018&quot;] = df[&quot;Q010&quot;] + df[&quot;Q018&quot;]
    df.loc[:, &quot;Q018+Q024&quot;] = df[&quot;Q018&quot;] + df[&quot;Q024&quot;]
    
    df.loc[:, &quot;Q018*Q006&quot;] = df[&quot;Q018&quot;] * df[&quot;Q006&quot;]
    df.loc[:, &quot;Q010*Q018&quot;] = df[&quot;Q010&quot;] * df[&quot;Q018&quot;]
    
    return df
  
def fe_mun(data):
    &#39;&#39;&#39;
    Gerar novas features a partir das localizacoes de municipio
    &#39;&#39;&#39;
    for c in list(data.columns[data.dtypes==&#39;category&#39;]):
        data.loc[:, c] = data.loc[:, c].astype(&#39;object&#39;)
    
    data.loc[:, &#39;FE_MUNICIPIO_PROVA_x_MUNICIPIO_RESIDENCIA&#39;] = np.where(data.NO_MUNICIPIO_PROVA == data.NO_MUNICIPIO_RESIDENCIA , 1, 0)
    data.loc[:, &#39;FE_MUNICIPIO_PROVA_x_MUNICIPIO_NASCIMENTO&#39;] = np.where(data.NO_MUNICIPIO_PROVA == data.NO_MUNICIPIO_NASCIMENTO , 1, 0)
    data.loc[:, &#39;FE_MUNICIPIO_PROVA_x_MUNICIPIO_ESC&#39;] = np.where(data.NO_MUNICIPIO_PROVA == data.NO_MUNICIPIO_ESC , 1, 0)
    data.loc[:, &#39;FE_MUNICIPIO_RESIDENCIA_x_MUNICIPIO_NASCIMENTO&#39;] = np.where(data.NO_MUNICIPIO_RESIDENCIA == data.NO_MUNICIPIO_NASCIMENTO , 1, 0)
    data.loc[:, &#39;FE_MUNICIPIO_RESIDENCIA_x_MUNICIPIO_ESC&#39;] = np.where(data.NO_MUNICIPIO_RESIDENCIA == data.NO_MUNICIPIO_ESC , 1, 0)
    data.loc[:, &#39;FE_MUNICIPIO_NASCIMENTO_x_MUNICIPIO_ESC&#39;] = np.where(data.NO_MUNICIPIO_RESIDENCIA == data.NO_MUNICIPIO_ESC , 1, 0)
    
    for c in list(data.columns[data.dtypes==&#39;object&#39;]):
        data.loc[:, c] = data.loc[:, c].astype(&#39;category&#39;)
    
    return data
  
def fe_in(df):
    &#39;&#39;&#39;
    Gerar features a partir das indicadoras
    &#39;&#39;&#39;
    df.loc[:, &#39;IN_DEFICIT_ATENCAO+IN_TEMPO_ADICIONAL&#39;] = df[&quot;IN_DEFICIT_ATENCAO&quot;] + df[&quot;IN_TEMPO_ADICIONAL&quot;]
    df.loc[:, &#39;IN_LEDOR+IN_TRANSCRICAO&#39;] = df[&quot;IN_LEDOR&quot;] + df[&quot;IN_TRANSCRICAO&quot;]

    return df
  
def prep_co_escola(df):
    &#39;&#39;&#39;
    Converter codigo da escola para categorico
    &#39;&#39;&#39;
    df.loc[:, &#39;CO_ESCOLA&#39;] = [str(x) for x in df.CO_ESCOLA]
    df.loc[:, &#39;CO_ESCOLA&#39;] = np.where(df[&#39;CO_ESCOLA&#39;]==&#39;nan&#39;, np.nan, df[&#39;CO_ESCOLA&#39;])
    df.loc[:, &#39;CO_ESCOLA&#39;] = df.loc[:, &#39;CO_ESCOLA&#39;].astype(&#39;category&#39;)
    
    return df
  
def fe_extra(df):
    &#39;&#39;&#39;
    Gerar novas features 
    &#39;&#39;&#39;
    df.loc[:, &quot;FE_IDADE_DISCRETA&quot;] = pd.cut(df.NU_IDADE, (0, 15, 18, 23, 36, 60, 120), labels=[&#39;ADOLESCENTE&#39;,&#39;ADOLESCENTE_2&#39;, &#39;JOVEM&#39;,&#39;JOVEM_2&#39;, &#39;ADULTO&#39;, &#39;IDOSO&#39;]).astype(&#39;category&#39;)
    df.loc[:, &#39;FE_OCUPACAO_PAIS&#39;] = df.Q003 + df.Q004
    df.loc[:, &#39;FE_ESCOLARIDADE_PAIS&#39;] = df.Q001 + df.Q002
    df.loc[:, &#39;FE_RENDA_POR_PESSOA&#39;] = df.Q006 / df.Q005
    df.loc[:, &#39;FE_CELULAR_POR_PESSOA&#39;] = df.Q022 / df.Q005
    df.loc[:, &#39;FE_COMPUTADOR_POR_PESSOA&#39;] = df.Q024 / df.Q005
    df.loc[:, &#39;FE_VISAO_RUIM&#39;] = df[[&#39;IN_BAIXA_VISAO&#39;, &#39;IN_CEGUEIRA&#39;, &#39;IN_VISAO_MONOCULAR&#39;, &#39;IN_SURDO_CEGUEIRA&#39;]].max(axis=1)
    df.loc[:, &#39;FE_AUDICAO_RUIM&#39;] = df[[&#39;IN_SURDEZ&#39;, &#39;IN_DEFICIENCIA_AUDITIVA&#39;, &#39;IN_SURDO_CEGUEIRA&#39;]].max(axis=1)
    df.loc[:, &#39;FE_TDAH_MAIS_TEMPO&#39;] = df.IN_TEMPO_ADICIONAL + df.IN_DEFICIT_ATENCAO
    df.loc[:, &#39;FE_TDAH_MEDICADO&#39;] = np.where((df.IN_DEFICIT_ATENCAO==1)&amp;(df.IN_MEDICAMENTOS==1), 1, 0)
    df.loc[:, &#39;FE_RECURSO_VISAO&#39;] =  df[[&#39;IN_BRAILLE&#39;, &#39;IN_AMPLIADA_24&#39;, &#39;IN_AMPLIADA_18&#39;, &#39;IN_LEDOR&#39;, &#39;IN_MAQUINA_BRAILE&#39;, &#39;IN_LAMINA_OVERLAY&#39;]].max(axis=1)
    df.loc[:, &#39;FE_RECURSO_SURDEZ&#39;] =  df[[&#39;IN_LIBRAS&#39;, &#39;IN_LEITURA_LABIAL&#39;, &#39;IN_TRANSCRICAO&#39;]].max(axis=1)
    acess = [&#39;IN_ACESSO&#39;, &#39;IN_MESA_CADEIRA_RODAS&#39;, &#39;IN_MESA_CADEIRA_SEPARADA&#39;, &#39;IN_APOIO_PERNA&#39;, &#39;IN_CADEIRA_ESPECIAL&#39;, &#39;IN_CADEIRA_CANHOTO&#39;, &#39;IN_CADEIRA_ACOLCHOADA&#39;, &#39;IN_MOBILIARIO_OBESO&#39;, &#39;IN_SALA_INDIVIDUAL&#39;, &#39;IN_SALA_ESPECIAL&#39;, &#39;IN_SALA_ACOMPANHANTE&#39;, &#39;IN_MOBILIARIO_ESPECIFICO&#39;, &#39;IN_MATERIAL_ESPECIFICO&#39;]
    df.loc[:, &#39;FE_ACESSIBILIDADE&#39;] =  df[acess].max(axis=1)

    return df</code></pre>
</details>
<p>¬†</p>
<p>Carregar features artificiais extra√≠das atrav√©s de um modelo KNN. N√£o apresentarei o c√≥digo aqui (talvez fique para um pr√≥ximo post) mas a id√©ia √© basicamente a seguinte:</p>
<div class="w3-panel w3-pale-yellow w3-border">
<p>¬† üß™ Feature Extraction com KNN</p>
<p>Ajuste um <code>KNeighborsRegressor</code> encontrando os K-vizinhos mais pr√≥ximos de cada inst√¢ncia out-of-fold via valida√ß√£o cruzada (para evitar data leak) nos dados de treino e depois ajuste um modelo em todos os dados de treino para obter os K-vizinhos mais pr√≥ximos nos dados de teste.</p>
</div>
<p>Quem sabe no futuro fa√ßo um post compartilhando esta estrat√©gia com mais detalhes.</p>
<pre class="python"><code>knn_train = pd.read_csv(&quot;../input/knn/KNN_feat_train_CH_LC.csv&quot;)
knn_test = pd.read_csv(&quot;../input/knn/KNN_feat_test_CH_LC.csv&quot;)

knn_train_cn_mt = pd.read_csv(&quot;../input/knn/KNN_feat_train_CN_MT.csv&quot;)
knn_test_cn_mt = pd.read_csv(&quot;../input/knn/KNN_feat_test_CN_MT.csv&quot;)

knn_train_rd = pd.read_csv(&quot;../input/knn/KNN_feat_train_RD.csv&quot;)
knn_test_rd = pd.read_csv(&quot;../input/knn/KNN_feat_test_RD.csv&quot;)</code></pre>
</div>
<div id="carregar-dados" class="section level2">
<h2>Carregar dados</h2>
<p>Importar uma vers√£o do dataset no formato <code>.parquet</code> que foi compactada com um truque para otimizar o consumo de mem√≥ria disponibilizada pelos organizadores <a href="https://www.kaggle.com/code/caneiro/mlo-make-parquet">neste notebook</a>.</p>
<pre class="python"><code>train = pd.read_parquet(&#39;train.parquet&#39;)
test = pd.read_parquet(&#39;test.parquet&#39;)
sub = pd.read_csv(&#39;../input/qualityeducation/sample_submission.csv&#39;)</code></pre>
<p>Definir objetos com targets</p>
<pre class="python"><code>targets = [&#39;NU_NOTA_LC&#39;, &#39;NU_NOTA_CH&#39;, &#39;NU_NOTA_CN&#39;,  &#39;NU_NOTA_MT&#39;, &#39;NU_NOTA_REDACAO&#39;]
presencas = [&#39;TP_PRESENCA_LC&#39;, &#39;TP_PRESENCA_CH&#39;, &#39;TP_PRESENCA_CN&#39;, &#39;TP_PRESENCA_MT&#39;, &#39;TP_STATUS_REDACAO&#39;]</code></pre>
<div class="w3-panel w3-pale-yellow w3-border">
<p>¬† ‚ö†Ô∏è Aten√ß√£o:</p>
<p>A feature de presen√ßa √© muito importante no p√≥s-processamento para atribuir nota zero aos alunos que n√£o foram realizar a prova mas n√£o faz sentido mant√™-la nos dados de treino pois ser√° sempre constante.</p>
</div>
<div id="dados-externos" class="section level3">
<h3>Dados externos</h3>
<p>Dados Externos utilizados:</p>
<ol style="list-style-type: decimal">
<li><a href="https://basedosdados.org/dataset/mundo-onu-adh">Atlas do Desenvolvimento Humano (ADH)</a></li>
</ol>
<p>Esta base tinha muita informa√ß√£o legal mas sua cobertura temporal estava bastante defasada (1991 - 2010) o que pode adicionar algum ru√≠do ao modelo.</p>
<p>As features selecionadas (sem muito crit√©rio) desta base foram:</p>
<pre class="python"><code>extra1 = pd.read_csv(&quot;municipio.csv&quot;)

extra1 = extra1[extra1.ano==2010]

features_extra1 = [&#39;expectativa_vida&#39;, &#39;razao_dependencia&#39;, &#39;expectativa_anos_estudo&#39;,
&#39;taxa_analfabetismo_11_a_14&#39;, &#39;taxa_analfabetismo_15_a_17&#39;, &#39;taxa_analfabetismo_18_mais&#39;,
&#39;taxa_atraso_0_basico&#39;, &#39;taxa_atraso_0_fundamental&#39;, &#39;taxa_atraso_0_medio&#39;,
&#39;taxa_freq_bruta_medio&#39;, &#39;taxa_freq_liquida_medio&#39;,
&#39;taxa_freq_medio_18_24&#39;, &#39;taxa_freq_medio_6_14&#39;, &#39;indice_gini&#39;,&#39;prop_pobreza_extrema&#39;, &#39;prop_pobreza&#39;,
&#39;prop_renda_10_ricos&#39;, &#39;prop_renda_20_pobres&#39;, &#39;razao_10_ricos_40_pobres&#39;,&#39;renda_pc&#39; , &#39;renda_pc_quintil_1&#39;,
&#39;indice_theil&#39;, &#39;prop_trabalhadores_conta_proria&#39;, 
&#39;prop_empregadores&#39;, &#39;prop_ocupados_agropecuaria&#39;, &#39;prop_ocupados_comercio&#39;,
&#39;prop_ocupados_construcao&#39;, &#39;prop_ocupados_formalizacao&#39;, &#39;prop_ocupados_medio&#39;,
&#39;prop_ocupados_servicos&#39;, &#39;prop_ocupados_superior&#39;,
&#39;prop_ocupados_renda_0&#39;, &#39;renda_media_ocupados&#39;, &#39;indice_treil_trabalho&#39;,
&#39;taxa_ocupados_carteira&#39;, &#39;taxa_agua_encanada&#39;, 
&#39;taxa_banheiro_agua_encanada&#39;, &#39;taxa_coleta_lixo&#39;, &#39;taxa_energia_eletrica&#39;,
&#39;taxa_agua_esgoto_inadequados&#39;, &#39;taxa_criancas_dom_sem_fund&#39;,
&#39;pea&#39;, &#39;indice_escolaridade&#39;, &#39;indice_frequencia_escolar&#39;, 
&#39;idhm&#39;, &#39;idhm_e&#39;, &#39;idhm_l&#39;, &#39;idhm_r&#39;]
extra1 = extra1[[&#39;id_municipio&#39;]+features_extra1]

train = pd.merge(train, extra1, how=&#39;left&#39;, left_on=&#39;CO_MUNICIPIO_RESIDENCIA&#39;, right_on=&#39;id_municipio&#39;)
test = pd.merge(test, extra1, how=&#39;left&#39;, left_on=&#39;CO_MUNICIPIO_RESIDENCIA&#39;, right_on=&#39;id_municipio&#39;)</code></pre>
<ol start="2" style="list-style-type: decimal">
<li><a href="https://www.gov.br/inep/pt-br/acesso-a-informacao/dados-abertos/microdados/censo-escolar">Microdados do Censo Escolar da Educaca√ß√£o B√°sica</a></li>
</ol>
<p>Base dispon√≠vel no mesmo site dos dados da competi√ß√£o e que tr√°s informa√ß√µes muito ricas das escolas do Brasil. Infelizmente quase 75% da informa√ß√£o da escola do aluno era missing ent√£o esta base n√£o conseguiu alavancar os ganhos do modelo de maneira consider√°vel.</p>
<p>Nesta base foquei principalmente nas features utilizadas para calcular o IIE (√çndice de Estrutura da Escola) que se baseia nos seguintes componentes:</p>
<table>
<colgroup>
<col width="32%" />
<col width="24%" />
<col width="42%" />
</colgroup>
<thead>
<tr class="header">
<th>Componente 1: Pedag√≥gica (IEE_Pedag√≥gico):</th>
<th>Componente 2: B√°sica (IEE_B√°sico):</th>
<th>Componente 3: Tecnol√≥gica (IEE_Tecnol√≥gico):</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Qualifica√ß√£o do docente (forma√ß√£o acad√™mica dos professores)</td>
<td>√Ågua filtrada (bin√°ria)</td>
<td>N√∫mero de computadores por aluno (computadores dispon√≠veis para uso dos alunos)</td>
</tr>
<tr class="even">
<td>N√∫mero de alunos por sala</td>
<td>Acesso √† rede p√∫blica de energia (bin√°ria)</td>
<td>N√∫mero de equipamentos multim√≠dia por aluno</td>
</tr>
<tr class="odd">
<td>N√∫mero de funcion√°rios por aluno</td>
<td>Acesso √† rede p√∫blica de esgoto (bin√°ria)</td>
<td>Acesso a internet (bin√°ria)</td>
</tr>
<tr class="even">
<td>Quadra de esportes coberta (bin√°ria)</td>
<td>Coleta peri√≥dica de lixo (bin√°ria)</td>
<td>Laborat√≥rio de Ci√™ncias (bin√°ria)</td>
</tr>
<tr class="odd">
<td>Biblioteca (bin√°ria)</td>
<td>Banheiro dentro do pr√©dio (bin√°ria)</td>
<td>Laborat√≥rio de Inform√°tica (bin√°ria)</td>
</tr>
</tbody>
</table>
<ul>
<li><a href="https://leosalesblog.wordpress.com/2018/02/03/escola-ruim-aluno-ruim-entendendo-a-relacao-entre-estrutura-escolar-e-desempenho-no-enem/">Fonte</a></li>
</ul>
<pre class="python"><code># Importar dados
extra2 = pd.read_csv(&#39;microdados_ed_basica_2021.csv&#39;, error_bad_lines=False, sep=&#39;;&#39;, encoding=&#39;latin1&#39;, dtype={&#39;CO_ORGAO_REGIONAL&#39;: &#39;str&#39;})
extra2 = extra2[extra2.isnull().sum(axis=1) / extra2.shape[1] &lt; .9]

# Tratamento nas features
extra2.loc[:, &#39;QT_TOTAL_ALUNOS&#39;] = extra2[[&#39;QT_MAT_BAS_ND&#39;, &#39;QT_MAT_BAS_BRANCA&#39;, &#39;QT_MAT_BAS_PRETA&#39;, &#39;QT_MAT_BAS_PARDA&#39;, &#39;QT_MAT_BAS_AMARELA&#39;, &#39;QT_MAT_BAS_INDIGENA&#39;]].sum(axis=1).fillna(0)
extra2.loc[:, &#39;QT_TOTAL_PROFESSORES&#39;] = (extra2.QT_DOC_BAS + extra2.QT_DOC_INF + extra2.QT_DOC_INF_CRE + extra2.QT_DOC_INF_PRE + extra2.QT_DOC_FUND + extra2.QT_DOC_FUND_AI + extra2.QT_DOC_FUND_AF + extra2.QT_DOC_MED + extra2.QT_DOC_PROF + extra2.QT_DOC_PROF_TEC + extra2.QT_DOC_EJA + extra2.QT_DOC_EJA_FUND + extra2.QT_DOC_EJA_MED + extra2.QT_DOC_ESP + extra2.QT_DOC_ESP_CC + extra2.QT_DOC_ESP_CE).fillna(0)
extra2.loc[:, &#39;QT_SALAS_UTILIZADAS&#39;] = (extra2.loc[:, &#39;QT_TOTAL_ALUNOS&#39;] / extra2.QT_SALAS_UTILIZADAS).fillna(0)
extra2.loc[:, &#39;QT_COMP_DISP_ALUNO&#39;] = extra2.QT_DESKTOP_ALUNO + extra2.QT_COMP_PORTATIL_ALUNO + extra2.QT_TABLET_ALUNO

# Selecao de faetures importantes
features_extra2 = [&#39;CO_ENTIDADE&#39;, &#39;QT_SALAS_UTILIZADAS&#39;, &#39;QT_TOTAL_PROFESSORES&#39;, &#39;IN_QUADRA_ESPORTES_COBERTA&#39;, &#39;IN_BIBLIOTECA&#39;,
       &#39;IN_AGUA_POTAVEL&#39;, &#39;IN_ENERGIA_REDE_PUBLICA&#39;, &#39;IN_ESGOTO_REDE_PUBLICA&#39;, &#39;IN_LIXO_SERVICO_COLETA&#39;, &#39;IN_BANHEIRO&#39;,
       &#39;QT_COMP_DISP_ALUNO&#39;, &#39;QT_EQUIP_MULTIMIDIA&#39;, &#39;IN_INTERNET&#39;, &#39;IN_LABORATORIO_CIENCIAS&#39;, &#39;IN_LABORATORIO_INFORMATICA&#39;]
extra2 = extra2[features_extra2]

# Remover outliers
for c in list(extra2.iloc[:, 1:].columns):
    trs = extra2.loc[extra2[c]!=88888, c].quantile(.99)
    extra2.loc[(extra2[c]==88888)|(extra2[c]&gt;trs), c] = trs
    
#Normalizar para calcular IEE
scaler = MinMaxScaler()
to_iee = scaler.fit_transform(extra2.iloc[:, 1:])
to_iee = pd.DataFrame(to_iee, columns=extra2.iloc[:, 1:].columns)

# Calcular IEE e componentes
extra2.loc[:, &#39;COMP1&#39;] = to_iee[[&#39;QT_SALAS_UTILIZADAS&#39;, &#39;QT_TOTAL_PROFESSORES&#39;, &#39;IN_QUADRA_ESPORTES_COBERTA&#39;, &#39;IN_BIBLIOTECA&#39;]].sum(axis=1)
extra2.loc[:, &#39;COMP2&#39;] = to_iee[[&#39;IN_AGUA_POTAVEL&#39;, &#39;IN_ENERGIA_REDE_PUBLICA&#39;, &#39;IN_ESGOTO_REDE_PUBLICA&#39;, &#39;IN_LIXO_SERVICO_COLETA&#39;, &#39;IN_BANHEIRO&#39;]].sum(axis=1)
extra2.loc[:, &#39;COMP3&#39;] = to_iee[[&#39;QT_COMP_DISP_ALUNO&#39;, &#39;QT_EQUIP_MULTIMIDIA&#39;, &#39;IN_INTERNET&#39;, &#39;IN_LABORATORIO_CIENCIAS&#39;, &#39;IN_LABORATORIO_INFORMATICA&#39;]].sum(axis=1)
extra2.loc[:, &#39;IEE&#39;] = extra2.COMP1 + extra2.COMP2 + extra2.COMP3

train = pd.merge(train, extra2, how=&#39;left&#39;, left_on=&#39;CO_ESCOLA&#39;, right_on=&#39;CO_ENTIDADE&#39;).drop(&#39;CO_ENTIDADE&#39;, axis=1)
test = pd.merge(test, extra2, how=&#39;left&#39;, left_on=&#39;CO_ESCOLA&#39;, right_on=&#39;CO_ENTIDADE&#39;).drop(&#39;CO_ENTIDADE&#39;, axis=1)</code></pre>
</div>
</div>
<div id="modelagem" class="section level2">
<h2>Modelagem</h2>
<p>Testei muitos modelos e muitas abordagens (inclusive com finalidade de estudo). Foram modelos estat√≠sticos (GAM considerando a distribui√ß√£o Beta(0,1)), redes neurais (TabNet) e √°rvores mas no final das contas os que tiveram melhor custo/benef√≠cio foram o LightGBM e o CatBoost.</p>
<p>Sobre o tuning, tomei a decis√£o de n√£o investir muito em otimiza√ß√£o autom√°tica de hiperpar√¢metros pois o tempo era curto e os ganhos seriam pequenos comparados com o potencial ganho com a variedade de features que poderiam ser geradas, ent√£o fiz apenas alguns testes manuais conforme via necessidade.</p>
<div id="pre-processing" class="section level4">
<h4>Pre processing</h4>
<p>A etapa que investi bastante tempo foi para criar novas vari√°veis. A seguir trago algumas features constru√≠das que foram utilizadas em determinados modelos, a partir dos dados dispon√≠veis:</p>
<ul>
<li>Renda somada dos pais;</li>
<li>N√≠vel de ocupa√ß√£o somado dos pais;</li>
<li>Renda dividido pelo n√∫mero de pessoas na casa;</li>
<li>Quantidade de celulares por pessoa na casa;</li>
<li>Quantidade de computadores por pessoa na casa;</li>
<li>Se a pessoa possui vis√£o ruim (se possui baixa vis√£o, cegueira ou monocular);</li>
<li>Se a pessoa possui audi√ß√£o ruim (Surdez, defici√™ncia auditiva);</li>
<li>Se o aluno possui TDAH e toma medicamento controlado;</li>
<li>Se o aluno possui TDAH e teve mais tempo de prova;</li>
<li>Se precisou de recurso de vis√£o ou audi√ß√£o (libras, baile, etc);</li>
<li>Se o munic√≠pio que nasceu √© o mesmo da escola;</li>
<li>Se o munic√≠pio que fez a prova √© o mesmo da escola;</li>
<li>Se o munic√≠pio da prova √© o mesmo da resid√™ncia;</li>
<li>Nota m√©dia dos alunos da respectiva escola nas outras provas (*);</li>
<li>Renda m√©dia dos alunos da respectiva escola (*).</li>
</ul>
<p>(*) Estas features precisaram ser calculadas de maneira muito cuidadosa para n√£o causar algum tipo de data leak!</p>
</div>
<div id="post-processing" class="section level4">
<h4>Post Processing</h4>
<p>Essa base tinha uma pegadinha que fazia muita diferen√ßa no resultado final. Existem duas possibilidades de um aluno tirar zero em uma prova: errar tudo ou n√£o comparecer.</p>
<p>Como temos a informa√ß√£o da presen√ßa do aluno na prova (o que na pr√°tica seria meio estranho) bastava dar zero para os alunos faltantes na hora de prever nos dados de teste para submeter.</p>
</div>
<div id="linguagens-e-c√≥digos" class="section level3">
<h3>Linguagens e C√≥digos</h3>
<p>Definir finalidade de algumas colunas:</p>
<pre class="python"><code># colunas que serao dropadas
to_drop = [&#39;IN_PROVA_DEITADO&#39;,
            &#39;NU_INSCRICAO&#39;,
            &#39;CO_MUNICIPIO_ESC&#39;,
            &#39;CO_UF_NASCIMENTO&#39;,
            &#39;CO_UF_RESIDENCIA&#39;,
            &#39;CO_UF_ESC&#39;,
            &#39;CO_UF_PROVA&#39;,
            &#39;CO_MUNICIPIO_PROVA&#39;,
            &#39;CO_MUNICIPIO_RESIDENCIA&#39;,
            &#39;CO_MUNICIPIO_NASCIMENTO&#39;]

# definir target e presenca
target = &quot;NU_NOTA_LC&quot;
presenca = &quot;TP_PRESENCA_LC&quot;

# demais notas para dropar (menos ch)
notas = list(set(targets)-set([target, &#39;NU_NOTA_CH&#39;]))</code></pre>
<p>Pr√©-processamento nos dados de treino</p>
<pre class="python"><code>X = train.copy()
X.loc[:, &#39;knn_feature&#39;] = knn_train.knn_oof
X = X.drop(to_drop, axis=1) 
X = X[X[presenca]==1]
X = X[~X[target].isnull()]

X = X.loc[:, ~X.columns.isin([target]+[presenca]+notas)]
X.loc[:, &#39;FE_RENDA&#39;] = X.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000, &#39;C&#39;:1500, &#39;D&#39;:2000,
&#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, &#39;I&#39;:6000, &#39;J&#39;:7000,&#39;K&#39;:8000,&#39;L&#39;:9000,
&#39;M&#39;:10000, &#39;N&#39;:12000, &#39;O&#39;:15000, &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
X = prep_data_questionarios(X)
X = fe_mun(X)
X = fe_questionario(X)
X = fe_in(X)
X = prep_co_escola(X)
X = fe_extra(X)

y = train.loc[(train[presenca]==1)&amp;(~train[target].isnull()), target].astype(np.float64)</code></pre>
<p>Pr√©-processamento nos dados de teste</p>
<pre class="python"><code>X_test = test.copy()
X_test.loc[:, &#39;knn_feature&#39;] = knn_test.knn_test
X_test = X_test.drop(to_drop, axis=1) 

X_test = X_test.loc[:, ~X_test.columns.isin([presenca])]
X_test.loc[:, &#39;FE_RENDA&#39;] = X_test.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000, &#39;C&#39;:1500, &#39;D&#39;:2000,
&#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, &#39;I&#39;:6000, &#39;J&#39;:7000, &#39;K&#39;:8000,&#39;L&#39;:9000,
&#39;M&#39;:10000, &#39;N&#39;:12000, &#39;O&#39;:15000, &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
X_test = prep_data_questionarios(X_test)
X_test = fe_mun(X_test)
X_test = fe_questionario(X_test)
X_test = fe_in(X_test)
X_test = prep_co_escola(X_test)
X_test = fe_extra(X_test)</code></pre>
<p>Feature engineering separada para evitar data leak:</p>
<pre class="python"><code># calcular estatisticas nos dados de treino
co_escola_renda_media = X.groupby(&#39;CO_ESCOLA&#39;).FE_RENDA.mean()
co_escola_idade_media = X.groupby(&#39;CO_ESCOLA&#39;).NU_IDADE.mean()
co_escola_nota_ch = X.groupby(&#39;CO_ESCOLA&#39;).NU_NOTA_CH.mean()
X = X.drop(&#39;NU_NOTA_CH&#39;, axis=1)

# instanciar objeto com as estatisticas por escola
co_escola_aux = pd.DataFrame({
    &#39;CO_ESCOLA&#39;: co_escola_renda_media.index,
    &#39;FE_ESCOLA_RENDA_MEDIA&#39;: co_escola_renda_media,
    &#39;FE_IDADE_MEDIA&#39;: co_escola_idade_media,
    &#39;FE_NOTA_CH&#39;: co_escola_nota_ch
}).reset_index(drop=True)

# Concatenar estatisticas nas bases de treino e teste
X = pd.merge(X, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)
X_test = pd.merge(X_test, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)

# Codigo da escola para categorico
X.loc[:, &#39;CO_ESCOLA&#39;] = X.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)
X_test.loc[:, &#39;CO_ESCOLA&#39;] = X_test.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)

# Features de contagem
X.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X_test.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X.NO_MUNICIPIO_RESIDENCIA.map({x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X_test.NO_MUNICIPIO_RESIDENCIA.map({ x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X.NO_MUNICIPIO_NASCIMENTO.map({ x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X_test.NO_MUNICIPIO_NASCIMENTO.map({x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})

X.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X_test.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})</code></pre>
<p>Ajustar modelo:</p>
<pre class="python"><code>cat_feat = X.columns[X.dtypes==&#39;category&#39;]
cat_indices = [X.columns.get_loc(x) for x in cat_feat]

for c in list(cat_feat):
    X.loc[:, c] = X.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)
    X_test.loc[:, c] = X_test.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)

X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.1, random_state=SEED)
    
clf = CatBoostRegressor(random_state=314,
                            cat_features=cat_indices,
                            verbose=0,
                            loss_function = &quot;RMSE&quot;,
                            od_type = &quot;Iter&quot;,
                            od_wait = 100,
                            iterations=3000,
                            use_best_model=True)

clf.fit(X, y, eval_set = (X_eval, y_eval), verbose=False, plot=True)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/lc_catboost.png" style="width:95.0%" />
</center>
<p>Salvar previs√µes:</p>
<pre class="python"><code>sub.loc[:, &#39;NU_NOTA_LC&#39;] = clf.predict(X_test)
# alunos que nao foram fazer a prova tiraram zero
sub.loc[test.TP_PRESENCA_LC!=1, &#39;NU_NOTA_LC&#39;] = 0</code></pre>
<p>Comparar distribui√ß√£o da target nos dados de treino com rela√ß√£o √†s previs√µes do modelo:</p>
<pre class="python"><code>sns.kdeplot(train.loc[:, target], shade=True, color=&#39;r&#39;, clip=[0,1000])
sns.kdeplot(sub.loc[:, target], shade=True, color=&#39;b&#39;, clip=[0,1000])
plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
plt.title(target)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/lc_pred.png" style="width:50.0%" />
</center>
</div>
<div id="ci√™ncias-humanas" class="section level3">
<h3>Ci√™ncias Humanas</h3>
<p>Novas features desenvolvidas especificamente para este modelo:</p>
<pre class="python"><code>def fe_ch(df):
    
    df.loc[:, &#39;FE_RENDA&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000,
    &#39;C&#39;:1500, &#39;D&#39;:2000, &#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, &#39;I&#39;:6000,
    &#39;J&#39;:7000, &#39;K&#39;:8000,&#39;L&#39;:9000, &#39;M&#39;:10000, &#39;N&#39;:12000, &#39;O&#39;:15000, 
    &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
    df.loc[:, &#39;FE_NU_IDADE*TP_ANO_CONCLUIU&#39;] = df.TP_ANO_CONCLUIU * df.NU_IDADE
    df.loc[:, &#39;FE_Q002+Q024&#39;] = df.loc[:, &#39;Q002&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, 
    &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}).astype(int) + 
    df.loc[:, &#39;Q024&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}).astype(int) 
    df.loc[:, &#39;FE_SCORE&#39;] = (1/df.TP_ANO_CONCLUIU) + np.sqrt(df.NU_IDADE) +
    np.where(df.TP_ESCOLA==3, 1, 0)
    
    return df</code></pre>
<p>Definir finalidade de algumas colunas:</p>
<pre class="python"><code># colunas que serao dropadas
to_drop = [&#39;IN_PROVA_DEITADO&#39;,
           &#39;NU_INSCRICAO&#39;,
           &#39;CO_MUNICIPIO_ESC&#39;,
           &#39;CO_UF_NASCIMENTO&#39;,
           &#39;CO_UF_RESIDENCIA&#39;,
           &#39;CO_UF_ESC&#39;,
           &#39;CO_UF_PROVA&#39;,
           &#39;CO_MUNICIPIO_PROVA&#39;,
           &#39;CO_MUNICIPIO_RESIDENCIA&#39;,
          &#39;CO_MUNICIPIO_NASCIMENTO&#39;]

# definir target e presenca
target = &quot;NU_NOTA_CH&quot;
presenca = &quot;TP_PRESENCA_CH&quot;

# demais notas para dropar (menos lc)
notas = list(set(targets)-set([target, &#39;NU_NOTA_LC&#39;]))</code></pre>
<p>Pr√©-processamento nos dados de treino</p>
<pre class="python"><code>X = train.copy()
X.loc[:, &#39;knn_feature&#39;] = knn_train.knn_oof
X = X.drop(to_drop, axis=1)
X = X[X[presenca]==1]
X = X[~X[target].isnull()]

X = X.loc[:, ~X.columns.isin([target]+[presenca]+notas)]
X = fe_ch(X)
X = prep_data_questionarios(X)
X = fe_mun(X)
X = fe_questionario(X)
#X = fe_in(X)
X = prep_co_escola(X)
X = fe_extra(X)

y = train.loc[(train[presenca]==1)&amp;(~train[target].isnull()), target].astype(np.float64)</code></pre>
<p>Pr√©-processamento nos dados de teste</p>
<pre class="python"><code>X_test = test.copy()
X_test.loc[:, &#39;knn_feature&#39;] = knn_test.knn_test
X_test = X_test.drop(to_drop, axis=1) 

X_test = X_test.loc[:, ~X_test.columns.isin([presenca])]
X_test = fe_ch(X_test)
X_test = prep_data_questionarios(X_test)
X_test = fe_mun(X_test)
X_test = fe_questionario(X_test)
#X_test = fe_in(X_test)
X_test = prep_co_escola(X_test)
X_test = fe_extra(X_test)</code></pre>
<p>Feature engineering separada para evitar data leak:</p>
<pre class="python"><code># calcular estatisticas nos dados de treino
co_escola_renda_media = X.groupby(&#39;CO_ESCOLA&#39;).FE_RENDA.mean()
co_escola_idade_media = X.groupby(&#39;CO_ESCOLA&#39;).NU_IDADE.mean()
co_escola_nota_lc = X.groupby(&#39;CO_ESCOLA&#39;).NU_NOTA_LC.mean()
X = X.drop(&#39;NU_NOTA_LC&#39;, axis=1)

# instanciar objeto com as estatisticas por escola
co_escola_aux = pd.DataFrame({
    &#39;CO_ESCOLA&#39;: co_escola_renda_media.index,
    &#39;FE_ESCOLA_RENDA_MEDIA&#39;: co_escola_renda_media,
    &#39;FE_IDADE_MEDIA&#39;: co_escola_idade_media,
    &#39;FE_NOTA_LC&#39;: co_escola_nota_lc
}).reset_index(drop=True)

# Concatenar estatisticas nas bases de treino e teste
X = pd.merge(X, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)
X_test = pd.merge(X_test, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)

# Codigo da escola para categorico
X.loc[:, &#39;CO_ESCOLA&#39;] = X.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)
X_test.loc[:, &#39;CO_ESCOLA&#39;] = X_test.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)

# Features de contagem
X.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X_test.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X.NO_MUNICIPIO_RESIDENCIA.map({x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X_test.NO_MUNICIPIO_RESIDENCIA.map({ x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X.NO_MUNICIPIO_NASCIMENTO.map({ x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X_test.NO_MUNICIPIO_NASCIMENTO.map({x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})

X.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X_test.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})</code></pre>
<p>Ajustar modelo:</p>
<pre class="python"><code>%%time

cat_feat = X.columns[X.dtypes==&#39;category&#39;]
cat_indices = [X.columns.get_loc(x) for x in cat_feat]

for c in list(cat_feat):
    X.loc[:, c] = X.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)
    X_test.loc[:, c] = X_test.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)

X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.1, random_state=SEED)

clf = CatBoostRegressor(random_state=314,
                            cat_features=cat_indices,
                            verbose=0,
                            loss_function = &quot;RMSE&quot;,
                            od_type = &quot;Iter&quot;,
                            od_wait = 100,iterations=3000,
                            use_best_model=True)

clf.fit(X, y, eval_set = (X_eval, y_eval), verbose=False, plot=True)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/ch_catboost.png" style="width:95.0%" />
</center>
<p>Salvar previs√µes:</p>
<pre class="python"><code>sub.loc[:, &#39;NU_NOTA_CH&#39;] = clf.predict(X_test)
# alunos que nao foram fazer a prova tiraram zero
sub.loc[test.TP_PRESENCA_CH!=1, &#39;NU_NOTA_CH&#39;] = 0</code></pre>
<p>Comparar distribui√ß√£o da target nos dados de treino com rela√ß√£o √†s previs√µes do modelo:</p>
<pre class="python"><code>sns.kdeplot(train.loc[:, target], shade=True, color=&#39;r&#39;, clip=[0,1000])
sns.kdeplot(sub.loc[:, target], shade=True, color=&#39;b&#39;, clip=[0,1000])
plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
plt.title(target)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/ch_pred.png" style="width:50.0%" />
</center>
</div>
<div id="ci√™ncias-da-natureza" class="section level3">
<h3>Ci√™ncias da Natureza</h3>
<p>Novas features desenvolvidas especificamente para este modelo:</p>
<pre class="python"><code>def fe_cn(df):
    df.loc[:, &#39;FE_RENDA&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000,
    &#39;C&#39;:1500, &#39;D&#39;:2000, &#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, 
    &#39;I&#39;:6000, &#39;J&#39;:7000, &#39;K&#39;:8000,&#39;L&#39;:9000, &#39;M&#39;:10000, &#39;N&#39;:12000, 
    &#39;O&#39;:15000, &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
    df.loc[:, &#39;FE_NU_IDADE*TP_ANO_CONCLUIU&#39;] = df.TP_ANO_CONCLUIU * df.NU_IDADE
    df.loc[:, &#39;FE_Q002+Q024&#39;] = df.loc[:, &#39;Q002&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2,
    &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}).astype(int) + 
    df.loc[:, &#39;Q024&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}).astype(int) 
    df.loc[:, &#39;FE_SCORE&#39;] = (1/df.TP_ANO_CONCLUIU) + np.sqrt(df.NU_IDADE) + np.where(df.TP_ESCOLA==3, 1, 0)
    
    df.loc[:, &#39;FE_UF_ESCOLA&#39;] = df.SG_UF_ESC.map({
      &#39;AM&#39;:&#39;Norte&#39;, &#39;RR&#39;:&#39;Norte&#39;, &#39;AP&#39;:&#39;Norte&#39;, &#39;PA&#39;:&#39;Norte&#39;, &#39;TO&#39;:&#39;Norte&#39;, &#39;RO&#39;:&#39;Norte&#39;, &#39;AC&#39;:&#39;Norte&#39;,
      &#39;MA&#39;:&#39;Nordeste&#39;, &#39;PI&#39;:&#39;Nordeste&#39;, &#39;CE&#39;:&#39;Nordeste&#39;, &#39;RN&#39;:&#39;Nordeste&#39;, &#39;PE&#39;:&#39;Nordeste&#39;, &#39;PB&#39;:&#39;Nordeste&#39;, &#39;SE&#39;:&#39;Nordeste&#39;, &#39;AL&#39;:&#39;Nordeste&#39;, &#39;BA&#39;:&#39;Nordeste&#39;,
      &#39;MT&#39;: &#39;CentroOeste&#39;, &#39;MS&#39;: &#39;CentroOeste&#39;, &#39;GO&#39;: &#39;CentroOeste&#39;,
      &#39;SP&#39;: &#39;Sudeste&#39;, &#39;RJ&#39;: &#39;Sudeste&#39;, &#39;ES&#39;: &#39;Sudeste&#39;, &#39;MG&#39;: &#39;Sudeste&#39;,
      &#39;PR&#39;: &#39;Sul&#39;, &#39;RS&#39;: &#39;Sul&#39;, &#39;SC&#39;: &#39;Sul&#39;}).astype(&#39;category&#39;)
    return df</code></pre>
<p>Definir finalidade de algumas colunas:</p>
<pre class="python"><code># colunas que serao dropadas
to_drop = [&#39;IN_PROVA_DEITADO&#39;,
           &#39;NU_INSCRICAO&#39;,
           &#39;CO_MUNICIPIO_ESC&#39;,
           &#39;CO_UF_NASCIMENTO&#39;,
           &#39;CO_UF_RESIDENCIA&#39;,
           &#39;CO_UF_ESC&#39;,
           &#39;CO_UF_PROVA&#39;,
           &#39;CO_MUNICIPIO_PROVA&#39;,
           &#39;CO_MUNICIPIO_RESIDENCIA&#39;,
          &#39;CO_MUNICIPIO_NASCIMENTO&#39;]

# definir target e presenca
target = &quot;NU_NOTA_CN&quot;
presenca = &quot;TP_PRESENCA_CN&quot;

# demais notas para dropar (menos mt)
notas = list(set(targets)-set([target, &#39;NU_NOTA_MT&#39;]))</code></pre>
<p>Pr√©-processamento nos dados de treino</p>
<pre class="python"><code>X = train.copy()
X = X.drop(to_drop, axis=1) 
X = X[X[presenca]==1]
X = X[~X[target].isnull()]

X = X.loc[:, ~X.columns.isin([target]+[presenca]+notas)]
X = fe_cn(X)
X = prep_data_questionarios(X)
X = fe_mun(X)
X = fe_questionario(X)
X = fe_in(X)
X = prep_co_escola(X)
X = fe_extra(X)

y = train.loc[(train[presenca]==1)&amp;(~train[target].isnull()), target].astype(np.float64)</code></pre>
<p>Pr√©-processamento nos dados de teste</p>
<pre class="python"><code>X_test = test.copy()
X_test = X_test.drop(to_drop, axis=1) 

X_test = X_test.loc[:, ~X_test.columns.isin([presenca])]
X_test = fe_cn(X_test)
X_test = prep_data_questionarios(X_test)
X_test = fe_mun(X_test)
X_test = fe_questionario(X_test)
X_test = fe_in(X_test)
X_test = prep_co_escola(X_test)
X_test = fe_extra(X_test)</code></pre>
<p>Feature engineering separada para evitar data leak:</p>
<pre class="python"><code># calcular estatisticas nos dados de treino
co_escola_renda_media = X.groupby(&#39;CO_ESCOLA&#39;).FE_RENDA.mean()
co_escola_idade_media = X.groupby(&#39;CO_ESCOLA&#39;).NU_IDADE.mean()
co_escola_nota_mt = X.groupby(&#39;CO_ESCOLA&#39;).NU_NOTA_MT.mean()
X = X.drop(&#39;NU_NOTA_MT&#39;, axis=1)

# instanciar objeto com as estatisticas por escola
co_escola_aux = pd.DataFrame({
    &#39;CO_ESCOLA&#39;: co_escola_renda_media.index,
    &#39;FE_ESCOLA_RENDA_MEDIA&#39;: co_escola_renda_media,
    &#39;FE_IDADE_MEDIA&#39;: co_escola_idade_media,
    &#39;FE_NOTA_MT&#39;: co_escola_nota_mt
}).reset_index(drop=True)

# Concatenar estatisticas nas bases de treino e teste
X = pd.merge(X, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)
X_test = pd.merge(X_test, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)

# Codigo da escola para categorico
X.loc[:, &#39;CO_ESCOLA&#39;] = X.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)
X_test.loc[:, &#39;CO_ESCOLA&#39;] = X_test.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)

# Features de contagem
X.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X_test.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X.NO_MUNICIPIO_RESIDENCIA.map({x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X_test.NO_MUNICIPIO_RESIDENCIA.map({ x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X.NO_MUNICIPIO_NASCIMENTO.map({ x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X_test.NO_MUNICIPIO_NASCIMENTO.map({x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})

X.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X_test.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})</code></pre>
<p>Ajustar modelo:</p>
<pre class="python"><code>%%time

cat_feat = X.columns[X.dtypes==&#39;category&#39;]
cat_indices = [X.columns.get_loc(x) for x in cat_feat]

for c in list(cat_feat):
    X.loc[:, c] = X.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)
    X_test.loc[:, c] = X_test.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)

X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.1, random_state=SEED)

clf = CatBoostRegressor(random_state=314,
                            cat_features=cat_indices,
                            verbose=0,
                            loss_function = &quot;RMSE&quot;,
                            od_type = &quot;Iter&quot;,
                            od_wait = 100,iterations=3000,
                            use_best_model=True)

clf.fit(X, y, eval_set = (X_eval, y_eval), verbose=False, plot=True)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/cn_catboost.png" style="width:95.0%" />
</center>
<p>Salvar previs√µes:</p>
<pre class="python"><code>sub.loc[:, &#39;NU_NOTA_CN&#39;] = clf.predict(X_test)
# alunos que nao foram fazer a prova tiraram zero
sub.loc[test.TP_PRESENCA_CN!=1, &#39;NU_NOTA_CN&#39;] = 0</code></pre>
<p>Comparar distribui√ß√£o da target nos dados de treino com rela√ß√£o √†s previs√µes do modelo:</p>
<pre class="python"><code>sns.kdeplot(train.loc[:, target], shade=True, color=&#39;r&#39;, clip=[0,1000])
sns.kdeplot(sub.loc[:, target], shade=True, color=&#39;b&#39;, clip=[0,1000])
plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
plt.title(target)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/cn_pred.png" style="width:50.0%" />
</center>
</div>
<div id="matem√°tica" class="section level3">
<h3>Matem√°tica</h3>
<p>Novas features desenvolvidas especificamente para este modelo:</p>
<pre class="python"><code>def fe_mt(df):
    
    df.loc[:, &#39;FE_RENDA&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000, &#39;C&#39;:1500, &#39;D&#39;:2000, &#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, &#39;I&#39;:6000, &#39;J&#39;:7000, &#39;K&#39;:8000,&#39;L&#39;:9000, &#39;M&#39;:10000, &#39;N&#39;:12000, &#39;O&#39;:15000, &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
    df.loc[:, &#39;FE_NU_IDADE*TP_ANO_CONCLUIU&#39;] = df.TP_ANO_CONCLUIU * df.NU_IDADE
    df.loc[:, &#39;FE_Q002+Q024&#39;] = df.loc[:, &#39;Q002&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}).astype(int) + df.loc[:, &#39;Q024&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}).astype(int) 
    df.loc[:, &#39;FE_SCORE&#39;] = (1/df.TP_ANO_CONCLUIU) + np.sqrt(df.NU_IDADE) + np.where(df.TP_ESCOLA==3, 1, 0)
    
    df.loc[:, &#39;FE_UF_ESCOLA&#39;] = df.SG_UF_ESC.map({&#39;AM&#39;:&#39;Norte&#39;, &#39;RR&#39;:&#39;Norte&#39;, &#39;AP&#39;:&#39;Norte&#39;, &#39;PA&#39;:&#39;Norte&#39;, &#39;TO&#39;:&#39;Norte&#39;, &#39;RO&#39;:&#39;Norte&#39;, &#39;AC&#39;:&#39;Norte&#39;,
                &#39;MA&#39;:&#39;Nordeste&#39;, &#39;PI&#39;:&#39;Nordeste&#39;, &#39;CE&#39;:&#39;Nordeste&#39;, &#39;RN&#39;:&#39;Nordeste&#39;, &#39;PE&#39;:&#39;Nordeste&#39;, &#39;PB&#39;:&#39;Nordeste&#39;, &#39;SE&#39;:&#39;Nordeste&#39;, &#39;AL&#39;:&#39;Nordeste&#39;, &#39;BA&#39;:&#39;Nordeste&#39;,
                &#39;MT&#39;: &#39;CentroOeste&#39;, &#39;MS&#39;: &#39;CentroOeste&#39;, &#39;GO&#39;: &#39;CentroOeste&#39;,
                &#39;SP&#39;: &#39;Sudeste&#39;, &#39;RJ&#39;: &#39;Sudeste&#39;, &#39;ES&#39;: &#39;Sudeste&#39;, &#39;MG&#39;: &#39;Sudeste&#39;,
                &#39;PR&#39;: &#39;Sul&#39;, &#39;RS&#39;: &#39;Sul&#39;, &#39;SC&#39;: &#39;Sul&#39;}).astype(&#39;category&#39;)
    
    
    return df</code></pre>
<p>Definir finalidade de algumas colunas:</p>
<pre class="python"><code># colunas que serao dropadas
to_drop = [&#39;IN_PROVA_DEITADO&#39;,
           &#39;NU_INSCRICAO&#39;,
           &#39;CO_MUNICIPIO_ESC&#39;,
           &#39;CO_UF_NASCIMENTO&#39;,
           &#39;CO_UF_RESIDENCIA&#39;,
           &#39;CO_UF_ESC&#39;,
           &#39;CO_UF_PROVA&#39;,
           &#39;CO_MUNICIPIO_PROVA&#39;,
           &#39;CO_MUNICIPIO_RESIDENCIA&#39;,
          &#39;CO_MUNICIPIO_NASCIMENTO&#39;]

# definir target e presenca
target = &quot;NU_NOTA_MT&quot;
presenca = &quot;TP_PRESENCA_MT&quot;

# demais notas para dropar (menos cn)
notas = list(set(targets)-set([target, &#39;NU_NOTA_CN&#39;]))</code></pre>
<p>Pr√©-processamento nos dados de treino</p>
<pre class="python"><code>X = train.copy()
X = X.drop(to_drop, axis=1) 
X = X[X[presenca]==1]
X = X[~X[target].isnull()]

X = X.loc[:, ~X.columns.isin([target]+[presenca]+notas)]
X = fe_mt(X)
X = prep_data_questionarios(X)
X = fe_mun(X)
#X = fe_questionario(X)
#X = fe_in(X)
X = prep_co_escola(X)
X = fe_extra(X)

y = train.loc[(train[presenca]==1)&amp;(~train[target].isnull()), target].astype(np.float64)</code></pre>
<p>Pr√©-processamento nos dados de teste</p>
<pre class="python"><code>X_test = test.copy()
X_test = X_test.drop(to_drop, axis=1) 

X_test = X_test.loc[:, ~X_test.columns.isin([presenca])]
X_test = fe_mt(X_test)
X_test = prep_data_questionarios(X_test)
X_test = fe_mun(X_test)
#X_test = fe_questionario(X_test)
#X_test = fe_in(X_test)
X_test = prep_co_escola(X_test)
X_test = fe_extra(X_test)</code></pre>
<p>Feature engineering separada para evitar data leak:</p>
<pre class="python"><code># calcular estatisticas nos dados de treino
co_escola_renda_media = X.groupby(&#39;CO_ESCOLA&#39;).FE_RENDA.mean()
co_escola_idade_media = X.groupby(&#39;CO_ESCOLA&#39;).NU_IDADE.mean()
co_escola_nota_cn = X.groupby(&#39;CO_ESCOLA&#39;).NU_NOTA_CN.mean()
X = X.drop(&#39;NU_NOTA_CN&#39;, axis=1)

# instanciar objeto com as estatisticas por escola
co_escola_aux = pd.DataFrame({
    &#39;CO_ESCOLA&#39;: co_escola_renda_media.index,
    &#39;FE_ESCOLA_RENDA_MEDIA&#39;: co_escola_renda_media,
    &#39;FE_IDADE_MEDIA&#39;: co_escola_idade_media,
    &#39;FE_NOTA_CN&#39;: co_escola_nota_cn
}).reset_index(drop=True)

# Concatenar estatisticas nas bases de treino e teste
X = pd.merge(X, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)
X_test = pd.merge(X_test, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)

# Codigo da escola para categorico
X.loc[:, &#39;CO_ESCOLA&#39;] = X.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)
X_test.loc[:, &#39;CO_ESCOLA&#39;] = X_test.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)

# Features de contagem
X.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X_test.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X.NO_MUNICIPIO_RESIDENCIA.map({x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X_test.NO_MUNICIPIO_RESIDENCIA.map({ x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X.NO_MUNICIPIO_NASCIMENTO.map({ x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X_test.NO_MUNICIPIO_NASCIMENTO.map({x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})

X.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X_test.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})</code></pre>
<p>Ajustar modelo:</p>
<pre class="python"><code>%%time

cat_feat = X.columns[X.dtypes==&#39;category&#39;]
cat_indices = [X.columns.get_loc(x) for x in cat_feat]

for c in list(cat_feat):
    X.loc[:, c] = X.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)
    X_test.loc[:, c] = X_test.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)

X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.1, random_state=SEED)
    
clf = CatBoostRegressor(random_state=314,
                            cat_features=cat_indices,
                            verbose=0,
                            loss_function = &quot;RMSE&quot;,
                            od_type = &quot;Iter&quot;,
                            od_wait = 100,iterations=3000,
                            use_best_model=True)

clf.fit(X, y, eval_set = (X_eval, y_eval), verbose=False, plot=True)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/mt_catboost.png" style="width:95.0%" />
</center>
<p>Salvar previs√µes:</p>
<pre class="python"><code>sub.loc[:, &#39;NU_NOTA_MT&#39;] = clf.predict(X_test)
# alunos que nao foram fazer a prova tiraram zero
sub.loc[test.TP_PRESENCA_CN!=1, &#39;NU_NOTA_MT&#39;] = 0</code></pre>
<p>Comparar distribui√ß√£o da target nos dados de treino com rela√ß√£o √†s previs√µes do modelo:</p>
<pre class="python"><code>sns.kdeplot(train.loc[:, target], shade=True, color=&#39;r&#39;, clip=[0,1000])
sns.kdeplot(sub.loc[:, target], shade=True, color=&#39;b&#39;, clip=[0,1000])
plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
plt.title(target)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/mt_pred.png" style="width:50.0%" />
</center>
</div>
<div id="reda√ß√£o" class="section level3">
<h3>Reda√ß√£o</h3>
<p>Novas features desenvolvidas especificamente para este modelo:</p>
<pre class="python"><code>def fe_rd(df):
    
    df.loc[:, &#39;FE_RENDA&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1000, &#39;C&#39;:1500, &#39;D&#39;:2000, &#39;E&#39;:2500, &#39;F&#39;:3000, &#39;G&#39;:4000, &#39;H&#39;:5000, &#39;I&#39;:6000, &#39;J&#39;:7000, &#39;K&#39;:8000,&#39;L&#39;:9000, &#39;M&#39;:10000, &#39;N&#39;:12000, &#39;O&#39;:15000, &#39;P&#39;:20000, &#39;Q&#39;:30000}).astype(int) 
    df.loc[:, &#39;FE_NU_IDADE*TP_ANO_CONCLUIU&#39;] = df.TP_ANO_CONCLUIU * df.NU_IDADE
    df.loc[:, &#39;FE_Q002+Q024&#39;] = df.loc[:, &#39;Q002&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;: 5, &#39;G&#39;: 6, &#39;H&#39;: -1}).astype(int) + df.loc[:, &#39;Q024&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}).astype(int) 
    df.loc[:, &#39;FE_SCORE&#39;] = (1/df.TP_ANO_CONCLUIU) + np.sqrt(df.NU_IDADE) + np.where(df.TP_ESCOLA==3, 1, 0)
    
    df.loc[:, &#39;FE_RENDA_FAMILIA_+_IDADE&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;:5, &#39;G&#39;:6, &#39;H&#39;:7, &#39;I&#39;:8, &#39;J&#39;:9, &#39;K&#39;:10,&#39;L&#39;:11, &#39;M&#39;:12, &#39;N&#39;:13, &#39;O&#39;:14, &#39;P&#39;:15, &#39;Q&#39;:16}).astype(int) + df.NU_IDADE        
    df.loc[:, &#39;FE_RENDA_FAMILIA_+_ANO_CONCLUIU&#39;] = df.loc[:, &#39;Q006&#39;].map({&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4, &#39;F&#39;:5, &#39;G&#39;:6, &#39;H&#39;:7, &#39;I&#39;:8, &#39;J&#39;:9, &#39;K&#39;:10,&#39;L&#39;:11, &#39;M&#39;:12, &#39;N&#39;:13, &#39;O&#39;:14, &#39;P&#39;:15, &#39;Q&#39;:16}).astype(int)+ df.TP_ANO_CONCLUIU  
    
    return df</code></pre>
<p>Definir finalidade de algumas colunas:</p>
<pre class="python"><code># colunas que serao dropadas
to_drop = [&#39;IN_PROVA_DEITADO&#39;,
           &#39;NU_INSCRICAO&#39;,
           &#39;CO_MUNICIPIO_ESC&#39;,
           &#39;CO_UF_NASCIMENTO&#39;,
           &#39;CO_UF_RESIDENCIA&#39;,
           &#39;CO_UF_ESC&#39;,
           &#39;CO_UF_PROVA&#39;,
           &#39;CO_MUNICIPIO_PROVA&#39;,
           &#39;CO_MUNICIPIO_RESIDENCIA&#39;,
          &#39;CO_MUNICIPIO_NASCIMENTO&#39;]

# definir target e presenca
target = &quot;NU_NOTA_REDACAO&quot;
presenca = &quot;TP_STATUS_REDACAO&quot;

# demais notas para dropar 
notas = list(set(targets)-set([target]))</code></pre>
<p>Pr√©-processamento nos dados de treino</p>
<pre class="python"><code>X = train.copy()
X = X.drop(to_drop, axis=1) 
X = X[X[presenca]==1]
X = X[~X[target].isnull()]


X = X.loc[:, ~X.columns.isin([target]+[presenca]+notas)]
X = fe_rd(X)
X = prep_data_questionarios(X)
X = fe_mun(X)
#X = fe_questionario(X)
X = fe_in(X)
X = prep_co_escola(X)
X = fe_extra(X)

y = train.loc[(train[presenca]==1)&amp;(~train[target].isnull()), target].astype(np.float64)</code></pre>
<p>Pr√©-processamento nos dados de teste</p>
<pre class="python"><code>X_test = test.copy()
X_test = X_test.drop(to_drop, axis=1) 

X_test = X_test.loc[:, ~X_test.columns.isin([presenca])]
X_test = fe_rd(X_test)
X_test = prep_data_questionarios(X_test)
X_test = fe_mun(X_test)
#X_test = fe_questionario(X_test)
X_test = fe_in(X_test)
X_test = prep_co_escola(X_test)
X_test = fe_extra(X_test)</code></pre>
<p>Feature engineering separada para evitar data leak:</p>
<pre class="python"><code># calcular estatisticas nos dados de treino
co_escola_renda_media = X.groupby(&#39;CO_ESCOLA&#39;).FE_RENDA.mean()
co_escola_idade_media = X.groupby(&#39;CO_ESCOLA&#39;).NU_IDADE.mean()

# instanciar objeto com as estatisticas por escola
co_escola_aux = pd.DataFrame({
    &#39;CO_ESCOLA&#39;: co_escola_renda_media.index,
    &#39;FE_ESCOLA_RENDA_MEDIA&#39;: co_escola_renda_media,
    &#39;FE_IDADE_MEDIA&#39;: co_escola_idade_media
}).reset_index(drop=True)

# Concatenar estatisticas nas bases de treino e teste
X = pd.merge(X, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)
X_test = pd.merge(X_test, co_escola_aux, how=&#39;left&#39;, on=&#39;CO_ESCOLA&#39;)

# Codigo da escola para categorico
X.loc[:, &#39;CO_ESCOLA&#39;] = X.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)
X_test.loc[:, &#39;CO_ESCOLA&#39;] = X_test.CO_ESCOLA.astype(&#39;object&#39;).astype(&#39;category&#39;)

# Features de contagem
X.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_PROVA&#39;] = X_test.NO_MUNICIPIO_PROVA.map({x: y for x, y in zip(X.NO_MUNICIPIO_PROVA.value_counts().index.values, X.NO_MUNICIPIO_PROVA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X.NO_MUNICIPIO_RESIDENCIA.map({x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_RESIDENCIA&#39;] = X_test.NO_MUNICIPIO_RESIDENCIA.map({ x: y for x, y in zip(X.NO_MUNICIPIO_RESIDENCIA.value_counts().index.values, X.NO_MUNICIPIO_RESIDENCIA.value_counts().values)})

X.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X.NO_MUNICIPIO_NASCIMENTO.map({ x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_MUNICIPIO_NASCIMENTO&#39;] = X_test.NO_MUNICIPIO_NASCIMENTO.map({x: y for x, y in zip(X.NO_MUNICIPIO_NASCIMENTO.value_counts().index.values, X.NO_MUNICIPIO_NASCIMENTO.value_counts().values)})

X.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})
X_test.loc[:, &#39;FE_COUNT_ESCOLA&#39;] = X_test.CO_ESCOLA.map({x: y for x, y in zip(X.CO_ESCOLA.value_counts().index.values, X.CO_ESCOLA.value_counts().values)})</code></pre>
<p>Ajustar modelo:</p>
<pre class="python"><code>%%time

cat_feat = X.columns[X.dtypes==&#39;category&#39;]
cat_indices = [X.columns.get_loc(x) for x in cat_feat]

for c in list(cat_feat):
    X.loc[:, c] = X.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)
    X_test.loc[:, c] = X_test.loc[:, c].astype(object).fillna(&quot;XXX&quot;).astype(&quot;category&quot;)

X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.1, random_state=SEED)

clf = CatBoostRegressor(random_state=314,
                            cat_features=cat_indices,
                            verbose=0,
                            loss_function = &quot;RMSE&quot;,
                            od_type = &quot;Iter&quot;,
                            od_wait = 100,iterations=3000,
                            use_best_model=True)

clf.fit(X, y, eval_set = (X_eval, y_eval), verbose=False, plot=True)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/redacao_catboost.png" style="width:95.0%" />
</center>
<p>Salvar previs√µes:</p>
<pre class="python"><code>sub.loc[:, &#39;NU_NOTA_REDACAO&#39;] = clf.predict(X_test)
# alunos que nao foram fazer a prova tiraram zero
sub.loc[test.TP_STATUS_REDACAO!=1, &#39;NU_NOTA_REDACAO&#39;] = 0</code></pre>
<p>Comparar distribui√ß√£o da target nos dados de treino com rela√ß√£o √†s previs√µes do modelo:</p>
<pre class="python"><code>sns.kdeplot(train.loc[:, target], shade=True, color=&#39;r&#39;, clip=[0,1000])
sns.kdeplot(sub.loc[:, target], shade=True, color=&#39;b&#39;, clip=[0,1000])
plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
plt.title(target)</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/redacao_pred.png" style="width:50.0%" />
</center>
</div>
</div>
</div>
<div id="submiss√£o" class="section level1">
<h1>Submiss√£o</h1>
<p>Veja a seguir como ficou a distribui√ß√£o das previs√µes comparada √† distribui√ß√£o da target nos dados de treino:</p>
<pre class="python"><code>plt.figure(figsize=(16, 5))

notas = [&#39;NU_NOTA_CH&#39;, &#39;NU_NOTA_CN&#39;, &#39;NU_NOTA_MT&#39;, &#39;NU_NOTA_LC&#39;, &#39;NU_NOTA_REDACAO&#39;]

for i in range(len(notas)):

    plt.subplot(1, 5, i+1)
    sns.kdeplot(train.loc[:, notas[i]], shade=True, color=&#39;r&#39;, clip=[0,1000])
    sns.kdeplot(sub.loc[:, notas[i]], shade=True, color=&#39;b&#39;, clip=[0,1000])
    plt.legend(labels=[&#39;train&#39;, &#39;predict&#39;])
    plt.title(notas[i])
plt.tight_layout()
plt.show()</code></pre>
<center>
<img src="/post/2022-04-20-solucao-final-education-quality-kaggle-competition/all_pred.png" style="width:95.0%" />
</center>
<p>Acredito que talvez um tuning do modelo poderia trazer mais qualidade √†s previs√µes mas com o tempo limitado n√£o pude investir muito nesta etapa.</p>
</div>
<div id="considera√ß√µes-finais" class="section level1">
<h1>Considera√ß√µes Finais</h1>
<p>Em resumo, essas foram as principais id√©ias para a solu√ß√£o da competi√ß√£o e acredito que um dos segredos era focar em feature engineering por 2 motivos:</p>
<ul>
<li>A base era muito grande e o processo de tuning seria muito custoso (a n√£o ser que tenha um √≥timo computador a disposi√ß√£o);</li>
<li>Os atributos n√£o eram an√¥nimos, o que d√° muita informa√ß√£o de contexto.</li>
</ul>
<p>Agrade√ßo aos organizadores e √† todos os participantes que tornaram esta competi√ß√£o t√£o divertida! Por mais competi√ß√µes como esta, que valorizam a comunidade brasileira de Data Science!</p>
<p>Espero que tenham gostado e at√© logo!</p>
<p>Abra√ßos!</p>
<p>Fellipe Gomes</p>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2022-04-20-solucao-final-education-quality-kaggle-competition/">Solu√ß√£o Final - ML Olympiad [2¬∫ lugar]</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">catboost</category>
      <category domain="tag">data-mining</category>
      <category domain="tag">data-science</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">machine-learning</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">pratica</category>
      <category domain="tag">r</category>
      <category domain="tag">regressao</category>
    </item>
    <item>
      <title>Vou te provar que da para fazer Grafos bonitos em R!</title>
      <link>https://gomesfellipe.github.io/post/2021-12-03-grafos-em-r/</link>
      <pubDate>Fri, 03 Dec 2021 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2021-12-03-grafos-em-r/</guid>
      <description>Neste post vamos coletar not√≠cias via web scrapping, detectar entidades dos textos e criar um grafo utilizando ggplot2</description>
      <content:encoded>&lt;![CDATA[
        

<div id="TOC">
<ul>
<li><a href="#introdu%C3%A7%C3%A3o-e-contexto" id="toc-introdu√ß√£o-e-contexto">Introdu√ß√£o e contexto</a>
<ul>
<li><a href="#o-que-s%C3%A3o-grafos" id="toc-o-que-s√£o-grafos">O que s√£o Grafos?</a></li>
<li><a href="#como-contruir-um" id="toc-como-contruir-um">Como contruir um?</a></li>
</ul></li>
<li><a href="#carregar-depend%C3%AAncias" id="toc-carregar-depend√™ncias">Carregar depend√™ncias</a></li>
<li><a href="#fonte-dos-dados" id="toc-fonte-dos-dados">Fonte dos dados</a></li>
<li><a href="#ner---named-entity-recognition" id="toc-ner---named-entity-recognition">NER - Named Entity Recognition</a></li>
<li><a href="#preparar-dados" id="toc-preparar-dados">Preparar dados</a></li>
<li><a href="#b%C3%B4nus" id="toc-b√¥nus">B√¥nus</a></li>
<li><a href="#conclus%C3%A3o" id="toc-conclus√£o">Conclus√£o</a></li>
<li><a href="#outras-bibliotecas-para-constru%C3%A7%C3%A3o-de-grafos" id="toc-outras-bibliotecas-para-constru√ß√£o-de-grafos">Outras bibliotecas para constru√ß√£o de grafos</a></li>
</ul>
</div>

<div id="introdu√ß√£o-e-contexto" class="section level1">
<h1>Introdu√ß√£o e contexto</h1>
<p>Durante os anos de 2020 e 2021 fiz um <a href="https://educacao-executiva.fgv.br/df/brasilia/cursos/mba-pos-graduacao/mba-presencial/mba-executivo-em-business-analytics-e-big-data">MBA Executivo em Business Analytics e Big Data</a> na FGV e uma das disciplinas que gostei bastante abordou a an√°lise de m√≠dias sociais com t√©cnicas de minera√ß√£o de texto e processamento de linguagem natural.</p>
<p>No trabalho final fomos desafiados a extrair dados da internet via api ou scraping, aplicar a metodologia apropriada para extrair informa√ß√µes de interesse e contruir um Grafo.</p>
<p>Como esse gr√°fico deu mais de trabalho do que eu esperava e fiquei bem satisfeito com o resultado final, resolvi fazer uma nova an√°lise para praticar e publicar aqui no blog, espero que gostem!</p>
<div id="o-que-s√£o-grafos" class="section level2">
<h2>O que s√£o Grafos?</h2>
<p>üìé Segundo o Wikipedia:</p>
<blockquote>
<p>‚ÄúA teoria dos grafos √© um ramo da matem√°tica que estuda as rela√ß√µes entre os objetos de um determinado conjunto‚Äù</p>
</blockquote>
<p>S√£o muito √∫teis para an√°lises de redes sociais, redes de amizades ou qualquer rede com rela√ß√µes de depend√™ncias. Existem muitos tipos de grafos como conectados, desconectados, esparsos, densos, direcionados, n√£o direcionados e por ai vai‚Ä¶</p>
<p>Al√©m disso existe toda uma nomenclatura espec√≠fica, mas n√£o entrarei em detalhes te√≥ricos neste post pois tamb√©m estou estudado sobre o tema! Caso queira aprofundar na teoria por tr√°s recomendo <a href="http://faculty.ucr.edu/~hanneman/nettext/index.html">este material</a> gratuito muito bom!</p>
</div>
<div id="como-contruir-um" class="section level2">
<h2>Como contruir um?</h2>
<p>No curso que fiz aprendemos a mexer no <a href="https://gephi.org/">Gephi</a> para a contru√ß√£o desses Grafos (ferramenta incr√≠vel, diga-se de passagem) por√©m ouvi dizer diversas vezes, tanto dentro quanto fora da FGV, que R e Python eram muito limitados para constru√ß√£o de Grafos bonitos e que esse software sempre a melhor op√ß√£o.</p>
<p>Apesar do enorme potencial do Gephi, fiquei um pouco entediado estudando-o pois n√£o sou grande f√£ de ferramentas <em>point-and-click</em> e quando o professor falou que a escolha da ferramenta para a constru√ß√£o do Grafo era livre, resolvi tentar faz√™-lo em R!</p>
</div>
</div>
<div id="carregar-depend√™ncias" class="section level1">
<h1>Carregar depend√™ncias</h1>
<p>Pacotes utilizados neste post:</p>
<pre class="r"><code>library(rvest)     # web scrapping
library(dplyr)     # manipulate data
library(purrr)     # functional prog
library(stringr)   # str toolkit
library(spacyr)    # ner
library(igraph)    # base graph
library(tidygraph) # tidy graph
library(ggraph)    # plot graph</code></pre>
</div>
<div id="fonte-dos-dados" class="section level1">
<h1>Fonte dos dados</h1>
<p>Os dados utilizados neste post foram coletados via web scrapping do site do <a href="https://g1.globo.com/">G1 - Globo</a>. Optei por trabalhar com textos jornal√≠sticos neste post pois apresentam a vantagem de serem bem escritos, o que facilita na tarefa de minera√ß√£o de texto.</p>
<p>Tamb√©m fiz um grafo analisando tweets sobre a CPI da pandemia <a href=".#b%C3%B4nus">que ser√° apresentado como b√¥nus no final deste post</a> e para quem tiver curiosidade de conferir <a href="https://github.com/gomesfellipe/cpi_da_pandemia">os c√≥digos</a> vai notar que foi necess√°rio um tratamento muito mais extensivo para corrigir os nomes de cada um dos senadores, deputados e personagens pol√≠ticos detectados.</p>
<p>Confira abaixo todos os c√≥digos necess√°rios para realizar tal extra√ß√£o:</p>
<details>
<summary>
(<em>Clique aqui para exibir as fun√ß√µes <code>scrape_post_links</code> e <code>scrape_post_body</code> </em>)
</summary>
<pre class="r"><code># Funcao para coletar os links de cada noticia
scrape_post_links &lt;- function(site) {
  cat(paste0(site, &quot;\n&quot;))
  
  source_html &lt;- read_html(site)
  
  links &lt;- source_html %&gt;%
    html_nodes(&quot;div.widget--info__text-container&quot;) %&gt;%
    html_nodes(&quot;a&quot;) %&gt;%
    html_attr(&quot;href&quot;)
  
  links &lt;- links[!is.na(links)]
  
  return(links)
}

# Funcao para coletar o texto da materia em cada link
scrape_post_body &lt;- function(site) { 
  
  text &lt;- tryCatch({
    cat(paste0(site, &quot;\n&quot;))
    body &lt;- site %&gt;%
      read_html %&gt;%
      html_nodes(&quot;article&quot;) %&gt;%
      html_nodes(&quot;p.content-text__container&quot;)  %&gt;%
      html_text %&gt;% 
      paste(collapse = &#39;&#39;)
    
  }, error = function(e){
    cat(paste(&quot;ERRO 404&quot;, &quot;\n&quot;))
    body &lt;- NA
  })
  
  return(body)
}

# criar matriz de adjacencias
get_adjacent_list &lt;- function(edge_list) {
  gtools::combinations(length(edge_list), 2, edge_list)  
}</code></pre>
</details>
<p>¬†</p>
<pre class="r"><code># raiz
root &lt;- &quot;https://g1.globo.com/busca/?q=economia+brasil&quot;

# gerar links das proximas 100 paginas
all_pages &lt;- c(root, paste0(root, &quot;&amp;page=&quot;, 1:50))

# coletar os links dos posts de cada pagina
all_links &lt;- map(all_pages, scrape_post_links) %&gt;% unlist()

# extrair urls
cleaned_links &lt;- map_chr(all_links, ~{
  .x %&gt;% 
    urltools::param_get() %&gt;% 
    pull(u) %&gt;% 
    urltools::url_decode()
})

# reter apenas links que falam de economia
cleaned_links &lt;- cleaned_links %&gt;% .[str_detect(.,  &quot;g1.globo.com/economia&quot;)]

# nao reter links do globoplay
cleaned_links &lt;- cleaned_links %&gt;% .[!str_detect(.,  &quot;globoplay&quot;)]

# coletar conteudo de cada link
data &lt;- map_chr(cleaned_links, scrape_post_body) %&gt;% unique()</code></pre>
</div>
<div id="ner---named-entity-recognition" class="section level1">
<h1>NER - Named Entity Recognition</h1>
<p>Utilizaremos um modelo de reconhecimento de entidades pr√©-treinado fornecido pela <a href="https://spacy.io/">Spacy</a> (que fornece essa e muitas outras solu√ß√µes interessantes quando se trata de processamento de linguagem natural).</p>
<p>Primeiramente vamos configurar o <code>spacyr</code> na m√°quina para utilizar o modelo pr√© treinado para reconhecimento de entidades em portugu√™s:</p>
<pre class="r"><code># Executar apenas 1 vez
spacyr::spacy_install()
spacy_download_langmodel(&quot;pt_core_news_sm&quot;)</code></pre>
<p>Inicializar modelo pr√©-treinado em portugu√™s:</p>
<pre class="r"><code>spacy_initialize(model=&quot;pt_core_news_sm&quot;)</code></pre>
<p>Aplicar modelo carregado para o reconhecimento de entidades:</p>
<pre class="r"><code>entities &lt;- spacy_extract_entity(data)</code></pre>
<p>Filtrar apenas entidades cujo tipo s√£o <strong>pessoas</strong> ou <strong>organiza√ß√µes</strong>:</p>
<pre class="r"><code>filtered_entities &lt;- 
  entities %&gt;% 
  filter(ent_type==&#39;ORG&#39;| ent_type==&#39;PER&#39;)</code></pre>
</div>
<div id="preparar-dados" class="section level1">
<h1>Preparar dados</h1>
<p>Precisamos criar uma lista de arestas:</p>
<pre class="r"><code>edges &lt;- 
  filtered_entities %&gt;%
  group_by(doc_id) %&gt;%
  summarise(entities = paste(text, collapse = &quot;,&quot;)) %&gt;% 
  pull(entities) %&gt;% 
  str_split(&quot;,&quot;) %&gt;% 
  map(~unique(unlist(.x))) %&gt;% 
  .[map_dbl(., length) != 1]</code></pre>
<p>Agora criaremos a matriz de adjac√™ncias, que envolvem todas as combina√ß√µes 2 a 2 das entidades detectadas em cada not√≠cia:</p>
<pre class="r"><code>adjacent_matrix &lt;-
  map_dfr(edges, ~ as.data.frame(get_adjacent_list(.x))) %&gt;% 
  as_tibble() %&gt;% 
  set_names(c(&#39;item1&#39;, &#39;item2&#39;))</code></pre>
<p>Aplicaremos algum tratamento para padronizar as entidades, reter apenas combina√ß√µes que aconteceram pelo menos 3 vezes e remover algum res√≠duo que veio no processo de NER:</p>
<pre class="r"><code># Padronizar entidades
adjacent_matrix &lt;- adjacent_matrix %&gt;% 
  mutate_all(~.x %&gt;% 
               str_replace_all(&quot;Funda√ß√£o Getulio Vargas&quot;, &quot;FGV&quot;) %&gt;% 
               str_replace_all(&quot;FMI&quot;, &quot;Fundo Monet√°rio Internacional&quot;) %&gt;% 
               str_replace_all(&quot;Paulo Guedes&quot;, &quot;Guedes&quot;) %&gt;% 
               str_replace_all(&quot;Estados Unidos( da Am[√©e]rica)?&quot;, &quot;EUA&quot;) %&gt;% 
               str_replace_all(&quot;Donald Trump&quot;, &quot;Trump&quot;) %&gt;% 
               str_replace_all(&quot;CEF&quot;, &quot;Caixa Econ√¥mica Federal&quot;) %&gt;% 
               str_replace_all(&quot;CMN&quot;, &quot;Conselho Monet√°rio Nacional&quot;) %&gt;% 
               str_replace_all(&quot;Cl[√°a]udio Considera&quot;, &quot;Cl√°udio&quot;) %&gt;% 
               str_replace_all(&quot;OCDE&quot;, &quot;Organiza√ß√£o para a Coopera√ß√£o e
                               Desenvolvimento Econ√¥mico&quot;) %&gt;% 
               str_replace_all(&quot;(Andr√© )?Brand√£o&quot;, &quot;Andr√© Brand√£o&quot;) %&gt;% 
               str_replace_all(&quot;(Maur[i√≠]cio )?Macri&quot;, &quot;Mauricio Macri&quot;) %&gt;% 
               str_remove_all(&quot;^(?i)(no|de)\\s&quot;)
             
             )

# remover residuos
{
  entities_to_drop &lt;- c(&quot;Assine&quot;, &quot;Google Podcasts&quot;, &quot;Spotify&quot;, &quot;Focus do&quot;,
                        &quot;Focus&quot;, &quot;Segundo&quot;, &quot;Ningu√©m&quot;, &quot;Haver√°&quot;, &quot;G1&quot;,
                        &quot;Come√ßa&quot;, &quot;LEIA&quot;, &quot;R$&quot;, &quot;Considera&quot;, &quot;Caixa Aqui&quot;)
  
  weighted_edgelist &lt;- adjacent_matrix %&gt;%
    filter_at(1:2, ~ !.x %in% entities_to_drop) %&gt;% 
    group_by(item1, item2) %&gt;%
    summarise(n=n()) %&gt;% 
    ungroup() %&gt;% 
    filter(n&gt;3) 
}</code></pre>
<p>Definir alguns objetos para o grafo:</p>
<pre class="r"><code># Instanciar objeto das setas
a &lt;- grid::arrow(type = &quot;closed&quot;, length = unit(.15, &quot;inches&quot;))

# Definir pesos conforme numero de ocorrencias
subt &lt;- weighted_edgelist

# Instanciar objeto dos vertices
vert &lt;- subt %&gt;% 
  tidyr::gather(item, word, item1, item2) %&gt;%
  group_by(word) %&gt;% 
  summarise(n = sum(n))

# Obter componentes para colorir os clusters do grafo
tidy_graph_components &lt;- 
  subt  %&gt;%
  select(item1, item2) %&gt;% 
  as.matrix() %&gt;%
  graph.edgelist(directed = FALSE)  %&gt;%
  as_tbl_graph() %&gt;% 
  activate(&quot;edges&quot;) %&gt;% 
  # definir pesos como numero de ocorrencias
  mutate(weight = subt$n) %&gt;% 
  activate(&quot;nodes&quot;) %&gt;% 
  # obter clusters:
  mutate(component = as.factor(tidygraph::group_edge_betweenness()))
  # outros tipos de agrupamentos:
  # tidygraph.data-imaginist.com/reference/group_graph.html 
  
# Atualizar vertice para incluir grupos
vert &lt;- vert %&gt;% 
  left_join( as.data.frame(activate(tidy_graph_components, &quot;nodes&quot;)) %&gt;% 
               rename(word = name))</code></pre>
<p>Finalmente, vamos criar o grafo utilizando <code>ggplot2</code>:</p>
<pre class="r"><code>set.seed(1)
subt %&gt;%
  graph_from_data_frame(vertices = vert) %&gt;%
  # https://www.data-imaginist.com/2017/ggraph-introduction-layouts/ # layouts
  ggraph(layout = &quot;fr&quot;) +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, &#39;inches&#39;), color = &quot;#D9D9D9A0&quot;) +
  geom_node_point() + 
  geom_node_text(aes(label = name, size = n, alpha = n, color = component),# color = &quot;#EAFF00&quot;,
                 repel = TRUE, point.padding = unit(0.2, &quot;lines&quot;),
                 show.legend = F) +
  scale_size(range = c(2,10)) +
  scale_alpha(range = c(0.5,1))+ 
  theme_dark() + 
  theme(
    panel.background = element_rect(fill = &quot;#2D2D2D&quot;),
    legend.key = element_rect(fill = &quot;#2D2D2D&quot;)
  ) +
  theme_graph(background = &quot;black&quot;)</code></pre>
<center>
<img src="/post/2021-12-03-grafos-em-r/grafo.png" style="width:95.0%" />
</center>
<p>üìå Interpreta√ß√£o</p>
<p>Este grafo resume algumas informa√ß√µes interessantes sobre como o cen√°rio da economia no brasil estava no dia 30 de novembro de 2021. Vejamos alguns pontos relevantes que podem ser envontrados no cen√°rio atual:</p>
<div class="w3-panel w3-pale-green w3-border">
<p>¬† ‚òû Bolsa familia</p>
<p>O Aux√≠lio Brasil √© referido como o ‚ÄúNovo Bolsa Fam√≠lia‚Äù pelos jornais e por isso deve ter sido criada tal rela√ß√£o no Grafo. J√° a Caixa Econ√¥mica Federal √© o agente que executa os pagamentos.</p>
</div>
<div class="w3-panel w3-pale-red w3-border">
<p>¬† ‚òû Guedes</p>
<p>Paulo Guedes √© nosso atual ministro da economia e envolta de seu nome aparecem diversos assuntos que est√£o em pauta atualmente como a PEC dos precat√≥rios, (a privatiza√ß√£o da) Petrobr√°s, Copom, IPCA, Aux√≠lio Brasil dentre outros.</p>
</div>
<div class="w3-panel w3-pale-yellow w3-border">
<p>¬† ‚òû Fundo Monet√°rio Internacional</p>
<p>O FMI <a href="https://pt.wikipedia.org/wiki/Fundo_Monet%C3%A1rio_Internacional">trabalha para melhorar as economias dos pa√≠ses</a> e al√©m da Argentina estar endividada e em acordo com o FMI, √© √©poca de elei√ß√£o, o que explica haver alguns personagens de sua pol√≠tica relacionados.</p>
</div>
<p>Salvar localmente em alta resolu√ß√£o:</p>
<pre class="r"><code>ggsave(filename = &#39;grafo.png&#39;, width = 8, height = 6, device=&#39;png&#39;, dpi=700)</code></pre>
<p>O legal de salvar em alta resolu√ß√£o √© poder dar zoom e navegar pelo grafo!</p>
</div>
<div id="b√¥nus" class="section level1">
<h1>B√¥nus</h1>
<p>Antes de criar este post trabalhei em um <a href="https://github.com/gomesfellipe/cpi_da_pandemia">outro grafo</a> com banco de dados de aproximadamente 27GB de tweets coletados e fornecidos gentilmente pelo <a href="https://twitter.com/trifenol">Janderson Toth</a> (Para quem n√£o o conhe√ße, recomendo fortemente <a href="https://br.linkedin.com/in/trifenol">segui-lo no linkedin</a> pois ele tem compartilhado uma s√©rie de posts com insights obtidos destes dados!)</p>
<center>
<img src="/post/2021-12-03-grafos-em-r/grafo2.png" style="width:95.0%" />
</center>
<p>Para quem tiver interesse, o c√≥digo est√° <a href="https://github.com/gomesfellipe/cpi_da_pandemia">dispon√≠vel no github</a>!</p>
</div>
<div id="conclus√£o" class="section level1">
<h1>Conclus√£o</h1>
<p>Convenhamos que, de fato, criar um grafo no R n√£o √© uma tarefa super simples. No Gelphi √© poss√≠vel criar grafos at√© mais bonitos que este, por√©m, no longo prazo, ganhamos em produtividade e em escalabilidade pois poder√≠amos reaproveitar muito c√≥digo e tranquilamente desenvolver uma rotina para criar novos grafos a partir de dados streaming, por exemplo, automatizando todo o processo!</p>
</div>
<div id="outras-bibliotecas-para-constru√ß√£o-de-grafos" class="section level1">
<h1>Outras bibliotecas para constru√ß√£o de grafos</h1>
<p>Depois de conversar com algumas pessoas que leram o post, achei que merecia um update com mais id√©ias de mais bibliotecas que poderiam ter sido utilizadas:</p>
<ul>
<li><a href="https://cran.r-project.org/web/packages/cheddar/vignettes/PlotsAndStats.pdf">cheddar</a></li>
<li><a href="https://cran.r-project.org/web/packages/bipartite/bipartite.pdf">bipartite</a></li>
<li><a href="https://pedroj.github.io/bipartite_plots/">ggbipart</a></li>
<li><a href="https://rich-iannone.github.io/DiagrammeR/">diagrameR</a></li>
<li><a href="https://cran.r-project.org/web/packages/visNetwork/vignettes/Introduction-to-visNetwork.html">visNetwork</a></li>
</ul>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2021-12-03-grafos-em-r/">Vou te provar que da para fazer Grafos bonitos em R!</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category>Texto e NLP</category>
      <category domain="tag">data-mining</category>
      <category domain="tag">estatistica</category>
      <category domain="tag">ggplot2</category>
      <category domain="tag">grafo</category>
      <category domain="tag">r</category>
      <category domain="tag">rstudio</category>
      <category domain="tag">strings</category>
      <category domain="tag">text-mining</category>
      <category domain="tag">tidyverse</category>
      <category domain="tag">web-scrappnig</category>
    </item>
    <item>
      <title>Como automatizar relat√≥rios longos e repetitivos com RMarkdown</title>
      <link>https://gomesfellipe.github.io/post/2019-09-13-relatorios-automaticos-com-rmarkdown/relatorios-automaticos-com-rmarkdown/</link>
      <pubDate>Fri, 13 Sep 2019 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2019-09-13-relatorios-automaticos-com-rmarkdown/relatorios-automaticos-com-rmarkdown/</guid>
      <description>Veja como fazer um relat√≥rio estat√≠stico &#34;extenso e repetitivo&#34; sem utilizar copiar e colar nenhuma vez</description>
      <content:encoded>&lt;![CDATA[
        


<div id="problema-de-neg√≥cio" class="section level1">
<h1>Problema de neg√≥cio</h1>
<p>Uma tarefa comum no dia a dia de um estat√≠stico (ou cientista de dados) √© a elabora√ß√£o de relat√≥rios para passsar ao restante da equipe e/ou tomadores de decis√£o os resultados encontrados e muitas vezes essa tarefa pode parecer desgastante quando os relat√≥rios s√£o muitos extensos e repetitivos.</p>
<p>Com a linguagem R, escrever relat√≥rios estat√≠sticos utilizando <a href="https://rmarkdown.rstudio.com/">RMarkdown</a> acaba sendo a escolha padr√£o por ser t√£o simples transformar as an√°lises em documentos, apresenta√ß√µes e dashboards de alta qualidade com poucas linhas de c√≥digo.</p>
<p>Assim, combinando conceitos de programa√ß√£o, como o <a href="https://pt.wikipedia.org/wiki/Loop_(programa%C3%A7%C3%A3o)">Loop</a> no R e a linguagem <a href="https://pt.wikipedia.org/wiki/Markdown">Markdown</a> para produ√ß√£o de relat√≥rios, temos uma poderosa ferramenta para <a href="https://pt.wikipedia.org/wiki/Automa%C3%A7%C3%A3o">Automa√ß√£o</a> de relat√≥rios.</p>
<div id="entendendo-o-problema" class="section level2">
<h2>Entendendo o problema</h2>
<p>Suponha que o seguinte gr√°fico seja apresentado √† voc√™:</p>
<center>
<img src="/post/2019-09-13-relatorios-automaticos-com-rmarkdown/figure-html/unnamed-chunk-1-1.gif" style="width:80.0%" />
</center>
<p>Este gr√°fico animado apresenta a evolu√ß√£o da esperan√ßa de vida ao nascer (em anos) em rela√ß√£o ao PIB per capita (em US$, ajustado pela infla√ß√£o) de 141 pa√≠ses dos 5 continentes durante o per√≠odo de 1952 at√© 2007, a cada 5 anos.</p>
<p>Entraremos em mais detalhes sobre as informa√ß√µes dete gr√°fico a seguir.</p>
</div>
</div>
<div id="fonte-dos-dados" class="section level1">
<h1>Fonte dos dados</h1>
<p>Os dados utilizados neste problema foram importados atrav√©s do pacote <a href="https://cran.r-project.org/web/packages/gapminder/index.html">gapminder</a> que √© um projeto que utiliza dados do site <a href="https://www.gapminder.org/">Gapminder.org</a>.</p>
<p>Segundo sua <a href="https://www.gapminder.org/about-gapminder/">descri√ß√£o no site</a>:</p>
<blockquote>
<p>‚ÄúGapminder √© uma funda√ß√£o independente sueca sem afilia√ß√µes pol√≠ticas, religiosas ou econ√¥micas. (‚Ä¶)‚Äù</p>
</blockquote>
<p>No site √© poss√≠vel obter dados gratuitos para se obter estat√≠sticas confi√°veis e al√©m dos disso a Funda√ß√£o Gapminder apresenta alguns outros projetos como o <a href="https://www.gapminder.org/dollar-street/matrix">Dollar Street</a> que apresenta 30.000 fotos de 264 fam√≠lias em 50 pa√≠ses classificados por renda.</p>
<p>Na p√°gina do projeto √© poss√≠vel ver e comparar os mais variados aspectos da popula√ß√£o ao redor do mundo que v√£o desde casas, itens mais amados, carros at√© banheiros, comida de pets e bebidas alco√≥licas.</p>
<p>O pacote fornece dados da Funda√ß√£o Gapminder como: valores de expectativa de vida, PIB per capta e popula√ß√£o, a cada cinco anos, de 1952 a 2007 (total de 12 anos). Veja as primeiras 5 linhas da base de dados contidos no pacote:</p>
<pre class="r"><code># Base de dados utilizada
head(gapminder)</code></pre>
<pre><code>## # A tibble: 6 x 6
##   country     continent  year lifeExp      pop gdpPercap
##   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;
## 1 Afghanistan Asia       1952    28.8  8425333      779.
## 2 Afghanistan Asia       1957    30.3  9240934      821.
## 3 Afghanistan Asia       1962    32.0 10267083      853.
## 4 Afghanistan Asia       1967    34.0 11537966      836.
## 5 Afghanistan Asia       1972    36.1 13079460      740.
## 6 Afghanistan Asia       1977    38.4 14880372      786.</code></pre>
<p>Essa base de dados possui 1705 linhas de 6 vari√°veis, onde:</p>
<ul>
<li><code>country</code>: factor com 142 levels</li>
<li><code>continent</code>: factor com 5 levels</li>
<li><code>year</code>: sequencia de 1952 at√© 2007 a cada 5 anos</li>
<li><code>lifeExp</code>: esperan√ßa de vida ao nascer, em anos</li>
<li><code>pop</code>: popula√ß√£o</li>
<li><code>gdpPercap</code>: PIB per capita (em US$, ajustado pela infla√ß√£o)</li>
</ul>
</div>
<div id="comportamento-geral-dos-dados" class="section level1">
<h1>Comportamento geral dos dados</h1>
<p>Antes de come√ßar a fazer os relat√≥rios para cada ano, vamos reproduzir a anima√ß√£o apresentada para n√≥s com o comportamento temporal utilizando o pacote <a href="https://github.com/thomasp85/gganimate">gganimate</a>:</p>
<pre class="r"><code># Carregar pacotes
library(ggplot2)
library(dplyr)
library(gapminder)
library(scales)
library(gganimate)

# Definir tema:
theme_set(theme_bw())

# Funcao para customizar legendas:
custom_legend &lt;- function(x){comma(x, big.mark = &quot;.&quot;,decimal.mark = &quot;,&quot;)}

# Comportamento geral:
gapminder %&gt;% 
  filter(country!=&quot;Kuwait&quot;) %&gt;% # remover 1 pais outlier
  ggplot(aes(x = gdpPercap, y = lifeExp, size = pop, 
             label = country, color = continent, alpha= log(gdpPercap))) %+%
  geom_point(show.legend = F) %+%
  geom_text(show.legend = F, size = 3, nudge_y = -0.7) %+%
  scale_size_continuous(labels = custom_legend) %+%
  scale_x_continuous(labels = custom_legend) %+%
  geom_smooth(se=F, color = &quot;black&quot;, show.legend = F, method = &quot;lm&quot;) %+% 
  transition_time(year) %+%
  scale_color_brewer(palette = &quot;Dark2&quot;) %+%
  labs(title = &quot;Year: {frame_time}&quot;)</code></pre>
<center>
<img src="/post/2019-09-13-relatorios-automaticos-com-rmarkdown/unnamed-chunk-1-1.gif" style="width:80.0%" />
</center>
<p>Analisando esta anima√ß√£o √© poss√≠vel notar:</p>
<ul>
<li>Jap√£o √© o pa√≠s que possui a maior expectativa de vida ao longo de todos os anos;</li>
<li>Os pa√≠ses do cont√≠nente africano s√£o os que apresentam expectativa de vida mais baixa e pior <code>gdpPercap</code>.</li>
<li>A Ar√°bia Saudita teve sua <code>gdpPercap</code> aumentada at√© 1978 por√©m a partir da√≠ diminiu bastante.</li>
<li>O pa√≠s com maior <code>gdpPercap</code> e expectativa de vida na Am√©rica √© o Estados Unidos;</li>
<li>A Noroega foi o pa√≠s que mais se descatou com os valores mais elevados e est√°veis ao longo destes 55 anos.</li>
</ul>
<p>Obs[1]: <a href="https://www.google.com/search?q=Kuwait&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=0ahUKEwj53ZXJ4bPjAhVAD7kGHQvwCWgQ_AUIESgC&amp;biw=1574&amp;bih=943">Kuwait</a> foi removida para este gr√°fico animado pois √© um pa√≠s outlier. Segundo o <a href="https://pt.wikipedia.org/wiki/Kuwait">Wikip√©dia</a>:</p>
<blockquote>
<p>‚ÄúO Kuwait tem um PIB (PPC) de US$ 167,9 bilh√µes[96] e uma renda per capita de US$ 81 800,[96] o que o torna o quinto pa√≠s mais rico do mundo.[52] O √≠ndice de desenvolvimento humano (IDH) do Kuwait √© de 0,816, um dos mais elevados do Oriente M√©dio e do mundo √°rabe. Com uma taxa de crescimento do PIB de 5,7%, o Kuwait tem uma das economias que mais crescem na regi√£o.[96]‚Äù</p>
</blockquote>
<p>Para quem tiver curiosidade, os dados de <code>Kuwait</code> podem ser obtidos da seguinte forma:</p>
<pre class="r"><code>gapminder %&gt;% filter(country == &quot;Kuwait&quot;)</code></pre>
<pre><code>## # A tibble: 12 x 6
##    country continent  year lifeExp     pop gdpPercap
##    &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;     &lt;dbl&gt;
##  1 Kuwait  Asia       1952    55.6  160000   108382.
##  2 Kuwait  Asia       1957    58.0  212846   113523.
##  3 Kuwait  Asia       1962    60.5  358266    95458.
##  4 Kuwait  Asia       1967    64.6  575003    80895.
##  5 Kuwait  Asia       1972    67.7  841934   109348.
##  6 Kuwait  Asia       1977    69.3 1140357    59265.
##  7 Kuwait  Asia       1982    71.3 1497494    31354.
##  8 Kuwait  Asia       1987    74.2 1891487    28118.
##  9 Kuwait  Asia       1992    75.2 1418095    34933.
## 10 Kuwait  Asia       1997    76.2 1765345    40301.
## 11 Kuwait  Asia       2002    76.9 2111561    35110.
## 12 Kuwait  Asia       2007    77.6 2505559    47307.</code></pre>
</div>
<div id="resolvendo-o-problema-de-neg√≥cio" class="section level1">
<h1>Resolvendo o problema de neg√≥cio</h1>
<p>Para resolver o problema de se fazer uma an√°lise sobre a expectativa de vida, PIB per capta e popula√ß√£o, para cada continente, para cada ano dispon√≠vel, (ou seja, analisar de 1952 a 2007 a cada cinco anos) faremos um total de 12 relat√≥rios.</p>
<p>Isso √© muito para se arriscar usar <code>ctrl+c</code> e <code>ctrl+v</code> 12 vezes e depois caso precise de alguma mudan√ßa, alterar o relat√≥rio 12 vezes.</p>
<p>Portanto utilizaremos uma estrat√©gia parecida com a que apresentei no √∫ltimo post sobre como <a href="https://gomesfellipe.github.io/post/2019-04-05-split-apply-combine/split-apply-combine/">Hackear o R com a estrat√©cia Split-Appy-Combine</a>.</p>
<p>Primeiramente vamos separar nosso dataset por ano utilizando a fun√ß√£o <code>tidyr::nest()</code>:</p>
<pre class="r"><code>library(tidyr) # funcao nest

# separar por ano:
nested_gapminder &lt;- gapminder %&gt;% nest(-year)</code></pre>
<p>Selecionei um dos anos como exemplo e utilizei os objetos <code>nested_gapminder$year[1]</code> e <code>nested_gapminder$data[[1]]</code> para desenvolver uma fun√ß√£o que realizasse todas as an√°lises que eu precisasse.</p>
<p>Essa fun√ß√£o foi salva em um script separado chamado <code>analise.R</code> e pode ser encontrada <a href="">neste link</a>. Para caregar a fun√ß√£o localmente basta utilizar a fun√ß√£o <code>source()</code>, veja;</p>
<pre class="r"><code>source(&quot;analise_gapminder.R&quot;)</code></pre>
<p>Veja nas se√ß√µes a seguir os outputs da fun√ß√£o antes de encapsul√°-la em um arquivo RMarkdown (.Rmd) para fazer o looping:</p>
<div id="resultados-para-o-ano-2007" class="section level2">
<h2>Resultados para o ano 2007</h2>
<p>A seguir vamos criar o objeto <code>x</code> que ser√° o data set referente ao ano <code>title</code>. Em seguida vamos aplicar a fun√ß√£o carregada anteriormente para obter os resultados das an√°lises e salvar no objeto <code>resutls</code></p>
<pre class="r"><code>library(magrittr) # pipe %$%

# Obter resultados
x       &lt;- nested_gapminder %&gt;% filter(year == 2007) %&gt;% unnest()
title   &lt;- nested_gapminder %&gt;% filter(year == 2007) %$% year
results &lt;- analise_gapminder(x, title)</code></pre>
<p>Vejamos como o Brasil esta em rela√ß√£o aos outros pa√≠ses com um gr√°fico que resume os resultados do modelo ajustado:</p>
<pre class="r"><code>results$grafico_geral_regressao</code></pre>
<center>
<img src="/post/2019-09-13-relatorios-automaticos-com-rmarkdown/unnamed-chunk-9-1.png" style="width:80.0%" />
</center>
<p>Comportamento dos dados por Continente</p>
<pre class="r"><code>results$grafico_por_continente</code></pre>
<center>
<img src="/post/2019-09-13-relatorios-automaticos-com-rmarkdown/unnamed-chunk-10-1.png" style="width:80.0%" />
</center>
<p>Ap√≥s ajustar o modelo de regress√£o, vamos obter algumas estat√≠sticas descritivas com mais gr√°ficos informativos!</p>
<p>O gr√°fico abaixo apresenta uma <a href="http://www.leg.ufpr.br/lib/exe/fetch.php/projetos:saudavel:loess.pdf">Regress√£o Local (LOESS)</a> com destaque nos pa√≠ses que tiveram <code>gdpPercap</code> e <code>lifeExp</code> acima da m√©dia</p>
<pre class="r"><code>results$grafico_zoom_acima_media</code></pre>
<center>
<img src="/post/2019-09-13-relatorios-automaticos-com-rmarkdown/unnamed-chunk-11-1.png" style="width:80.0%" />
</center>
<p>E agora podemos conferir um gr√°fico que apresenta uma <a href="http://www.leg.ufpr.br/lib/exe/fetch.php/projetos:saudavel:loess.pdf">Regress√£o Local (LOESS)</a> com destaque nos pa√≠ses que tiveram <code>gdpPercap</code> e <code>lifeExp</code> acima da m√©dia</p>
<pre class="r"><code>results$grafico_zoom_abaixo_media</code></pre>
<center>
<img src="/post/2019-09-13-relatorios-automaticos-com-rmarkdown/unnamed-chunk-12-1.png" style="width:80.0%" />
</center>
<p>Maravilha! Muitas informa√ß√µes interessantes mas n√£o resolvemos o problema por inteiro. Resta aplicar as mesmas an√°lises para os demais anos do nosso dataset.</p>
</div>
</div>
<div id="automatizar-as-analises-para-os-pr√≥ximos-anos" class="section level1">
<h1>Automatizar as analises para os pr√≥ximos anos</h1>
<p>A linha a seguir √© a que realiza toda a m√°gica!</p>
<p>A fun√ß√£o <code>knit_child()</code> compila o c√≥digo R e retorna uma sa√≠da pura (Latex, html ou word sem c√≥digo R), ent√£o se fizermos um looping da seguinte maneira teremos replicado nossas an√°lises para todos os demais anos:</p>
<pre><code>rmarkdown::render(&quot;gapminder_automatico_master.Rmd&quot;)</code></pre>
<p>Veja o conte√∫do do script <code>gapminder_automatico_master.Rmd</code>:</p>
<script src="https://gist.github.com/gomesfellipe/86af044b4e8a874756a2f4c379cfc01b.js"></script>
<p>Note que este script chama outro arquivo <code>.Rmd</code> chamado <code>gapminder_automatico_child.Rmd</code>, que tem o seguinte conte√∫do:</p>
<script src="https://gist.github.com/gomesfellipe/2a9d666e041907ca88dd2188cbc72924.js"></script>
<p>Veja os resultados do looping:</p>
<iframe src="/post/2019-09-13-relatorios-automaticos-com-rmarkdown/gapminder_automatico_master.pdf" width="600" height="827" style="border: none;">
</iframe>
</div>
<div id="conclus√£o" class="section level1">
<h1>Conclus√£o</h1>
<p>A Abordagem para criar chunks <em>filhos</em> de RMarkdown com a fun√ß√£o <code>knit_child()</code> abre muitas portas para an√°lises de dados! Neste post fizemos um exemplo simples de automa√ß√£o de relat√≥rios por√©m esses resultados podem ser cada vez mais customiz√°veis e utilizados em RPA - <a href="https://en.wikipedia.org/wiki/Robotic_process_automation">Robotic Process Automation</a> - de forma que seja poss√≠vel automatizar processos que antes s√≥ poderiam ser executados por humanos!</p>
</div>
<div id="referencias" class="section level1">
<h1>Referencias</h1>
<ul>
<li><a href="https://cran.r-project.org/web/packages/gganimate/vignettes/gganimate.html" class="uri">https://cran.r-project.org/web/packages/gganimate/vignettes/gganimate.html</a></li>
<li><a href="https://cran.r-project.org/web/packages/ggforce/vignettes/Visual_Guide.html" class="uri">https://cran.r-project.org/web/packages/ggforce/vignettes/Visual_Guide.html</a></li>
<li><a href="https://cran.r-project.org/web/packages/ggExtra/vignettes/ggExtra.html" class="uri">https://cran.r-project.org/web/packages/ggExtra/vignettes/ggExtra.html</a></li>
<li><a href="https://cran.r-project.org/web/packages/gapminder/gapminder.pdf" class="uri">https://cran.r-project.org/web/packages/gapminder/gapminder.pdf</a></li>
<li><a href="https://www.gapminder.org/data/" class="uri">https://www.gapminder.org/data/</a></li>
<li><a href="https://stackoverflow.com/questions/43873345/knit-child-in-a-loop-variable-as-title" class="uri">https://stackoverflow.com/questions/43873345/knit-child-in-a-loop-variable-as-title</a></li>
</ul>
</div>
<div id="apendice" class="section level1">
<h1>Apendice</h1>
<div id="fun√ß√£o-analise.r" class="section level2">
<h2>Fun√ß√£o <code>analise.R</code></h2>
<p>Veja o conte√∫do da fun√ß√£o <code>analise.R</code> preparada para esta analise:</p>
<pre class="r"><code># Funcao para analise por ano:
analise_gapminder &lt;- function(x, title){
  
  # Carregar dependencias:
  require(broom)
  require(ggforce)
  require(ggpmisc)
  require(ggExtra)
  
  # Funcao para customizar legendas:
  custom_legend &lt;- function(x){comma(x, big.mark = &quot;.&quot;,decimal.mark = &quot;,&quot;)}
  
  # Obter dados do Brasil:
  brazil &lt;- x %&gt;% filter(country == &quot;Brazil&quot;)
  
  # Resultados do ajuste de regressao ---------------------------------------
  mytable &lt;- 
    lm(lifeExp ~ gdpPercap, data = x) %&gt;% 
    tidy() %&gt;% 
    mutate_if(is.numeric, ~round(.x, 4)) %&gt;% 
    `colnames&lt;-`(c(&quot;Termo&quot;, &quot;Estimativa&quot;, &quot;Desv.Pad.&quot;, &quot;Estatistica&quot;, &quot;Valor p&quot;))
  
  # r2:
  r2 &lt;- round(summary(lm(lifeExp ~ gdpPercap, data = x))$r.squared,4)*100
  
  # residuos do modelo:
  res &lt;- lm(lifeExp ~ gdpPercap, data = x)$residuals
  
  # resutado para teste de kolmogorov-smirnov
  ks_test &lt;- ks.test(res, &quot;pnorm&quot;, mean(res), sd(res))$p.value %&gt;% round(5)
  
  # Grafico geral com regressao e boxplots ----------------------------------
  grafico_geral_regressao &lt;- 
    x %&gt;% 
    ggplot(aes(x = gdpPercap, y = lifeExp, size = pop, label = country, color = country)) %+%
    geom_point(show.legend = F) %+%
    geom_text(show.legend = F, size = 3, nudge_y = -0.5) %+%
    scale_size_continuous(labels = custom_legend) %+%
    scale_x_log10(labels = custom_legend) %+%
    scale_color_manual(values = country_colors) %+%
    geom_smooth(se=F, color = &quot;black&quot;, show.legend = F, method = &quot;lm&quot;) %+%
    annotate(&quot;segment&quot;, color=&quot;blue&quot;, arrow=arrow(length=unit(0.05,&quot;npc&quot;)),
             x=brazil$gdpPercap, xend=brazil$gdpPercap,
             y=brazil$lifeExp-6, yend=brazil$lifeExp-1) %+%
    annotate(&quot;text&quot;, color=&quot;blue&quot;, label = &quot;Brasil&quot;,
             x=brazil$gdpPercap, y=brazil$lifeExp-7) %+%
    labs(title = paste0(title, &quot;: lifeExp ~ gdpPercap&quot;),
         subtitle = &quot;Regress√£o linear e destaque no Brasil&quot;,
         caption = paste0(&quot;R¬≤ do modelo: &quot;, r2, &quot;\n&quot;,&quot;p valor para ks.test: &quot;, ks_test),
         x = &quot;gdpPercap (Transforma√ß√£o log10)&quot;) %+%
    annotate(geom = &quot;table&quot;, x = Inf, y = -Inf,
             label = list(mytable), 
             vjust = 0, hjust = 1) %&gt;%  
    ggMarginal(type = &quot;boxplot&quot;, fill=&quot;transparent&quot;,size = 10)
  
  # Comportamento separado por continente -----------------------------------
  grafico_por_continente &lt;- 
    x %&gt;% 
    filter(continent != &quot;Oceania&quot;) %&gt;% 
    ggplot(aes(x = gdpPercap, y = lifeExp, size = pop, label = country, color = country)) %+%
    geom_point(show.legend = F) %+%
    geom_text(show.legend = F, size = 3, nudge_y = -0.5) %+%
    facet_wrap(~continent, scales = &quot;free&quot;) %+%
    scale_x_continuous(labels = custom_legend) %+%
    scale_color_manual(values = country_colors) %+% 
    geom_smooth(method = &quot;lm&quot;, color = &quot;black&quot;, se=F, show.legend = F) %+%
    labs(title = paste0(title, &quot;: lifeExp ~ gdpPercap, por continente&quot;))
  
  # Acima da media ----------------------------------------------------------
  grafico_zoom_acima_media &lt;- 
    x %&gt;% 
    ggplot(aes(x = gdpPercap, y = lifeExp, size = pop, label = country, color = country)) %+%
    geom_point(show.legend = F) %+% 
    geom_text(show.legend = F, size = 3, nudge_y = -0.5) %+%
    scale_size_continuous(labels = custom_legend) %+%
    scale_x_continuous(labels = custom_legend) %+%
    scale_color_manual(values = country_colors) %+%
    facet_zoom(y = lifeExp   &gt; median(x$lifeExp),
               x = gdpPercap &gt; median(x$gdpPercap), split = T) %+%
    geom_smooth(se=F, color = &quot;red&quot;, show.legend = F, method = &quot;loess&quot;)  %+%
    labs(title = paste0(title, &quot;: lifeExp ~ gdpPercap com zoom nos pa√≠ses acima da mediana&quot;))
  
  # Abaixo da media ---------------------------------------------------------
  grafico_zoom_abaixo_media &lt;- 
    x %&gt;% 
    ggplot(aes(x = gdpPercap, y = lifeExp, size = pop, label = country, color = country)) %+%
    geom_point(show.legend = F) %+%
    geom_text(show.legend = F, size = 3, nudge_y = -0.5) %+%
    scale_size_continuous(labels = custom_legend) %+%
    scale_x_continuous(labels = custom_legend) %+%
    scale_color_manual(values = country_colors) %+%
    facet_zoom(y = lifeExp   &lt; median(x$lifeExp),
               x = gdpPercap &lt; median(x$gdpPercap), split = T) %+%
    geom_smooth(se=F, color = &quot;red&quot;, show.legend = F, method = &quot;loess&quot;)   %+%
    labs(title = paste0(title, &quot;: lifeExp ~ gdpPercap com zoom nos pa√≠ses abaixo da mediana&quot;))
  
  # Output ------------------------------------------------------------------
  list(
    brazil  = brazil,
    mytable = mytable,
    r2      = r2,
    grafico_geral_regressao   = grafico_geral_regressao,
    grafico_por_continente    = grafico_por_continente,
    grafico_zoom_acima_media  = grafico_zoom_acima_media,
    grafico_zoom_abaixo_media = grafico_zoom_abaixo_media,
    ks_test = ks_test
  )
}</code></pre>
</div>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2019-09-13-relatorios-automaticos-com-rmarkdown/relatorios-automaticos-com-rmarkdown/">Como automatizar relat√≥rios longos e repetitivos com RMarkdown</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">data-mining</category>
      <category domain="tag">estatistica</category>
      <category domain="tag">otimizacao</category>
      <category domain="tag">pratica</category>
      <category domain="tag">r</category>
      <category domain="tag">regressao</category>
      <category domain="tag">relatorios</category>
      <category domain="tag">reports</category>
      <category domain="tag">rmarkdown</category>
      <category domain="tag">rstudio</category>
    </item>
    <item>
      <title>Hackeando o R: estrat√©gia split-apply-combine</title>
      <link>https://gomesfellipe.github.io/post/2019-04-05-split-apply-combine/split-apply-combine/</link>
      <pubDate>Fri, 05 Apr 2019 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2019-04-05-split-apply-combine/split-apply-combine/</guid>
      <description>Veja como aplicar essa estrat√©gia de maneira eficiente utilizando os pacotes do tidyverse: dplyr&#43;tidyr&#43;purrr</description>
      <content:encoded>&lt;![CDATA[
        


<div id="o-m√©todo-split-apply-combine" class="section level1">
<h1>O m√©todo split-apply-combine</h1>
<p>Geralmente em uma an√°lise de dados precisamos compreender, al√©m do comportamento geral dos dados, o seu comportamento de acordo com alguns segmentos.</p>
<p>No famoso paper <a href="https://vita.had.co.nz/papers/plyr.pdf">The Split-Apply-Combine Strategy for Data Analysis</a>, <a href="http://hadley.nz/">Hadley Wickham</a> descreve a abordagem ‚Äúsplit-apply-combine‚Äù (dividir-aplicar-combinar) como uma das mais comuns em uma an√°lise de dados. Em R essa tarefa pode ser feita por diversos caminhos, veja alguns dos modos de se fazer utilizando fun√ß√µes base do R e abordagens mais antigas:</p>
<ul>
<li><code>split()</code> + <code>lapply()</code> + <code>do.call(rbind, ...)</code></li>
<li><code>ddply()</code> do pacote <code>plyr</code></li>
<li><code>group_by</code> + <code>do()</code></li>
<li><code>split()</code> + <code>map_dfr()</code></li>
</ul>
<p>Todos esses exemplos atendem √† maioria dos casos que deseja-se utilizar a abordagem ‚Äúsplit-apply-combine‚Äù, por√©m, veja por exemplo este <a href="https://community.rstudio.com/t/should-i-move-away-from-do-and-rowwise/2857">t√≥pico na community.rstudio.com</a> criado no final de 2017 em que ocorre um comunicado que a fun√ß√£o <code>do()</code> ser√° descontinuada</p>
<p>Ou ainda, confira quando foi o √∫ltimo lan√ßamento de atualiza√ß√£o do pacote <a href="https://cran.r-project.org/web/packages/plyr/index.html"><code>plyr</code> no CRAN</a> (foi em junho de 2016).</p>
<p>Com a proposta de mais efici√™ncia e legibilidade do c√≥digo, atualmente existem maneiras mais sofisticadas e modernas de se realizar esta tarefa com pacotes que foram atualizados j√° este ano de 2019. Veja nas se√ß√µes a seguir o aumento de produtividade que √© poss√≠vel se obter combinando os pacotes <code>dplyr</code>, <code>tidyr</code> e <code>purrr</code> da cole√ß√£o de pacotes do <a href="https://www.tidyverse.org/"><code>tidyverse</code></a>.</p>
<div id="usando-s√≥-o-dplyr" class="section level2">
<h2>Usando s√≥ o dplyr</h2>
<p>Usamos ‚Äúsplit-apply-combine‚Äù implicitamente o tempo todo quando utilizamos as fun√ß√µes <code>groupy_by()</code> + <code>summarise()</code> do pacote <a href="https://dplyr.tidyverse.org/"><code>dplyr</code></a></p>
<p>Poder√≠amos facilmente reproduzir o exemplo da imagem do post com os seguintes comandos:</p>
<pre class="r"><code>library(dplyr)
data &lt;- tibble(x = c(&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;C&quot;, &quot;C&quot;),
               y = c(0,1,2,3,4,5)) 

data %&gt;%                       # input data
  group_by(x) %&gt;%              # split
  summarise(data = mean(y))    # apply/combine</code></pre>
<pre><code>## # A tibble: 3 x 2
##   x      data
##   &lt;chr&gt; &lt;dbl&gt;
## 1 A       0.5
## 2 B       2.5
## 3 C       4.5</code></pre>
<p>Essa sequ√™ncia de c√≥digos aplica a abordagem implicitamente, agrupando os dados de acordo com a vari√°vel selecionada e em seguida aplicando a opera√ß√£o e combinando os resultados em uma matriz resumida</p>
</div>
<div id="usando-dplyr-tidyr-purrr" class="section level2">
<h2>Usando dplyr + tidyr + purrr</h2>
<p>Poder√≠amos ter realizado a mesma opera√ß√£o de forma expl√≠cita com o aux√≠lio das fun√ß√µes <code>nest()</code>, <code>map()</code>, <code>mutate()</code> e <code>unnest()</code> dos pacotes <code>dplyr</code> <code>tidyr</code> e <code>purrr</code>, veja:</p>
<pre class="r"><code># Pacotes necess√°rios
library(tidyr)
library(purrr)

# Dados
data &lt;- tibble(x = c(&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;C&quot;, &quot;C&quot;),
               y = c(0,1,2,3,4,5)) 
# Codigos
data %&gt;%                                     # Input Data
  nest(-x) %&gt;%                               # Split
  mutate(data = map(data, ~mean(.x$y))) %&gt;%  # Apply
  unnest()                                   # Combine</code></pre>
<pre><code>## # A tibble: 3 x 2
##   x      data
##   &lt;chr&gt; &lt;dbl&gt;
## 1 A       0.5
## 2 B       2.5
## 3 C       4.5</code></pre>
<p>Note que obtemos a mesma sa√≠da do c√≥digo anterior</p>
<div id="split-apply-combine-com-fun√ß√µes-complexas" class="section level3">
<h3>Split-Apply-Combine com fun√ß√µes complexas</h3>
<p>Voc√™ deve estar se perguntando:</p>
<p>‚Äú<em>T√°, eu tenho um atalho para usar a estrat√©gia ‚Äùsplit-apply-combine‚Äù com pacote <code>dplyr</code>, por que eu preciso usar os dados aninhados?</em>‚Äù</p>
<p>Trabalhar com dados aninhados permite aplicar qualquer tipo de fun√ß√£o em parti√ß√µes do conjunto de dados e juntar os resultados em um objeto do tipo <a href="https://tibble.tidyverse.org/"><code>tibble</code></a> cujo <code>print()</code> √© um <em>‚Äúm√©todo aprimorado que os torna mais f√°ceis de usar com grandes conjuntos de dados contendo objetos complexos‚Äù</em>.</p>
<p>Veja o seguinte exemplo:</p>
<p>Primeiramente, imagine que voc√™ queira calcular a m√©dia de <code>mpg</code> por <code>cyl</code> dos dados <code>mtcars</code> (nativos do R), bastaria utilizar a sequ√™ncia de c√≥digos:</p>
<pre class="r"><code>mtcars %&gt;%                     # input data
  group_by(cyl) %&gt;%            # split
  summarise(media = mean(mpg)) # apply/combine</code></pre>
<pre><code>## # A tibble: 3 x 2
##     cyl media
##   &lt;dbl&gt; &lt;dbl&gt;
## 1     4  26.7
## 2     6  19.7
## 3     8  15.1</code></pre>
<p>Vejamos a seguir o uso da estrat√©gia em situa√ß√µes mais complexas</p>
<div id="em-ajustes-de-modelos" class="section level4">
<h4>Em ajustes de modelos</h4>
<p>E se precis√°ssemos calcular algo mais elaborado, como por exemplo ajustar <span class="math inline">\(k=3\)</span> regress√µes lineares: <span class="math inline">\(y_k= b_{0_k} + b_{1_k}*x_k\)</span> (com <span class="math inline">\(y_k=\)</span> <code>mpg</code>, <span class="math inline">\(x_k=\)</span><code>disp</code> para cada <span class="math inline">\(k=\)</span><code>cyl</code>) para estudar os coeficientes estimados, o que aconteceria se utiliz√°ssemos o c√≥digo abaixo ?</p>
<p><em>Spoiler</em>: Note que pelo fato da sa√≠da da fun√ß√£o <code>lm</code> n√£o retornar apenas uma √∫nica vari√°vel para sumarizar obteremos um <code>Error</code>:</p>
<pre class="r"><code>mtcars %&gt;%                       # input data
  group_by(cyl) %&gt;%              # split
  summarise(lm = lm(mpg ~ disp)) # apply/combine</code></pre>
<pre><code>## Error: Problem with `summarise()` input `lm`.
## x Input `lm` must be a vector, not a `lm` object.
## ‚Ñπ Input `lm` is `lm(mpg ~ disp)`.
## ‚Ñπ The error occurred in group 1: cyl = 4.</code></pre>
<p>O erro nos diz: ‚Äú<em>A coluna <code>lm</code> deve ter o comprimento 1, n√£o 12</em>‚Äù ou seja, o resultado precisa ser um valor de resumo e n√£o todo o resultado do ajuste dos modelos.</p>
<p>Agora vejamos utilizando a abordagem <code>split-apply-combine</code> que ir√° nos permitir aplicar qualquer tipo de fun√ß√£o nos dados agrupados por pela vari√°vel <code>cyl</code>:</p>
<pre class="r"><code>as_tibble(mtcars) %&gt;%                                                      # input data
  nest(-cyl) %&gt;%                                                           # split
  mutate(lm = map(data, ~lm(mpg ~ disp, data = .x) %&gt;% broom::tidy())) %&gt;% # apply
  unnest(lm)                                                               # combine</code></pre>
<pre><code>## # A tibble: 6 x 7
##     cyl data               term        estimate std.error statistic    p.value
##   &lt;dbl&gt; &lt;list&gt;             &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
## 1     6 &lt;tibble [7 √ó 10]&gt;  (Intercept) 19.1       2.91        6.55  0.00124   
## 2     6 &lt;tibble [7 √ó 10]&gt;  disp         0.00361   0.0156      0.232 0.826     
## 3     4 &lt;tibble [11 √ó 10]&gt; (Intercept) 40.9       3.59       11.4   0.00000120
## 4     4 &lt;tibble [11 √ó 10]&gt; disp        -0.135     0.0332     -4.07  0.00278   
## 5     8 &lt;tibble [14 √ó 10]&gt; (Intercept) 22.0       3.35        6.59  0.0000259 
## 6     8 &lt;tibble [14 √ó 10]&gt; disp        -0.0196    0.00932    -2.11  0.0568</code></pre>
<p>Com o aux√≠lio do pacote <a href="https://cran.r-project.org/web/packages/broom/vignettes/broom.html"><code>broom</code></a> obtemos sa√≠das de dados arrumados e juntamos os resultados finais da regress√£o em uma √∫nica tabela de maneira pr√°tica.</p>
</div>
<div id="na-constru√ß√£o-de-gr√°ficos" class="section level4">
<h4>Na constru√ß√£o de gr√°ficos</h4>
<p>Veja um outro exemplo de uso aplicando uma fun√ß√£o para criar gr√°ficos, agora com ggplot:</p>
<pre class="r"><code>library(ggplot2)
library(gridExtra)

plot_list &lt;- 
  mtcars %&gt;%      # input data
  nest(-cyl) %&gt;%  # split/apply ‚Üì
  mutate(plots = map(data, ~ggplot(.x, aes(x=disp, y=mpg))+geom_point()+geom_smooth(method = &quot;lm&quot;))) %$% 
  plots # magrittr

# Combine para printar:
invoke(grid.arrange,plot_list, ncol=1) # ou: grid.arrange(grobs = plot_list, ncol=1)

# Combine para salvar:
walk2(paste0(&quot;plot&quot;,1:3,&quot;.png&quot;), plot_list, ~ggsave(.x,.y))</code></pre>
<p><img src="/post/2019-04-05-split-apply-combine/unnamed-chunk-6-1.png" /></p>
</div>
<div id="criando-tabelas" class="section level4">
<h4>Criando tabelas</h4>
<p>Por fim, um exemplo utilizando o pacote flextable.</p>
<p>Utilizaremos a fun√ß√£o <a href="https://github.com/gomesfellipe/functions/blob/master/flextable_custom.R"><code>flextable_custom()</code></a> que adaptei para gerar uma tabela j√° customizada com o pacote flextable e a fun√ß√£o <a href="https://github.com/gomesfellipe/functions/blob/master/save_flextable.R"><code>save_flextable()</code></a> inspirada em uma pergunta que fiz no stackoverflow sobre <a href="https://stackoverflow.com/questions/50225669/how-to-save-flextable-as-png-in-r">Como salvar uma tabela flextable como png no R?</a>.</p>
<p>Veja:</p>
<pre class="r"><code>library(flextable)
source(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/flextable_custom.R&quot;)
source(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/save_flextable.R&quot;)

tabela_list &lt;- 
  head(mtcars,7) %&gt;%           # input data
  nest(-cyl) %$% data %&gt;%      # apply                                       
  map(~flextable_custom(.x))   # apply / combine

# Veja a tabela:
tabela_list[[1]]

# Combine para salvar:
walk2(paste0(&quot;tab&quot;,1:3,&quot;.png&quot;), tabela_list, ~save_flextable(.y,.x))</code></pre>
<p><img src="/post/2019-04-05-split-apply-combine/img1.png" /></p>
</div>
</div>
</div>
</div>
<div id="conclus√£o" class="section level1">
<h1>Conclus√£o</h1>
<p>Vimos aqui como funciona a estrat√©gia e alguns exemplos de uso, por√©m, existem infinitas outras aplica√ß√µes para esse tipo de abordagem com os dados arrumados. Dependendo da tarefa esta abordagem pode ser bem produtiva e poupar muitas linhas de c√≥digo!</p>
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias</h1>
<p>Al√©m das refer√™ncias deixarem aqui algumas sugest√µes de leitura:</p>
<ul>
<li><a href="https://github.com/tidyverse/purrr" class="uri">https://github.com/tidyverse/purrr</a></li>
<li><a href="https://tibble.tidyverse.org/" class="uri">https://tibble.tidyverse.org/</a></li>
<li><a href="https://vita.had.co.nz/papers/plyr.pdf" class="uri">https://vita.had.co.nz/papers/plyr.pdf</a></li>
<li><a href="https://adv-r.hadley.nz/functionals.html#purrr-style" class="uri">https://adv-r.hadley.nz/functionals.html#purrr-style</a></li>
<li><a href="https://davisvaughan.github.io/furrr/" class="uri">https://davisvaughan.github.io/furrr/</a></li>
</ul>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2019-04-05-split-apply-combine/split-apply-combine/">Hackeando o R: estrat√©gia split-apply-combine</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category>Texto e NLP</category>
      <category domain="tag">data-mining</category>
      <category domain="tag">estatistica</category>
      <category domain="tag">flextable</category>
      <category domain="tag">pratica</category>
      <category domain="tag">r</category>
      <category domain="tag">rstudio</category>
      <category domain="tag">split-aply-combine</category>
      <category domain="tag">tabelas</category>
    </item>
    <item>
      <title>An√°lise de sobreviv√™ncia com dados do jogo PUBG dispon√≠veis no Kaggle</title>
      <link>https://gomesfellipe.github.io/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle/</link>
      <pubDate>Wed, 28 Nov 2018 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle/</guid>
      <description>O que interefere na probabilidade de um indiv√≠duo sobreviver? Quais fatores apresentam efeito no risco de morte em um intervalo de tempo? Neste post buscaremos evid√™ncias estat√≠sticas para responder estas perguntas em dados abertos do PUBG hospedados no Kaggle</description>
      <content:encoded>&lt;![CDATA[
        


<div id="an√°lise-de-sobreviv√™ncia-e-pubg" class="section level1">
<h1>An√°lise de sobreviv√™ncia e PUBG</h1>
<p>An√°lise de sobreviv√™ncia √© um termo que se refere a situa√ß√µes m√©dicas e √© caracterizada pela sua vari√°vel resposta, que pode ser apresentada de tr√™s formas: probabilidade de sobreviv√™ncia, taxa de incid√™cia e taxa de incid√™ncia acumulada.</p>
<p>Na engenharia este termo tamb√©m √© conhecido como confiabilidade, no entanto, condi√ß√µes parecidas podem ocorrer em (inusitadas) outras √°reas.</p>
<p>PUBG √© um jogo online multiplayer de batalha em que 100 jogadores s√£o lan√ßados em uma ilha e tem como objetivo principal <strong>sobreviver</strong>, a √°rea de jogo diminui progressivamente, confinando os sobreviventes a um espa√ßo cada vez menor e for√ßando encontros e o vencedor √© o √∫ltimo jogador (ou time) a permanecer vivo.</p>
<p>Um √∫nico jogo dura aproximadamente de 30-35 minutos e neste tempo o jogador coleta itens (arma, cura, boost), abate outros jogadores, comete e leva dano de seus advers√°rios, pode dirigir ve√≠culos dentre outras a√ß√µes enquanto tentam sobrevier ao mesmo tempo.</p>
<p>Quest√µes que surgiram em mente ap√≥s um per√≠odo de estudos de an√°lise de sobreviv√™ncia e confiabilidade e ouvindo pessoas falarem sobre esta modalidade de jogo:</p>
<ul>
<li>O que interefere na probabilidade de um indiv√≠duo sobreviver?</li>
<li>O que tem efeito no risco de um jogador ser abatido em um intervalo de tempo?</li>
</ul>
<p>Faremos uma abordagem estat√≠stica aqui, ap√≥s uma breve an√°lise explorat√≥ria os dados ser√£o avaliados utilizando o modelo de Kaplan-Meier, que √© um estimador de forma n√£o param√©trica para a fun√ß√£o de sobreviv√™ncia e o modelo semiparam√©trico de regress√£o de riscos proporcionais de Cox.</p>
</div>
<div id="a-base-de-dados" class="section level1">
<h1>A Base de dados</h1>
<p>A base de dados utilizada foi obtida atrav√©s do Kaggle em ‚ÄúPUBG Match Deaths and Statistics‚Äù: <a href="https://www.kaggle.com/skihikingkevin/pubg-match-deaths" class="uri">https://www.kaggle.com/skihikingkevin/pubg-match-deaths</a> que conta com mais de 65 milh√µes de registros de mortes no jogo PlayerUnknown Battleground‚Äôs matches - PUBG.</p>
<p><a href="https://www.kaggle.com/gomes555/analise-de-sobrevivencia-km-e-cox/">Existe uma vers√£o deste post no kaggle</a> e al√©m desta base, existe uma competi√ß√£o em andamento que vai at√© o dia 30 de Janeiro no link:<a href="https://www.kaggle.com/c/pubg-finish-placement-prediction" class="uri">https://www.kaggle.com/c/pubg-finish-placement-prediction</a> que desafia os jogadores a prever o posicionamento do vencedor em percentil, onde 1 corresponde ao 1¬∫ lugar e 0 corresponde ao √∫ltimo lugar do jogo. Fiz uma participa√ß√£o com um <a href="https://www.kaggle.com/gomes555/xgboost-caret-for-fun">script testando os resultados do algor√≠tmo xgboost com caret</a> e tamb√©m testei uns <a href="https://www.kaggle.com/gomes555/tidyverse-machine-learning-for-fun">ajustes com random forest utilizando o tidyverse</a>. Esses scripts s√£o abertos e est√£o prontos para uso, <a href="https://www.kaggle.com/gomes555">n√£o me renderam a melhor posi√ß√£o</a> mas a intens√£o aqui √©, principalmente, aprender e testar os m√©todos pois S√£o muitas possibilidade para aprender e praticar. Voltando a base de dados:</p>
<p>Segundo a <a href="https://www.kaggle.com/skihikingkevin/pubg-match-deaths#aggregate.zip">descri√ß√£o da base no kaggle</a>:</p>
<p><code>agg_match_stats_x.csv</code> fornece informa√ß√µes de correspond√™ncia mais agregadas sobre os dados de mortes, como tamanho da fila, fpp/tpp, morte do jogador, etc.</p>
<p>As colunas s√£o as seguintes:</p>
<div class="col2">
<ul>
<li><code>match_id</code> : O id √∫nico de correspond√™ncia gerado por pubg.op.gg. √â poss√≠vel fazer uma jun√ß√£o disso com os dados das mortes para ver todas as informa√ß√µes</li>
<li><code>party_size</code> : o n√∫mero m√°ximo de jogadores por equipe. por exemplo, 2 implica que era um sistema de fila dupla</li>
<li><code>player_dist_ride</code> : unidades de distancia total (metros?) que o jogador percorreu em um ve√≠culo</li>
<li><code>player_dist_walk</code> : unidades de distancia total (metros?) percorrida pelo jogador a p√©</li>
<li><code>match_mode</code> : se o jogo foi jogado em primeira pessoa (fpp) ou em terceira pessoa (tpp)</li>
<li><code>team_placement</code> : a classifica√ß√£o final da equipe dentro da partida</li>
<li><code>player_dmg</code> : Total de pontos de vida que o jogador distribuiu</li>
<li><code>player_assists</code> : N√∫mero de assist√™ncias que o jogador marcou</li>
<li><code>game_size</code> : o n√∫mero total de equipes que estavam no jogo</li>
<li><code>player_dbno</code> : N√∫mero de knockdowns que o jogador marcou</li>
<li><code>player_kills</code> : N√∫mero de mortes que o jogador marcou</li>
<li><code>team_id</code> : o ID da equipe √† qual o jogador pertencia</li>
<li><code>date</code> : a data e a hora em que a partida ocorreu</li>
<li><code>player_name</code> : nome do jogador</li>
</ul>
<hr />
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/img.png" /></p>
</div>
<p>A rotinas abaixo carregam os pacotes, fun√ß√µes customizadas e salva em extens√£o <code>.rds</code>uma amostra da base de dados utilizadas ao longo do post:</p>
<pre class="r"><code># Carregar pacotes --------------------------------------------------------
packages &lt;- c(&quot;data.table&quot;, &quot;dplyr&quot;, &quot;purrr&quot;, &quot;survival&quot;  , &quot;survminer&quot;,
              &quot;ggfortify&quot;,&quot;GGally&quot;, &quot;ggplot2&quot;,&quot;moments&quot;, &quot;gridExtra&quot;,&quot;ggExtra&quot;,
              &quot;cowplot&quot;,&quot;lubridate&quot;, &quot;scales&quot;, &quot;knitr&quot;, &quot;kableExtra&quot;, &quot;grid&quot;,
              &quot;broom&quot;, &quot;formattable&quot;, &quot;grid&quot;)
purrr::walk(packages,library, character.only = TRUE, warn.conflicts = FALSE)
rm(packages)

# Funcoes customizadas do github ------------------------------------------
source(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/inicio_e_fim_da_base.R&quot;)
source(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/grafico_descritivo.R&quot;)
source(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/sumario_custom_num.R&quot;)

# Opcoes do documento -----------------------------------------------------
# options(scipen = 99999)

# Tema dos graficos -------------------------------------------------------
theme_set(theme_bw()+
            theme(axis.text.x = element_text(size=17),
                  axis.text.y = element_text(size=17),
                  axis.title.y = element_text(size=20), legend.position = &quot;bottom&quot;))

# Tema das tabelas kable --------------------------------------------------
kable2 &lt;- function(x,linhas=NULL,colunas=NULL, ...){
  k &lt;- 
    kable(x,digits = 4,...) %&gt;%
    kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F) %&gt;%
    kable_styling(c(&quot;striped&quot;, &quot;bordered&quot;)) 
  
  if (!is.null(linhas)) {
    # destque na linha:
    k &lt;-  k %&gt;% row_spec(linhas, bold = T, color = &quot;white&quot;, background = &quot;#FFE8BD&quot;)
  }
  
  if (!is.null(colunas)) {
    # destque na colunas:
    k &lt;-  k %&gt;% column_spec(colunas,bold=T, color=&quot;white&quot;, background = &quot;#FFE8BD&quot;)
  }
  k %&gt;%
    scroll_box(width = &quot;850px&quot;)
}</code></pre>
<p>Em uma an√°lise de sobreviv√™ncia √© comum a presen√ßa de observa√ß√µes censuradas, (isto √©, quando ocorre a perda de informa√ß√£o decorrente de n√£o se ter observado a data de ocorr√™ncia do desfecho). No caso dessa base de dados n√£o existe uma vari√°vel que define a censura, pois apenas a morte do jogador √© registrada e √© poss√≠vel que se os jogadores se desconectarem do jogo mesmo que n√£o sejam mortos seja contado como morte de qualquer jeito. Os detalhes por tr√°s da aquisi√ß√£o de dados n√£o trazem essa informa√ß√£o portanto pode n√£o ser poss√≠vel distinguir a censura do desfecho e isso √© um detalhe relevante que deve ser levado em conta.</p>
<pre class="r"><code># Carregar base -----------------------------------------------------------
set.seed(2)   # reprodutivel
pubg_tpp1 &lt;-  # Informacoes dos criterios de selecao no corpo do texto
  map_df(paste0(&quot;agg_match_stats_&quot;,0:4,&quot;.csv&quot;), 
         ~ fread(.x, showProgress = T,
                 data.table = T)[match_mode == &quot;tpp&quot; &amp; party_size == 1 &amp; year(date) == 2018 &amp; player_dist_walk&gt;10 &amp; player_dmg != 0 ][, !c(&quot;match_mode&quot;,&quot;party_size&quot;,&quot;game_size&quot;,&quot;date&quot;, &quot;team_id&quot;,&quot;player_dbno&quot;, &quot;team_placement&quot;), with=FALSE][,player_survive_time := player_survive_time/60] %&gt;% 
           group_by(match_id) %&gt;%
           do(sample_n(.,1)) %&gt;% 
           ungroup() 
  )

# Salvar base coletada ----------------------------------------------------
saveRDS(pubg_tpp1,&quot;pubg_tpp1.rds&quot;)</code></pre>
<!-- <iframe src="https://giphy.com/embed/3oKIPmaM8aFolCcuI0" width="100%" height="270" frameBorder="0" class="giphy-embed" allowFullScreen></iframe> -->
<div class="col2">
<p>Descri√ß√£o da rotina acima e os crit√©rios para a sele√ß√£o da amostra:</p>
<ol style="list-style-type: decimal">
<li>percorre as 5 bases dispon√≠veis: <code>paste0("agg_match_stats_",0:4,".csv")</code></li>
<li>seleciona partidas em terceira pessoa: <code>match_mode == "tpp"</code></li>
<li>com tamanho da equipe = 1 (individual): <code>party_size == 1</code></li>
<li>do ano de 2018: <code>year(date) == 2018</code></li>
<li>andaram mais que 10 unidades de distancia (metros?): <code>player_dist_walk&gt;10</code></li>
<li>fizeram algum dano (evitar jogadores ausentes): <code>player_dmg != 0</code><br />
</li>
<li>remove colunas n√£o utilizadas na analise</li>
<li>converte do tempo para minutos: <code>player_survive_time := player_survive_time/60</code></li>
<li>agrupa por partida: <code>group_by(match_id)</code></li>
<li>seleciona um jogador de cada partida: <code>do(sample_n(.,1))</code></li>
</ol>
<iframe src="https://giphy.com/embed/g4OqNwXDrnfOcbaaUM" width="240" height="300" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
</div>
<p>Note que apenas um jogador de cada partida √© selecionado na inten√ß√£o de obter independ√™ncia entre observa√ß√µes, isso reduziu drasticamente seu tamanho. Agora que a base j√° foi importada e filtrada, faremos a leitura de 200 linhas aleat√≥rias com a finalidade de diminuir o tempo computacional das opera√ß√µes realizadas em seguida.</p>
<pre class="r"><code>set.seed(1)
pubg_tpp1 &lt;- readRDS(&quot;pubg_tpp1.rds&quot;) %&gt;% sample_n(200)%&gt;% 
  select(-one_of(c(&quot;match_id&quot;, &quot;player_name&quot;)))</code></pre>
<p>Veja a seguir de forma visual como as vari√°veis num√©ricas se correlacionam:</p>
<pre class="r"><code>pubg_tpp1 %&gt;% 
  rev %&gt;% 
  grafico_descritivo()</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-4-1.png" style="width:80.0%" /></p>
</center>
<div id="vari√°vel-resposta" class="section level3">
<h3>Vari√°vel resposta</h3>
<p>Vejamos o que acontece ao analisar o tempo de sobreviv√™ncia de cada jogador</p>
<iframe src="https://giphy.com/embed/3oKIP5KxPss1gjwpG0" width="100%" height="270" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
<p>A seguir, a distribui√ß√£o da vari√°vel resposta <code>player_survive_time</code> :</p>
<pre class="r"><code>plot_grid(pubg_tpp1 %&gt;% 
            ggplot(aes(x=player_survive_time))+
            geom_histogram(aes(y = ..density..), bins = 30, fill=&quot;white&quot;, color=&quot;black&quot;)+
            geom_density(alpha=.2, fill=&quot;white&quot;)+
            scale_x_continuous(labels = scales::comma, limits = c(0,40), breaks = seq(0,40,5))+
            labs(x=&quot;&quot;,y=&quot;&quot;, title = &quot;Tempo de sobreviv√™ncia dos jogadores selecionados&quot;)
          ,
          pubg_tpp1 %&gt;% 
            ggplot(aes(x=&quot; &quot;, y=player_survive_time))+
            geom_boxplot()+
            labs(x=&quot;&quot;)+
            coord_flip()
          ,
          ncol = 1, nrow = 2, align = &quot;v&quot;, rel_heights = c(3,1))</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-6-1.png" style="width:80.0%" /></p>
</center>
<p>Note que possue uma <a href="https://binged.it/2BAYX3s">assimetria positiva</a></p>
</div>
<div id="data-wrangling" class="section level3">
<h3>Data Wrangling</h3>
<p>Primeiramente, vejamos as vari√°veis se relacionam entre si e com a vari√°vel resposta com os coeficientes de correla√ß√£o de Pearson:</p>
<pre class="r"><code># Correlations
pubg_tpp1 %&gt;% 
  select_if(is.numeric) %&gt;% 
  cor() %&gt;% 
  corrplot::corrplot(method = &quot;number&quot;,type = &quot;upper&quot;,diag = F, order = &quot;hclust&quot;,number.cex = 0.7, title = &quot;Correlation correlated numerics&quot;, mar=c(0,0,1,0))</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-7-1.png" style="width:80.0%" /></p>
</center>
<p>√â poss√≠vel notar que apenas a vari√°vel <code>player_assists</code> n√£o correlaciona-se com a vari√°vel resposta nem com as demais vari√°veis e <code>player_dmg</code> e <code>player_kills</code> s√£o fortemente correlacionadas, isso indica que pode ser interessante remover uma delas ou juntar toda essa informa√ß√£o em uma √∫nica vari√°vel, veremos‚Ä¶</p>
<p>Al√©m disso nota-se que a dist√¢ncia percorrida a p√© √© fortemente correlacionada com a vari√°vel resposta enquanto que a dist√¢ncia de quem andou de carro n√£o √© t√£o correlacionada. Uma transforma√ß√£o na vari√°vel <code>player_dist_ride</code> para uma dummy <code>drive</code> indicando se o indiv√≠duo dirigiu ou n√£o pode representar melhor esta informa√ß√£o.</p>
<p>Vejamos algumas caracter√≠sticas peculiares:</p>
<pre class="r"><code>pubg_tpp1 %&gt;% 
  select(player_kills, player_dist_ride, player_assists) %&gt;% 
  map_dfr(~quantile(.x,  probs = seq(0,1,0.25)) %&gt;% round(2)) %&gt;% 
  t  %&gt;% tidy() %&gt;% 
  `colnames&lt;-`(c(&quot;vari√°vel&quot;,percent(seq(0,1,0.25)))) %&gt;% 
  kable2()</code></pre>
<p>Praticamente metade da amostra n√£o registrou abates nem possui marca√ß√£o de <code>player_dist_ride</code>. Como a vari√°vel <code>player_dmg</code> apresentou correla√ß√£o com a vari√°vel resposta <code>player_survive_time</code>, vamos fazer algumas transforma√ß√µes:</p>
<ol style="list-style-type: decimal">
<li>Criar uma vari√°vel dummy <code>drive</code> se jogador usou carro</li>
<li>Somar a <code>player_dist_ride</code> e <code>player_dist_walk</code> em uma √∫nica vari√°vel: <code>player_dist</code></li>
<li>Juntar <code>player_kills</code>, <code>player_dmg</code> e <code>player_assists</code> em uma √∫nica vari√°vel: <code>player_performance</code></li>
</ol>
<div id="player-performance" class="section level4">
<h4>Player performance</h4>
<p>Como criar a vari√°vel <code>player_performance</code>?</p>
<iframe src="https://giphy.com/embed/xT9IgnOQS8e8uKkflK" width="100%" height="270" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
<p>Tentei inventar uma metodologia e com certeza devem existir maneiras mais eficientes de se fazer isso, por√©m, deixa eu explicar o que eu pensei, considere a formula:</p>
<p><span class="math display">\[
Playerperformance = log(WPlayerDmg + WPlayerAssists + WPlayerKills)
\]</span></p>
<p>onde:</p>
<p><span class="math display">\[
WPlayerKills = log(PlayerKills+0.5)\\
WPlayerDmg = log(PlayerDmg)\\
WPlayerAssists = PlayerAssists
\]</span></p>
<p>Note que:</p>
<ul>
<li><span class="math inline">\(WPlayerAssists\)</span>: N√£o √© feita qualquer transforma√ß√£o;</li>
<li><span class="math inline">\(WPlayerDmg\)</span>: A distribui√ß√£o fica ‚Äúquase sim√©trica‚Äù ap√≥s a transforma√ß√£o log;</li>
<li><span class="math inline">\(WPlayerKills\)</span>: adiciona-se 0.5 para poder tirar o log pois podem existir zeros nessa vari√°vel e al√©m disso, quem n√£o marcou abate ser√° penalizado com <span class="math inline">\(-1\)</span> na soma final do score: <code>player_performance</code>.</li>
</ul>
<p>Veja a seguir de forma visual a distribui√ß√£o das vari√°veis que far√£o parte da vari√°vel <code>player_performance</code> na parte de cima e na parte inferior o que acontece ap√≥s sua soma, gerando a nova vari√°vel <code>player_performance</code> :</p>
<pre class="r"><code>performance &lt;- tibble(w_player_kills = log(pubg_tpp1$player_kills+0.5),
                      w_player_dmg = log(pubg_tpp1$player_dmg),
                      w_player_assists = pubg_tpp1$player_assists) %&gt;% 
  mutate(player_performance = log(w_player_dmg + w_player_assists + w_player_kills))

grid.arrange(
  performance %&gt;% 
    select(-player_performance) %&gt;% 
    tidyr::gather() %&gt;% 
    ggplot(aes(x=value))+
    geom_histogram(aes(y = ..density..), bins = 30, fill=&quot;white&quot;, color=&quot;black&quot;)+
    geom_density(alpha=.2, fill=&quot;white&quot;)+
    scale_x_continuous(labels = scales::comma, limits = c(-1.5,8), breaks = seq(-1,8,1))+
    labs(x=&quot;&quot;, y=&quot;&quot;)+
    theme(axis.title.x=element_blank(),
          axis.text.x=element_blank(),
          axis.ticks.x=element_blank())+
    facet_wrap(~key, scales = &quot;free&quot;)
  ,
  performance %&gt;% 
    select(player_performance) %&gt;% 
    tidyr::gather() %&gt;% 
    ggplot(aes(x=value))+
    geom_histogram(aes(y = ..density..), fill=&quot;white&quot;, color=&quot;black&quot;,bins = 15)+
    geom_density(alpha=.2, fill=&quot;white&quot;)+
    scale_x_continuous(limits = c(-1.,2.5), breaks = seq(-1,3,0.5))+
    labs(x=&quot;&quot;, y=&quot;&quot;, title = &quot;performance&quot;),
  ncol=1
)</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-9-1.png" style="width:80.0%" /></p>
</center>
</div>
<div id="transforma√ß√µes-na-base" class="section level4">
<h4>Transforma√ß√µes na base</h4>
<p>A seguir faremos as mudan√ßas diretamente no dataset que estamos trabalhando:</p>
<pre class="r"><code>pubg_tpp1 &lt;- 
  pubg_tpp1 %&gt;% 
  mutate(player_dist = log(player_dist_ride + player_dist_walk)) %&gt;%  
  mutate(player_assists_d = if_else(player_assists ==0, 0, 1)) %&gt;% 
  mutate(player_performance = performance$player_performance )%&gt;% 
  mutate(drive = ifelse(player_dist_ride==0, &quot;no&quot;, &quot;yes&quot;) %&gt;% as.factor()) %&gt;% 
  mutate(player_kills_d = ifelse(player_kills==0, &quot;no&quot;, &quot;yes&quot;) %&gt;% as.factor()) </code></pre>
<p>A manipula√ß√£o acima cria as seguintes vari√°veis:</p>
<ol style="list-style-type: decimal">
<li><code>player_dist</code> como o log da soma de <code>player_dist_ride</code> e <code>player_dist_walk</code></li>
<li><code>player_assists_d</code> como uma dummy: 1 se o jogador deu assist√™ncia; 0 c.c.</li>
<li><code>player_performaec</code> como a combina√ß√£o de <code>player_dmg</code>, <code>player_assists</code> e <code>player_kills</code></li>
<li><code>drive</code> como uma dummy: 1 se o jogador dirigiu; 0 c.c.</li>
<li><code>player_kills_d</code> como uma dummy: 1 se jogador matou algu√©m; 0 c.c.</li>
</ol>
<p>Vejamos como ocorre a distribui√ß√£o das vari√°veis num√©ricas ap√≥s as transforma√ß√µes:</p>
<pre class="r"><code>g1 &lt;- 
  pubg_tpp1 %&gt;% 
  # select_if(~ !length(table(.x))==2 &amp; is.numeric(.x)) %&gt;% colnames() %&gt;% 
  select(player_survive_time,player_performance,player_dist) %&gt;% colnames() %&gt;% 
  map2(c(&quot;Densidade&quot;, &quot;&quot;, &quot;&quot;),
       ~ plot_grid(
         pubg_tpp1 %&gt;% 
           ggplot(aes_string(x=.x)) + 
           geom_histogram(aes(y=..density..),colour=&quot;black&quot;, fill=&quot;white&quot;, bins = 15) +
           geom_density(alpha=.2, fill=&quot;lightgrey&quot;) +
           scale_x_continuous()+
           ggtitle(.x)+
           labs(x=&quot;&quot;, y=.y)+
           theme(axis.title.x=element_blank(),
                 axis.text.x=element_blank(),
                 axis.ticks.x=element_blank())
         ,
         pubg_tpp1 %&gt;% 
           ggplot(aes_string(, y=.x))+
           geom_boxplot(aes(x=&quot; &quot;))+
           labs(x=&quot;&quot;, y=&quot;&quot;)+
           coord_flip()+
           theme(axis.title.x=element_blank(),
                 axis.text.x=element_blank(),
                 axis.ticks.x=element_blank()),
         
         ncol = 1, nrow = 2, align = &quot;v&quot;, rel_heights = c(3,1)
       )
  )

dat &lt;- 
  pubg_tpp1 %&gt;% 
  select_if(~.x %&gt;% table %&gt;% length == 2) %&gt;% 
  mutate_at(2,~if_else(.x==0, &quot;no&quot;, &quot;yes&quot;)) %&gt;% 
  .[,-1]

g2 &lt;- map2(colnames(dat),
           c( &quot;Porcentagem&quot;, &quot;&quot;,&quot;&quot;),
           ~ dat[,.x] %&gt;% 
             tidyr::gather() %&gt;% 
             group_by(key, value) %&gt;% 
             summarise(n = n()) %&gt;% 
             mutate(prop = n/sum(n)) %&gt;% 
             ggplot(aes(x = key, y = prop,fill = value)) + 
             geom_bar(position = &quot;fill&quot;,stat = &quot;identity&quot;, alpha=0.7) +
             scale_y_continuous(labels = percent_format())+
             labs(x=&quot;&quot;, y = .y)+
             scale_fill_manual(values = c(&quot;grey&quot;, &quot;#FCC14B&quot;), name = &quot;Legenda:&quot;)
)

grid.arrange(g1[[1]], g1[[2]], g1[[3]],g2[[1]], g2[[2]], g2[[3]], ncol=3, heights=c(3/5, 2/5))</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-11-1.png" style="width:80.0%" /></p>
</center>
<!-- A distribui√ß√£o dos dados ordenada pela vari√°vel resposta `player_survive_time` : -->
<!-- ```{r} -->
<!-- # Sorted -->
<!-- pubg_tpp1 %>%  -->
<!--   select(player_survive_time, everything()) %>%  -->
<!--   mutate_if(~length(unique(.x))==2, as.factor) %>%  -->
<!--   tabplot::tableplot(sortCol = player_survive_time,decreasing = T) -->
<!-- ``` -->
<p>Apos a transforma√ß√£o a distribui√ß√£o e demais informa√ß√µes dos dados, vejamos novamente a distribui√ß√£o das vari√°veis da amostra com os gr√°ficos de dispers√£o, densidade e correla√ß√µes levando em conta se dirigiu ou n√£o:</p>
<pre class="r"><code>grafico_descritivo(x = pubg_tpp1,
                   colNames = c(&#39;player_survive_time&#39;, &quot;player_performance&quot;, &#39;player_dist&#39;,
                                &#39;player_assists_d&#39;,&quot;player_kills_d&quot;, &#39;drive&#39;),
                   color=&#39;drive&#39;,
                   colors = c(&quot;grey&quot;, &quot;#FCC14B&quot;))</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-13-1.png" style="width:80.0%" /></p>
</center>
<p>O fato do jogador ter dirigido ou n√£o exibiu padr√µes interessantes, pode ser que seja significante no ajuste do modelo final.</p>
</div>
</div>
</div>
<div id="an√°lise-de-sobrevivencia" class="section level1">
<h1>An√°lise de sobrevivencia</h1>
<p>O passo inicial de qualquer an√°lise estat√≠stica consiste em uma descri√ß√£o dos dados e o principal componente da an√°lise descritiva envolvendo dados de tempo de vida √© a fun√ß√£o de sobreviv√™ncia: <span class="math inline">\(S(t) = P(T&gt;t)\)</span>, que determina a probabilidade de um indiv√≠duo sobreviver por mais do que um determinado tempo <span class="math inline">\(t\)</span>, ou por no m√≠nimo um tempo igual a <span class="math inline">\(t\)</span>.</p>
<p>A descri√ß√£o dos dados j√° foi realizada, agora faremos a descri√ß√£o envolvendo a fun√ß√£o de sobreviv√™ncia.</p>
<iframe src="https://giphy.com/embed/xT0xeMrCEGPiU5uw0w" width="100%" height="266" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
<div id="kaplan-meier" class="section level2">
<h2>Kaplan-Meier</h2>
<p>Para isso existem algumas alternativas como o estimador de Kaplan-Meier, que utiliza os conceitos de independ√™ncia e de probabilidade condicional para deduzir a probabilidade de sobreviver at√© o tempo <span class="math inline">\(t\)</span>.</p>
<p>Veja a seguir s√£o ajustados os modelos univariados de Kaplan-Meier para cada uma das coivar√°veis da amostra:</p>
<pre class="r"><code>surv &lt;- Surv(pubg_tpp1$player_survive_time)
resultado_km &lt;-
  list(geral            = survfit(surv ~ 1 ,data = pubg_tpp1),
       player_assists_d = survfit(surv ~ player_assists_d ,data = pubg_tpp1),
       drive            = survfit(surv ~ drive,data = pubg_tpp1 ),
       player_kills_d   = survfit(surv ~ player_kills_d,data = pubg_tpp1))</code></pre>
<p>Veja os resultados da fun√ß√£o de sobreviv√™ncia sem levar em considera√ß√£o nenhuma das coivar√°veis:</p>
<pre class="r"><code>surv_summary(resultado_km[[1]], pubg_tpp1) %&gt;% .[1:5,-ncol(.)] %&gt;% cbind(variable = &quot;Geral&quot;) %&gt;% 
  select(variable, everything())%&gt;%
  kable2()</code></pre>
<p>A fun√ß√£o <code>surv_summary()</code> retorna um quadro de dados com as seguintes colunas:</p>
<ul>
<li>time: o tempo em que a curva tem um passo.</li>
<li>n.risk: o n√∫mero de sujeitos em risco em t.</li>
<li>n.evento: o n√∫mero de eventos que ocorrem no tempo t.</li>
<li>n.censor: n√∫mero de eventos censurados.</li>
<li>surv: estimativa da probabilidade de sobreviv√™ncia.</li>
<li>std.err: erro padr√£o de sobreviv√™ncia.</li>
<li>superior: extremidade superior do intervalo de confian√ßa</li>
<li>inferior: extremidade inferior do intervalo de confian√ßa</li>
<li>estratos: indica a estratifica√ß√£o da estimativa de curvas. Os n√≠veis de estratos (um fator) s√£o os r√≥tulos das curvas (se houver).</li>
</ul>
<div id="log-rank" class="section level3">
<h3>Log-rank</h3>
<p>Al√©m da an√°lise visual das estimativas √© importante comparar as curvas de sobreviv√™ncia com testes de hip√≥teses para obter-se signific√¢ncia estat√≠stica para nossas afirma√ß√µes.</p>
<p>O teste log rank √© um teste n√£o param√©trico, que n√£o faz suposi√ß√µes sobre as distribui√ß√µes de sobreviv√™ncia. Essencialmente, o teste log rank compara o n√∫mero observado de eventos em cada grupo com o que seria esperado se a hip√≥tese nula fosse verdadeira. Considere ent√£o <span class="math inline">\(H_0: S_1(t)=S_2(t)\)</span> para todo <span class="math inline">\(t\)</span> no per√≠odo de acompanhamento (ou seja, se as curvas de sobreviv√™ncia fossem id√™nticas). A estat√≠stica utilizada no teste √© um <span class="math inline">\(T\)</span> com distribui√ß√£o aproximadamente <span class="math inline">\(\chi^2\)</span> com 1 grau de liberdade.</p>
<p>O objeto criado abaixo guarda o valor p para o teste de log-rank de cada em cada um dos modelos:</p>
<pre class="r"><code>resultado_log_rank &lt;- 
  c(geral = &quot;&quot;,
    player_assists_d=round(1-pchisq(survdiff(surv~player_assists_d,data = pubg_tpp1)$chisq,1),5),
    drive=round(1-pchisq(survdiff(surv~drive,data=pubg_tpp1)$chisq,1),5),
    player_kills_d=round(1-pchisq(survdiff(surv~player_kills_d,data=pubg_tpp1)$chisq,1),5)
  )</code></pre>
<p>Os gr√°ficos gerados a partir dos modelos ajustados acima bem como o resultado dos testes de log-rank s√£o exibidos na imagem a seguir:</p>
<pre class="r"><code>survplot &lt;- map2(resultado_km,
                 case_when(resultado_log_rank == &#39;0&#39; ~ &quot;log-rank: \n p &lt; 0,00001&quot;,
                           resultado_log_rank == &quot;&quot; ~ &quot;log-rank n√£o se aplica&quot;,
                           resultado_log_rank != &#39;0&#39; | resultado_log_rank != &#39;&#39; ~ 
                             paste0(&quot;log-rank: \n p =&quot;,as.numeric(resultado_log_rank))),
                 ~ autoplot(.x)+
                   ggtitle(stringr::str_remove_all(names(.x$strata)[1],&quot;(=no|=yes)&quot;))+
                   annotate(&quot;label&quot;,y = 0.20, x = 5,
                            label = .y,
                            size = 4, colour = &quot;red&quot;,hjust=0.1)+ 
                   scale_fill_manual(values = c(&quot;grey&quot;, &quot;#FCC14B&quot;))+
                   scale_color_manual(values = c(&quot;grey&quot;, &quot;#FCC14B&quot;))+
                   theme(legend.position = c(0.85,0.7))+
                   scale_x_continuous(limits = c(0,30), breaks = seq(0,30,5))
                 
)
grid.arrange(survplot[[1]], survplot[[2]] ,survplot[[3]], survplot[[4]], ncol=2)</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-17-1.png" style="width:80.0%" /></p>
</center>
<p>O eixo horizontal (eixo x) representa o tempo em minutos, e o eixo vertical (eixo y) mostra a probabilidade de sobreviv√™ncia ou a propor√ß√£o de jogadores que sobrevivem. As linhas representam curvas de sobreviv√™ncia dos dois grupos.</p>
<p>Uma queda vertical nas curvas indica um evento. No tempo zero, a probabilidade de sobreviv√™ncia √© de 1,0 (ou 100% dos jogadores vivos).</p>
<p>Interpreta√ß√£o: Pelo gr√°fico, aparentemente n√£o existe diferen√ßa no tempo de sobreviv√™ncia com estratifica√ß√£o dos dados de acordo com quem deu assist√™ncia ou n√£o, j√° para o teste que compara igualdade de fun√ß√µes de sobreviv√™ncia das demais vari√°veis, existem evidencias estat√≠sticas para rejeitar a hip√≥tese de que n√£o h√° diferen√ßa na sobrevida entre os dois grupos</p>
</div>
</div>
<div id="fun√ß√£o-de-risco-hazard-ou-taxa-de-falha" class="section level2">
<h2>Fun√ß√£o de risco (hazard) ou taxa de falha</h2>
<p>Fun√ß√£o de risco (hazard) ou taxa de falha √© o risco ‚Äúinstant√¢neo‚Äù denotada por <span class="math inline">\(\lambda(t)\)</span> √© uma taxa, n√£o uma probabilidade e pode assumir qualquer valor real maior que zero.</p>
<p>No exemplo representa a taxa de incid√™ncia ou risco acumulado para um indiv√≠duo morrer at√© o momento <span class="math inline">\(t\)</span>, dado que sobreviveu at√© este momento. √â muito informativa quando comparada com a fun√ß√£o de sobreviv√™ncia pois diferentes <span class="math inline">\(S(t)\)</span> podem ter formas semelhantes, enquanto que respectivas <span class="math inline">\(\lambda(t)\)</span> podem diferir drasticamente.</p>
<pre class="r"><code>survplot &lt;-
  map(resultado_km  ,
      ~ ggsurvplot(.x, conf.int = TRUE, 
                   palette = c(&quot;grey&quot;, &quot;#FCC14B&quot;),
                   risk.table = F,break.time.by = 5,
                   fun = &quot;cumhaz&quot;,title = stringr::str_remove_all(names(.x$strata)[1],&quot;(=no|=yes)&quot;))
  )
arrange_ggsurvplots(survplot, print = TRUE,
                    ncol = 2, nrow = 2)</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-18-1.png" style="width:80.0%" /></p>
</center>
<p>O risco cumulativo <span class="math inline">\(H( t)\)</span> pode ser interpretado como a for√ßa cumulativa da mortalidade.
Em outras palavras, corresponde ao n√∫mero de eventos que seriam esperados para cada indiv√≠duo
pelo tempo t se o evento fosse um processo repetitivo.</p>
</div>
<div id="modelo-de-cox" class="section level2">
<h2>Modelo de cox</h2>
<p>√â caracterizado pela presen√ßa dos coeficientes <span class="math inline">\(\beta\)</span>s que medem os efeitos (semelhantes √† an√°lise de regress√£o log√≠stica m√∫ltipla e linear m√∫ltipla) das vari√°veis explicativas sobre a fun√ß√£o de risco. Em um modelo de regress√£o de riscos proporcionais de Cox, a medida do efeito √© a <em>taxa de risco</em>, que √© o risco de falha, dado que o participante sobreviveu at√© um tempo espec√≠fico.</p>
<p>Algumas das suposi√ß√µes para o correto uso do modelo de regress√£o de riscos proporcionais de Co incluem:</p>
<ul>
<li>independ√™ncia dos tempos de sobreviv√™ncia entre indiv√≠duos distintos na amostra,</li>
<li>rela√ß√£o multiplicativa entre os preditores e o risco,</li>
<li>uma taxa de risco constante ao longo do tempo.</li>
</ul>
<p>O modelo de riscos proporcionais de Cox √© chamado de modelo semi-param√©trico , porque n√£o h√° suposi√ß√µes sobre o formato da fun√ß√£o de risco de linha de base. No entanto, existem outras suposi√ß√µes, como observado acima.</p>
<p>√â poss√≠vel utilizar as estat√≠sticas de Wald, da raz√£o de verossimilhan√ßa e escore para fazer infer√™ncias sobre os par√¢metros do modelo</p>
<p>Veja a seguir a signific√¢ncia dos coeficiente estimado em modelos univariados para cada vari√°vel candidata ao modelo:</p>
<pre class="r"><code># Modelos univariados
covariates    &lt;- c(&quot;player_kills&quot;,&quot;player_dist_ride&quot;,&quot;player_performance&quot;,
                   &quot;player_dist_walk&quot;,&quot;player_dmg&quot;, &quot;player_dist&quot;, 
                   &quot;player_assists_d&quot;,&quot;drive&quot;, &quot;player_kills_d&quot;)
univ_formulas &lt;- map(covariates,~ as.formula(paste(&#39;Surv(player_survive_time) ~&#39;, .x)))
univ_models   &lt;- map( univ_formulas, ~coxph(.x, data = pubg_tpp1))

# estrair resultados 
map2_df(univ_models,
        covariates,
        function(x,y){ 
          x                = summary(x)
          p.value          = signif(x$wald[&quot;pvalue&quot;], digits=2)
          wald.test        = signif(x$wald[&quot;test&quot;], digits=2)
          beta             = signif(x$coef[1], digits=2);#coeficient beta
          HR               = signif(x$coef[2], digits=2);#exp(beta)
          HR.confint.lower = signif(x$conf.int[,&quot;lower .95&quot;], 2)
          HR.confint.upper = signif(x$conf.int[,&quot;upper .95&quot;],2)
          HR               = paste0(HR, &quot; (&quot;, HR.confint.lower, &quot;-&quot;, HR.confint.upper, &quot;)&quot;)
          res              = tibble(y,beta, HR, wald.test, p.value)
          colnames(res)    = c(&quot;covariates&quot;,&quot;beta&quot;, &quot;HR (95% CI for HR)&quot;, &quot;wald.test&quot;, &quot;p.value&quot;)
          res
        }) %&gt;% 
  kable2(linhas = 7)</code></pre>
<p>Modelo de Cox usando uma vari√°vel categ√≥rica retorna uma raz√£o de risco, que, acima de 1 indica uma covari√°vel que est√° positivamente associada √† probabilidade do evento e, portanto, negativamente associada ao tempo de sobrevida. O oposto vale para HR menor que um e HR = 1 indica que a covari√°vel n√£o tem efeito.</p>
<pre class="r"><code>final_model  &lt;- 
  coxph(Surv(player_survive_time) ~ player_performance+player_dist+drive,
        data = pubg_tpp1,x=T,method=&quot;breslow&quot;)

summary(final_model)</code></pre>
<pre><code>## Call:
## coxph(formula = Surv(player_survive_time) ~ player_performance + 
##     player_dist + drive, data = pubg_tpp1, x = T, method = &quot;breslow&quot;)
## 
##   n= 200, number of events= 200 
## 
##                       coef exp(coef) se(coef)       z Pr(&gt;|z|)    
## player_performance -0.7469    0.4738   0.1787  -4.179 2.92e-05 ***
## player_dist        -1.7599    0.1721   0.1150 -15.307  &lt; 2e-16 ***
## driveyes            0.8832    2.4186   0.2091   4.225 2.39e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
##                    exp(coef) exp(-coef) lower .95 upper .95
## player_performance    0.4738     2.1105    0.3338    0.6726
## player_dist           0.1721     5.8117    0.1374    0.2156
## driveyes              2.4186     0.4135    1.6055    3.6434
## 
## Concordance= 0.883  (se = 0.008 )
## Likelihood ratio test= 362.7  on 3 df,   p=&lt;2e-16
## Wald test            = 274  on 3 df,   p=&lt;2e-16
## Score (logrank) test = 407.8  on 3 df,   p=&lt;2e-16</code></pre>
<p>No modelo ajustado note-se que existe uma associa√ß√£o negativa entre <code>player_performance</code> e mortalidade e entre <code>player_dist</code> e mortalidade (ou seja, o risco de morte diminui para jogadores que percorrem maiores dist√¢ncias e possuem melhor performance).</p>
<p>As estimativas dos par√¢metros representam o aumento no log esperado do risco relativo para cada aumento de uma unidade no preditor, mantendo os outros preditores constantes.</p>
<p>Para interpretabilidade, calcularemos as taxas de risco exponenciando das estimativas dos par√¢metros. Para a <code>player_performance</code>, <span class="math inline">\(exp(-0.7469196)= 0.4738239\)</span>. Isso implica que diminui para <span class="math inline">\(47.38\)</span> do valor original do risco esperado em rela√ß√£o a um aumento de uma unidade na performance, mantendo as demais vari√°veis constantes. A interpreta√ß√£o de <code>player_dist</code> em escala logar√≠timica √© feita de maneira semelhante.`</p>
<p>J√° para os jogadores onde <code>drive</code> = 1 (que dirigiram durante a partida) existe uma rela√ß√£o positiva, como <span class="math inline">\(exp(0.8831835)= 2.4185871\)</span>. O risco esperado corresponde √† <span class="math inline">\(2.4185871\)</span> do valor original nos que dirigiram em compara√ß√£o aos que n√£o dirigiram, mantendo as demais vari√°veis constantes.</p>
<pre class="r"><code>map2_df(1:3,final_model$coefficients %&gt;% names(),~
          tibble(
            variable = .y,
            beta             = signif(summary(final_model)$coef[.x,1], digits=2), #coeficient beta
            HR               = signif(summary(final_model)$coef[.x,2], digits=2), #exp(beta)
            HR.confint.lower = signif(summary(final_model)$conf.int[.x,&quot;lower .95&quot;], 2),
            HR.confint.upper = signif(summary(final_model)$conf.int[.x,&quot;upper .95&quot;],2)) %&gt;% 
          mutate(HR= paste0(HR, &quot; (&quot;, HR.confint.lower, &quot;-&quot;, HR.confint.upper, &quot;)&quot;)
          )
) %&gt;% kable2()</code></pre>
<p>Em suma:</p>
<ul>
<li>HR = 1: sem efeito</li>
<li>HR &lt;1: Redu√ß√£o do risco</li>
<li>HR&gt; 1: aumento do risco</li>
</ul>
<iframe src="https://giphy.com/embed/2Us3iTghyffcfeI35h" width="100%" height="200" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
<div id="res√≠duos-de-martingal-e-deviance" class="section level3">
<h3>Res√≠duos de Martingal e Deviance</h3>
<p>Como foi visto, o modelo de regress√£o de riscos proporcionais de Cox faz diversas suposi√ß√µes que precisam ser conferidas ap√≥s o ajuste do modelo para chegar a qualidade de seus resultados pois um modelo mais ajustado pode trazer resultados enganosos e que n√£o fa√ßam sentido algum</p>
<iframe src="https://giphy.com/embed/l0CLSXnSgbYma8EOA" width="100%" height="269" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
<p>Gr√°ficos dos res√≠duos Martingal ou deviance contra os tempos fornecem
uma forma de verificar a adequa√ß√£o do modelo ajustado, bem como
ajudar na detec√ß√£o de observa√ß√µes at√≠picas.</p>
<p><strong>Deviance</strong></p>
<p>Esses res√≠duos, que s√£o uma tentativa de tornar os res√≠duos
Martingal mais sim√©tricos em torno do zero, facilitam, em geral,
a detec√ß√£o de pontos at√≠picos (outliers).
Se o modelo for apropriado, esses res√≠duos devem apresentar um
comportamento aleat√≥rio em torno de zero.</p>
<p><strong>Martingal</strong></p>
<p>Esses res√≠duos s√£o vistos como uma estimativa do numero de falhas em excesso
observada nos dados mas n√£o predito pelo modelo. Os mesmos s√£o usados, em geral,
para examinar a melhor forma funcional (linear, quadr√°tica, etc.)
para uma dada covariavel em um modelo de regress√£o assumido para os dados do estudo.</p>
<pre class="r"><code>res &lt;- 
  tibble(residuo_deviance = resid(final_model,type=&quot;deviance&quot;) ,
         residuo_martingal = resid(final_model,type=&quot;martingal&quot;),
         linear_predictors = final_model$linear.predictors)

# Graficos:
grid.arrange(
  ggplot(res, aes(x=linear_predictors, y=residuo_martingal))+ geom_point()+geom_hline(yintercept=0, color=&#39;coral&#39;)+ylab(&quot;Res√≠duos Martingual&quot;),
  ggplot(res, aes(x=linear_predictors, y=residuo_deviance))+ geom_point()+geom_hline(yintercept=0, color=&#39;coral&#39;)+ylab(&quot;Deviance&quot;),
  ncol=2
)</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-22-1.png" style="width:80.0%" /></p>
</center>
<p>Interpreta√ß√£o:</p>
<ul>
<li><strong>Martingal</strong>: Parecido com deviance mais acentuado;</li>
<li><strong>Deviance</strong>: Modelo n√£o eh tao ruim assim, se fosse um modelo linear talvez dever√≠amos tomar cuidado.</li>
</ul>
<div id="residuos-de-schoenfeld" class="section level4">
<h4>Residuos de Schoenfeld</h4>
<p>Em princ√≠pio, os res√≠duos de Schoenfeld s√£o independentes do tempo.
Um gr√°fico que mostra um padr√£o n√£o aleat√≥rio contra o tempo √©
evid√™ncia de viola√ß√£o da suposi√ß√£o de hip√≥tese.</p>
<p>Para testar a suposi√ß√£o de riscos proporcionais:</p>
<pre class="r"><code>final_model %&gt;% cox.zph %&gt;% ggcoxzph</code></pre>
<center>
<p><img src="/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle_files/unnamed-chunk-23-1.png" style="width:80.0%" /></p>
</center>
<p>A partir da inspe√ß√£o gr√°fica, n√£o h√° padr√£o com o tempo.
A suposi√ß√£o de riscos proporcionais parece ser suportada
pelas covari√°veis</p>
</div>
</div>
</div>
<div id="considera√ß√µes-finais" class="section level2">
<h2>Considera√ß√µes finais</h2>
<iframe src="https://giphy.com/embed/ZacieLN2WI2AedWrz9" width="100%" height="216" frameBorder="0" class="giphy-embed" allowFullScreen>
</iframe>
<p>Como era de se esperar, o risco de ser abatido diminui para jogadores que possuem melhor performance e tamb√©m para os jogadores que percorrem maiores dist√¢ncias (o que mostra que ficar parado no jogo em uma zona pode n√£o ser a melhor ideia, j√° √© quanto mais se movimenta maior a quantidade de itens que podem ser coletados).</p>
<p>Interessante notar que a curva de <strong>sobreviv√™ncia</strong> para os jogadores que dirigiram apresenta resultado oposto ao <strong>risco</strong> esperado nos que dirigiram, isso ocorre pois esses dois modelos calculam medidas diferentes.</p>
</div>
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias</h1>
<ul>
<li>Carvalho,M.A., Andreozzi,V.L., Codec¬∏o,C.T., Campos,D.P., Barbosa,M.T.S., Shimakura,S.E., An√°lise de sobreviv√™ncia: Teoria e aplica√ß√µes em sa√∫de, Segunda Edi√ß√£o, Editora FIOCRUZ, Rio de Janeiro, 2011.</li>
<li>Colosimo,E.A., Giolo,S.R., An√°lise de sobreviv√™ncia aplicada, ABE-Projeto Fisher, S√£o Paulo, 2010</li>
<li>Lewis,E.E., Introduction to reliability engineering, John Wiley, New York, 1987</li>
<li><a href="http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Survival/BS704_Survival6.html" class="uri">http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Survival/BS704_Survival6.html</a></li>
<li><a href="http://www.sthda.com/english/wiki/cox-model-assumptions" class="uri">http://www.sthda.com/english/wiki/cox-model-assumptions</a></li>
</ul>
<p>Cuiriosidades / Leituras futuras:</p>
<ul>
<li>Evaluating Random Forests for Survival Analysis Using Prediction Error Curves: <a href="https://www.jstatsoft.org/article/view/v050i11" class="uri">https://www.jstatsoft.org/article/view/v050i11</a></li>
<li>randomForestSRC: <a href="https://cran.r-project.org/web/packages/randomForestSRC/index.html" class="uri">https://cran.r-project.org/web/packages/randomForestSRC/index.html</a></li>
<li>WTTE-RNN - Less hacky churn prediction: <a href="https://ragulpr.github.io/2016/12/22/WTTE-RNN-Hackless-churn-modeling/" class="uri">https://ragulpr.github.io/2016/12/22/WTTE-RNN-Hackless-churn-modeling/</a></li>
<li>Weibull Time To Event Recurrent Neural Network: <a href="https://github.com/ragulpr/wtte-rnn/" class="uri">https://github.com/ragulpr/wtte-rnn/</a></li>
<li>Neural Networks as Statistical Methods in Survival Analysis: <a href="https://www.stats.ox.ac.uk/pub/bdr/NNSM.pdf" class="uri">https://www.stats.ox.ac.uk/pub/bdr/NNSM.pdf</a></li>
<li>Continuous and Discrete Time Survival Analysis: Neural Network
Approaches: <a href="http://pcwww.liv.ac.uk/~afgt/eleuteri_lyon07.pdf" class="uri">http://pcwww.liv.ac.uk/~afgt/eleuteri_lyon07.pdf</a></li>
<li>Cox Proportional Hazards Model - h2O Documentation: <a href="http://s3.amazonaws.com/h2o-release/h2o/master/1579/docs-website/datascience/coxph.html" class="uri">http://s3.amazonaws.com/h2o-release/h2o/master/1579/docs-website/datascience/coxph.html</a></li>
<li>Introduction to H2OCoxPH: <a href="https://www.slideshare.net/0xdata/introduction-to-h2ocoxph" class="uri">https://www.slideshare.net/0xdata/introduction-to-h2ocoxph</a></li>
</ul>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2018-11-28-pubg-sobrevivencia-kaggle/pubg-sobrevivencia-kaggle/">An√°lise de sobreviv√™ncia com dados do jogo PUBG dispon√≠veis no Kaggle</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Fundamentos de Data Science</category>
      <category>Intelig√™ncia Artificial</category>
      <category>Machine Learning</category>
      <category>Programa√ß√£o e Ferramentas</category>
      <category domain="tag">analise-de-sobrevivencia</category>
      <category domain="tag">data-mining</category>
      <category domain="tag">estatistica</category>
      <category domain="tag">gamificacao</category>
      <category domain="tag">gamification</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">modelagem-estatistica</category>
      <category domain="tag">r</category>
      <category domain="tag">survivor</category>
    </item>
    <item>
      <title>Um estudo sobre modelos de aprendizagem baseados em √°rvores com desafio do Kaggle</title>
      <link>https://gomesfellipe.github.io/post/2018-08-31-modelos-em-arvore/modelos-em-arvore/</link>
      <pubDate>Fri, 31 Aug 2018 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2018-08-31-modelos-em-arvore/modelos-em-arvore/</guid>
      <description>Um estudo aplicado de modelos de aprendizagem baseados em √°rvores utilizando a base de dados do Kaggle para prever o pre√ßo final de casas residenciais em Ames, Iowa, utilizando uma variedade de aspectos</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="kaggle" class="section level1">
<h1>Kaggle</h1>
<p>Segundo o <a href="https://en.wikipedia.org/wiki/Kaggle">Wikip√©dia</a>: ‚ÄúKaggle √© a maior comunidade mundial de cientistas de dados e machine learning.‚Äù Aprendo muito estudando as resolu√ß√µes de alguns competidores pois l√° √© poss√≠vel conferir tanto as metodologias utilizadas pelos competidores quando os c√≥digos e √© not√°vel o cuidado dos participantes para que seja poss√≠vel a reprodutibilidade dos resultados, o que pode impulsionar o aprendizado.</p>
<p>O Kaggle trabalha com a ideia de <a href="https://en.wikipedia.org/wiki/Gamification">gamifica√ß√£o</a>, que √© um assunto do qual j√° escrevi em um post sobre <a href="https://gomesfellipe.github.io/post/2018-02-17-cheatsheet-gamificacao-r/cheatsheet-gamificacao-r/">gamifica√ß√£o e porque aprender R √© t√£o divertido</a> e gosto deste conceito de se criar jogos para motivar e engajar as pessoas em atividades profissionais e a ideia de se estar em um jogo possibilita doses de motiva√ß√£o especialmente a quem gosta de competir.</p>
<p>A plataforma √© focada em competi√ß√µes que envolvem modelagem preditiva, que julgam apenas o seu desempenho preditivo, embora a inteligibilidade n√£o deixe de ser importante. Neste post farei tamb√©m a modelagem descritiva com modelos de aprendizagem baseados em √°rvores, na qual o principal objetivo ser√° obter informa√ß√µes sobre os dados para o ajuste dos modelos preditivos que iremos submeter √† competi√ß√£o do Kaggle <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/">House Prices: Advanced Regression Techniques</a>.</p>
<p>A diferen√ßa entre modelos preditivos e descritivos n√£o √© t√£o rigorosa assim pois algumas das t√©cnicas podem ser utilizadas para ambos e geralmente um modelo pode servir para ambos os prop√≥sitos (mesmo que de de forma insuficiente).</p>
<p>Al√©m dos modelos de machine learning baseados em √°rvores, tamb√©m ser√° ajustado um modelo de regress√£o linear multivariado para compararmos os resultados dos ajustes e submeter nossas previs√µes no site do <a href="https://kaggle.com">kaggle</a>.</p>
<p>Os pacotes que ser√£o utilizados ser√£o os seguintes:</p>
<pre class="r"><code>library(purrr)       # Programacao funciona
library(broom)       # Arrumar outputs
library(dplyr)       # Manipulacao de dados
library(magrittr)    # pipes
library(funModeling) # df_status()
library(plyr)        # revalue()
library(gridExtra)   # Juntar ggplots
library(reshape)     # funcao melt()
library(rpart)       # Arvore de Decisoes
library(rpart.plot)  # Plot da Arvore de Decisoes
library(data.table)  # aux na manipulacao do heatmap
library(readr)       # Leitura da base de dados
library(stringr)     # Manipulacao de strings
library(ggplot2)     # Graficos elegantes
library(caret)       # Machine Learning 
library(GGally)      # up ggplot
library(ggfortify)   # autoplot()</code></pre>
<div id="base-de-dados" class="section level2">
<h2>Base de dados</h2>
<p>A base de dados deste post vem de uma competi√ß√£o √≥tima para estudantes de ci√™ncia de dados de dados com alguma experi√™ncia com R ou Python e no√ß√µes b√°sicas de machine learning e estat√≠stica.</p>
<p>Pode ser √∫til para aqueles que desejam expandir seu conjunto de habilidades em uma tarefa de regress√£o, quando a vari√°vel <span class="math inline">\(y\)</span> que desejamos estimar √© do tipo num√©rico (cont√≠nuo ou discreto).</p>
<p>Trata-se do <a href="https://ww2.amstat.org/publications/jse/v19n3/decock.pdf">conjunto de dados Ames Housing</a> que foi compilado por Dean De Cock para uso em educa√ß√£o de ci√™ncia de dados.</p>
<pre class="r"><code>train &lt;- read_csv(&quot;train.csv&quot;)
test  &lt;- read_csv(&quot;test.csv&quot;)
full  &lt;- bind_rows(train, test)

id    &lt;- test$Id
full %&lt;&gt;% select(-Id)</code></pre>
<div id="descri√ß√£o-da-competi√ß√£o" class="section level3">
<h3>Descri√ß√£o da Competi√ß√£o</h3>
<p>Traduzido do site oficial do kaggle:</p>
<p>"Pe√ßa a um comprador que descreva a casa dos seus sonhos, e eles provavelmente n√£o come√ßar√£o com a altura do teto do por√£o ou a proximidade de uma ferrovia leste-oeste. Mas o conjunto de dados desta competi√ß√£o de playground prova que muito mais influencia as negocia√ß√µes de pre√ßo do que o n√∫mero de quartos ou uma cerca branca.</p>
<p>Com 79 vari√°veis explicativas descrevendo (quase) todos os aspectos de casas residenciais em Ames, Iowa, esta competi√ß√£o desafia voc√™ a prever o pre√ßo final de cada casa."</p>
<p>Portanto, primeiramente vamos entender o comportamento da vari√°vel resposta, depois buscar quais dessas 79 vari√°veis explicativas s√£o mais importantes para representar a varia√ß√£o do pre√ßo de venda das casas atrav√©s dos m√©todos baseados em √°rvores e por fim ajustar os modelos propostos e submeter nossas estimativas no site!</p>
</div>
</div>
</div>
<div id="an√°lise-explorat√≥ria-dos-dados" class="section level1">
<h1>An√°lise explorat√≥ria dos dados</h1>
<p>Antes de pensar em ajustar algum modelo √© extremamente necess√°rio entender como se comportam os dados, portanto, tanto a vari√°vel resposta quanto as vari√°veis explicativas ser√£o avaliadas.</p>
<div id="vari√°vel-resposta" class="section level2">
<h2>Vari√°vel resposta:</h2>
<p><code>SalePrice</code> - o pre√ßo de venda da propriedade em d√≥lares. Essa √© a vari√°vel de destino que estamos tentando prever.</p>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Note que a distribui√ß√£o dos dados referentes ao pre√ßo de venda se distribui de maneira assim√©trica e n√£o possuem evid√™ncias de normalidade dos dados. Apesar dos m√©todos baseados em √°rvore se tratarem de t√©cnicas n√£o param√©tricas essa transforma√ß√£o ser√° feita pois ao final deste post desejo comparar os resultados com um modelo de regress√£o linear m√∫ltipla.</p>
</div>
</div>
<div id="√°rvore-de-decis√£o" class="section level1">
<h1>√Årvore de decis√£o</h1>
<p>Uma t√©cnica muito popular que √© mais comumente usada para resolver tarefas de classifica√ß√£o de dados por√©m a √°rvore conhecida como <a href="https://tinyurl.com/ybhlsgom">CART (Classification and Regression Trees)(Breiman, 1986)</a> lida com todos os tipos de atributos (incluindo atributos num√©ricos que s√£o tratados a partir da cria√ß√£o de intervalos). Para seu ajuste √© poss√≠vel realizar podas e produzir √°rvores bin√°rias.</p>
<p>A constru√ß√£o da √°rvore √© realizada por meio do algoritmo que iterativamente analisa os atributos descritivos de um conjunto de dados previamente rotulado. Sua popularidade como apoio para a tomada de decis√£o se deve principalmente ao fato da f√°cil visualiza√ß√£o do conhecimento gerado e o f√°cil entendimento.</p>
<p>Outra caracter√≠stica legal da √°rvore de decis√µes √© que ela permite ajustar um modelo sem um pr√©-processamento detalhado, pois √© f√°cil de ajustar, aceita valores faltantes e √© de f√°cil interpreta√ß√£o, veja:</p>
<pre class="r"><code>library(rpart)

control &lt;- rpart.control(minsplit =10, # o n√∫mero m√≠nimo de observa√ß√µes em um n√≥
                         cp = 0.006    # parametro de complexidade q controla o tamanho da arvore
)
rpartFit &lt;- rpart(exp(SalePrice) ~ . , train, method = &quot;anova&quot;, control = control) 

rpart.plot::rpart.plot(rpartFit,cex = 0.6)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-5-1.png" width="1200" /></p>
<p>No topo, vemos o primeiro n√≥ com 100% das observa√ß√µes, que representa o total da base (100%). Em seguida, vemos que a primeira vari√°vel que determina o pre√ßo de venda das casas <code>SalePrice</code> √© a vari√°vel <code>OverallQual</code>. As casas que apresentaram <code>OverallQual</code> &lt; 7.5 ocorrem em maior propor√ß√£o do que as que tiveram <code>OverallQual</code>&gt;7.5. A interpreta√ß√£o pode continuar dessa forma recursivamente.</p>
<p>√â poss√≠vel notar que as vari√°veis <code>OverallQual</code>,<code>Neighborhood</code>,<code>1stFlrSF</code>,<code>2ndFlrSF</code>,<code>GrLivArea</code>, <code>BsmtFinSF1</code> foram as que melhor representaram os dados de acordo com os par√¢metros que determinamos para ajustar esta √°rvore, vejamos com mais detalhes se existe rela√ß√£o linear e intensidade e dire√ß√£o dessa rela√ß√£o com o <a href="https://pt.wikipedia.org/wiki/Coeficiente_de_correla%C3%A7%C3%A3o_de_Pearson">coeficiente de correla√ß√£o de Pearson</a> entre estas vari√°veis dois a dois e em rela√ß√£o √† vari√°vel resposta:</p>
<pre class="r"><code>devtools::source_url(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/correlations_for_ggpairs.R&quot;)

train %&gt;% 
  select(SalePrice,OverallQual,`1stFlrSF`,`2ndFlrSF`,GrLivArea,BsmtFinSF1) %&gt;% 
  ggpairs(lower = list(continuous = my_fn))+
  theme_bw()</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Com esta figura temos muitas informa√ß√µes, destaca-se que todas essas vari√°veis possuem algum tipo de rela√ß√£o linear com a vari√°vel resposta, a menor correla√ß√£o observada foi com o <code>BsmtFinSF1</code> e a vari√°vel que apresentou a maior correla√ß√£o foi a <code>OverallQual</code>. Aten√ß√£o para a correla√ß√£o entre <code>SalePrice</code> e <code>OverallQual</code>, pois <code>Overallqual</code> parece ser uma vari√°vel ordinal e uma outra medida de correla√ß√£o que melhor representaria esta rela√ß√£o √© o <a href="https://pt.wikipedia.org/wiki/Coeficiente_de_correla%C3%A7%C3%A3o_de_postos_de_Spearman">coeficiente de correla√ß√£o de Spearman</a>, veja:</p>
<pre class="r"><code>cor(full$SalePrice, full$OverallQual, method = &quot;spearman&quot;, use = &quot;complete.obs&quot;)</code></pre>
<pre><code>## [1] 0.8098286</code></pre>
<p>Um pouco diferente do resultado da correla√ß√£o de Pearson pois avalia rela√ß√µes lineares, j√° a correla√ß√£o de Spearman avalia rela√ß√µes mon√≥tonas, sejam elas lineares ou n√£o.</p>
<div id="an√°lise-explorat√≥ria-e-input-de-nas" class="section level2 tabset">
<h2>An√°lise explorat√≥ria e input de <code>NA</code>s</h2>
<p>Arrumar a base de dados √© uma tarefa longa e que geralmente consome grande parte no tempo em um projeto de ci√™ncia de dados. N√£o adianta usar o algor√≠timo mais poderoso de machine learning se a base de dados n√£o estiver arrumada de maneira que possibilite a an√°lise dos dados.</p>
<p>Para obter informa√ß√µes da amostra, confira no <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data">link do dataset da competi√ß√£o no Kaggle</a>. Na p√°gina √© poss√≠vel conferir <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/download/data_description.txt">a descri√ß√£o da amostra</a> e nela nota-se que alguns dos valores faltantes possuem significado, ent√£o √© necess√°rio rotul√°-los para que o R possa interpretar estes valores da maneira correta.</p>
<div id="status-da-amostra" class="section level3">
<h3>Status da amostra</h3>
<p>Conferindo o status da amostra com a fun√ß√£o <code>df_status()</code> do pacote <a href="https://cran.r-project.org/web/packages/funModeling/index.html"><code>funModeling</code></a>:</p>
<pre class="r"><code>full %&gt;% 
  df_status(print_results = F) %&gt;% 
  as_tibble() %&gt;%
  arrange(-p_na, -p_zeros)</code></pre>
<pre><code>## # A tibble: 80 x 9
##    variable     q_zeros p_zeros  q_na  p_na q_inf p_inf type      unique
##    &lt;chr&gt;          &lt;int&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;int&gt;
##  1 PoolQC             0       0  2909 99.7      0     0 character      3
##  2 MiscFeature        0       0  2814 96.4      0     0 character      4
##  3 Alley              0       0  2721 93.2      0     0 character      2
##  4 Fence              0       0  2348 80.4      0     0 character      4
##  5 SalePrice          0       0  1459 50.0      0     0 numeric      663
##  6 FireplaceQu        0       0  1420 48.6      0     0 character      5
##  7 LotFrontage        0       0   486 16.6      0     0 numeric      128
##  8 GarageYrBlt        0       0   159  5.45     0     0 numeric      103
##  9 GarageFinish       0       0   159  5.45     0     0 character      3
## 10 GarageQual         0       0   159  5.45     0     0 character      5
## # ‚Ä¶ with 70 more rows</code></pre>
<p>Note que as vari√°veis problem√°ticas foram ordenadas de forma decrescente (maior n√∫mero de dados faltantes e zeros) vamos tratar uma de cada vez partindo da vari√°vel mais cr√≠tica</p>
</div>
<div id="pool" class="section level3">
<h3>Pool</h3>
<ul>
<li><code>PoolQC</code> √© a vari√°vel que possui mais <code>NA</code> e a descri√ß√£o da base informa que:</li>
</ul>
<p><code>PoolQC</code>: qualidade da piscina</p>
<ul>
<li>Ex Excelente</li>
<li>Gd Good</li>
<li>TA M√©dia / T√≠pica</li>
<li>Fa Pequena</li>
<li>NA sem piscina</li>
</ul>
<p>√â poss√≠vel observar que se trata de uma vari√°vel ordinal, portanto vamos criar uma vari√°vel auxiliar (pois esta descri√ß√£o se repete em outras vari√°veis):</p>
<pre class="r"><code># Criando vari√°vel auxilar ordinal
Qualidade &lt;- c(&#39;None&#39; = 0, &#39;Po&#39; = 1, &#39;Fa&#39; = 2, &#39;TA&#39; = 3, &#39;Gd&#39; = 4, &#39;Ex&#39; = 5)

full %&lt;&gt;%
  mutate(PoolQC =  ifelse(PoolQC %&gt;% is.na, &quot;None&quot;, PoolQC) %&gt;% as.factor() ) %&gt;% 
  mutate(PoolQC = as.integer(revalue(PoolQC, Qualidade)))</code></pre>
<p>Al√©m disso, existe outra vari√°vel relacionada √† piscina, veja:</p>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Pool&quot;)]) %&gt;% 
  table </code></pre>
<pre><code>##         PoolQC
## PoolArea    1    2    3    4
##      0      0    0    0 2906
##      144    1    0    0    0
##      228    1    0    0    0
##      368    0    0    0    1
##      444    0    0    0    1
##      480    0    0    1    0
##      512    1    0    0    0
##      519    0    1    0    0
##      555    1    0    0    0
##      561    0    0    0    1
##      576    0    0    1    0
##      648    0    1    0    0
##      738    0    0    1    0
##      800    0    0    1    0</code></pre>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Pool&quot;)]) %&gt;%
  map(~sum(is.na(.x)))</code></pre>
<pre><code>## $PoolArea
## [1] 0
## 
## $PoolQC
## [1] 0</code></pre>
<pre class="r"><code># Arrumando inconsist√´ncias:
full %&lt;&gt;% 
  mutate(PoolQC = ifelse(PoolQC == 0 &amp; PoolArea !=0, 2, PoolQC))

# Arrumando inconsist√´ncias:
full %&lt;&gt;% 
  mutate(Pool = ifelse(PoolQC == 0 &amp; PoolArea ==0, &quot;no&quot;, &quot;yes&quot;))</code></pre>
</div>
<div id="misc" class="section level3">
<h3>Misc</h3>
<p>Se referem aos recursos diversos</p>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Misc&quot;)],
         SalePrice
  ) %&gt;%
  map(~sum(is.na(.x)))</code></pre>
<pre><code>## $MiscFeature
## [1] 2814
## 
## $MiscVal
## [1] 0
## 
## $SalePrice
## [1] 1459</code></pre>
<p><code>MiscFeature</code>: recurso diverso n√£o coberto em outras categorias</p>
<ul>
<li>Elevador elev</li>
<li>Gar2 2nd Garage (se n√£o for descrito na se√ß√£o de garagem)</li>
<li>Othr Outro</li>
<li>Galp√£o derramado (mais de 100 SF)</li>
<li>TenC Campo de t√©nis</li>
<li>NA Nenhum</li>
</ul>
<p>Desta vez n√£o se trata de uma vari√°vel ordinal, vejamos:</p>
<pre class="r"><code>full %&lt;&gt;%
  mutate(MiscFeature =  if_else(MiscFeature %&gt;% is.na, &quot;None&quot;, MiscFeature) %&gt;% as.factor) 

# Breve resumo:
g1 &lt;- 
  full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Misc&quot;)], SalePrice) %&gt;% 
  ggplot(aes(y=MiscVal,x= reorder(MiscFeature, -MiscVal,FUN = median) ,fill=MiscFeature))+
  geom_boxplot()+
  theme_bw() +
  scale_fill_viridis_d() +
  labs(x = &quot;Recurso Diverso&quot;)

g2 &lt;- 
  full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Misc&quot;)], SalePrice) %&gt;% 
  ggplot(aes(y=SalePrice,x= reorder(MiscFeature, -MiscVal,FUN = median) ,fill=MiscFeature))+
  geom_boxplot()+
  theme_bw() +
  scale_fill_viridis_d() +
  labs(x = &quot;Pre√ßo de Venda&quot;)

grid.arrange(g1, g2)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<pre class="r"><code>rm(g1,g2)</code></pre>
<p>Al√©m disso, <code>MiscVal</code>: Valor do recurso variado</p>
</div>
<div id="alley" class="section level3">
<h3>Alley</h3>
<p><code>Alley</code>: Tipo de acesso ao beco para a propriedade</p>
<ul>
<li>Grvl Cascalho</li>
<li>Pave pavimentado</li>
<li>NA Nenhum acesso de beco</li>
</ul>
<p>Basta realizar o input:</p>
<pre class="r"><code>full %&lt;&gt;% 
  mutate(Alley = Alley %&gt;% str_replace_na(&quot;None&quot;)) %&gt;% 
  mutate(Alley = as.factor(Alley))</code></pre>
<pre class="r"><code>full[!is.na(full$SalePrice),] %&gt;% 
  select(Alley, SalePrice) %&gt;% 
  ggplot(aes(y=SalePrice,x= reorder(Alley, -SalePrice,FUN = median) ,fill=Alley))+
  geom_boxplot()+
  theme_bw() +
  scale_fill_viridis_d() +
  labs(x = &quot;tipo de Acesso&quot;)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
</div>
<div id="fence" class="section level3">
<h3>Fence</h3>
<p><code>Fence</code>: qualidade da cerca</p>
<ul>
<li>GdPrv Boa privacidade</li>
<li>MnPrv minima privacidade</li>
<li>GdWo boa madeira</li>
<li>MnWw M√≠nima Madeira / Fio</li>
<li>NA Sem cerca</li>
</ul>
<p>Input ser√° da seguinte forma:</p>
<pre class="r"><code>full %&lt;&gt;% 
  mutate(Fence = Fence %&gt;% str_replace_na(&quot;None&quot;))</code></pre>
<pre class="r"><code>full[1:nrow(train),] %&gt;% 
  select(Fence, SalePrice) %&gt;% 
  ggplot(aes(y=SalePrice,x= reorder(Fence, -SalePrice, median) ,fill=Fence))+
  geom_boxplot()+
  theme_bw() +
  scale_fill_viridis_d() +
  labs(x = &quot;tipo de Acesso&quot;)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<pre class="r"><code>full %&lt;&gt;% mutate(Fence = as.factor(Fence))</code></pre>
<p>Aparentemente n√£o parece existir uma rela√ß√£o ordinal sobre o tipo de cerca quanto ao pre;o de venda da casa, portanto foi convertida para fator</p>
</div>
<div id="fireplace" class="section level3">
<h3>FirePlace</h3>
<p>Vari√°veis relacionadas com lareira. Segundo a descri√ß√£o, temos:</p>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Fireplace&quot;)], SalePrice)</code></pre>
<pre><code>## # A tibble: 2,919 x 3
##    Fireplaces FireplaceQu SalePrice
##         &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;
##  1          0 &lt;NA&gt;             12.2
##  2          1 TA               12.1
##  3          1 TA               12.3
##  4          1 Gd               11.8
##  5          1 TA               12.4
##  6          0 &lt;NA&gt;             11.9
##  7          1 Gd               12.6
##  8          2 TA               12.2
##  9          2 TA               11.8
## 10          2 TA               11.7
## # ‚Ä¶ with 2,909 more rows</code></pre>
<p><code>Fireplaces</code>: Numero de lareiras</p>
<p><code>FireplaceQu</code>: Qualidade da lareira</p>
<ul>
<li>Ex Excellente - Excepcional Lareira de Alvenaria</li>
<li>Gd Boa - Lareira de alvenaria no n√≠vel principal</li>
<li>TA M√©dia - lareira pr√©-fabricada na sala principal ou Lareira de alvenaria no por√£o</li>
<li>Fa Pequena - Lareira pr√©-fabricada no por√£o</li>
<li>Po Pobre - Fog√£o Ben Franklin</li>
<li>NA sem lareira</li>
</ul>
<p>Nota-se que se trata de uma vari√°vel ordinal de acordo com a qualidade, portanto:</p>
<pre class="r"><code>full %&lt;&gt;% 
  mutate(FireplaceQu =  if_else(FireplaceQu %&gt;% is.na, &quot;None&quot;, FireplaceQu) ) %&gt;% 
  mutate(FireplaceQu = as.integer(revalue(FireplaceQu, Qualidade)))</code></pre>
<p>Conferindo se existem inconsist√™ncias:</p>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Fireplace&quot;)]) %&gt;% 
  table </code></pre>
<pre><code>##           FireplaceQu
## Fireplaces    0    1    2    3    4    5
##          0 1420    0    0    0    0    0
##          1    0   46   63  495  627   37
##          2    0    0   10   92  112    5
##          3    0    0    1    4    5    1
##          4    0    0    0    1    0    0</code></pre>
</div>
<div id="lot" class="section level3">
<h3>Lot</h3>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Lot&quot;)], SalePrice) %&gt;% 
  map_dbl(~sum(is.na(.x)))</code></pre>
<pre><code>## LotFrontage     LotArea    LotShape   LotConfig   SalePrice 
##         486           0           0           0        1459</code></pre>
<p>Segundo a descri√ß√£o:</p>
<p><code>LotFrontage</code>: Ruas linearmente conectadas √† propriedade</p>
<p><code>LotArea</code> : Tamanho do lote em p√©s quadrados</p>
<p><code>LotShape</code>: forma geral da propriedade</p>
<ul>
<li>Regue Regular<br />
</li>
<li>IR1 ligeiramente irregular</li>
<li>IR2 moderadamente irregular</li>
<li>IR3 Irregular</li>
</ul>
<p><code>LotConfig</code>: configura√ß√£o de lote</p>
<ul>
<li>Inside Lote muito para dentro</li>
<li>Corner Canto de esquina</li>
<li>CulDSac Cul-de-sac</li>
<li>FR2 Frente em 2 lados da propriedade</li>
<li>FR3 Frente em 3 lados da propriedade</li>
</ul>
<p>Input para o <code>LotFrontage</code> ser√° feito considerando a configura√ß√£o do lote, veja:</p>
<pre class="r"><code>inputsLot &lt;- full %&gt;% 
  select(LotFrontage, LotConfig) %&gt;% 
  group_by(LotConfig) %&gt;%
  dplyr::summarise(Media = mean(LotFrontage, na.rm = T),
            Mediana = median(LotFrontage, na.rm = T))

full$LotFrontage[is.na(full$LotFrontage) &amp; full$LotConfig == inputsLot$LotConfig[1]] &lt;- inputsLot$Mediana[1] 
full$LotFrontage[is.na(full$LotFrontage) &amp; full$LotConfig == inputsLot$LotConfig[2]] &lt;- inputsLot$Mediana[2] 
full$LotFrontage[is.na(full$LotFrontage) &amp; full$LotConfig == inputsLot$LotConfig[3]] &lt;- inputsLot$Mediana[3] 
full$LotFrontage[is.na(full$LotFrontage) &amp; full$LotConfig == inputsLot$LotConfig[4]] &lt;- inputsLot$Mediana[4] 
full$LotFrontage[is.na(full$LotFrontage) &amp; full$LotConfig == inputsLot$LotConfig[5]] &lt;- inputsLot$Mediana[5] </code></pre>
<p>Arrumando vari√°veis nominais e ordinais:</p>
<pre class="r"><code>full %&lt;&gt;% 
  mutate(LotShape = as.integer(revalue(full$LotShape, c(&#39;IR3&#39;=0, &#39;IR2&#39;=1, &#39;IR1&#39;=2, &#39;Reg&#39;=3))))</code></pre>
</div>
<div id="garages" class="section level3">
<h3>Garages</h3>
<p>Vari√°veis relacionadas, segundo a descri√ß√£o, temos:</p>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Garage&quot;)], SalePrice) %&gt;% 
  map_dbl(~sum(is.na(.x)))</code></pre>
<pre><code>##   GarageType  GarageYrBlt GarageFinish   GarageCars   GarageArea   GarageQual 
##          157          159          159            1            1          159 
##   GarageCond    SalePrice 
##          159         1459</code></pre>
<p><code>GarageType</code>: localiza√ß√£o da garagem</p>
<ul>
<li>2Types Mais de um tipo de garagem</li>
<li>Attchd anexa a casa</li>
<li>Basement tipo porao</li>
<li>BuiltIn (garagem parte da casa - normalmente tem sala acima da garagem)</li>
<li>CarPort Porta do carro</li>
<li>Detchd nao anexa a casa</li>
<li>NA Sem Garagem</li>
</ul>
<p><code>GarageYrBlt</code>: garagem do ano foi constru√≠da</p>
<p><code>GarageFinish</code>: acabamento interior da garagem</p>
<ul>
<li>Fin Finished</li>
<li>RFn √Åspero Finalizado<br />
</li>
<li>Unf inacabado</li>
<li>NA Sem Garagem</li>
</ul>
<p><code>GarageCars</code>: Tamanho da garagem na capacidade do carro</p>
<p><code>GarageArea</code>: Tamanho da garagem em p√©s quadrados</p>
<p><code>GarageQual</code>: GarageQuality</p>
<ul>
<li>Ex Excelente</li>
<li>Gd Good</li>
<li>TA T√≠pico / M√©dio</li>
<li>FA Justo</li>
<li>Po Poor</li>
<li>NA Sem Garagem</li>
</ul>
<p><code>GarageCond</code>: condi√ß√£o de garagem</p>
<ul>
<li>Ex Excelente</li>
<li>Gd Good</li>
<li>TA T√≠pico / M√©dio</li>
<li>Fa Justo</li>
<li>Po Poor</li>
<li>NA Sem Garagem</li>
</ul>
<pre class="r"><code>full %&lt;&gt;% 
  mutate(GarageType   =  if_else(GarageType %&gt;% is.na, &quot;None&quot;, GarageType) ) %&gt;% 
  mutate(GarageYrBlt  = if_else(GarageYrBlt %&gt;% is.na,YearBuilt, GarageYrBlt) ) %&gt;% 
  mutate(GarageFinish =  if_else(GarageFinish %&gt;% is.na, &quot;None&quot;, GarageFinish) ) %&gt;% 
  mutate(GarageFinish = as.integer(revalue(GarageFinish, c(&#39;None&#39;=0, &#39;Unf&#39;=1, &#39;RFn&#39;=2, &#39;Fin&#39;=3)))) %&gt;% 
  mutate(GarageCars   = ifelse(GarageCars %&gt;% is.na, 0, GarageCars) ) %&gt;% 
  mutate(GarageArea   = ifelse(GarageArea %&gt;% is.na, 0, GarageArea)) %&gt;% 
  mutate(GarageQual   = if_else(GarageQual %&gt;% is.na, &quot;None&quot;, GarageQual)) %&gt;% 
  mutate(GarageQual   = as.integer(revalue(GarageQual, Qualidade))) %&gt;% 
  mutate(GarageCond   = if_else(GarageCond %&gt;% is.na, &quot;None&quot;, GarageCond)) %&gt;% 
  mutate(GarageCond   = as.integer(revalue(GarageCond, Qualidade))) 
  
table(full$GarageCond)</code></pre>
<pre><code>## 
##    0    1    2    3    4    5 
##  159   14   74 2654   15    3</code></pre>
</div>
<div id="bsmt" class="section level3">
<h3>Bsmt</h3>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;Bsmt&quot;)], SalePrice) %&gt;% 
  map_dbl(~sum(is.na(.x)))</code></pre>
<pre><code>##     BsmtQual     BsmtCond BsmtExposure BsmtFinType1   BsmtFinSF1 BsmtFinType2 
##           81           82           82           79            1           80 
##   BsmtFinSF2    BsmtUnfSF  TotalBsmtSF BsmtFullBath BsmtHalfBath    SalePrice 
##            1            1            1            2            2         1459</code></pre>
<p><code>BsmtQual</code>: Avalia a altura do por√£o</p>
<ul>
<li>Ex Excelente (100+ polegadas)<br />
</li>
<li>Gd Bom (90-99 polegadas)</li>
<li>TA T√≠pica (80-89 polegadas)</li>
<li>Fa Justo (70-79 polegadas)</li>
<li>Po Pobre (&lt;70 polegadas</li>
<li>NA Sem Por√£o</li>
</ul>
<p><code>BsmtCond</code>: Avalia o estado geral do por√£o</p>
<ul>
<li>Ex Excelente</li>
<li>Gd Bom</li>
<li>TA T√≠pica - umidade ligeira permitida</li>
<li>Fa Razo√°vel - umidade ou alguma rachadura ou sedimenta√ß√£o</li>
<li>Po Insuficiente - Craqueamento severo, sedimenta√ß√£o ou umidade</li>
<li>NA Sem Por√£o</li>
</ul>
<p><code>BsmtExposure</code>: Refere-se a paralisa√ß√µes ou paredes no n√≠vel do jardim</p>
<ul>
<li>Gd Good Exposi√ß√£o</li>
<li>Av M√©dia Exposi√ß√£o (n√≠veis divididos ou foyers normalmente pontua√ß√£o m√©dia ou acima)<br />
</li>
<li>Mn Exposi√ß√£o M√≠nima</li>
<li>No N√£o Exposi√ß√£o</li>
<li>NA Sem por√£o</li>
</ul>
<p><code>BsmtFinType1</code>: Avalia√ß√£o da √°rea acabada do por√£o</p>
<ul>
<li>GLQ Bons Viver</li>
<li>ALQ M√©dia Living Quarters</li>
<li>BLQ Abaixo da m√©dia Living Quarters<br />
</li>
<li>Rec M√©dia Rec Room</li>
<li>LwQ Baixa Qualidade</li>
<li>Unf unfinshed</li>
<li>NA nenhum por√£o</li>
</ul>
<p><code>BsmtFinSF1</code>: pes quadrados do tipo 1 terminado</p>
<p><code>BsmtFinType2</code>: Avalia√ß√£o do por√£o √°rea terminado (se v√°rios tipos)</p>
<ul>
<li>GLQ Bons aposentos</li>
<li>ALQ Medianos</li>
<li>BLQ abaixo da media</li>
<li>Rec Aposentos m√©dia qualidade</li>
<li>LwQ Baixa Qualidade</li>
<li>Unf</li>
<li>N√£o Sem Por√£o</li>
</ul>
<p><code>BsmtFinSF2</code>: P√©s quadrados acabados do Tipo 2</p>
<p><code>BsmtUnfSF</code>: P√©s quadrados inacabados da √°rea do por√£o</p>
<p><code>TotalBsmtSF</code>: Total p√©s quadrados da √°rea do por√£o</p>
<p>Input das vari√°veis n√£o num√©ricas com <code>None</code> e convertendo para ordinal as vari√°veis com rela√ß√£o de ordem. Para os faltantes das vari√°veis num√©ricas foram imputados o valor 0 (zeros).</p>
<pre class="r"><code># Categ√≥ricos:
full[,names(full)[names(full) %&gt;% str_detect(&quot;Bsmt&quot;)]] &lt;- 
  full[,names(full)[names(full) %&gt;% str_detect(&quot;Bsmt&quot;)]] %&gt;%
  select(names(full)[names(full) %&gt;% str_detect(&quot;Bsmt&quot;)]) %&gt;%
  mutate_if( ~ !is.numeric(.x) , ~ ifelse(is.na(.x), &quot;None&quot;, .x)) %&gt;% 
  mutate(BsmtQual = as.integer(revalue(BsmtQual, Qualidade))) %&gt;% 
  mutate(BsmtCond = as.integer(revalue(BsmtCond, Qualidade))) %&gt;% 
  mutate(BsmtExposure = as.integer(revalue(BsmtExposure, c(&#39;None&#39;=0, &#39;No&#39;=1, &#39;Mn&#39;=2, &#39;Av&#39;=3, &#39;Gd&#39;=4)))) %&gt;% 
  mutate(BsmtFinType1 = as.integer(revalue(BsmtFinType1,c(&#39;None&#39;=0, &#39;Unf&#39;=1, &#39;LwQ&#39;=2, &#39;Rec&#39;=3, &#39;BLQ&#39;=4, &#39;ALQ&#39;=5, &#39;GLQ&#39;=6)))) 

# Num√©ricos:
full[,names(full)[names(full) %&gt;% str_detect(&quot;Bsmt&quot;)]] &lt;- 
  full[,names(full)[names(full) %&gt;% str_detect(&quot;Bsmt&quot;)]] %&gt;%
  select(names(full)[names(full) %&gt;% str_detect(&quot;Bsmt&quot;)]) %&gt;%
  mutate_if( ~ is.numeric(.x) , ~ ifelse(is.na(.x), 0, .x))</code></pre>
</div>
<div id="masvnr" class="section level3">
<h3>MasVnr</h3>
<pre class="r"><code>full %&gt;% 
  select(names(full)[names(full) %&gt;% str_detect(&quot;MasVnr&quot;)], SalePrice) %&gt;% 
  map_dbl(~sum(is.na(.x)))</code></pre>
<pre><code>## MasVnrType MasVnrArea  SalePrice 
##         24         23       1459</code></pre>
<p><code>MasVnrType</code>: Alvenaria tipo de verniz</p>
<ul>
<li>BrkCmn Brick Common</li>
<li>BrkFace Face de tijolos</li>
<li>CBlock Bloco cinza</li>
<li>None Nenhum</li>
<li>Stone Pedra</li>
</ul>
<p><code>MasVnrArea</code>: √Årea de folheado de alvenaria em p√©s quadrados</p>
<pre class="r"><code>full %&lt;&gt;% 
  mutate(MasVnrType = if_else(is.na(MasVnrType), &quot;None&quot;, MasVnrType)) %&gt;% 
  mutate(MasVnrType = as.integer(revalue(MasVnrType, c(&#39;None&#39;=0, &#39;BrkCmn&#39;=0, &#39;BrkFace&#39;=1, &#39;Stone&#39;=2)))) %&gt;% 
  mutate(MasVnrArea = if_else(is.na(MasVnrArea), 0, 1))</code></pre>
</div>
<div id="vari√°veis-restantes-com-poucos-na" class="section level3">
<h3>Vari√°veis restantes com poucos <code>NA</code></h3>
<p>A estrat√©gia adotada para imputar estes dados ser√° tomada de maneira arbitr√°ria. Os valores faltantes ser√£o preenchidos com o valor comum mais frequente daquela vari√°vel. As vari√°veis que restam s√£o:</p>
<pre class="r"><code>full %&gt;% 
  df_status(print_results = F) %&gt;% 
  as_tibble() %&gt;%
  arrange(-p_na, -p_zeros)</code></pre>
<pre><code>## # A tibble: 81 x 9
##    variable    q_zeros p_zeros  q_na  p_na q_inf p_inf type      unique
##    &lt;chr&gt;         &lt;int&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;int&gt;
##  1 SalePrice         0     0    1459 50.0      0     0 numeric      663
##  2 MSZoning          0     0       4  0.14     0     0 character      5
##  3 Utilities         0     0       2  0.07     0     0 character      2
##  4 Functional        0     0       2  0.07     0     0 character      7
##  5 Exterior1st       0     0       1  0.03     0     0 character     15
##  6 Exterior2nd       0     0       1  0.03     0     0 character     16
##  7 Electrical        0     0       1  0.03     0     0 character      5
##  8 KitchenQual       0     0       1  0.03     0     0 character      4
##  9 SaleType          0     0       1  0.03     0     0 character      9
## 10 PoolArea       2906    99.6     0  0        0     0 numeric       14
## # ‚Ä¶ with 71 more rows</code></pre>
<p>Vejamos:</p>
<p><code>MSZoning</code>: Identifica a classifica√ß√£o geral de zoneamento da venda.</p>
<ul>
<li>Ser√° convertida para fator, vari√°vel nominal</li>
</ul>
<p><code>KitchenQual</code>: Qualidade da cozinha</p>
<ul>
<li>Ser√° convertida para ordinal</li>
</ul>
<p><code>Utilities</code>: Tipo de utilidade dispon√≠vel</p>
<ul>
<li>Ser√° removida</li>
</ul>
<p><code>Functional</code>: Funcionalidade dom√©stica</p>
<ul>
<li>Ser√° considerada como ordinal</li>
</ul>
<p><code>Exterior1st</code>: revestimento Exterior em casa</p>
<ul>
<li>Convertida para fator, vari√°vel nominal</li>
</ul>
<p><code>Electrical</code>: Sistema el√©trico</p>
<ul>
<li>Convertida para fator, vari√°vel nominal</li>
</ul>
<p><code>SaleType</code>: Tipo de venda</p>
<ul>
<li>Convertida para fator, vari√°vel nominal</li>
</ul>
<pre class="r"><code>full &lt;- full %&gt;% 
  mutate(MSZoning    = ifelse(is.na(MSZoning),
                            full$MSZoning %&gt;% table %&gt;% sort %&gt;% names %&gt;% last, MSZoning)) %&gt;% 
  mutate(MSZoning    = as.factor(MSZoning)) %&gt;% 
  mutate(KitchenQual = ifelse(is.na(KitchenQual),
                            full$KitchenQual %&gt;% 
                              table %&gt;% sort %&gt;% names %&gt;% last, KitchenQual)) %&gt;% 
  mutate(KitchenQual = as.integer(revalue(as.character(full$KitchenQual), Qualidade))) %&gt;% 
  select(-Utilities) %&gt;% 
  mutate(Exterior1st = ifelse(is.na(Exterior1st),
                            full$Exterior1st %&gt;% table %&gt;% sort %&gt;% names %&gt;% last, Exterior1st)) %&gt;% 
  mutate(Exterior1st = as.factor(Exterior1st)) %&gt;% 
  mutate(Exterior2nd = ifelse(is.na(Exterior2nd),
                            full$Exterior2nd %&gt;% table %&gt;% sort %&gt;% names %&gt;% last, Exterior2nd)) %&gt;% 
  mutate(Exterior2nd = as.factor(Exterior2nd)) %&gt;% 
  mutate(Electrical  = ifelse(is.na(Electrical),
                            full$Electrical %&gt;% table %&gt;% sort %&gt;% names %&gt;% last, Electrical)) %&gt;% 
  mutate(Electrical  = as.factor(Electrical)) %&gt;% 
  mutate(SaleType    = ifelse(is.na(SaleType ),
                            full$SaleType  %&gt;% table %&gt;% sort %&gt;% names %&gt;% last, SaleType )) %&gt;% 
  mutate(SaleType    = as.factor(SaleType )) 


full[is.na(full$Functional),&quot;Functional&quot;] &lt;- full$Functional %&gt;% table %&gt;% sort %&gt;% names %&gt;% last
full$Functional = as.integer(revalue(full$Functional, c(&#39;Sal&#39;=0, &#39;Sev&#39;=1, &#39;Maj2&#39;=2, &#39;Maj1&#39;=3, &#39;Mod&#39;=4, &#39;Min2&#39;=5, &#39;Min1&#39;=6, &#39;Typ&#39;=7)))
full[is.na(full$KitchenQual),&quot;KitchenQual&quot;] &lt;- full$KitchenQual %&gt;% table %&gt;% sort %&gt;% names %&gt;% last %&gt;% as.numeric()
full$KitchenQual = as.integer(revalue(as.character(full$KitchenQual), Qualidade))
# full[is.na(full$Electrical),&quot;Electrical&quot;] &lt;- 3

to_remove &lt;- full %&gt;% map(~table(.x) %&gt;% length()) %&gt;% .[.== 1] %&gt;% names()
full &lt;- full %&gt;% select(-one_of(to_remove))</code></pre>
<p>Status da base no momento:</p>
<pre class="r"><code>full %&gt;% 
  df_status(print_results = F) %&gt;% 
  as_tibble() %&gt;%
  arrange(-p_na,-p_zeros, type)</code></pre>
<pre><code>## # A tibble: 79 x 9
##    variable      q_zeros p_zeros  q_na  p_na q_inf p_inf type    unique
##    &lt;chr&gt;           &lt;int&gt;   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt;
##  1 SalePrice           0     0    1459  50.0     0     0 numeric    663
##  2 PoolArea         2906    99.6     0   0       0     0 numeric     14
##  3 3SsnPorch        2882    98.7     0   0       0     0 numeric     31
##  4 LowQualFinSF     2879    98.6     0   0       0     0 numeric     36
##  5 MiscVal          2816    96.5     0   0       0     0 numeric     38
##  6 BsmtHalfBath     2744    94       0   0       0     0 numeric      3
##  7 ScreenPorch      2663    91.2     0   0       0     0 numeric    121
##  8 BsmtFinSF2       2572    88.1     0   0       0     0 numeric    272
##  9 EnclosedPorch    2460    84.3     0   0       0     0 numeric    183
## 10 HalfBath         1834    62.8     0   0       0     0 numeric      3
## # ‚Ä¶ with 69 more rows</code></pre>
<p>Transformando o <code>character</code> para <code>factor</code>:</p>
<pre class="r"><code>full %&lt;&gt;% mutate_if(is.character, as.factor)</code></pre>
<p>Transformando novamente nossa base de treino e de teste:</p>
<pre class="r"><code>train &lt;- full[1:nrow(train),] %&gt;% as.data.frame() 
test  &lt;- full[(nrow(train)+1):nrow(full),] %&gt;% select(-SalePrice) %&gt;% as.data.frame()

# # Input Missing
# train_miss_model = preProcess(train, &quot;knnImpute&quot;)
# train = predict(train_miss_model, train)
# test = predict(train_miss_model, test)
# 
# train$SalePrice &lt;- y</code></pre>
</div>
</div>
</div>
<div id="machine-learning-com-algor√≠tmos-de-aprendizagem-baseados-em-√°rvores" class="section level1">
<h1>Machine Learning com algor√≠tmos de aprendizagem baseados em √°rvores</h1>
<p>Os m√©todos baseados em √°rvores fornecem modelos preditivos de alta precis√£o, estabilidade e facilidade de interpreta√ß√£o. Ao contr√°rio dos modelos lineares, eles s√£o capazes de lidar bem com rela√ß√µes n√£o-lineares al√©m de poderem ser adaptados para resolver tanto problemas de classifica√ß√£o quanto problemas de regress√£o.</p>
<p>Algoritmos como √°rvores de decis√£o, random forest e ‚Äúgradient boosting‚Äù est√£o sendo muito usados em todos os tipos de problemas de data science e √© not√°vel o uso desses algor√≠timos para resolver os desafios do <a href="https://www.kaggle.com/">Kaggle</a>. Para resolver este problema utilizaremos estes tr√™s algoritmos e ao final, pegando carona na sele√ß√£o de vari√°veis para os algoritmos de √°rvore, ser√° ajustado um modelo de regress√£o linear para compararmos e conferirmos a signific√¢ncia estat√≠stica de cada uma das vari√°veis.</p>
<div id="varimp-com-random-forest" class="section level2">
<h2>VarImp com Random Forest</h2>
<p>Um dos benef√≠cios da floresta aleat√≥ria √© o poder de lidar com grande conjunto de dados com maior dimensionalidade e identificar as vari√°veis a import√¢ncia das vari√°veis, que pode ser uma caracter√≠stica muito √∫til por√©m deve ser feita com cautela.</p>
<p>Veja uma reflex√£o (traduzida) da <a href="https://www.stat.berkeley.edu/~breiman/RandomForests/reg_philosophy.htm">nota de Leo Breiman (Universidade da Calif√≥rnia em Berkeley)</a></p>
<blockquote>
<p>‚ÄúUma nota filos√≥fica: RF √© um exemplo de uma ferramenta que √© √∫til para fazer an√°lises de dados cient√≠ficos; Mas os algoritmos mais inteligentes n√£o substituem a intelig√™ncia humana e o conhecimento dos dados do problema; Pegue a sa√≠da de florestas aleat√≥rias n√£o como verdade absoluta, mas como suposi√ß√µes geradas por um computador inteligente que podem ser √∫teis para levar a uma compreens√£o mais profunda do problema.‚Äù</p>
</blockquote>
<p>O ajuste da √°rvore ser√° feito com o pacote <code>caret</code> e o estudo de estimativas de erro foi definido como o <a href="https://en.wikipedia.org/wiki/Out-of-bag_error">Out of bag</a> que remove a necessidade de um conjunto de teste pois √© o erro m√©dio de previs√£o em cada amostra de treinamento <span class="math inline">\(x_i\)</span> , usando apenas as √°rvores que n√£o tinham <span class="math inline">\(x_i\)</span> em sua amostra de <a href="https://www.ime.usp.br/~chang/home/mae5704/aula-bootstrap.pdf">bootstrap</a>.</p>
<pre class="r"><code>set.seed(1)
control &lt;- trainControl(method = &quot;oob&quot;,verboseIter = F)

rfFit1 &lt;- train(SalePrice ~. ,
      data=train,
      method=&quot;rf&quot;,
      metric = &quot;Rsquared&quot;,
      trControl = control,
      preProcess = c(&quot;knnImpute&quot;)
      )

randomForest::varImpPlot(rfFit1$finalModel)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<pre class="r"><code>rfFit1$finalModel$importance %&gt;% 
  as.data.frame %&gt;%
  mutate(row = rownames(.)) %&gt;% 
  arrange(desc(IncNodePurity)) %&gt;% 
  as_tibble()</code></pre>
<pre><code>## # A tibble: 217 x 2
##    IncNodePurity row        
##            &lt;dbl&gt; &lt;chr&gt;      
##  1         77.9  OverallQual
##  2         35.0  GrLivArea  
##  3         14.8  YearBuilt  
##  4         11.5  KitchenQual
##  5          9.75 TotalBsmtSF
##  6          9.29 GarageCars 
##  7          6.74 `1stFlrSF` 
##  8          6.33 GarageArea 
##  9          5.02 ExterQualTA
## 10          4.04 BsmtFinSF1 
## # ‚Ä¶ with 207 more rows</code></pre>
<p>Ap√≥s inspecionar a import√¢ncia das vari√°veis vamos selecionar as seguintes vari√°veis:</p>
<pre class="r"><code>full %&lt;&gt;% 
  select(
    SalePrice  , Neighborhood, OverallQual , GrLivArea   , YearBuilt   ,  KitchenQual, 
    GarageCars ,  GarageArea , `1stFlrSF`  , ExterQual   , BsmtFinSF1  , FireplaceQu, 
    BsmtQual   , `2ndFlrSF`  , CentralAir  , GarageFinish, YearRemodAdd, FullBath, 
    GarageYrBlt, Fireplaces  , LotFrontage , BsmtUnfSF   , TotalBsmtSF , BsmtFinType1,
    OpenPorchSF, GarageType  , BsmtExposure, OverallCond , TotalBsmtSF , LotArea
  )</code></pre>
<p>Portanto, vamos definir novamente o conjunto de dados de treino e de teste:</p>
<pre class="r"><code>train &lt;- full[1:nrow(train),] %&gt;% as.data.frame()
test  &lt;- full[(nrow(train)+1):nrow(full),-1] %&gt;% as.data.frame()</code></pre>
</div>
<div id="vari√°veis-num√©ricas" class="section level2">
<h2>Vari√°veis num√©ricas</h2>
<p>Ap√≥s a sele√ß√£o dessas vari√°veis, vamos entender como elas est√£o correlacionadas dois a dois com o <a href="https://pt.wikipedia.org/wiki/Coeficiente_de_correla%C3%A7%C3%A3o_de_Pearson">coeficiente de correla√ß√£o de pearson</a>, exibindo a matrix em um <a href="https://en.wikipedia.org/wiki/Heat_map">Heatmap</a> (ou mapa de calor ), que √© uma representa√ß√£o gr√°fica de dados em que os valores individuais contidos em uma matriz representados como cores.</p>
<pre class="r"><code>cormat &lt;- 
  full %&gt;% 
  select(SalePrice, everything()) %&gt;% 
  select_if(is.numeric) %&gt;% 
  as.data.frame() %&gt;% 
  cor(use = &quot;na.or.complete&quot;) %&gt;% 
  melt

cormat %&gt;%   
  ggplot( aes(reorder(Var1,value), reorder(Var2,value), fill=value))+
  geom_tile(color=&quot;white&quot;)+
  scale_fill_gradient2(low=&quot;blue&quot;, high=&quot;red&quot;, mid=&quot;white&quot;, midpoint=0, limit=c(-1,1), space=&quot;Lab&quot;, name=&quot;Pearson\nCorrelation&quot;)+
  theme_bw()+
  theme(axis.text.x=element_text(angle=45, vjust=1, size=10, hjust=1))+
  coord_fixed()+
  labs(x=&quot;&quot;,y=&quot;&quot;)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-37-1.png" width="1152" /></p>
<p>√â poss√≠vel notar que existem vari√°veis explicativas correlacionadas o que indica que a presen√ßa de algumas vari√°veis pode possivelmente interferir no ajuste final do modelo linear multivariado.</p>
</div>
<div id="vari√°veis-categ√≥ricas" class="section level2">
<h2>Vari√°veis categ√≥ricas</h2>
<p>J√° a rela√ß√£o das var√°veis categ√≥ricas n√£o podem ser calculada com o coeficiente de correla√ß√£o calculado anteriormente, para avaliar como elas est√£o associadas ser√° calculado a medida de associa√ß√£o <a href="https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V">V de Cram√©r</a>. Novamente a matrix dos resultados ser√£o novamente apresentados em um <a href="https://en.wikipedia.org/wiki/Heat_map">Heatmap</a> (ou mapa de calor ) que foi inspirado <a href="http://analysingstuffs.xyz/2017/12/01/visualizing-the-correlations-between-categorical-variables-with-r-a-cramers-v-heatmap/">neste post</a> (neste post tamb√©m √© apresentada uma fun√ß√£o para o c√°lculo da matrix, adaptei de forma que se tornasse mais geral e disponibilizei no meu github <a href="https://github.com/gomesfellipe/functions/blob/master/interaction_all.R">neste link</a>).</p>
<pre class="r"><code># Carrega funcao que calcula o V de Cramer:
devtools::source_url(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/cv_test.R&quot;)
# Carrega a funcao que realiza as intera√ß√µes dos calculos dois a dois:
devtools::source_url(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/interaction_all.R&quot;)</code></pre>
<p>Veja:</p>
<pre class="r"><code>cvmat &lt;- 
train %&gt;%
  select_if(~!is.numeric(.x)) %&gt;% 
  as.data.table() %&gt;%
  interaction_all(cv_test) %&gt;% 
  as_tibble() 

cvmat %&gt;% 
  ggplot( aes(variable_x, variable_y, fill=v_cramer))+
  geom_tile(color=&quot;white&quot;)+
  scale_fill_gradient2(low=&quot;blue&quot;, high=&quot;red&quot;, mid=&quot;white&quot;, midpoint=0, limit=c(-1,1), space=&quot;Lab&quot;, name=&quot;Cramer&#39;s V&quot;)+
  theme_bw()+
  theme(axis.text.x=element_text(angle=45, vjust=1, size=10, hjust=1))+
  coord_fixed()+
  labs(x=&quot;&quot;,y=&quot;&quot;)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
</div>
</div>
<div id="ajustando-modelos" class="section level1">
<h1>Ajustando modelos</h1>
<div id="arvore-de-decisao" class="section level2">
<h2>Arvore de decisao</h2>
<p>O modelo de √°rvore de decis√£o j√° foi comentado e deixei algumas refer√™ncias ao final do post portanto vejamos a seguir o ajusto no R. Segundo a <a href="https://cran.r-project.org/web/packages/rpart/rpart.pdf">documenta√ß√£o</a>:</p>
<p><code>cp</code>: par√¢metro de complexidade. No nosso caso isso significa que o <a href="https://pt.wikipedia.org/wiki/R%C2%B2"><span class="math inline">\(R^2\)</span></a> total deve aumentar em cp em cada etapa. O principal papel desse par√¢metro √© economizar tempo de computa√ß√£o removendo as divis√µes que obviamente n√£o valem a pena. Essencialmente, informamos ao programa que qualquer divis√£o que n√£o melhore o ajuste por <code>cp</code> provavelmente ser√° eliminada por <a href="https://pt.wikipedia.org/wiki/Valida%C3%A7%C3%A3o_cruzada">valida√ß√£o cruzada</a>, e que, portanto, o programa n√£o precisa busc√°-la.</p>
<p>Para pesquisa de grade existem duas maneiras de ajustar um algoritmo no pacote <code>caret</code>: permitir que o sistema fa√ßa isso automaticamente ou especificar o <code>tuneGride</code> manualmente onde cada par√¢metro do algoritmo pode ser especificado como um vetor de valores poss√≠veis. Confira o ajuste manual em R:</p>
<pre class="r"><code>set.seed(1)

control &lt;- trainControl(method = &quot;cv&quot;, number = 5,verboseIter = F)

tunegrid &lt;- expand.grid(cp=seq(0.001, 0.01, 0.001))

rpartFit2 &lt;- 
  train(y=train$SalePrice, x=train[,-1],
        method=&quot;rpart&quot;,
        trControl=control,
        tuneGrid=tunegrid,
        metric = &quot;Rsquared&quot;
  )
rpartFit2</code></pre>
<pre><code>## CART 
## 
## 1460 samples
##   28 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 1169, 1169, 1168, 1168, 1166 
## Resampling results across tuning parameters:
## 
##   cp     RMSE       Rsquared   MAE      
##   0.001  0.1918932  0.7757730  0.1386651
##   0.002  0.1943654  0.7690391  0.1410967
##   0.003  0.2016485  0.7513005  0.1457213
##   0.004  0.2029596  0.7462748  0.1457752
##   0.005  0.2098812  0.7279462  0.1534384
##   0.006  0.2090073  0.7291130  0.1539830
##   0.007  0.2110066  0.7227211  0.1544402
##   0.008  0.2120734  0.7198280  0.1555415
##   0.009  0.2142488  0.7143975  0.1570535
##   0.010  0.2148236  0.7126454  0.1575360
## 
## Rsquared was used to select the optimal model using the largest value.
## The final value used for the model was cp = 0.001.</code></pre>
<p>Podemos conferir os resultados novamente de maneira visual:</p>
<pre class="r"><code>rpart.plot(rpartFit2$finalModel, cex = 0.5)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-41-1.png" width="1200" /></p>
<p>Gerando arquivo para submiss√£o no kaggle:</p>
<pre class="r"><code>id %&gt;% cbind(predict(rpartFit2, test) %&gt;% exp) %&gt;% 
  `colnames&lt;-`(c(&quot;Id&quot;, &quot;SalePrice&quot;)) %&gt;%
  write.csv(&quot;rpartFit2.csv&quot;,row.names = F)</code></pre>
</div>
<div id="bagging" class="section level2">
<h2>Bagging</h2>
<p><a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating">‚ÄúBagging‚Äù</a> √© usado quando desejamos reduzir a varia√ß√£o de uma √°rvore de decis√£o. Ela combina o resultado de v√°rios modelos onde todas as vari√°veis s√£o considerados para divis√£o um n√≥. Em R:</p>
<pre class="r"><code>set.seed(1)

control &lt;- trainControl(method = &quot;cv&quot;, number = 5,verboseIter = F)

treebagFit &lt;- train(y=train$SalePrice, 
                    x=train[,-1], 
                    method = &quot;treebag&quot;,
                    metric = &quot;Rsquared&quot;,
                    trControl=control
)
treebagFit</code></pre>
<pre><code>## Bagged CART 
## 
## 1460 samples
##   28 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 1169, 1169, 1168, 1168, 1166 
## Resampling results:
## 
##   RMSE       Rsquared   MAE      
##   0.1831872  0.7946059  0.1288626</code></pre>
<p>Note que o <span class="math inline">\(R^2\)</span> aumentou e o <a href="https://en.wikipedia.org/wiki/Root-mean-square_deviation"><span class="math inline">\(RMSE\)</span></a> diminuiu ap√≥s o uso desta t√©cnica.</p>
<p>Resultados para enviar para o Kaggle:</p>
<pre class="r"><code>id %&gt;% cbind(predict(treebagFit, test)%&gt;% exp) %&gt;% 
  `colnames&lt;-`(c(&quot;Id&quot;, &quot;SalePrice&quot;)) %&gt;%
  write.csv(&quot;treebagFit.csv&quot;,row.names = F)</code></pre>
</div>
<div id="random-forest" class="section level2">
<h2>Random Forest</h2>
<p>A principal diferen√ßa entre ‚Äúbagging‚Äù e o algoritmo Random Forest √© que em <code>randomForest</code>, apenas um subconjunto de caracter√≠sticas √© selecionado aleatoriamente em cada divis√£o em uma √°rvore de decis√£o enquanto que no bagging todos os recursos s√£o usados.</p>
<p>Para pesquisa de grade especificaremos um vetor com os poss√≠veis valores, <a href="https://cran.r-project.org/web/packages/randomForest/randomForest.pdf">pois o default adotado para o par√¢metro</a> <code>mtry</code> √© <code>mtry</code> = p/3 (N√∫mero de vari√°veis amostradas aleatoriamente como candidatos em cada divis√£o), onde p √© o n√∫mero de vari√°veis e pode ser que o modelo se ajuste melhor aos dados ao utilizar outro valor.</p>
<p>Veja:</p>
<pre class="r"><code>set.seed(1)

tunegrid &lt;- expand.grid(mtry = seq(4, ncol(train) * 0.8, 2))

control &lt;- trainControl(method = &quot;cv&quot;, number = 5,verboseIter = F)

rfFit &lt;- train(SalePrice ~. ,
               data=train,
               method=&quot;rf&quot;,
               metric = &quot;Rsquared&quot;,
               tuneGrid=tunegrid,
               trControl=control
)
rfFit</code></pre>
<pre><code>## Random Forest 
## 
## 1460 samples
##   28 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 1169, 1169, 1168, 1168, 1166 
## Resampling results across tuning parameters:
## 
##   mtry  RMSE       Rsquared   MAE       
##    4    0.1455656  0.8781772  0.09755474
##    6    0.1417368  0.8817193  0.09435674
##    8    0.1405084  0.8826370  0.09350712
##   10    0.1395367  0.8834153  0.09290816
##   12    0.1385338  0.8845102  0.09181049
##   14    0.1386865  0.8840165  0.09223527
##   16    0.1381776  0.8846283  0.09155563
##   18    0.1384532  0.8837305  0.09222536
##   20    0.1380863  0.8840803  0.09173754
##   22    0.1383788  0.8835938  0.09189772
## 
## Rsquared was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 16.</code></pre>
<p>Note que o <span class="math inline">\(R^2\)</span> aumentou e o <span class="math inline">\(RMSE\)</span> apresentou resultados ainda mais satisfat√≥rios.</p>
<p>Veja visualmente a import√¢ncia de ada vari√°vel:</p>
<pre class="r"><code>randomForest::varImpPlot(rfFit$finalModel)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
<p>Resultados para enviar para o Kaggle:</p>
<pre class="r"><code>id %&gt;% cbind(predict(rfFit, test) %&gt;% exp) %&gt;% 
  `colnames&lt;-`(c(&quot;Id&quot;, &quot;SalePrice&quot;)) %&gt;%
  write.csv(&quot;rfFit.csv&quot;,row.names = F) </code></pre>
</div>
<div id="gbm" class="section level2">
<h2>GBM</h2>
<p>Diferentemente do ‚Äúbagging‚Äù, o ‚Äúboosting‚Äù √© uma t√©cnica de ensemble (conjunto) na qual os preditores n√£o s√£o feitos independentemente, mas sequencialmente. Na imagem a seguir √© poss√≠vel ver uma representa√ß√£o visual dessa diferen√ßa:</p>
<p><img src="https://cdn-images-1.medium.com/max/1600/1*PaXJ8HCYE9r2MgiZ32TQ2A.png" /></p>
<p>A imagem foi obtida <a href="https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d">neste artigo: Gradient Boosting from scratch</a>, recomendo a leitura pois da uma boa intui√ß√£o de como o algoritmo funciona.</p>
<p>Para a pesquisa de grade vamos permitir que o sistema fa√ßa isso automaticamente configurando apenas o <code>tuneLength</code> para indicar o n√∫mero de valores diferentes para cada par√¢metro do algoritmo.</p>
<pre class="r"><code>set.seed(1)

control &lt;- trainControl(method = &quot;cv&quot;, number = 5,verboseIter = F)

gbmFit &lt;- train(SalePrice~.,data=train,
                method = &quot;gbm&quot;,
                trControl=control,
                tuneLength=5,
                metric = &quot;Rsquared&quot;,
                verbose = FALSE
)
gbmFit</code></pre>
<pre><code>## Stochastic Gradient Boosting 
## 
## 1460 samples
##   28 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 1169, 1169, 1168, 1168, 1166 
## Resampling results across tuning parameters:
## 
##   interaction.depth  n.trees  RMSE       Rsquared   MAE       
##   1                   50      0.1736970  0.8346902  0.12145158
##   1                  100      0.1474386  0.8663694  0.10371271
##   1                  150      0.1400060  0.8775141  0.09804851
##   1                  200      0.1381902  0.8803999  0.09607709
##   1                  250      0.1375854  0.8817130  0.09502881
##   2                   50      0.1511051  0.8640075  0.10557294
##   2                  100      0.1379357  0.8815852  0.09546142
##   2                  150      0.1360260  0.8846503  0.09326628
##   2                  200      0.1355702  0.8852090  0.09248558
##   2                  250      0.1362827  0.8841734  0.09254710
##   3                   50      0.1434808  0.8743589  0.09910961
##   3                  100      0.1363881  0.8838715  0.09355652
##   3                  150      0.1346606  0.8868808  0.09163759
##   3                  200      0.1339427  0.8880370  0.09062153
##   3                  250      0.1336666  0.8886732  0.08979366
##   4                   50      0.1376575  0.8824442  0.09516571
##   4                  100      0.1334392  0.8884173  0.09192150
##   4                  150      0.1330866  0.8890336  0.09156893
##   4                  200      0.1334706  0.8886198  0.09096598
##   4                  250      0.1335809  0.8884950  0.09101981
##   5                   50      0.1384852  0.8813449  0.09535954
##   5                  100      0.1350803  0.8863344  0.09231165
##   5                  150      0.1340246  0.8878172  0.09112111
##   5                  200      0.1342892  0.8874590  0.09088714
##   5                  250      0.1349331  0.8867525  0.09104875
## 
## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1
## 
## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10
## Rsquared was used to select the optimal model using the largest value.
## The final values used for the model were n.trees = 150, interaction.depth =
##  4, shrinkage = 0.1 and n.minobsinnode = 10.</code></pre>
<p>Note que este foi o modelo que apresentou os melhores resultados quanto s√≥ <span class="math inline">\(R^2\)</span> e ao <span class="math inline">\(RMSE\)</span> em compara√ß√£o com os outros modelos.</p>
<p>Submiss√£o para Kaggle:</p>
<pre class="r"><code>id %&gt;% cbind(predict(gbmFit, test) %&gt;% exp) %&gt;%
  `colnames&lt;-`(c(&quot;Id&quot;, &quot;SalePrice&quot;)) %&gt;%
  write.csv(&quot;gbmFit.csv&quot;, row.names = F)</code></pre>
</div>
<div id="regress√£o-linear" class="section level2">
<h2>Regress√£o Linear</h2>
<p>Por fim faremos o ajuste de um modelo de regress√£o linear multivariado utilizando o pacote caret.</p>
<p>Utilizaremos valida√ß√£o cruzada separando nossa amostra em 5 e utilizaremos o m√©todo <code>lmStepAIC</code> que realiza a sele√ß√£o do modelo escalonado pelo crit√©rio de informa√ß√£o de Akaike - <a href="https://en.wikipedia.org/wiki/Akaike_information_criterion">AIC</a>.</p>
<pre class="r"><code>set.seed(1)

control &lt;- trainControl(method = &quot;cv&quot;, number = 5,verboseIter = F)

lmFit &lt;- train(SalePrice~.,data=train,
               method = &quot;lmStepAIC&quot;,
               trControl=control,
               metric = &quot;Rsquared&quot;,trace=F
)
lmFit</code></pre>
<pre><code>## Linear Regression with Stepwise Selection 
## 
## 1460 samples
##   28 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 1169, 1169, 1168, 1168, 1166 
## Resampling results:
## 
##   RMSE      Rsquared   MAE       
##   0.147716  0.8632513  0.09574552</code></pre>
<p>Note que o ajuste do modelo se apresenta de maneira satisfat√≥ria com <span class="math inline">\(R^2\)</span> e <span class="math inline">\(RMSE\)</span> semelhantes aos modelos de <code>bagging</code> e <code>boosting</code> e al√©m disso, diferente dos modelos baseados em √°rvore, com este ajuste √© poss√≠vel notar a signific√¢ncia estat√≠stica de cada par√¢metro ajustado, o que possibilita tanto o uso tanto como modelo preditivo quanto como modelo descritivo. Veja:</p>
<pre class="r"><code>ggcoef(
  lmFit$finalModel,                      #O modelo a ser conferido
  vline_color = &quot;red&quot;,          #Reta em zero  
  errorbar_color = &quot;blue&quot;,      #Cor da barra de erros
  errorbar_height = .25,
  shape = 18,                   #Altera o formato dos pontos centrais
  size=2,                      #Altera o tamanho do ponto
  color=&quot;black&quot;,
  exclude_intercept = TRUE,                #Altera a cor do ponto
  mapping = aes(x = estimate, y = term, size = p.value))+
  scale_size_continuous(trans = &quot;reverse&quot;)+ #Essa linha faz com que inverta o tamanho
  theme_bw()</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-51-1.png" width="672" /></p>
<p>Note que o intercepto <span class="math inline">\(\beta_0\)</span> foi retirado da imagem pois √© muito superior aos demais coeficientes. Note tamb√©m que <span class="math inline">\(\beta_i\)</span> informa qu√£o sens√≠vel √© <span class="math inline">\(y\)</span>, no caso <code>log(SalePrice)</code> √†s varia√ß√µes de cara umas das <span class="math inline">\(x_{i,j}\)</span> vari√°veis explicativas. Mais concretamente, se <span class="math inline">\(x_{i,j}\)</span> aumenta em uma unidade, o valor de <span class="math inline">\(y\)</span> varia em <span class="math inline">\(\beta_1\)</span> unidades.</p>
<p>Uma r√°pida <a href="http://www.portalaction.com.br/analise-de-regressao/analise-dos-residuos">An√°lise dos Res√≠duos</a>:</p>
<pre class="r"><code>lmFit$finalModel %&gt;% 
  autoplot(which = 1:2) + 
  theme_bw()</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-52-1.png" width="1500" /></p>
<p>√â poss√≠vel notar que parece haver alguns outliers em ambas as figuras. Na primeira √© poss√≠vel notar uma nuvem de pontos aleat√≥rios em torno de zero por√©m na segunda figura nota-se que alguns valores n√£o est√£o de acordo com os quantils te√≥ricos de uma distribui√ß√£o normal, o que pode prejudicar nossa interpreta√ß√£o dos coeficientes do modelo. Vamos encerrar o modelo por aqui mesmo e ver como ele se sai na competi√ß√£o do Kaggle, preparando a submiss√£o:</p>
<pre class="r"><code>id %&gt;% cbind(predict(lmFit, test) %&gt;% exp ) %&gt;% 
  `colnames&lt;-`(c(&quot;Id&quot;, &quot;SalePrice&quot;)) %&gt;%
  write.csv(&quot;lmFit.csv&quot;,row.names = F)</code></pre>
<p>O score obtido com esta submiss√£o no Kaggle foi muito pr√≥ximo dos modelos baseados e √°rvore e o tempo computacional para este ajuste foi bem menor.</p>
</div>
<div id="comparando-ajustes" class="section level2">
<h2>Comparando ajustes</h2>
<p>Vejamos a seguir uma compara√ß√£o entre estes modelos com as fun√ß√µes fornecidas pelo pacote `caret:.</p>
<pre class="r"><code>resamps &lt;- resamples(list(rpart = rpartFit2,
                          treebag = treebagFit,
                          rf = rfFit,
                          gbm = gbmFit,
                          lm = lmFit 
                          )) 
bwplot(resamps)</code></pre>
<p><img src="/post/2018-08-31-modelos-em-arvore/modelos-em-arvore_files/figure-html/unnamed-chunk-54-1.png" width="672" /></p>
<p>Com este gr√°fico √© poss√≠vel notar que o modelo de regress√£o linear m√∫ltipla apresentou resultados semelhantes aos de bagging e boosting.</p>
<p>√â importante frisar que a maneira como as vari√°veis foram selecionadas para o modelo de regress√£o linear m√∫ltipla atrav√©s da import√¢ncia das vari√°veis obtida com o modelo randomForest n√£o √© um padr√£o e existem diversos outros modos estat√≠sticos de se de determinar a signific√¢ncia e a rela√ß√£o das vari√°veis para o modelo.</p>
<p>Um poss√≠vel problema neste m√©todo √© que n√£o detecta a multicolinearidade, que ocorre quando as vari√°veis explicativas est√£o fortemente correlacionadas entre si e a an√°lise de regress√£o linear pode ficar confusa e desprovida de significado, pois h√° dificuldade em distinguir o efeito de uma ou outra vari√°vel explicativa sobre a vari√°vel resposta <span class="math inline">\(Y\)</span> devido √† vari√¢ncias muito elevadas ou sinais inconsistentes.</p>
<p>Essa proposta de aprender se divertindo e de maneira produtiva me deixa muito empolgado, espero que tenham se divertido como eu me diverti fazendo este post!</p>
</div>
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias:</h1>
<ul>
<li><a href="https://www.datacamp.com/courses/machine-learning-with-tree-based-models-in-r">DataCamp Course:Machine Learning with Tree-Based Models in R</a></li>
<li><a href="https://tinyurl.com/y796aa4t">Data Science <em>for</em> Business</a></li>
<li><a href="https://lethalbrains.com/learn-ml-algorithms-by-coding-decision-trees-439ac503c9a4">Learn ML Algorithms by coding: Decision Trees</a></li>
<li><a href="https://www.datacamp.com/community/tutorials/decision-trees-R">DataCamp Tutorials: Decision Trees in R</a></li>
<li><a href="https://topepo.github.io/caret/">The caret Package - Max Kuhn</a></li>
<li><a href="https://www.vooo.pro/insights/um-tutorial-completo-sobre-a-modelagem-baseada-em-tree-arvore-do-zero-em-r-python/">Um tutorial completo sobre modelagem baseada em √°rvores de decis√£o (c√≥digos R e Python)</a></li>
<li><a href="https://machinelearningmastery.com/tuning-machine-learning-models-using-the-caret-r-package/">Tuning Machine Learning Models Using the Caret R Package</a></li>
<li><a href="https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d">Gradient Boosting from scratch</a></li>
<li><a href="https://machinelearningmastery.com/tune-machine-learning-algorithms-in-r/">Tune Machine Learning Algorithms in R (random forest case study)</a></li>
<li><a href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_manual.htm">Random Forests - Leo Breiman and Adele Cutler</a></li>
<li><a href="https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf">An Introduction to Recursive Partitioning Using the RPART Routines - CRAN</a></li>
</ul>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2018-08-31-modelos-em-arvore/modelos-em-arvore/">Um estudo sobre modelos de aprendizagem baseados em √°rvores com desafio do Kaggle</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Analise Explorat√≥ria</category>
      <category>Aprendizado Supervisionado</category>
      <category>Data mining</category>
      <category>Estatistica</category>
      <category>Machine Learning</category>
      <category>Pr√°tica</category>
      <category>Probabilidade</category>
      <category>R</category>
      <category>modelo baseado em arvores</category>
      <category>kaggle</category>
      <category>Regress√£o</category>
      <category domain="tag">Data Mining</category>
      <category domain="tag">Estatistica</category>
      <category domain="tag">gomesfellipe</category>
      <category domain="tag">kaggle</category>
      <category domain="tag">Correlacoes</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">modelagem estatistica</category>
      <category domain="tag">Pr√°tica</category>
      <category domain="tag">R</category>
      <category domain="tag">regression</category>
      <category domain="tag">caret</category>
      <category domain="tag">xgboost</category>
      <category domain="tag">random forest</category>
      <category domain="tag">decisiontree</category>
    </item>
    <item>
      <title>Brasil x Argentina, tidytext e Machine Learning</title>
      <link>https://gomesfellipe.github.io/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml/</link>
      <pubDate>Sun, 24 Jun 2018 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml/</guid>
      <description>Aplicando t√©cnincas de Text Mining como pacote tidy text para explorar a rivalidade entre Brasil e Argentina! Veja tamb√©m como a an√°lise de sentimentos pode ser divertida al√©m de poss√≠veis aplica√ß√µes de machine learning</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="brasil-vs-argentina-e-text-mining" class="section level1">
<h1>Brasil vs Argentina e Text Mining</h1>
<p>A copa do mundo esta ai novamente e como n√£o poderia ser diferente, com ela surgem novos <a href="http://cio.com.br/noticias/2015/10/27/tome-nota-2-5-quintilhoes-de-bytes-sao-criados-todos-os-dias/">quintilh√µes de bytes todos os dias</a>, saber analisar esses dados √© um grande desafio pois a maioria dessa informa√ß√£o se encontra de forma n√£o estruturada e al√©m do desafio de captar esses dados ainda existem mais desafios que podem ser ainda maiores, como o de process√°-los e obter respostas deles.</p>
<p>Dada a rivalidade hist√≥rica entre Brasil e Argentina achei que seria interessante avaliar como anda o comportamento das pessoas do Brasil nas m√≠dias sociais em rela√ß√£o a esses dois pa√≠ses. Para o post n√£o ficar muito longo, escolhi que iria recolher informa√ß√µes apenas do Twitter devido a praticidade, foram coletados os √∫ltimos 4.000 tweets com o termo ‚Äúbrasil‚Äù e os √∫ltimos ‚Äú4.000‚Äù tweets com o termo ‚Äúargentina‚Äù no Twitter atrav√©s da sua API com o pacote os <code>twitteR</code> e <code>ROAuth</code>. O c√≥digo pode ser conferido <a href="https://github.com/gomesfellipe/functions/blob/master/getting_twitter_data.R">neste link</a>.</p>
<p>An√°lise de textos sempre foi um tema que me interessou muito, no final do ano de 2017 quando era estagi√°rio me pediram para ajudar em uma pesquisa que envolvia a an√°lise de palavras criando algumas nuvens de palavras. Pesquisando sobre t√©cnicas de textmining descobri tantas abordagens diferentes que resolvi juntar tudo que tinha encontrado em uma √∫nica fun√ß√£o (que ser√° apresentada a seguir) para a confec√ß√£o dessas nuvens, utilizarei esta fun√ß√£o para ter uma primeira impress√£o dos dados.</p>
<p>Al√©m disso, como seria um problema a tarefa de criar as nuvens de palavras s√≥ poderia ser realizada por algu√©m com conhecimento em R, na √©poca estava come√ßando meus estudo sobre shiny e como treinamento desenvolvi um app que esta hospedado no link: <a href="https://gomesfellipe.shinyapps.io/appwordcloud/" class="uri">https://gomesfellipe.shinyapps.io/appwordcloud/</a> e o c√≥digo esta aberto e dispon√≠vel para quem se interessar no meu github <a href="https://github.com/gomesfellipe/appwordcloud/blob/master/appwordcloud.Rmd">neste link</a></p>
<p>Por√©m, ap√≥s ler e estudar o livro <a href="https://www.tidytextmining.com/">Text Mining with R - A Tidy Approach</a> por <span class="citation"><a href="#ref-tidytext" role="doc-biblioref">Silge; Robinson</a> (<a href="#ref-tidytext" role="doc-biblioref">2018</a>)</span> hoje em dia eu olho para tr√°s e vejo que poderia ter feito tanto a fun√ß√£o quanto o aplicativo de maneira muito mais eficiente portanto esse post tr√°s alguns dos meus estudos sobre esse livro maravilhoso e tamb√©m algum estudo sobre Machine Learning com o pacote <a href="https://cran.r-project.org/web/packages/caret"><code>caret</code></a></p>
<div id="importando-a-dados" class="section level2">
<h2>Importando a dados</h2>
<p>Como j√° foi dito, a base de dados foi obtida atrav√©s da API do twitter e o c√≥digo pode ser obtido <a href="https://github.com/gomesfellipe/functions/blob/master/getting_twitter_data.R">neste link</a>.</p>
<pre class="r"><code>library(dplyr)
library(kableExtra)
library(magrittr)

base &lt;- read.csv(&quot;original_books.csv&quot;) %&gt;% as_tibble()</code></pre>
</div>
<div id="nuvem-de-palavras-r√°pida-com-fun√ß√£o-customizada" class="section level2">
<h2>Nuvem de palavras r√°pida com fun√ß√£o customizada</h2>
<p>Para uma primeira impress√£o dos dados, vejamos o que retorna uma nuvem de palavras criada com a fun√ß√£o <a href="https://github.com/gomesfellipe/functions/blob/master/wordcloud_sentiment.R"><code>wordcloud_sentiment()</code></a> que desenvolvi antes de conhecer a ‚ÄúA Tidy Approach‚Äù para Text Mining:</p>
<pre class="r"><code>devtools::source_url(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/wordcloud_sentiment.R&quot;)

# Obtendo nuvem e salvando tabela num objeto com nome teste:
df &lt;- wordcloud_sentiment(base$text,
                      type = &quot;text&quot;,
                      sentiment = F,
                      excludeWords = c(&quot;nao&quot;,letters,LETTERS),
                      ngrams = 2,
                      tf_idf = F,
                      max = 100,
                      freq = 10,
                      horizontal = 0.9,
                      textStemming = F,
                      print=T)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-2-1.png" width="1056" /></p>
<p>N√£o poderia esquecer, al√©m da nuvem, a fun√ß√£o tamb√©m retorna um dataframe com a frequ√™ncia das palavras:</p>
<pre class="r"><code>df %&gt;% as_tibble()</code></pre>
<pre><code>## # A tibble: 29,064 x 2
##    words          freq  
##    &lt;chr&gt;          &lt;chr&gt; 
##  1 =              &quot;2795&quot;
##  2 brasil copa    &quot;2061&quot;
##  3 copa mundo     &quot;1959&quot;
##  4 hat trick      &quot;1327&quot;
##  5 = hoje         &quot;1248&quot;
##  6 hoje brasil    &quot;1215&quot;
##  7 mundo          &quot; 852&quot;
##  8 isl ndia       &quot; 820&quot;
##  9 pra copa       &quot; 813&quot;
## 10 estreia brasil &quot; 782&quot;
## # ‚Ä¶ with 29,054 more rows</code></pre>
<p>E outra fun√ß√£o interessante √© a de criar uma nuvem a partir de um webscraping muito (muito mesmo) introdut√≥rio, para isso foi pegar todo o texto da p√°gina sobre a copa do mundo no Wikip√©dia, veja:</p>
<pre class="r"><code># Obtendo nuvem e salvando tabela num objeto com nome teste:
df_html &lt;- wordcloud_sentiment(&quot;https://pt.wikipedia.org/wiki/Copa_do_Mundo_FIFA&quot;,
                      type = &quot;url&quot;,
                      sentiment = F,
                      excludeWords = c(&quot;nao&quot;,letters,LETTERS),
                      ngrams = 2,
                      tf_idf = F,
                      max = 100,
                      freq = 6,
                      horizontal = 0.9,
                      textStemming = F,
                      print=T)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Essa fun√ß√£o √© bem ‚Äúprematura,‚Äù existem infinitas maneiras de melhorar ela e n√£o alterei ela ainda por falta de tempo.</p>
</div>
<div id="a-tidy-approach" class="section level2">
<h2>A Tidy Approach</h2>
<p>O formato tidy, em que cada linha corresponde a uma observa√ß√£o e cada coluna √† uma vari√°vel, veja:</p>
<center>
<img src="http://garrettgman.github.io/images/tidy-1.png" style="width:70.0%" />
</center>
<p>Agora a tarefa ser√° simplificada com a abordagem tidy, al√©m das fun√ß√µes do livro <a href="https://www.tidytextmining.com/">Text Mining with R</a> utilizarei a fun√ß√£o <a href="https://github.com/gomesfellipe/functions/blob/master/clean_tweets.R"><code>clean_tweets</code></a> que adaptei inspirado nesse post dessa pagina: <a href="https://sites.google.com/site/miningtwitter/home">Quick guide to mining twitter with R</a> quando estudava sobre textmining.</p>
<div id="arrumando-e-transformando-a-base-de-dados" class="section level3">
<h3>Arrumando e transformando a base de dados</h3>
<p>Utilizando as fun√ß√µes do pacote <code>tidytext</code> em conjunto com os pacotes <code>stringr</code> e <code>abjutils</code>, ser√° poss√≠vel limpar e arrumar a base de dados.</p>
<p>Al√©m disso ser√£o removidas as stop words de nossa base, com a fun√ß√£o <code>stopwords::stopwords("pt")</code> podemos obter as stopwords da nossa l√≠ngua</p>
<pre class="r"><code>library(stringr)
library(tidytext)
library(abjutils)

devtools::source_url(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/clean_tweets.R&quot;)

original_books = base %&gt;% 
  mutate(text = clean_tweets(text) %&gt;% enc2native() %&gt;% rm_accent())

#Removendo stopwords:
excludewords=c(&quot;[:alpha:]&quot;,&quot;[:alnum:]&quot;,&quot;[:digit:]&quot;,&quot;[:xdigit:]&quot;,&quot;[:space:]&quot;,&quot;[:word:]&quot;,
               LETTERS,letters,1:10,
               &quot;hat&quot;,&quot;trick&quot;,&quot;bc&quot;,&quot;de&quot;,&quot;tem&quot;,&quot;twitte&quot;,&quot;fez&quot;,
               &#39;pra&#39;,&quot;vai&quot;,&quot;ta&quot;,&quot;so&quot;,&quot;ja&quot;,&quot;rt&quot;)

stop_words = data_frame(word = c(stopwords::stopwords(&quot;pt&quot;), excludewords))

tidy_books &lt;- original_books %&gt;%
  unnest_tokens(word, text) %&gt;% 
  anti_join(stop_words)</code></pre>
<p>Portando a base de dados ap√≥s a limpeza e a remo√ß√£o das stop words:</p>
<pre class="r"><code>#Palavras mais faladas:
tidy_books %&gt;% count(word, sort = TRUE) </code></pre>
<pre><code>## # A tibble: 3,900 x 2
##    word          n
##    &lt;chr&gt;     &lt;int&gt;
##  1 copa       6993
##  2 brasil     4164
##  3 argentina  3487
##  4 mundo      2030
##  5 hoje       1825
##  6 letras     1562
##  7 messi      1493
##  8 estreia    1107
##  9 est         866
## 10 isl         828
## # ‚Ä¶ with 3,890 more rows</code></pre>
<pre class="r"><code>#Apos a limpeza, caso precise voltar as frases:
original_books = tidy_books%&gt;%
  group_by(book,line)%&gt;%
  summarise(text=paste(word,collapse = &quot; &quot;))</code></pre>
<div id="palavras-mais-frequentes" class="section level4">
<h4>Palavras mais frequentes</h4>
<p>Vejamos as palavras mais faladas nessa pesquisa:</p>
<pre class="r"><code>library(ggplot2)

tidy_books %&gt;%
  count(word, sort = TRUE) %&gt;%
  filter(n &gt; 400) %&gt;%
  mutate(word = reorder(word, n)) %&gt;%
  
  ggplot(aes(word, n, fill = I(&quot;yellow&quot;), colour = I(&quot;green&quot;))) +
  geom_col(position=&quot;dodge&quot;) +
  xlab(NULL) +
  labs(title = &quot;Frequencia total das palavras pesquisadas&quot;)+
  coord_flip()+ theme(
  panel.background = element_rect(fill = &quot;#74acdf&quot;,
                                colour = &quot;lightblue&quot;,
                                size = 0.5, linetype = &quot;solid&quot;),
  panel.grid.major = element_line(size = 0.5, linetype = &#39;solid&#39;,
                                colour = &quot;white&quot;), 
  panel.grid.minor = element_line(size = 0.25, linetype = &#39;solid&#39;,
                                colour = &quot;white&quot;)
  )</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<div id="palavras-mais-frequentes-para-cada-termo" class="section level4">
<h4>Palavras mais frequentes para cada termo</h4>
<p>Vejamos as nuvens de palavras mais frequentes de acordo com cada um dos termos pesquisados:</p>
<pre class="r"><code>#Criando nuvem de palavra:
library(wordcloud)

par(mfrow=c(1,2))
tidy_books %&gt;%
  filter(book==&quot;br&quot;)%&gt;%
  count(word) %&gt;%
  with(wordcloud(word, n, max.words = 100,random.order = F,min.freq = 15,random.color = F,colors = c(&quot;#009b3a&quot;, &quot;#fedf00&quot;,&quot;#002776&quot;),scale = c(2,1),rot.per = 0.05))

tidy_books %&gt;%
  filter(book==&quot;arg&quot;)%&gt;%
  count(word) %&gt;%
  with(wordcloud(word, n, max.words = 100,min.freq = 15,random.order = F,random.color = F,colors = c(&quot;#75ade0&quot;, &quot;#ffffff&quot;,&quot;#f6b506&quot;),scale = c(2,1),rot.per = 0.05))</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-8-1.png" width="1056" /></p>
<pre class="r"><code>par(mfrow=c(1,1))</code></pre>
</div>
</div>
<div id="an√°lise-de-sentimentos" class="section level3">
<h3>An√°lise de sentimentos</h3>
<p>A an√°lise de sentimentos utilizando a abordagem tidy foi poss√≠vel gra√ßas ao pacote <a href="https://cran.r-project.org/package=lexiconPT"><code>lexiconPT</code></a>, que esta dispon√≠vel no CRAN e que conheci ao ler o <a href="https://sillasgonzaga.github.io/2017-09-23-sensacionalista-pt01/">post: ‚ÄúO Sensacionalista e Text Mining: An√°lise de sentimento usando o lexiconPT‚Äù</a> do blog <a href="https://sillasgonzaga.github.io/">Paix√£o por dados</a> que gosto tanto de acompanhar.</p>
<pre class="r"><code># Analise de sentimentos:
library(lexiconPT)

sentiment = data.frame(word = sentiLex_lem_PT02$term ,
                       polarity = sentiLex_lem_PT02$polarity) %&gt;% 
  mutate(sentiment = if_else(polarity&gt;0,&quot;positive&quot;,if_else(polarity&lt;0,&quot;negative&quot;,&quot;neutro&quot;)),
         word = as.character(word)) %&gt;% 
  as_tibble()


library(tidyr)

book_sentiment &lt;- tidy_books %&gt;%
  inner_join(sentiment) %&gt;%
  count(book,word, index = line , sentiment) %&gt;%
  spread(sentiment, n, fill = 0) %&gt;%
  mutate(sentiment = positive - negative) %T&gt;%
  print</code></pre>
<pre><code>## # A tibble: 2,953 x 7
##    book  word      index negative neutro positive sentiment
##    &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
##  1 arg   abandonar   857        1      0        0        -1
##  2 arg   absurdo     849        1      0        0        -1
##  3 arg   absurdo    1863        1      0        0        -1
##  4 arg   afogado    2275        1      0        0        -1
##  5 arg   afogado    3659        1      0        0        -1
##  6 arg   alegria    1134        0      0        1         1
##  7 arg   almo        186        0      0        1         1
##  8 arg   almo       2828        0      0        1         1
##  9 arg   almo       3433        0      0        1         1
## 10 arg   almo       3569        0      0        1         1
## # ‚Ä¶ with 2,943 more rows</code></pre>
<p>Cada palavra possui um valor associado a sua polaridade , vejamos como ficou distribu√≠do o n√∫mero de palavras de cada sentimento de acordo com cada termo escolhido para a pesquisa:</p>
<pre class="r"><code>book_sentiment%&gt;%
  count(sentiment,book)%&gt;%
  arrange(book) %&gt;%
  
  ggplot(aes(x = factor(sentiment),y = n,fill=book))+
  geom_bar(stat=&quot;identity&quot;,position=&quot;dodge&quot;)+
  facet_wrap(~book) +
  theme_bw()+ 
    scale_fill_manual(values=c(&quot;#75ade0&quot;, &quot;#009b3a&quot;))</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<div id="comparando-sentimentos-dos-termos-de-pesquisa" class="section level4">
<h4>Comparando sentimentos dos termos de pesquisa</h4>
<p>Para termos associados a palavra ‚ÄúBrasil‚Äù no twitter:</p>
<pre class="r"><code># Nuvem de compara√ß√£o:
library(reshape2)

tidy_books %&gt;%
  filter(book==&quot;br&quot;)%&gt;%
  inner_join(sentiment) %&gt;%
  count(word, sentiment, sort = TRUE) %&gt;%
  acast(word ~ sentiment, value.var = &quot;n&quot;, fill = 0) %&gt;%
  comparison.cloud(colors = c(&quot;red&quot;, &quot;gray80&quot;,&quot;green&quot;),
                   max.words = 200)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Para termos associados a palavra ‚ÄúArgentina‚Äù no twitter:</p>
<pre class="r"><code>tidy_books %&gt;%
  filter(book==&quot;arg&quot;)%&gt;%
  inner_join(sentiment) %&gt;%
  count(word, sentiment, sort = TRUE) %&gt;%
  acast(word ~ sentiment, value.var = &quot;n&quot;, fill = 0) %&gt;%
  comparison.cloud(colors = c(&quot;red&quot;, &quot;gray80&quot;,&quot;green&quot;),
                   max.words = 200)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
<div id="propor√ß√£o-de-palavras-positivas-e-negativas-por-texto" class="section level4">
<h4>Propor√ß√£o de palavras positivas e negativas por texto</h4>
<pre class="r"><code># Propor√ß√£o de palavras negativas:
bingnegative &lt;- sentiment %&gt;% 
  filter(sentiment == &quot;negative&quot;)

bingpositive &lt;- sentiment %&gt;% 
  filter(sentiment == &quot;positive&quot;)

wordcounts &lt;- tidy_books %&gt;%
  group_by(book, line) %&gt;%
  summarize(words = n())</code></pre>
<div id="para-negativas" class="section level5">
<h5>Para negativas;</h5>
<pre class="r"><code>tidy_books %&gt;%
  semi_join(bingnegative) %&gt;%
  group_by(book, line) %&gt;%
  summarize(negativewords = n()) %&gt;%
  left_join(wordcounts, by = c(&quot;book&quot;, &quot;line&quot;)) %&gt;%
  mutate(ratio = negativewords/words) %&gt;%
  top_n(5) %&gt;%
  ungroup() %&gt;% arrange(desc(ratio)) %&gt;% filter(book==&quot;br&quot;)</code></pre>
<pre><code>## # A tibble: 5 x 5
##   book   line negativewords words ratio
##   &lt;chr&gt; &lt;int&gt;         &lt;int&gt; &lt;int&gt; &lt;dbl&gt;
## 1 br     2003             1     3 0.333
## 2 br     2775             1     3 0.333
## 3 br     2580             2     7 0.286
## 4 br      126             1     4 0.25 
## 5 br     2335             1     4 0.25</code></pre>
<p>A frase mais negativa do brasil e da argentina::</p>
<pre class="r"><code>base %&gt;%
  filter(book==&quot;br&quot;,line==2580) %&gt;% mutate(text = as.character(text))%&gt;% select(text) %&gt;% c() </code></pre>
<pre><code>## $text
## [1] &quot;um medo? \x97 de nois criar expectativa e o Brasil perder a copa https://t.co/0chcNWHh0m&quot;</code></pre>
<pre class="r"><code>base %&gt;%
  filter(book==&quot;arg&quot;,line==572) %&gt;% mutate(text = as.character(text))%&gt;% select(text) %&gt;% c()  </code></pre>
<pre><code>## $text
## [1] &quot;RT @DavidmeMelo: @SantiiSanchez16 @Flamengo Perder a copa para o time mais sujo e mais corrupto da argentina \xe9 assim mesmo https://t.co/zIC\x85&quot;</code></pre>
</div>
<div id="para-positivas" class="section level5">
<h5>Para positivas:</h5>
<pre class="r"><code>tidy_books %&gt;%
  semi_join(bingpositive) %&gt;%
  group_by(book, line) %&gt;%
  summarize(positivewords = n()) %&gt;%
  left_join(wordcounts, by = c(&quot;book&quot;, &quot;line&quot;)) %&gt;%
  mutate(ratio = positivewords/words) %&gt;%
  top_n(5) %&gt;%
  ungroup() %&gt;% arrange(desc(ratio))</code></pre>
<pre><code>## # A tibble: 22 x 5
##    book   line positivewords words ratio
##    &lt;chr&gt; &lt;int&gt;         &lt;int&gt; &lt;int&gt; &lt;dbl&gt;
##  1 arg    2120             3     9 0.333
##  2 br     2374             1     3 0.333
##  3 arg    3272             2     7 0.286
##  4 arg    2301             1     4 0.25 
##  5 br      126             1     4 0.25 
##  6 br      553             2     8 0.25 
##  7 br     1499             2     8 0.25 
##  8 br     2054             2     8 0.25 
##  9 br     2591             1     4 0.25 
## 10 arg    2130             1     5 0.2  
## # ‚Ä¶ with 12 more rows</code></pre>
<p>A frase mais positiva do brasil e da argentina:</p>
<pre class="r"><code>base %&gt;%
  filter(book==&quot;br&quot;,line==2374) %&gt;% mutate(text = as.character(text))%&gt;% select(text) %&gt;% c() </code></pre>
<pre><code>## $text
## [1] &quot;Tirei Brasil, \xe9 uma honra https://t.co/OgNCot4Wu0&quot;</code></pre>
<pre class="r"><code>base %&gt;%
  filter(book==&quot;arg&quot;,line==2120) %&gt;% mutate(text = as.character(text))%&gt;% select(text) %&gt;% c()  </code></pre>
<pre><code>## $text
## [1] &quot;@_LeoFerreiraH Quero que a Argentina passe para possivelmente enfrentar o Brasil, ganhar da Argentina j\xe1 \xe9 bom, na\x85 https://t.co/bxHJUeGVpc&quot;</code></pre>
</div>
</div>
</div>
</div>
<div id="tf-idf" class="section level2">
<h2>TF-IDF</h2>
<p>Segundo <span class="citation"><a href="#ref-tidytext" role="doc-biblioref">Silge; Robinson</a> (<a href="#ref-tidytext" role="doc-biblioref">2018</a>)</span> no livro <a href="https://www.tidytextmining.com/tfidf.html">tidytextminig</a>:</p>
<blockquote>
<p>The statistic tf-idf is intended to measure how important a word is to a document in a collection (or corpus) of documents, for example, to one novel in a collection of novels or to one website in a collection of websites.</p>
</blockquote>
<p>Traduzido pelo Google tradutor:</p>
<blockquote>
<p>A estat√≠stica tf-idf destina-se a medir a import√¢ncia de uma palavra para um documento em uma cole√ß√£o (ou corpus) de documentos, por exemplo, para um romance em uma cole√ß√£o de romances ou para um site em uma cole√ß√£o de sites.</p>
</blockquote>
<p>Matematicamente:</p>
<p><span class="math display">\[
idf(\text{term}) = \ln{\left(\frac{n_{\text{documents}}}{n_{\text{documents containing term}}}\right)}
\]</span></p>
<p>E que com o pacote <code>tidytext</code> podemos obter usando o comando <code>bind_tf_idf()</code>, veja:</p>
<pre class="r"><code># Obtendo numero de palavras
book_words &lt;- original_books %&gt;%
  unnest_tokens(word, text) %&gt;%
  count(book, word, sort = TRUE) %&gt;%
  ungroup()%&gt;%
  anti_join(stop_words)

total_words &lt;- book_words %&gt;% 
  group_by(book) %&gt;% 
  summarize(total = sum(n))

book_words &lt;- left_join(book_words, total_words)

# tf-idf:
book_words &lt;- book_words %&gt;%
  bind_tf_idf(word, book, n)

book_words %&gt;%
  arrange(desc(tf_idf))</code></pre>
<pre><code>## # A tibble: 4,773 x 7
##    book  word              n total      tf   idf  tf_idf
##    &lt;chr&gt; &lt;chr&gt;         &lt;int&gt; &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;
##  1 br    letras         1562 30429 0.0513  0.693 0.0356 
##  2 br    ansioso         688 30429 0.0226  0.693 0.0157 
##  3 arg   classificou     666 40781 0.0163  0.693 0.0113 
##  4 arg   segundo         654 40781 0.0160  0.693 0.0111 
##  5 arg   especialistas   649 40781 0.0159  0.693 0.0110 
##  6 arg   nalti           649 40781 0.0159  0.693 0.0110 
##  7 arg   repito          649 40781 0.0159  0.693 0.0110 
##  8 br    icon            248 30429 0.00815 0.693 0.00565
##  9 arg   ncio            287 40781 0.00704 0.693 0.00488
## 10 arg   penalti         284 40781 0.00696 0.693 0.00483
## # ‚Ä¶ with 4,763 more rows</code></pre>
<p>O que nos tr√°s algo como: ‚Äútermos mais relevantes.‚Äù</p>
<p>Visualmente:</p>
<pre class="r"><code>book_words %&gt;%
  arrange(desc(tf_idf)) %&gt;%
  mutate(word = factor(word, levels = rev(unique(word)))) %&gt;% 
  group_by(book) %&gt;% 
  top_n(15) %&gt;% 
  ungroup %&gt;%
  
  ggplot(aes(word, tf_idf, fill = book)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = &quot;tf-idf&quot;) +
  facet_wrap(~book, ncol = 2, scales = &quot;free&quot;) +
  coord_flip()+
  theme_bw()+ 
    scale_fill_manual(values=c(&quot;#75ade0&quot;, &quot;#009b3a&quot;))</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
</div>
<div id="bi-grams" class="section level2">
<h2>bi grams</h2>
<p>OS bi grams s√£o sequencias de palavras, a seguir ser√° procurada as sequencias de duas palavras, o que nos permite estudar um pouco melhor o contexto do seu uso.</p>
<pre class="r"><code># Bi grams
book_bigrams &lt;- original_books %&gt;%
  unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2)

book_bigrams %&gt;%
  count(bigram, sort = TRUE)</code></pre>
<pre><code>## # A tibble: 15,106 x 3
## # Groups:   book [2]
##    book  bigram                    n
##    &lt;chr&gt; &lt;chr&gt;                 &lt;int&gt;
##  1 br    brasil copa            2039
##  2 br    copa mundo             1459
##  3 br    hoje brasil            1215
##  4 arg   argentina copa         1122
##  5 arg   isl ndia                818
##  6 br    estreia brasil          764
##  7 br    ansioso estreia         684
##  8 br    est ansioso             680
##  9 arg   classificou argentina   660
## 10 arg   copa segundo            649
## # ‚Ä¶ with 15,096 more rows</code></pre>
<p>Separando as coluna de bi grams:</p>
<pre class="r"><code>bigrams_separated &lt;- book_bigrams %&gt;%
  separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;)

bigrams_filtered &lt;- bigrams_separated %&gt;%
  filter(!word1 %in% stop_words$word) %&gt;%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts &lt;- bigrams_filtered %&gt;% 
  count(word1, word2, sort = TRUE)

bigram_counts</code></pre>
<pre><code>## # A tibble: 15,106 x 4
## # Groups:   book [2]
##    book  word1       word2         n
##    &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;     &lt;int&gt;
##  1 br    brasil      copa       2039
##  2 br    copa        mundo      1459
##  3 br    hoje        brasil     1215
##  4 arg   argentina   copa       1122
##  5 arg   isl         ndia        818
##  6 br    estreia     brasil      764
##  7 br    ansioso     estreia     684
##  8 br    est         ansioso     680
##  9 arg   classificou argentina   660
## 10 arg   copa        segundo     649
## # ‚Ä¶ with 15,096 more rows</code></pre>
<p>Caso seja preciso juntar novamente:</p>
<pre class="r"><code>bigrams_united &lt;- bigrams_filtered %&gt;%
  unite(bigram, word1, word2, sep = &quot; &quot;)

bigrams_united</code></pre>
<pre><code>## # A tibble: 71,208 x 2
## # Groups:   book [2]
##    book  bigram             
##    &lt;chr&gt; &lt;chr&gt;              
##  1 arg   isl ndia           
##  2 arg   ndia pouco         
##  3 arg   pouco mil          
##  4 arg   mil habitantes     
##  5 arg   habitantes montaram
##  6 arg   montaram sele      
##  7 arg   sele est           
##  8 arg   est copa           
##  9 arg   copa fizeram       
## 10 arg   fizeram gol        
## # ‚Ä¶ with 71,198 more rows</code></pre>
<div id="analisando-bi-grams-com-tf-idf" class="section level3">
<h3>Analisando bi grams com tf-idf</h3>
<p>Tamb√©m √© poss√≠vel aplicar a transforma√ß√£o <code>tf-idf</code> em bigrams, veja:</p>
<pre class="r"><code>#bi grams com tf idf
bigram_tf_idf &lt;- bigrams_united %&gt;%
  count(book, bigram) %&gt;%
  bind_tf_idf(bigram, book, n) %&gt;%
  arrange(desc(tf_idf))

bigram_tf_idf</code></pre>
<pre><code>## # A tibble: 15,106 x 6
## # Groups:   book [2]
##    book  bigram                    n     tf   idf  tf_idf
##    &lt;chr&gt; &lt;chr&gt;                 &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;
##  1 br    hoje brasil            1215 0.0399 0.693 0.0277 
##  2 br    ansioso estreia         684 0.0225 0.693 0.0156 
##  3 br    est ansioso             680 0.0223 0.693 0.0155 
##  4 br    letras letras           620 0.0204 0.693 0.0141 
##  5 arg   classificou argentina   660 0.0162 0.693 0.0112 
##  6 arg   copa segundo            649 0.0159 0.693 0.0110 
##  7 arg   messi repito            649 0.0159 0.693 0.0110 
##  8 arg   repito classificou      649 0.0159 0.693 0.0110 
##  9 arg   segundo especialistas   649 0.0159 0.693 0.0110 
## 10 br    brasil letras           313 0.0103 0.693 0.00713
## # ‚Ä¶ with 15,096 more rows</code></pre>
</div>
<div id="analisando-contexto-de-palavras-negativas" class="section level3">
<h3>Analisando contexto de palavras negativas:</h3>
<p>Uma das abordagens interessantes ao estudar as bi-grams √© a de avaliar o contexto das palavras negativas, veja:</p>
<pre class="r"><code>bigrams_separated %&gt;%
  filter(word1 == &quot;nao&quot;) %&gt;%
  count(word1, word2, sort = TRUE)</code></pre>
<pre><code>## # A tibble: 35 x 4
## # Groups:   book [2]
##    book  word1 word2         n
##    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;
##  1 br    nao   copa         10
##  2 arg   nao   abrir         3
##  3 arg   nao   convoca       3
##  4 arg   nao   ruim          3
##  5 br    nao   acredito      2
##  6 arg   nao   achei         1
##  7 arg   nao   acordem       1
##  8 arg   nao   argentina     1
##  9 arg   nao   assisti       1
## 10 arg   nao   compara       1
## # ‚Ä¶ with 25 more rows</code></pre>
<pre class="r"><code>not_words &lt;- bigrams_separated %&gt;%
  filter(word1 == &quot;nao&quot;) %&gt;%
  inner_join(sentiment, by = c(word2 = &quot;word&quot;)) %&gt;%
  count(word2, sentiment, sort = TRUE) %&gt;%
  ungroup()

not_words</code></pre>
<pre><code>## # A tibble: 3 x 4
##   book  word2    sentiment     n
##   &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;int&gt;
## 1 arg   ruim     negative      3
## 2 arg   vencer   positive      1
## 3 br    amistoso positive      1</code></pre>
<p>A palavra n√£o antes de uma palavra ‚Äúpositiva,‚Äù como por exemplo ‚Äún√£o gosto‚Äù pode ser anulada ao somar-se suas polaridades (‚Äún√£o‚Äù = - 1, ‚Äúgosto‚Äù = +1 e ‚Äún√£o gosto‚Äù = -1 + 1) o leva a necessidade de ser tomar um cuidado especial com essas palavras em uma an√°lise de texto mais detalhada, veja de forma visual:</p>
<pre class="r"><code>not_words %&gt;%
  mutate(sentiment=ifelse(sentiment==&quot;positive&quot;,1,ifelse(sentiment==&quot;negative&quot;,-1,0)))%&gt;%
  mutate(contribution = n * sentiment) %&gt;%
  arrange(desc(abs(contribution))) %&gt;%
  head(20) %&gt;%
  mutate(word2 = reorder(word2, contribution)) %&gt;%
  
  ggplot(aes(word2, n * sentiment, fill = n * sentiment &gt; 0)) +
  geom_col() +
  xlab(&quot;Words preceded by \&quot;not\&quot;&quot;) +
  ylab(&quot;Sentiment score * number of occurrences&quot;) +
  coord_flip()+
  theme_bw()</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="machine-learning" class="section level1">
<h1>Machine Learning</h1>
<p>Estava pesquisando sobre algor√≠timos recomendados para a an√°lise de texto quando encontrei um artigo da data camp chamado: <a href="https://www.datacamp.com/community/tutorials/R-nlp-machine-learning"><em>Lyric Analysis with NLP &amp; Machine Learning with R</em></a>, do qual a autora exp√µe a seguinte tabela:</p>
<center>
<img src="http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1517331396/MLImage_cygwsb.jpg" style="width:60.0%" />
</center>
<p>Portanto resolvi fazer uma brincadeira e ajustar 4 dos modelos propostos para a tarefa supervisionada de classifica√ß√£o: K-NN, Tress (tentarei o ajuste do algor√≠timo Random Forest), Logistic Regression (Modelo estat√≠stico) e Naive-Bayes (por meio do c√°lculo de probabilidades condicionais) para ver se conseguia recuperar a classifica√ß√£o de quais os termos de pesquisa que eu utilizei para obter esses dados</p>
<p>Al√©m de t√©cnicas apresentadas no livro do pacote <code>caret</code>, por <span class="citation"><a href="#ref-caret" role="doc-biblioref">Kuhn</a> (<a href="#ref-caret" role="doc-biblioref">2018</a>)</span>, muito do que apliquei aqui foi baseado no livro ‚ÄúIntrodu√ß√£o a minera√ß√£o de dados‚Äù por <span class="citation"><a href="#ref-miner" role="doc-biblioref">Silva; Peres; Boscarioli</a> (<a href="#ref-miner" role="doc-biblioref">2016</a>)</span>, que foi bastante √∫til na minha introdu√ß√£o sobre o tema Machine Learning.</p>
<p>Vou utilizar uma fun√ß√£o chamada <code>plot_pred_type_distribution()</code>,apresentada neste post de titulo: <a href="https://www.r-bloggers.com/illustrated-guide-to-roc-and-auc/">Illustrated Guide to ROC and AUC</a> e fiz uma pequena altera√ß√£o para que ela funcionasse para o dataset deste post . A fun√ß√£o adaptada pode ser encontrada <a href="https://github.com/gomesfellipe/functions/blob/master/plot_pred_type_distribution.R">neste link</a> no meu github e a fun√ß√£o original <a href="https://github.com/joyofdata/joyofdata-articles/blob/master/roc-auc/plot_pred_type_distribution.R">neste link do github do autor</a>.</p>
<div id="pacote-caret" class="section level2">
<h2>Pacote caret</h2>
<p>Basicamente o ajuste de todos os modelos envolveram o uso do pacote <code>caret</code> e muitos dos passos aqui foram baseados nas instru√ß√µes fornecidas no <a href="https://topepo.github.io/caret/index.html">livro do pacote</a>. O pacote facilita bastante o ajuste dos par√¢metros no ajuste de modelos.</p>
</div>
<div id="transformar-e-arrumar" class="section level2">
<h2>Transformar e arrumar</h2>
<p>Uma <a href="https://www.kaggle.com/kailex/tidy-xgboost-glmnet-text2vec-lsa">solu√ß√£o do kaggle</a> para o desafio <a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge">Toxic Comment Classification Challenge</a> me chamou aten√ß√£o, do qual o participante da competi√ß√£o criou colunas que sinalizassem os caracteres especiais de cada frase, utilizarei esta t√©cnica para o ajuste e novamente utilizarei o pacote de l√©xicos do apresentado no <a href="https://sillasgonzaga.github.io/2017-09-23-sensacionalista-pt01/">post do blog Paix√£o por dados</a></p>
<p>Veja a base transformada e arrumada:</p>
<pre class="r"><code># Ref: https://cfss.uchicago.edu/text_classification.html 
# https://www.r-bloggers.com/illustrated-guide-to-roc-and-auc/
devtools::source_url(&quot;https://raw.githubusercontent.com/gomesfellipe/functions/master/plot_pred_type_distribution.R&quot;)

base &lt;- base %&gt;% 
  mutate(length = str_length(text),
         ncap = str_count(text, &quot;[A-Z]&quot;),
         ncap_len = ncap / length,
         nexcl = str_count(text, fixed(&quot;!&quot;)),
         nquest = str_count(text, fixed(&quot;?&quot;)),
         npunct = str_count(text, &quot;[[:punct:]]&quot;),
         nword = str_count(text, &quot;\\w+&quot;),
         nsymb = str_count(text, &quot;&amp;|@|#|\\$|%|\\*|\\^&quot;),
         nsmile = str_count(text, &quot;((?::|;|=)(?:-)?(?:\\)|D|P))&quot;),
         text = clean_tweets(text) %&gt;% enc2native() %&gt;% rm_accent())%&gt;%
  unnest_tokens(word, text) %&gt;% 
  anti_join(stop_words)%&gt;%
  group_by(book,line,length, ncap, ncap_len, nexcl, nquest, npunct, nword, nsymb, nsmile)%&gt;%
  summarise(text=paste(word,collapse = &quot; &quot;)) %&gt;% 
  select(text,everything())%T&gt;% 
  print()</code></pre>
<pre><code>## # A tibble: 7,995 x 12
## # Groups:   book, line, length, ncap, ncap_len, nexcl, nquest, npunct, nword,
## #   nsymb [7,995]
##    text  book   line length  ncap ncap_len nexcl nquest npunct nword nsymb
##    &lt;chr&gt; &lt;chr&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt;    &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;
##  1 isl ‚Ä¶ arg       1     NA     7  NA          0      0      6    24     1
##  2 pau ‚Ä¶ arg       2    108     6   0.0556     0      0      2    20     1
##  3 mess‚Ä¶ arg       3     NA    10  NA          0      0      3    24     1
##  4 minu‚Ä¶ arg       4     NA     2  NA          0      0      2    24     1
##  5 requ‚Ä¶ arg       5    129    23   0.178      0      0     15    21     1
##  6 bras‚Ä¶ arg       6     NA    11  NA          0      0     12    20     1
##  7 dupl‚Ä¶ arg       7    123    84   0.683      0      0      8    21     1
##  8 mess‚Ä¶ arg       8     NA    10  NA          0      0      3    24     1
##  9 mess‚Ä¶ arg       9     NA    10  NA          0      0      3    24     1
## 10 mess‚Ä¶ arg      10     NA    10  NA          0      0      3    24     1
## # ‚Ä¶ with 7,985 more rows, and 1 more variable: nsmile &lt;int&gt;</code></pre>
<p>Ap√≥s arrumar e transformar as informa√ß√µes que ser√£o utilizadas na classifica√ß√£o, ser√° criado um corpus sem a abordagem tidy para obter a matriz de documentos e termos, e depois utilizar a coluna de classifica√ß√£o, veja:</p>
<pre class="r"><code>library(tm)       #Pacote de para text mining
corpus &lt;- Corpus(VectorSource(base$text))

#Criando a matrix de termos:
book_dtm = DocumentTermMatrix(corpus, control = list(minWordLength=2,minDocFreq=3)) %&gt;% 
  weightTfIdf(normalize = T) %&gt;%    # Transforma√ß√£o tf-idf com pacote tm
  removeSparseTerms( sparse = .95)  # obtendo matriz esparsa com pacote tm

#Transformando em matrix, permitindo a manipulacao:
matrix = as.matrix(book_dtm)
dim(matrix)</code></pre>
<pre><code>## [1] 7995   18</code></pre>
<p>Pronto, agora j√° podemos juntar tudo em um data frame e separa em treino e teste para a classifica√ß√£o dos textos obtidos do twitter:</p>
<pre class="r"><code>#Criando a base de dados:
full=data.frame(cbind(
  base[,&quot;book&quot;],
  matrix,
  base[,-c(1:3)]
  )) %&gt;% na.omit()</code></pre>
</div>
<div id="treino-e-teste" class="section level2">
<h2>Treino e teste</h2>
<p>Ser√° utilizado tanto o m√©todo de hold-out e de cross-validation</p>
<pre class="r"><code>set.seed(825)
particao = sample(1:2,nrow(full), replace = T,prob = c(0.7,0.3))

train = full[particao==1,] 
test = full[particao==2,] 

library(caret)</code></pre>
</div>
<div id="ajustando-modelos" class="section level2">
<h2>Ajustando modelos</h2>
<div id="knn" class="section level3">
<h3>KNN</h3>
<p>√â uma t√©cnica de aprendizado baseado em inst√¢ncia, isto quer dizer que a classifica√ß√£o de uma observa√ß√£o com a classe desconhecida √© realizada a partir da compara√ß√£o com outras observa√ß√µes cada vez que uma observa√ß√£o √© apresentado ao modelo e tamb√©m √© conhecido como ‚Äúlazy evaluation,‚Äù j√° que um modelo n√£o √© induzido previamente.</p>
<p>Diversas medidas de dist√¢ncia podem ser utilizadas, utilizarei aqui a euclideana e al√©m disso a escolha do par√¢metro <span class="math inline">\(k\)</span> (de k vizinhos mais pr√≥ximos) deve ser feita com cuidado pois um <span class="math inline">\(k\)</span> pequeno pode expor o algor√≠timo a uma alta sensibilidade a um ru√≠do.</p>
<p>Utilizarei aqui o pacote <code>caret</code> como ferramenta para o ajuste deste modelo pois ela permite que eu configure que seja feita a valida√ß√£o cruzada em conjunto com a padroniza√ß√£o, pois esses complementos beneficiam no ajuste de modelos que calculam dist√¢ncias.</p>
<pre class="r"><code># knn -------
set.seed(825)
antes = Sys.time()
book_knn &lt;- train(book ~.,
                  data=train,
                 method = &quot;knn&quot;,
                 trControl = trainControl(method = &quot;cv&quot;,number = 10), # validacao cruzada
                 preProc = c(&quot;center&quot;, &quot;scale&quot;))                      
time_knn &lt;- Sys.time() - antes 
Sys.time() - antes</code></pre>
<pre><code>## Time difference of 2.465522 secs</code></pre>
<pre class="r"><code>plot(book_knn)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/knn-1.png" width="672" /></p>
<pre class="r"><code>previsao  = predict(book_knn, test)
confusionMatrix(previsao, factor(test$book))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction arg  br
##        arg 105   8
##        br    5 371
##                                          
##                Accuracy : 0.9734         
##                  95% CI : (0.955, 0.9858)
##     No Information Rate : 0.7751         
##     P-Value [Acc &gt; NIR] : &lt;2e-16         
##                                          
##                   Kappa : 0.9245         
##                                          
##  Mcnemar&#39;s Test P-Value : 0.5791         
##                                          
##             Sensitivity : 0.9545         
##             Specificity : 0.9789         
##          Pos Pred Value : 0.9292         
##          Neg Pred Value : 0.9867         
##              Prevalence : 0.2249         
##          Detection Rate : 0.2147         
##    Detection Prevalence : 0.2311         
##       Balanced Accuracy : 0.9667         
##                                          
##        &#39;Positive&#39; Class : arg            
## </code></pre>
<pre class="r"><code>df = cbind(fit = if_else(previsao==&quot;br&quot;,1,0), class = if_else(test$book==&quot;br&quot;,1,0)) %&gt;% as.data.frame()
plot_pred_type_distribution(df,0.5)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/knn-2.png" width="672" /></p>
<p>Como podemos ver, segundo a valida√ß√£o cruzada realizada com o pacote <code>caret</code>, o n√∫mero 5 de vizinhos mais pr√≥ximos foi o que apresentou o melhor resultado. Al√©m disso o modelo apresentou uma acur√°cia de 97,18% e isto parece bom dado que a sensibilidade (taxa de verdadeiros positivos) e a especificidade (taxa de verdadeiros negativos) foram altas tamb√©m, o que foi refor√ßado com o gr√°fico ilustrado da matriz de confus√£o.</p>
<p>O tempo computacional para o ajuste do modelo foi de:2.46385908126831 segundos</p>
</div>
<div id="random-forest" class="section level3">
<h3>Random Forest</h3>
<p>O modelo de Random Forest tem se tornado muito popular devido ao seu bom desempenho e pela sua alta capacidade de se adaptar aos dados. O modelo funciona atrav√©s da combina√ß√£o de v√°rias √°rvores de decis√µes e no seu ajuste alguns par√¢metros precisam ser levados em conta.</p>
<p>O par√¢metro que sera levado em conta para o ajuste ser√° apenas o <code>ntree</code>, que representa o n√∫mero de √°rvores ajustadas. Este par√¢metro deve ser escolhido com cuidado pois pode ser t√£o grande quanto voc√™ quiser e continua aumentando a precis√£o at√© certo ponto por√©m pode ser mais limitado pelo tempo computacional dispon√≠vel.</p>
<pre class="r"><code>set.seed(824)
# Random Forest
antes = Sys.time()
book_rf &lt;- train(book ~.,
                  data=train,
                     method = &quot;rf&quot;,trace=F,
                     ntree = 200,
                     trControl = trainControl(method = &quot;cv&quot;,number = 10))
time_rf &lt;- Sys.time() - antes 
Sys.time() - antes</code></pre>
<pre><code>## Time difference of 8.994044 secs</code></pre>
<pre class="r"><code>library(randomForest)
varImpPlot(book_rf$finalModel)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/rf-1.png" width="672" /></p>
<pre class="r"><code>previsao  = predict(book_rf, test)
confusionMatrix(previsao, factor(test$book))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction arg  br
##        arg 110   0
##        br    0 379
##                                      
##                Accuracy : 1          
##                  95% CI : (0.9925, 1)
##     No Information Rate : 0.7751     
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16  
##                                      
##                   Kappa : 1          
##                                      
##  Mcnemar&#39;s Test P-Value : NA         
##                                      
##             Sensitivity : 1.0000     
##             Specificity : 1.0000     
##          Pos Pred Value : 1.0000     
##          Neg Pred Value : 1.0000     
##              Prevalence : 0.2249     
##          Detection Rate : 0.2249     
##    Detection Prevalence : 0.2249     
##       Balanced Accuracy : 1.0000     
##                                      
##        &#39;Positive&#39; Class : arg        
## </code></pre>
<pre class="r"><code># https://www.r-bloggers.com/illustrated-guide-to-roc-and-auc/
df = cbind(fit = if_else(previsao==&quot;br&quot;,1,0), class = if_else(test$book==&quot;br&quot;,1,0)) %&gt;% as.data.frame()
plot_pred_type_distribution(df,0.5)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/rf-2.png" width="672" /></p>
<p>Segundo o gr√°fico de import√¢ncia, parece que as palavras ‚Äúbrasil,‚Äù ‚Äúargentina,‚Äù ‚Äúcopa‚Äù e ‚Äúmessi‚Äù foram as que apresentaram maior impacto do preditor (lembrando que essa medida n√£o √© um efeito espec√≠fico), o que mostra que a presen√ßa das palavras que estamos utilizando para classificar tiveram um impacto na classifica√ß√£o bastante superior aos demais.</p>
<p>Quanto a acur√°cia, o random forest apresentou valor um pouco maior do que o do algor√≠timo K-NN e al√©m disso apresentou altos valores para a sensibilidade (taxa de verdadeiros positivos) e a especificidade (taxa de verdadeiros negativos), o que foi refor√ßado com o gr√°fico ilustrado da matriz de confus√£o, por√©m o tempo computacional utilizado para ajustar este modelo foi muito maior, o que leva a questionar se esse pequeno aumento na taxa de acerto vale a pena aumentando tanto no tempo de processamento (outra alternativa seria diminuir o tamanho do n√∫mero de √°rvores para ver se melhoraria na qualidade do ajuste).</p>
<p>O tempo computacional para o ajuste do modelo foi de: 8.99299788475037 segundos</p>
</div>
<div id="naive-bayes" class="section level3">
<h3>Naive Bayes</h3>
<p>Este √© um algor√≠timo que trata-se de um classificador estat√≠stico baseado no <strong>Teorema de Bayes</strong> e recebe o nome de ing√™nuo (<em>naive</em>) porque pressup√µe que o valor de um atributo que exerce algum efeito sobre a distribui√ß√£o da vari√°vel resposta √© independente do efeito que outros atributos.</p>
<p>O c√°lculo para a classifica√ß√£o √© feito por meio do c√°lculo de probabilidades condicionais, ou seja, probabilidade de uma observa√ß√£o pertencer a cada classe dado os exemplares existentes no conjunto de dados usado para o treinamento.</p>
<pre class="r"><code># Naive Bayes ----
set.seed(825)
antes = Sys.time()
book_nb &lt;- train(book ~.,
                  data=train,
                 method= &quot;nb&quot;,
                 laplace =1,       
                 trControl = trainControl(method = &quot;cv&quot;,number = 10))
time_nb &lt;- Sys.time() - antes 
Sys.time() - antes</code></pre>
<pre><code>## Time difference of 7.141471 secs</code></pre>
<pre class="r"><code>previsao  = predict(book_nb, test)
confusionMatrix(previsao, factor(test$book))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction arg  br
##        arg 108   6
##        br    2 373
##                                          
##                Accuracy : 0.9836         
##                  95% CI : (0.968, 0.9929)
##     No Information Rate : 0.7751         
##     P-Value [Acc &gt; NIR] : &lt;2e-16         
##                                          
##                   Kappa : 0.9537         
##                                          
##  Mcnemar&#39;s Test P-Value : 0.2888         
##                                          
##             Sensitivity : 0.9818         
##             Specificity : 0.9842         
##          Pos Pred Value : 0.9474         
##          Neg Pred Value : 0.9947         
##              Prevalence : 0.2249         
##          Detection Rate : 0.2209         
##    Detection Prevalence : 0.2331         
##       Balanced Accuracy : 0.9830         
##                                          
##        &#39;Positive&#39; Class : arg            
## </code></pre>
<pre class="r"><code># https://www.r-bloggers.com/illustrated-guide-to-roc-and-auc/
df = cbind(fit = if_else(previsao==&quot;br&quot;,1,0), class = if_else(test$book==&quot;br&quot;,1,0)) %&gt;% as.data.frame()
plot_pred_type_distribution(df,0.5)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/nb-1.png" width="672" /></p>
<p>Apesar a aparente acur√°cia alta, o valor calculado para a especificidade (verdadeiros negativos) foi elevado o que aponta que o ajuste do modelo n√£o se apresentou de forma eficiente</p>
<p>O tempo computacional foi de 7.1403751373291 segundos</p>
</div>
<div id="glm---logit" class="section level3">
<h3>GLM - Logit</h3>
<p>Este √© um modelo estat√≠stico que j√° abordei aqui no blog no post sobre <a href="https://gomesfellipe.github.io/post/2018-05-26-smarteademachinelearning/smarteademachinelearning/">AED de forma r√°pida e um pouco de machine learning</a> e seguindo a recomenda√ß√£o do artigo da datacamp vejamos quais resultados obtemos com o ajuste deste modelo:</p>
<pre class="r"><code># Modelo log√≠stico ----
set.seed(825)
antes = Sys.time()
book_glm &lt;- train(book ~.,
                  data=train,
                  method = &quot;glm&quot;,                                         # modelo generalizado
                  family = binomial(link = &#39;logit&#39;),                      # Familia Binomial ligacao logit
                  trControl = trainControl(method = &quot;cv&quot;, number = 10))   # validacao cruzada
time_glm &lt;- Sys.time() - antes 
Sys.time() - antes</code></pre>
<pre><code>## Time difference of 1.378149 secs</code></pre>
<pre class="r"><code>library(ggfortify)

autoplot(book_glm$finalModel, which = 1:6, data = train,
         colour = &#39;book&#39;, label.size = 3,
         ncol = 3) + theme_classic()</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/glm-1.png" width="672" /></p>
<pre class="r"><code>previsao  = predict(book_glm, test)
confusionMatrix(previsao, factor(test$book))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction arg  br
##        arg 109   0
##        br    1 379
##                                           
##                Accuracy : 0.998           
##                  95% CI : (0.9887, 0.9999)
##     No Information Rate : 0.7751          
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.9941          
##                                           
##  Mcnemar&#39;s Test P-Value : 1               
##                                           
##             Sensitivity : 0.9909          
##             Specificity : 1.0000          
##          Pos Pred Value : 1.0000          
##          Neg Pred Value : 0.9974          
##              Prevalence : 0.2249          
##          Detection Rate : 0.2229          
##    Detection Prevalence : 0.2229          
##       Balanced Accuracy : 0.9955          
##                                           
##        &#39;Positive&#39; Class : arg             
## </code></pre>
<pre class="r"><code>df = cbind(fit = if_else(previsao==&quot;br&quot;,1,0), class = if_else(test$book==&quot;br&quot;,1,0)) %&gt;% as.data.frame()
plot_pred_type_distribution(df,0.5)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/glm-2.png" width="672" /></p>
</div>
</div>
</div>
<div id="comparando-modelos" class="section level1">
<h1>Comparando modelos</h1>
<p>Agora que temos 4 modelos ajustados e cada um apresentando resultados diferentes, vejamos qual deles seria o mais interessante para caso fosse necess√°rio recuperar a classifica√ß√£o dos termos pesquisados atrav√©s da API, veja a seguir um resumo das medidas obtidas:</p>
<pre class="r"><code># &quot;Dados esses modelos, podemos fazer declara√ß√µes estat√≠sticas sobre suas diferen√ßas de desempenho? Para fazer isso, primeiro coletamos os resultados de reamostragem usando resamples.&quot; - caret
resamps &lt;- resamples(list(knn = book_knn,
                          rf = book_rf,
                          nb = book_nb,
                          glm = book_glm)) 
summary(resamps)</code></pre>
<pre><code>## 
## Call:
## summary.resamples(object = resamps)
## 
## Models: knn, rf, nb, glm 
## Number of resamples: 10 
## 
## Accuracy 
##          Min.   1st Qu.    Median      Mean   3rd Qu. Max. NA&#39;s
## knn 0.9553571 0.9821824 0.9823009 0.9831305 0.9889381    1    0
## rf  0.9823009 1.0000000 1.0000000 0.9973451 1.0000000    1    0
## nb  0.9107143 0.9623894 0.9823009 0.9768726 1.0000000    1    0
## glm 0.9910714 0.9911504 1.0000000 0.9964523 1.0000000    1    0
## 
## Kappa 
##          Min.   1st Qu.    Median      Mean   3rd Qu. Max. NA&#39;s
## knn 0.8730734 0.9500663 0.9512773 0.9530901 0.9691458    1    0
## rf  0.9513351 1.0000000 1.0000000 0.9926689 1.0000000    1    0
## nb  0.7791798 0.8998204 0.9525409 0.9398109 1.0000000    1    0
## glm 0.9752868 0.9753544 1.0000000 0.9901350 1.0000000    1    0</code></pre>
<p>Como podemos ver, o modelo que apresentou a menor acur√°cia e o menor coeficiente kappa foi o Naive Bayes enquanto que o que apresentou as maiores medidas de qualidade do ajuste foi o modelo ajustado com o algor√≠timo Random Forest e tanto o modelo ajustado pelo algor√≠timo knn quanto o modelo linear generalizado com fun√ß√£o de liga√ß√£o ‚Äúlogit‚Äù tamb√©m apresentaram acur√°cia e coeficiente kappa pr√≥ximos do apresentado no ajuste do Random Forest.</p>
<p>Portanto, apesar dos ajustes, caso dois modelos n√£o apresentem diferen√ßa estatisticamente significante e o tempo computacional gasto para o ajuste de ambos for muito diferente pode ser que ser que tenhamos um modelo candidato para:</p>
<pre class="r"><code>c( knn= time_knn,rf = time_rf,nb = time_nb,glm = time_glm)</code></pre>
<pre><code>## Time differences in secs
##      knn       rf       nb      glm 
## 2.463859 8.992998 7.140375 1.377073</code></pre>
<p>O modelo linear generalizado foi o que apresentou o menor tempo computacional e foi o que apresentou o terceiro maior registro para os as medidas de qualidade do ajuste dos modelos, portanto esse modelo ser√° avaliado com mais cuidado em seguida para saber se ele ser√° o modelo selecionado</p>
<p><strong>Obs.:</strong> Sou suspeito para falar mas dentre esses modelos eu teria prefer√™ncia por este modelo de qualquer maneira por n√£o se tratar de uma ‚Äúcaixa preta,‚Äù da qual todos os efeitos de cada par√¢metro ajustado podem ser interpretado, al√©m de obter medidas como raz√µes de chance que ajudam bastante na compreens√£o dos dados.</p>
<p>Comparando de forma visual:</p>
<pre class="r"><code>splom(resamps)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<p>Assim fica mais claro o como o ajuste dos modelos Random Forest, K-NN e GLM se destacaram quando avaliados em rela√ß√£o a acur√°cia apresentada.</p>
<p>Vejamos a seguir como foi a distribui√ß√£o dessas medidas de acordo com cada modelo atrav√©s de boxplots:</p>
<pre class="r"><code>bwplot(resamps)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>Note que al√©m de apresentar os ajustes com menor acur√°cia (e elevada taxa de falsos negativos) o algor√≠timo Naive Bayes foi o que apresentou a maior varia√ß√£o interquartil das medidas de qualidade do ajuste do modelo.</p>
<p>Para finalizar a an√°lise visual vamos obter as diferen√ßas entre os modelos com a fun√ß√£o <code>diff()</code> e em seguida conferir de maneira visual o comportamento dessas informa√ß√µes:</p>
<pre class="r"><code>difValues &lt;- diff(resamps)

# plot:
bwplot(difValues)</code></pre>
<p><img src="/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>Observe que tanto o modelo log√≠stico quando o ajuste com o algor√≠timo K-NN apresentaram valores muito pr√≥ximos dos valores do ajuste do Random Forest e como j√° vimos o Random Forest foi o modelo que levou maior tempo computacional para ser ajustado, portanto vamos conferir a seguir se existe diferen√ßa estatisticamente significante entre os valores obtidos atrav√©s de cada um dos ajustes e decidir qual dos modelos se apresentou de maneira mais adequada para nosso caso:</p>
<pre class="r"><code>resamps$values %&gt;% 
  select_if(is.numeric) %&gt;% 
  purrr::map(function(x) shapiro.test(x))</code></pre>
<pre><code>## $`knn~Accuracy`
## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.87602, p-value = 0.1174
## 
## 
## $`knn~Kappa`
## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.87418, p-value = 0.1118
## 
## 
## $`rf~Accuracy`
## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.53165, p-value = 8.564e-06
## 
## 
## $`rf~Kappa`
## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.53234, p-value = 8.727e-06
## 
## 
## $`nb~Accuracy`
## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.80077, p-value = 0.01482
## 
## 
## $`nb~Kappa`
## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.81793, p-value = 0.02392
## 
## 
## $`glm~Accuracy`
## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.6429, p-value = 0.0001803
## 
## 
## $`glm~Kappa`
## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.64123, p-value = 0.0001722</code></pre>
<p>Como a hip√≥tese de normalidade n√£o foi rejeitada para nenhuma das amostras de acur√°cias registradas, vejamos se existe diferen√ßa estatisticamente significante entre as m√©dias dessas medidas de qualidade para cada modelo:</p>
<pre class="r"><code>t.test(resamps$values$`rf~Accuracy`,resamps$values$`knn~Accuracy`, paired = T)  </code></pre>
<pre><code>## 
##  Paired t-test
## 
## data:  resamps$values$`rf~Accuracy` and resamps$values$`knn~Accuracy`
## t = 3.9961, df = 9, p-value = 0.003129
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.0061678 0.0222614
## sample estimates:
## mean of the differences 
##               0.0142146</code></pre>
<p>Rejeita a hip√≥tese de que as m√©dias das acur√°cias calculadas para o ajuste do algor√≠timo Random Forest e K-NN foram iguais</p>
<pre class="r"><code>t.test(resamps$values$`rf~Accuracy`,resamps$values$`glm~Accuracy`, paired = T)  </code></pre>
<pre><code>## 
##  Paired t-test
## 
## data:  resamps$values$`rf~Accuracy` and resamps$values$`glm~Accuracy`
## t = 0.43326, df = 9, p-value = 0.675
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.003768926  0.005554640
## sample estimates:
## mean of the differences 
##            0.0008928571</code></pre>
<p>Novamente, rejeita-se a hip√≥tese de que as m√©dias das acur√°cias calculadas para o ajuste do algor√≠timo Random Forest e do modelo de log√≠stico foram iguais</p>
<pre class="r"><code>t.test(resamps$values$`knn~Accuracy`,resamps$values$`glm~Accuracy`, paired = T)</code></pre>
<pre><code>## 
##  Paired t-test
## 
## data:  resamps$values$`knn~Accuracy` and resamps$values$`glm~Accuracy`
## t = -4.0077, df = 9, p-value = 0.003074
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.020841197 -0.005802292
## sample estimates:
## mean of the differences 
##             -0.01332174</code></pre>
<p>J√° para a compara√ß√£o entre as m√©dias das acur√°cias calculadas para o algor√≠timo K-NN e para o modelo log√≠stico n√£o houve evid√™ncias estat√≠sticas para se rejeitas a hip√≥tese de que ambas as m√©dias s√£o iguais, o que nos sugere o modelo log√≠stico como o segundo melhor candidato como modelo de classifica√ß√£o para este problema com estes dados.</p>
<p>Ent√£o a escolha ficar√° a crit√©rio do que √© mais importante. Caso o tempo computacional fosse uma medida que tivesse mais import√¢ncia do que a pequena superioridade de acur√°cia apresentada pelo algor√≠timo Random Forest, escolheria o modelo log√≠stico, por√©m como neste caso os 7.61592507362366 segundos a mais para ajustar o modelo n√£o fazem diferen√ßa para mim, fico com o modelo Random Forest.</p>
<p>Este post tr√°s alguns dos conceitos que venho estudado e existem muitos t√≥picos apresentados aqui que podem (e devem) ser estudados com mais profundidade, espero que tenha gostado!</p>
</div>
<div id="refer√™ncias" class="section level1">
<h1>Refer√™ncias</h1>
<p>obs.: links mensionados no corpo do texto</p>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-caret" class="csl-entry">
Kuhn, Max. 2018. <em>The Caret Package</em>. <a href="https://topepo.github.io/caret/index.html">https://topepo.github.io/caret/index.html</a>.
</div>
<div id="ref-tidytext" class="csl-entry">
Silge; Robinson, Julia; David. 2018. <em>Text Mining with R</em>. <em>A Tidy Approach</em>. <a href="https://www.tidytextmining.com/">https://www.tidytextmining.com/</a>.
</div>
<div id="ref-miner" class="csl-entry">
Silva; Peres; Boscarioli, Leandro Augusto; Sarajane Marques; Clodis. 2016. <em>Introdu√ß√£o √† Minera√ß√£o de Dados</em>. <em>Com Aplica√ß√µes Em R</em>. Vol. 3. Elsevier Editora Ltda.
</div>
</div>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2018-06-24-brasil-argentina-tidytext-ml/brasil-argentina-tidytext-ml/">Brasil x Argentina, tidytext e Machine Learning</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Analise Explorat√≥ria</category>
      <category>Aprendizado N√£o Supervisionado</category>
      <category>Data mining</category>
      <category>Estatistica</category>
      <category>Machine Learning</category>
      <category>Modelagem Estatistica</category>
      <category>Pr√°tica</category>
      <category>R</category>
      <category>Text Mining</category>
      <category>An√°lise de Sentimentos</category>
      <category domain="tag">Data Mining</category>
      <category domain="tag">Estatistica</category>
      <category domain="tag">gomesfellipe</category>
      <category domain="tag">twitter</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">Pr√°tica</category>
      <category domain="tag">R</category>
      <category domain="tag">text mining</category>
    </item>
    <item>
      <title>AED de forma r√°pida e um pouco de Machine Learning</title>
      <link>https://gomesfellipe.github.io/post/2018-05-26-smarteademachinelearning/smarteademachinelearning/</link>
      <pubDate>Sat, 26 May 2018 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2018-05-26-smarteademachinelearning/smarteademachinelearning/</guid>
      <description>Veja como √© poss√≠vel realizar a AED de forma muito r√°pida com o pacote SmartEAD, al√©m de uma breve aplica√ß√£o de t√©cnicas de machine learning e estat√≠stica para ilustrar alguns poss√≠veis cen√°rios da analise da dados</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/pagedtable/css/pagedtable.css" rel="stylesheet" />
<script src="/rmarkdown-libs/pagedtable/js/pagedtable.js"></script>


<style>
  .col2 {
    columns: 2 200px;         /* number of columns and width in pixels*/
    -webkit-columns: 2 200px; /* chrome, safari */
    -moz-columns: 2 200px;    /* firefox */
  }
  .col3 {
    columns: 3 100px;
    -webkit-columns: 3 100px;
    -moz-columns: 3 100px;
  }
</style>
<!-- Resumo: Neste post mostro como √© poss√≠vel realizar a AED de forma muito r√°pida com o pacote SmartEAD, e aplico algumas t√©cnicas de machine learning e estat√≠stica para ilustrar alguns poss√≠veis cen√°rios-->
<div id="a-an√°lise-explorat√≥ria-dos-dados" class="section level1">
<h1>A an√°lise explorat√≥ria dos dados</h1>
<div class="col2">
<p>A an√°lise explorat√≥ria dos dados (AED) foi um termo que ganhou bastante popularidade quando Tukey publicou o livro Exploratory Data Analysis em 1977 que tratava uma ‚Äúbusca por conhecimento antes da an√°lise de dados de fato‚Äù. Ocorre quando busca-se obter informa√ß√µes ocultas sobre os dados, tais como: varia√ß√£o, anomalias, distribui√ß√£o, tend√™ncias, padr√µes e rela√ß√µes</p>
<p>Ao iniciar uma an√°lise de dados, come√ßamos pela AED para a partir dai decidir como buscar qual solu√ß√£o para o problema. √â importante frisar que a AED e a constru√ß√£o de gr√°ficos <strong>n√£o</strong> s√£o a mesma coisa, mesmo a AED sendo altamente baseada em produ√ß√£o de gr√°ficos como de dispers√£o, histogramas, boxplots etc.</p>
<p>Por vezes a AED no R pode envolver a produ√ß√£o de longos scripts utilizando fun√ß√µes como as do pacote <code>ggplot2</code> e mesmo sabendo que desejamos sempre criar o gr√°fico de maneira mais informativa e atraente poss√≠vel, as vezes precisamos ter uma no√ß√£o geral dos dados de forma r√°pida, n√£o necessariamente t√£o detalhada e customizada de cara.</p>
<p>A vezes queremos apenas ter uma primeira impress√£o dos dados e em seguida pensar em quais os gr√°ficos mais se adequariam para a entrega dos resultados que mesmo as fun√ß√µes base do R dependendo do caso tamb√©m envolvem a confec√ß√£o de longos scripts.</p>
<p>Existem pacotes que auxiliam na hora de se fazer uma r√°pida an√°lise explorat√≥ria, como o <a href="https://github.com/ropenscilabs/skimr">skimr</a> e o <a href="https://github.com/boxuancui/DataExplorer">DataExplorer</a>. Por√©m estava pesquisando de existiam mais op√ß√µes para uma r√°pida abordagem de AED e me deparei com esta <a href="https://cran.r-project.org/web/packages/SmartEDA/vignettes/Report_r1.html">vinheta</a>, por Dayanand, Kiran, Ravi.</p>
<p>Essa vinheta apresenta o pacote <a href="https://cran.r-project.org/web/packages/SmartEDA"><code>SmartEAD</code></a> que tr√°s uma s√©rie de fun√ß√µes que auxiliam na AED de forma bem pr√°tica. O pacote est√° dispon√≠vel no CRAN.</p>
<p>Para testar o pacote foi utilizada uma base de dados do artigo <a href="http://people.stern.nyu.edu/wgreene/Lugano2013/Fair-ExtramaritalAffairs.pdf">A Theory of Extramarital Affairs</a>, publicado pela <a href="http://www.jstor.org/publisher/ucpress">The University of Chicago Press</a>.</p>
<p>Gostei tanto da proposta do pacote que resolvi preparar este post que conta com a explana√ß√£o de alguns t√≥picos apresentados pelo autor, algumas explica√ß√µes da teoria estat√≠stica apresentada na an√°lise descritiva e explorat√≥ria dos dados e al√©m da aplica√ß√£o de algumas t√©cnicas estat√≠sticas e de machine learning para o entendimento da base de dados.</p>
</div>
<p></br></p>
</div>
<div id="smarteda" class="section level1">
<h1>SmartEDA</h1>
<p>Como ele pode ajud√°-lo a criar uma an√°lise de dados explorat√≥ria? O <code>SmartEDA</code> inclui v√°rias fun√ß√µes personalizadas para executar uma an√°lise explorat√≥ria inicial em qualquer dado de entrada. A sa√≠da gerada pode ser obtida em formato resumido e gr√°fico e os resultados tamb√©m podem ser exportados como relat√≥rios.</p>
<p>O pacote SmartEDA ajuda a construir uma boa base de compreens√£o de dados, algumas de suas funcionalidades s√£o:</p>
<ul>
<li>O pacote SmartEDA far√° com que voc√™ seja capaz de aplicar diferentes tipos de EDA sem ter que lembre-se dos diferentes nomes dos pacotes R e escrever longos scripts R com esfor√ßo manual para preparar o relat√≥rio da EDA, permitindo o entendimento dos dados de maneira mais r√°pida</li>
<li>N√£o h√° necessidade de categorizar as vari√°veis em caractere, num√©rico, fator etc. As fun√ß√µes do SmartEDA categorizam automaticamente todos os recursos no tipo de dados correto (caractere, num√©rico, fator etc.) com base nos dados de entrada.</li>
</ul>
<p>O pacote SmartEDA ajuda a obter a an√°lise completa dos dados explorat√≥rios apenas executando a fun√ß√£o em vez de escrever um longo c√≥digo r.</p>
<div id="carregando-o-pacote" class="section level2">
<h2>Carregando o pacote:</h2>
<pre class="r"><code># install.packages(&quot;SmartEDA&quot;)
library(&quot;SmartEDA&quot;)</code></pre>
<p>outros pactes que ser√£o utilizados no post (incluindo um script com algumas fun√ß√µes, que estar√° dispon√≠vel no meu github <a href="https://github.com/gomesfellipe/gomesfellipe.github.io/blob/master/post/2018-05-26-smarteademachinelearning/functions.R">neste link</a>).</p>
<pre class="r"><code>library(knitr)        # Para tabelas interativas
library(DT)           # Para tabelas interativas
library(dplyr)        # Para manipulacao de dados
library(plotly)       # Para gerar uma tabela
library(psych)        # para an√°lise fatorial
source(&quot;functions.R&quot;) # script com funcoes customizadas</code></pre>
<div id="base-de-dados-utilizada" class="section level3">
<h3>Base de dados utilizada:</h3>
<div class="col2">
<p>Estava √† procura de uma base de dados para testar as funcionalidades do pacote <code>SmartEAD</code> quando um colega de trabalho me mostrou um artigo chamado <a href="http://people.stern.nyu.edu/wgreene/Lugano2013/Fair-ExtramaritalAffairs.pdf">A Theory of Extramarital Affairs</a>, publicado pela <a href="http://www.jstor.org/publisher/ucpress">The University of Chicago Press</a>. Neste artigo √© desenvolvido um <a href="https://en.wikipedia.org/wiki/Tobit_model">modelo pelo estimador de Tobit</a> que explica a aloca√ß√£o de um tempo do indiv√≠duo entre o trabalho e dois tipos de atividades de lazer: tempo passou com o c√¥njuge e tempo gasto com o amante.</p>
<p>N√£o conhecia o modelo proposto e em uma r√°pida pesquisa no Google notei que alguns dos dados utilizados nesse artigo est√£o dispon√≠veis no pacote <a href="ftp://cran.r-project.org/pub/R/web/packages/AER">AER</a> de Econometria Aplicada com R, que cont√©m fun√ß√µes, conjuntos de dados, exemplos, demonstra√ß√µes e vinhetas para o livro <a href="http://jrsyzx.njau.edu.cn/__local/C/94/F1/35C7CC5EDA214D4AAE7FE2BA0FD_0D3DFF32_3CDD40.pdf?e=.pdf">Applied Econometrics with R</a> e como esses dados j√° foram tratados e est√£o ‚Äúprontos para an√°lise‚Äù, resolvi usar essa amostra pela conveni√™ncia.</p>
<p>Portanto farei aqui uma an√°lise explorat√≥ria e ao final de cada caso (<em>sem vari√°vel reposta</em>, <em>com vari√°vel resposta num√©rica</em> e <em>com vari√°vel resposta bin√°ria</em>), para ter uma breve intui√ß√£o de como se comportam os dados irei primeiro utilizar um <em>algor√≠timo de machine learning n√£o supervisionado</em> para o agrupamento das observa√ß√µes (sem considerar q j√° conhecemos a vari√°vel resposta), depois ajustar um* modelo de regress√£o linear simples* considerando a vari√°vel resposta como num√©rica e por fim o ajuste de um <em>algor√≠timo de machine learning supervisonado de classifica√ß√£o</em> ap√≥s discretizar a vari√°vel resposta.</p>
<p>A base de dados pode ser conferida a seguir:</p>
</div>
<pre class="r"><code>library(AER)
data(Affairs)
Affairs %&gt;% rmarkdown::paged_table()</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["affairs"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["gender"],"name":[2],"type":["fct"],"align":["left"]},{"label":["age"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["yearsmarried"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["children"],"name":[5],"type":["fct"],"align":["left"]},{"label":["religiousness"],"name":[6],"type":["int"],"align":["right"]},{"label":["education"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["occupation"],"name":[8],"type":["int"],"align":["right"]},{"label":["rating"],"name":[9],"type":["int"],"align":["right"]}],"data":[{"1":"0","2":"male","3":"37.0","4":"10.000","5":"no","6":"3","7":"18","8":"7","9":"4","_rn_":"4"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"no","6":"4","7":"14","8":"6","9":"4","_rn_":"5"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"1","7":"12","8":"1","9":"4","_rn_":"11"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"5","7":"18","8":"6","9":"5","_rn_":"16"},{"1":"0","2":"male","3":"22.0","4":"0.750","5":"no","6":"2","7":"17","8":"6","9":"3","_rn_":"23"},{"1":"0","2":"female","3":"32.0","4":"1.500","5":"no","6":"2","7":"17","8":"5","9":"5","_rn_":"29"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"2","7":"12","8":"1","9":"3","_rn_":"44"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"2","7":"14","8":"4","9":"4","_rn_":"45"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"4","7":"16","8":"1","9":"2","_rn_":"47"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"no","6":"4","7":"14","8":"4","9":"5","_rn_":"49"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"2","7":"20","8":"7","9":"2","_rn_":"50"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"4","7":"18","8":"6","9":"4","_rn_":"55"},{"1":"0","2":"male","3":"47.0","4":"15.000","5":"yes","6":"5","7":"17","8":"6","9":"4","_rn_":"64"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"17","8":"5","9":"4","_rn_":"80"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"no","6":"4","7":"14","8":"5","9":"4","_rn_":"86"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"1","7":"17","8":"5","9":"5","_rn_":"93"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"2","7":"18","8":"4","9":"3","_rn_":"108"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"3","7":"16","8":"5","9":"4","_rn_":"114"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"16","8":"5","9":"5","_rn_":"115"},{"1":"0","2":"female","3":"27.0","4":"10.000","5":"yes","6":"2","7":"14","8":"1","9":"5","_rn_":"116"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"16","8":"5","9":"5","_rn_":"123"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"16","8":"5","9":"5","_rn_":"127"},{"1":"0","2":"female","3":"27.0","4":"10.000","5":"yes","6":"4","7":"16","8":"5","9":"4","_rn_":"129"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"3","7":"14","8":"1","9":"5","_rn_":"134"},{"1":"0","2":"male","3":"37.0","4":"4.000","5":"yes","6":"2","7":"20","8":"6","9":"4","_rn_":"137"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"18","8":"5","9":"5","_rn_":"139"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"no","6":"4","7":"16","8":"1","9":"5","_rn_":"147"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"5","7":"20","8":"6","9":"4","_rn_":"151"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"3","7":"16","8":"5","9":"5","_rn_":"153"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"3","7":"17","8":"5","9":"4","_rn_":"155"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"3","_rn_":"162"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"3","7":"16","8":"5","9":"5","_rn_":"163"},{"1":"0","2":"male","3":"27.0","4":"0.417","5":"no","6":"4","7":"17","8":"6","9":"4","_rn_":"165"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"5","7":"14","8":"5","9":"4","_rn_":"168"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"yes","6":"1","7":"18","8":"6","9":"4","_rn_":"170"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"4","7":"16","8":"5","9":"3","_rn_":"172"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"3","7":"12","8":"1","9":"4","_rn_":"184"},{"1":"0","2":"female","3":"22.0","4":"4.000","5":"no","6":"4","7":"17","8":"5","9":"5","_rn_":"187"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"yes","6":"1","7":"14","8":"3","9":"5","_rn_":"192"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"3","7":"16","8":"1","9":"5","_rn_":"194"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"yes","6":"5","7":"20","8":"6","9":"5","_rn_":"210"},{"1":"0","2":"male","3":"52.0","4":"15.000","5":"yes","6":"5","7":"18","8":"6","9":"3","_rn_":"217"},{"1":"0","2":"female","3":"22.0","4":"0.417","5":"no","6":"5","7":"14","8":"1","9":"4","_rn_":"220"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"2","7":"18","8":"6","9":"1","_rn_":"224"},{"1":"0","2":"female","3":"32.0","4":"7.000","5":"yes","6":"5","7":"17","8":"5","9":"3","_rn_":"227"},{"1":"0","2":"male","3":"22.0","4":"4.000","5":"no","6":"3","7":"16","8":"5","9":"5","_rn_":"228"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"18","8":"6","9":"5","_rn_":"239"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"2","7":"18","8":"5","9":"4","_rn_":"241"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"yes","6":"4","7":"16","8":"3","9":"5","_rn_":"245"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"2","7":"20","8":"6","9":"4","_rn_":"249"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"5","7":"14","8":"3","9":"5","_rn_":"262"},{"1":"0","2":"male","3":"32.0","4":"7.000","5":"yes","6":"2","7":"20","8":"6","9":"4","_rn_":"265"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"5","7":"20","8":"6","9":"5","_rn_":"267"},{"1":"0","2":"male","3":"27.0","4":"10.000","5":"yes","6":"4","7":"20","8":"6","9":"4","_rn_":"269"},{"1":"0","2":"male","3":"22.0","4":"4.000","5":"no","6":"1","7":"18","8":"5","9":"5","_rn_":"271"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"14","8":"3","9":"1","_rn_":"277"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"yes","6":"5","7":"16","8":"4","9":"4","_rn_":"290"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"17","8":"1","9":"5","_rn_":"292"},{"1":"0","2":"female","3":"27.0","4":"0.750","5":"no","6":"4","7":"17","8":"5","9":"4","_rn_":"293"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"yes","6":"4","7":"20","8":"6","9":"4","_rn_":"295"},{"1":"0","2":"female","3":"47.0","4":"15.000","5":"yes","6":"5","7":"14","8":"7","9":"2","_rn_":"299"},{"1":"0","2":"male","3":"37.0","4":"10.000","5":"yes","6":"3","7":"20","8":"6","9":"4","_rn_":"320"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"2","7":"16","8":"5","9":"5","_rn_":"321"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"no","6":"2","7":"18","8":"4","9":"5","_rn_":"324"},{"1":"0","2":"male","3":"32.0","4":"7.000","5":"no","6":"4","7":"20","8":"6","9":"4","_rn_":"334"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"2","7":"17","8":"3","9":"5","_rn_":"351"},{"1":"0","2":"male","3":"37.0","4":"10.000","5":"yes","6":"4","7":"20","8":"6","9":"4","_rn_":"355"},{"1":"0","2":"female","3":"47.0","4":"15.000","5":"yes","6":"3","7":"17","8":"6","9":"5","_rn_":"361"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"5","7":"16","8":"5","9":"5","_rn_":"362"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"no","6":"2","7":"16","8":"6","9":"4","_rn_":"366"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"no","6":"3","7":"17","8":"5","9":"5","_rn_":"370"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"5","7":"14","8":"4","9":"5","_rn_":"374"},{"1":"0","2":"female","3":"22.0","4":"0.125","5":"no","6":"2","7":"12","8":"5","9":"5","_rn_":"378"},{"1":"0","2":"male","3":"47.0","4":"15.000","5":"yes","6":"4","7":"14","8":"4","9":"3","_rn_":"381"},{"1":"0","2":"male","3":"32.0","4":"15.000","5":"yes","6":"1","7":"14","8":"5","9":"5","_rn_":"382"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"yes","6":"4","7":"16","8":"5","9":"5","_rn_":"383"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"yes","6":"3","7":"16","8":"5","9":"5","_rn_":"384"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"3","7":"17","8":"6","9":"5","_rn_":"400"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"3","7":"16","8":"5","9":"5","_rn_":"403"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"2","7":"14","8":"7","9":"2","_rn_":"409"},{"1":"0","2":"male","3":"17.5","4":"1.500","5":"yes","6":"3","7":"18","8":"6","9":"5","_rn_":"412"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"5","_rn_":"413"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"2","7":"16","8":"3","9":"4","_rn_":"416"},{"1":"0","2":"male","3":"42.0","4":"4.000","5":"no","6":"4","7":"17","8":"3","9":"3","_rn_":"418"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"yes","6":"4","7":"12","8":"1","9":"5","_rn_":"422"},{"1":"0","2":"female","3":"22.0","4":"0.417","5":"no","6":"1","7":"17","8":"6","9":"4","_rn_":"435"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"4","7":"17","8":"5","9":"5","_rn_":"439"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"no","6":"3","7":"18","8":"5","9":"2","_rn_":"445"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"yes","6":"3","7":"14","8":"1","9":"5","_rn_":"447"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"3","7":"14","8":"1","9":"4","_rn_":"448"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"4","7":"14","8":"3","9":"4","_rn_":"449"},{"1":"0","2":"male","3":"37.0","4":"10.000","5":"yes","6":"2","7":"14","8":"5","9":"3","_rn_":"478"},{"1":"0","2":"male","3":"37.0","4":"10.000","5":"yes","6":"4","7":"16","8":"5","9":"4","_rn_":"482"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"5","7":"20","8":"5","9":"3","_rn_":"486"},{"1":"0","2":"male","3":"27.0","4":"0.417","5":"no","6":"1","7":"16","8":"3","9":"4","_rn_":"489"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"5","7":"14","8":"1","9":"5","_rn_":"490"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"3","7":"16","8":"6","9":"1","_rn_":"491"},{"1":"0","2":"male","3":"37.0","4":"10.000","5":"yes","6":"1","7":"16","8":"6","9":"4","_rn_":"492"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"3","7":"17","8":"5","9":"5","_rn_":"503"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"5","_rn_":"508"},{"1":"0","2":"female","3":"27.0","4":"10.000","5":"yes","6":"5","7":"14","8":"1","9":"5","_rn_":"509"},{"1":"0","2":"male","3":"37.0","4":"10.000","5":"yes","6":"2","7":"18","8":"6","9":"4","_rn_":"512"},{"1":"0","2":"female","3":"22.0","4":"0.125","5":"no","6":"4","7":"12","8":"4","9":"5","_rn_":"515"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"5","7":"20","8":"6","9":"5","_rn_":"517"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"18","8":"6","9":"4","_rn_":"532"},{"1":"0","2":"male","3":"22.0","4":"4.000","5":"yes","6":"4","7":"14","8":"6","9":"4","_rn_":"533"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"yes","6":"4","7":"18","8":"5","9":"4","_rn_":"535"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"4","7":"20","8":"5","9":"4","_rn_":"537"},{"1":"0","2":"male","3":"32.0","4":"15.000","5":"yes","6":"3","7":"14","8":"6","9":"3","_rn_":"538"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"14","8":"5","9":"4","_rn_":"543"},{"1":"0","2":"female","3":"32.0","4":"7.000","5":"yes","6":"4","7":"17","8":"1","9":"5","_rn_":"547"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"17","8":"6","9":"5","_rn_":"550"},{"1":"0","2":"female","3":"32.0","4":"1.500","5":"no","6":"5","7":"18","8":"5","9":"5","_rn_":"558"},{"1":"0","2":"male","3":"42.0","4":"10.000","5":"yes","6":"5","7":"20","8":"7","9":"4","_rn_":"571"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"no","6":"3","7":"16","8":"5","9":"4","_rn_":"578"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"no","6":"4","7":"20","8":"6","9":"5","_rn_":"583"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"14","8":"3","9":"2","_rn_":"586"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"no","6":"5","7":"18","8":"6","9":"4","_rn_":"594"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"4","7":"16","8":"1","9":"5","_rn_":"597"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"12","8":"2","9":"4","_rn_":"602"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"2","7":"16","8":"2","9":"5","_rn_":"603"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"5","7":"18","8":"5","9":"4","_rn_":"604"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"4","7":"17","8":"5","9":"3","_rn_":"612"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"2","7":"16","8":"1","9":"2","_rn_":"613"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"3","7":"16","8":"5","9":"5","_rn_":"621"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"5","7":"20","8":"6","9":"5","_rn_":"627"},{"1":"0","2":"female","3":"22.0","4":"0.125","5":"no","6":"2","7":"14","8":"4","9":"5","_rn_":"630"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"no","6":"4","7":"16","8":"5","9":"5","_rn_":"631"},{"1":"0","2":"male","3":"32.0","4":"1.500","5":"no","6":"2","7":"18","8":"6","9":"5","_rn_":"632"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"no","6":"2","7":"17","8":"6","9":"5","_rn_":"639"},{"1":"0","2":"female","3":"27.0","4":"10.000","5":"yes","6":"4","7":"16","8":"1","9":"3","_rn_":"645"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"4","7":"18","8":"6","9":"5","_rn_":"647"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"no","6":"2","7":"16","8":"6","9":"5","_rn_":"648"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"no","6":"2","7":"18","8":"6","9":"3","_rn_":"651"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"3","7":"14","8":"5","9":"3","_rn_":"655"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"3","7":"18","8":"5","9":"4","_rn_":"667"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"2","7":"18","8":"6","9":"5","_rn_":"670"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"2","7":"16","8":"1","9":"4","_rn_":"671"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"4","7":"20","8":"5","9":"5","_rn_":"673"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"no","6":"1","7":"20","8":"5","9":"4","_rn_":"701"},{"1":"0","2":"female","3":"27.0","4":"10.000","5":"yes","6":"2","7":"12","8":"1","9":"4","_rn_":"705"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"5","7":"18","8":"6","9":"4","_rn_":"706"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"yes","6":"5","7":"12","8":"5","9":"3","_rn_":"709"},{"1":"0","2":"male","3":"52.0","4":"15.000","5":"yes","6":"2","7":"18","8":"5","9":"4","_rn_":"717"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"no","6":"3","7":"20","8":"6","9":"3","_rn_":"719"},{"1":"0","2":"male","3":"37.0","4":"4.000","5":"yes","6":"1","7":"18","8":"5","9":"4","_rn_":"723"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"4","7":"14","8":"5","9":"4","_rn_":"724"},{"1":"0","2":"female","3":"52.0","4":"15.000","5":"yes","6":"5","7":"12","8":"1","9":"3","_rn_":"726"},{"1":"0","2":"female","3":"57.0","4":"15.000","5":"yes","6":"4","7":"16","8":"6","9":"4","_rn_":"734"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"yes","6":"1","7":"16","8":"5","9":"4","_rn_":"735"},{"1":"0","2":"male","3":"37.0","4":"7.000","5":"yes","6":"4","7":"20","8":"6","9":"3","_rn_":"736"},{"1":"0","2":"male","3":"22.0","4":"0.750","5":"no","6":"2","7":"14","8":"4","9":"3","_rn_":"737"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"yes","6":"2","7":"18","8":"5","9":"3","_rn_":"739"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"3","_rn_":"743"},{"1":"0","2":"male","3":"22.0","4":"0.750","5":"yes","6":"2","7":"14","8":"4","9":"3","_rn_":"745"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"3","_rn_":"747"},{"1":"0","2":"female","3":"52.0","4":"15.000","5":"yes","6":"5","7":"17","8":"1","9":"1","_rn_":"751"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"14","8":"1","9":"2","_rn_":"752"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"yes","6":"4","7":"14","8":"5","9":"3","_rn_":"754"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"yes","6":"2","7":"16","8":"5","9":"5","_rn_":"760"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"2","7":"18","8":"6","9":"5","_rn_":"763"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"2","7":"18","8":"5","9":"5","_rn_":"774"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"5","7":"18","8":"6","9":"5","_rn_":"776"},{"1":"0","2":"female","3":"47.0","4":"15.000","5":"yes","6":"5","7":"12","8":"5","9":"4","_rn_":"779"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"3","7":"17","8":"1","9":"4","_rn_":"784"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"yes","6":"4","7":"17","8":"1","9":"2","_rn_":"788"},{"1":"0","2":"female","3":"57.0","4":"15.000","5":"yes","6":"2","7":"18","8":"5","9":"2","_rn_":"794"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"4","7":"14","8":"5","9":"4","_rn_":"795"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"3","7":"14","8":"3","9":"4","_rn_":"798"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"4","7":"9","8":"2","9":"2","_rn_":"800"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"5","_rn_":"803"},{"1":"0","2":"female","3":"22.0","4":"0.125","5":"no","6":"4","7":"14","8":"4","9":"5","_rn_":"807"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"4","7":"14","8":"1","9":"5","_rn_":"812"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"3","7":"18","8":"5","9":"4","_rn_":"820"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"no","6":"2","7":"18","8":"6","9":"5","_rn_":"823"},{"1":"0","2":"male","3":"32.0","4":"0.125","5":"yes","6":"2","7":"18","8":"5","9":"2","_rn_":"830"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"no","6":"3","7":"16","8":"5","9":"4","_rn_":"843"},{"1":"0","2":"female","3":"27.0","4":"10.000","5":"yes","6":"2","7":"16","8":"1","9":"4","_rn_":"848"},{"1":"0","2":"female","3":"32.0","4":"7.000","5":"yes","6":"4","7":"16","8":"1","9":"3","_rn_":"851"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"14","8":"5","9":"4","_rn_":"854"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"5","7":"17","8":"6","9":"2","_rn_":"856"},{"1":"0","2":"male","3":"32.0","4":"1.500","5":"yes","6":"4","7":"14","8":"6","9":"5","_rn_":"857"},{"1":"0","2":"female","3":"32.0","4":"4.000","5":"yes","6":"3","7":"17","8":"5","9":"3","_rn_":"859"},{"1":"0","2":"female","3":"37.0","4":"7.000","5":"no","6":"4","7":"18","8":"5","9":"5","_rn_":"863"},{"1":"0","2":"female","3":"22.0","4":"0.417","5":"yes","6":"3","7":"14","8":"3","9":"5","_rn_":"865"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"14","8":"1","9":"5","_rn_":"867"},{"1":"0","2":"male","3":"27.0","4":"0.750","5":"no","6":"3","7":"16","8":"5","9":"5","_rn_":"870"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"2","7":"20","8":"5","9":"5","_rn_":"873"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"yes","6":"4","7":"16","8":"4","9":"5","_rn_":"875"},{"1":"0","2":"male","3":"32.0","4":"15.000","5":"yes","6":"1","7":"14","8":"5","9":"5","_rn_":"876"},{"1":"0","2":"male","3":"22.0","4":"0.750","5":"no","6":"3","7":"17","8":"4","9":"5","_rn_":"877"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"17","8":"1","9":"4","_rn_":"880"},{"1":"0","2":"male","3":"27.0","4":"0.417","5":"yes","6":"4","7":"20","8":"5","9":"4","_rn_":"903"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"20","8":"5","9":"4","_rn_":"904"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"2","7":"14","8":"1","9":"3","_rn_":"905"},{"1":"0","2":"male","3":"22.0","4":"4.000","5":"yes","6":"1","7":"18","8":"5","9":"4","_rn_":"908"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"17","8":"5","9":"3","_rn_":"909"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"14","8":"4","9":"5","_rn_":"910"},{"1":"0","2":"male","3":"52.0","4":"15.000","5":"yes","6":"4","7":"14","8":"6","9":"2","_rn_":"912"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"4","7":"17","8":"5","9":"5","_rn_":"914"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"yes","6":"5","7":"14","8":"3","9":"5","_rn_":"915"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"yes","6":"2","7":"14","8":"3","9":"5","_rn_":"916"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"3","7":"16","8":"6","9":"5","_rn_":"920"},{"1":"0","2":"male","3":"27.0","4":"0.750","5":"no","6":"2","7":"18","8":"3","9":"3","_rn_":"921"},{"1":"0","2":"female","3":"22.0","4":"7.000","5":"yes","6":"2","7":"14","8":"5","9":"2","_rn_":"925"},{"1":"0","2":"female","3":"27.0","4":"0.750","5":"no","6":"2","7":"17","8":"5","9":"3","_rn_":"926"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"12","8":"1","9":"2","_rn_":"929"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"1","7":"14","8":"1","9":"5","_rn_":"931"},{"1":"0","2":"female","3":"37.0","4":"10.000","5":"no","6":"2","7":"12","8":"4","9":"4","_rn_":"945"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"18","8":"5","9":"3","_rn_":"947"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"3","7":"12","8":"3","9":"3","_rn_":"949"},{"1":"0","2":"male","3":"22.0","4":"4.000","5":"no","6":"2","7":"18","8":"5","9":"5","_rn_":"950"},{"1":"0","2":"male","3":"52.0","4":"7.000","5":"yes","6":"2","7":"20","8":"6","9":"2","_rn_":"961"},{"1":"0","2":"male","3":"27.0","4":"0.750","5":"no","6":"2","7":"17","8":"5","9":"5","_rn_":"965"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"no","6":"2","7":"17","8":"4","9":"5","_rn_":"966"},{"1":"0","2":"male","3":"42.0","4":"1.500","5":"no","6":"5","7":"20","8":"6","9":"5","_rn_":"967"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"no","6":"4","7":"17","8":"6","9":"5","_rn_":"987"},{"1":"0","2":"male","3":"22.0","4":"4.000","5":"no","6":"4","7":"17","8":"5","9":"3","_rn_":"990"},{"1":"0","2":"female","3":"22.0","4":"4.000","5":"yes","6":"1","7":"14","8":"5","9":"4","_rn_":"992"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"5","7":"20","8":"4","9":"5","_rn_":"995"},{"1":"0","2":"female","3":"37.0","4":"10.000","5":"yes","6":"3","7":"16","8":"6","9":"3","_rn_":"1009"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"4","7":"17","8":"6","9":"5","_rn_":"1021"},{"1":"0","2":"female","3":"47.0","4":"15.000","5":"yes","6":"4","7":"17","8":"5","9":"5","_rn_":"1026"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"no","6":"4","7":"16","8":"5","9":"4","_rn_":"1027"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"3","7":"12","8":"1","9":"4","_rn_":"1030"},{"1":"0","2":"female","3":"22.0","4":"7.000","5":"yes","6":"1","7":"14","8":"3","9":"5","_rn_":"1031"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"4","7":"17","8":"5","9":"4","_rn_":"1034"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"yes","6":"2","7":"16","8":"2","9":"4","_rn_":"1037"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"14","8":"5","9":"5","_rn_":"1038"},{"1":"0","2":"male","3":"42.0","4":"4.000","5":"yes","6":"3","7":"14","8":"4","9":"5","_rn_":"1039"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"5","7":"14","8":"5","9":"4","_rn_":"1045"},{"1":"0","2":"female","3":"32.0","4":"7.000","5":"yes","6":"4","7":"17","8":"5","9":"5","_rn_":"1046"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"4","7":"18","8":"6","9":"5","_rn_":"1054"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"no","6":"4","7":"18","8":"6","9":"4","_rn_":"1059"},{"1":"0","2":"male","3":"22.0","4":"0.750","5":"no","6":"4","7":"18","8":"6","9":"5","_rn_":"1063"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"4","7":"14","8":"5","9":"3","_rn_":"1068"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"5","7":"18","8":"1","9":"5","_rn_":"1070"},{"1":"0","2":"female","3":"52.0","4":"15.000","5":"yes","6":"5","7":"9","8":"5","9":"5","_rn_":"1072"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"yes","6":"3","7":"14","8":"5","9":"5","_rn_":"1073"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"16","8":"4","9":"4","_rn_":"1077"},{"1":"0","2":"male","3":"32.0","4":"7.000","5":"yes","6":"2","7":"20","8":"5","9":"4","_rn_":"1081"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"3","7":"18","8":"1","9":"4","_rn_":"1083"},{"1":"0","2":"male","3":"32.0","4":"15.000","5":"yes","6":"1","7":"16","8":"5","9":"5","_rn_":"1084"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"3","7":"18","8":"5","9":"5","_rn_":"1086"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"4","7":"12","8":"3","9":"4","_rn_":"1087"},{"1":"0","2":"male","3":"22.0","4":"0.750","5":"yes","6":"3","7":"14","8":"2","9":"4","_rn_":"1089"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"3","7":"16","8":"5","9":"3","_rn_":"1096"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"4","7":"14","8":"3","9":"5","_rn_":"1102"},{"1":"0","2":"female","3":"52.0","4":"15.000","5":"yes","6":"3","7":"16","8":"5","9":"4","_rn_":"1103"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"5","7":"20","8":"6","9":"4","_rn_":"1107"},{"1":"0","2":"female","3":"47.0","4":"15.000","5":"yes","6":"4","7":"12","8":"2","9":"3","_rn_":"1109"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"2","7":"20","8":"6","9":"4","_rn_":"1115"},{"1":"0","2":"male","3":"32.0","4":"7.000","5":"yes","6":"4","7":"17","8":"5","9":"5","_rn_":"1119"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"17","8":"1","9":"4","_rn_":"1124"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"no","6":"1","7":"18","8":"6","9":"5","_rn_":"1126"},{"1":"0","2":"female","3":"22.0","4":"4.000","5":"yes","6":"3","7":"9","8":"1","9":"4","_rn_":"1128"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"14","8":"1","9":"5","_rn_":"1129"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"2","7":"20","8":"6","9":"4","_rn_":"1130"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"4","7":"9","8":"2","9":"4","_rn_":"1133"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"2","7":"18","8":"1","9":"5","_rn_":"1140"},{"1":"0","2":"female","3":"22.0","4":"4.000","5":"yes","6":"3","7":"14","8":"1","9":"5","_rn_":"1143"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"14","8":"5","9":"3","_rn_":"1146"},{"1":"0","2":"male","3":"32.0","4":"7.000","5":"yes","6":"1","7":"18","8":"6","9":"4","_rn_":"1153"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"14","8":"5","9":"5","_rn_":"1156"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"yes","6":"3","7":"12","8":"1","9":"3","_rn_":"1157"},{"1":"0","2":"male","3":"52.0","4":"15.000","5":"yes","6":"2","7":"14","8":"5","9":"5","_rn_":"1158"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"2","7":"14","8":"1","9":"1","_rn_":"1160"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"2","7":"14","8":"5","9":"5","_rn_":"1161"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"4","7":"20","8":"4","9":"5","_rn_":"1166"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"3","7":"18","8":"4","9":"5","_rn_":"1177"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"5","_rn_":"1178"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"no","6":"3","7":"18","8":"5","9":"5","_rn_":"1180"},{"1":"0","2":"female","3":"22.0","4":"0.125","5":"no","6":"2","7":"16","8":"6","9":"3","_rn_":"1187"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"yes","6":"2","7":"20","8":"6","9":"3","_rn_":"1191"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"no","6":"4","7":"18","8":"5","9":"4","_rn_":"1195"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"2","7":"12","8":"5","9":"1","_rn_":"1207"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"yes","6":"5","7":"18","8":"6","9":"3","_rn_":"1208"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"2","7":"17","8":"5","9":"5","_rn_":"1209"},{"1":"0","2":"male","3":"47.0","4":"15.000","5":"no","6":"4","7":"20","8":"6","9":"4","_rn_":"1211"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"no","6":"1","7":"18","8":"5","9":"5","_rn_":"1215"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"4","_rn_":"1221"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"4","7":"18","8":"1","9":"4","_rn_":"1226"},{"1":"0","2":"female","3":"32.0","4":"7.000","5":"yes","6":"4","7":"17","8":"5","9":"4","_rn_":"1229"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"3","7":"14","8":"1","9":"3","_rn_":"1231"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"3","7":"16","8":"1","9":"4","_rn_":"1234"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"no","6":"3","7":"16","8":"4","9":"2","_rn_":"1235"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"no","6":"3","7":"16","8":"3","9":"5","_rn_":"1242"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"3","7":"16","8":"4","9":"2","_rn_":"1245"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"3","7":"12","8":"1","9":"2","_rn_":"1260"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"2","7":"18","8":"5","9":"4","_rn_":"1266"},{"1":"0","2":"female","3":"37.0","4":"7.000","5":"yes","6":"3","7":"14","8":"4","9":"4","_rn_":"1271"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"no","6":"2","7":"16","8":"5","9":"5","_rn_":"1273"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"5","7":"20","8":"5","9":"4","_rn_":"1276"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"4","7":"16","8":"5","9":"3","_rn_":"1280"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"4","7":"16","8":"1","9":"5","_rn_":"1282"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"no","6":"2","7":"17","8":"5","9":"3","_rn_":"1285"},{"1":"0","2":"female","3":"22.0","4":"0.417","5":"no","6":"4","7":"14","8":"5","9":"5","_rn_":"1295"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"no","6":"2","7":"18","8":"5","9":"5","_rn_":"1298"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"18","8":"5","9":"3","_rn_":"1299"},{"1":"0","2":"male","3":"37.0","4":"10.000","5":"yes","6":"5","7":"20","8":"7","9":"4","_rn_":"1304"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"2","7":"14","8":"4","9":"2","_rn_":"1305"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"yes","6":"2","7":"16","8":"5","9":"5","_rn_":"1311"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"yes","6":"2","7":"16","8":"6","9":"4","_rn_":"1314"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"no","6":"3","7":"18","8":"4","9":"5","_rn_":"1319"},{"1":"0","2":"female","3":"22.0","4":"4.000","5":"yes","6":"4","7":"14","8":"3","9":"4","_rn_":"1322"},{"1":"0","2":"female","3":"17.5","4":"0.750","5":"no","6":"2","7":"18","8":"5","9":"4","_rn_":"1324"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"yes","6":"4","7":"20","8":"4","9":"5","_rn_":"1327"},{"1":"0","2":"female","3":"32.0","4":"0.750","5":"no","6":"5","7":"14","8":"3","9":"3","_rn_":"1328"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"17","8":"5","9":"3","_rn_":"1330"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"no","6":"3","7":"14","8":"4","9":"5","_rn_":"1332"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"no","6":"2","7":"17","8":"3","9":"2","_rn_":"1333"},{"1":"0","2":"female","3":"22.0","4":"7.000","5":"yes","6":"4","7":"14","8":"1","9":"5","_rn_":"1336"},{"1":"0","2":"male","3":"47.0","4":"15.000","5":"yes","6":"5","7":"14","8":"6","9":"5","_rn_":"1341"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"1","7":"16","8":"4","9":"4","_rn_":"1344"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"5","7":"14","8":"1","9":"3","_rn_":"1352"},{"1":"0","2":"male","3":"42.0","4":"4.000","5":"yes","6":"4","7":"18","8":"5","9":"5","_rn_":"1358"},{"1":"0","2":"female","3":"32.0","4":"4.000","5":"yes","6":"2","7":"14","8":"1","9":"5","_rn_":"1359"},{"1":"0","2":"male","3":"52.0","4":"15.000","5":"yes","6":"2","7":"14","8":"7","9":"4","_rn_":"1361"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"16","8":"1","9":"4","_rn_":"1364"},{"1":"0","2":"male","3":"52.0","4":"15.000","5":"yes","6":"4","7":"12","8":"2","9":"4","_rn_":"1368"},{"1":"0","2":"female","3":"22.0","4":"0.417","5":"no","6":"3","7":"17","8":"1","9":"5","_rn_":"1384"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"16","8":"5","9":"5","_rn_":"1390"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"4","7":"20","8":"6","9":"4","_rn_":"1393"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"4","7":"14","8":"1","9":"5","_rn_":"1394"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"no","6":"2","7":"16","8":"3","9":"5","_rn_":"1402"},{"1":"0","2":"male","3":"32.0","4":"4.000","5":"no","6":"1","7":"20","8":"6","9":"5","_rn_":"1407"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"3","7":"20","8":"6","9":"4","_rn_":"1408"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"no","6":"2","7":"16","8":"6","9":"5","_rn_":"1412"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"5","7":"14","8":"5","9":"5","_rn_":"1413"},{"1":"0","2":"male","3":"37.0","4":"1.500","5":"yes","6":"4","7":"18","8":"5","9":"3","_rn_":"1416"},{"1":"0","2":"male","3":"32.0","4":"1.500","5":"no","6":"2","7":"18","8":"4","9":"4","_rn_":"1417"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"4","7":"14","8":"1","9":"4","_rn_":"1418"},{"1":"0","2":"female","3":"47.0","4":"15.000","5":"yes","6":"4","7":"18","8":"5","9":"4","_rn_":"1419"},{"1":"0","2":"female","3":"27.0","4":"10.000","5":"yes","6":"5","7":"12","8":"1","9":"5","_rn_":"1420"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"3","7":"16","8":"4","9":"5","_rn_":"1423"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"12","8":"4","9":"2","_rn_":"1424"},{"1":"0","2":"female","3":"27.0","4":"0.750","5":"no","6":"4","7":"16","8":"5","9":"5","_rn_":"1432"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"16","8":"1","9":"5","_rn_":"1433"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"3","7":"16","8":"1","9":"5","_rn_":"1437"},{"1":"0","2":"female","3":"27.0","4":"10.000","5":"yes","6":"2","7":"16","8":"1","9":"5","_rn_":"1438"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"no","6":"2","7":"20","8":"6","9":"5","_rn_":"1439"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"2","7":"14","8":"1","9":"3","_rn_":"1446"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"yes","6":"2","7":"17","8":"4","9":"4","_rn_":"1450"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"yes","6":"2","7":"14","8":"1","9":"5","_rn_":"1451"},{"1":"0","2":"male","3":"22.0","4":"4.000","5":"yes","6":"4","7":"14","8":"2","9":"4","_rn_":"1452"},{"1":"0","2":"male","3":"42.0","4":"0.125","5":"no","6":"4","7":"17","8":"6","9":"4","_rn_":"1453"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"yes","6":"4","7":"18","8":"6","9":"5","_rn_":"1456"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"yes","6":"3","7":"16","8":"6","9":"3","_rn_":"1464"},{"1":"0","2":"female","3":"52.0","4":"15.000","5":"yes","6":"4","7":"14","8":"1","9":"3","_rn_":"1469"},{"1":"0","2":"male","3":"27.0","4":"1.500","5":"no","6":"5","7":"20","8":"5","9":"2","_rn_":"1473"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"no","6":"2","7":"16","8":"5","9":"5","_rn_":"1481"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"no","6":"3","7":"17","8":"5","9":"5","_rn_":"1482"},{"1":"0","2":"male","3":"22.0","4":"0.125","5":"no","6":"5","7":"16","8":"4","9":"4","_rn_":"1496"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"4","7":"16","8":"1","9":"5","_rn_":"1497"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"4","7":"12","8":"1","9":"5","_rn_":"1504"},{"1":"0","2":"female","3":"47.0","4":"15.000","5":"yes","6":"2","7":"14","8":"5","9":"5","_rn_":"1513"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"3","7":"14","8":"5","9":"3","_rn_":"1515"},{"1":"0","2":"male","3":"42.0","4":"7.000","5":"yes","6":"2","7":"16","8":"5","9":"5","_rn_":"1534"},{"1":"0","2":"male","3":"22.0","4":"0.750","5":"no","6":"4","7":"16","8":"6","9":"4","_rn_":"1535"},{"1":"0","2":"male","3":"27.0","4":"0.125","5":"no","6":"3","7":"20","8":"6","9":"5","_rn_":"1536"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"yes","6":"3","7":"20","8":"6","9":"5","_rn_":"1540"},{"1":"0","2":"female","3":"22.0","4":"0.417","5":"no","6":"5","7":"14","8":"4","9":"5","_rn_":"1551"},{"1":"0","2":"female","3":"47.0","4":"15.000","5":"yes","6":"5","7":"14","8":"1","9":"4","_rn_":"1555"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"3","7":"14","8":"1","9":"5","_rn_":"1557"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"4","7":"17","8":"5","9":"5","_rn_":"1566"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"3","7":"20","8":"6","9":"5","_rn_":"1567"},{"1":"0","2":"female","3":"32.0","4":"7.000","5":"yes","6":"4","7":"17","8":"1","9":"5","_rn_":"1576"},{"1":"0","2":"female","3":"37.0","4":"10.000","5":"yes","6":"4","7":"16","8":"1","9":"5","_rn_":"1584"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"1","7":"18","8":"1","9":"4","_rn_":"1585"},{"1":"0","2":"female","3":"22.0","4":"4.000","5":"no","6":"3","7":"14","8":"1","9":"4","_rn_":"1590"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"14","8":"3","9":"2","_rn_":"1594"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"yes","6":"5","7":"18","8":"5","9":"2","_rn_":"1595"},{"1":"0","2":"male","3":"32.0","4":"7.000","5":"yes","6":"2","7":"18","8":"5","9":"5","_rn_":"1603"},{"1":"0","2":"female","3":"27.0","4":"1.500","5":"no","6":"4","7":"17","8":"1","9":"3","_rn_":"1608"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"no","6":"4","7":"14","8":"5","9":"5","_rn_":"1609"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"yes","6":"4","7":"14","8":"5","9":"4","_rn_":"1615"},{"1":"0","2":"female","3":"32.0","4":"7.000","5":"yes","6":"3","7":"16","8":"1","9":"5","_rn_":"1616"},{"1":"0","2":"female","3":"47.0","4":"15.000","5":"yes","6":"3","7":"16","8":"5","9":"4","_rn_":"1617"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"3","7":"16","8":"1","9":"5","_rn_":"1620"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"yes","6":"2","7":"14","8":"5","9":"5","_rn_":"1621"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"1","7":"16","8":"5","9":"5","_rn_":"1637"},{"1":"0","2":"male","3":"52.0","4":"15.000","5":"yes","6":"4","7":"16","8":"5","9":"5","_rn_":"1638"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"yes","6":"4","7":"20","8":"6","9":"5","_rn_":"1650"},{"1":"0","2":"male","3":"47.0","4":"15.000","5":"yes","6":"4","7":"16","8":"6","9":"4","_rn_":"1654"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"2","7":"14","8":"1","9":"2","_rn_":"1665"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"4","7":"14","8":"4","9":"5","_rn_":"1670"},{"1":"0","2":"female","3":"32.0","4":"10.000","5":"yes","6":"2","7":"16","8":"5","9":"4","_rn_":"1671"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"2","7":"16","8":"5","9":"4","_rn_":"1675"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"16","8":"5","9":"5","_rn_":"1688"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"3","7":"18","8":"6","9":"4","_rn_":"1691"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"5","7":"14","8":"4","9":"5","_rn_":"1695"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"4","7":"16","8":"4","9":"4","_rn_":"1698"},{"1":"0","2":"female","3":"57.0","4":"15.000","5":"yes","6":"3","7":"18","8":"5","9":"2","_rn_":"1704"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"3","7":"18","8":"6","9":"2","_rn_":"1705"},{"1":"0","2":"female","3":"32.0","4":"7.000","5":"yes","6":"2","7":"14","8":"1","9":"2","_rn_":"1711"},{"1":"0","2":"male","3":"22.0","4":"4.000","5":"no","6":"5","7":"12","8":"4","9":"5","_rn_":"1719"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"1","7":"16","8":"6","9":"5","_rn_":"1723"},{"1":"0","2":"female","3":"22.0","4":"0.750","5":"no","6":"1","7":"14","8":"4","9":"5","_rn_":"1726"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"4","7":"12","8":"1","9":"5","_rn_":"1749"},{"1":"0","2":"male","3":"22.0","4":"1.500","5":"no","6":"2","7":"18","8":"5","9":"3","_rn_":"1752"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"5","7":"17","8":"2","9":"5","_rn_":"1754"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"4","7":"12","8":"1","9":"5","_rn_":"1758"},{"1":"0","2":"male","3":"42.0","4":"15.000","5":"yes","6":"5","7":"18","8":"5","9":"4","_rn_":"1761"},{"1":"0","2":"male","3":"32.0","4":"1.500","5":"no","6":"2","7":"20","8":"7","9":"3","_rn_":"1773"},{"1":"0","2":"male","3":"57.0","4":"15.000","5":"no","6":"4","7":"9","8":"3","9":"1","_rn_":"1775"},{"1":"0","2":"male","3":"37.0","4":"7.000","5":"no","6":"4","7":"18","8":"5","9":"5","_rn_":"1786"},{"1":"0","2":"male","3":"52.0","4":"15.000","5":"yes","6":"2","7":"17","8":"5","9":"4","_rn_":"1793"},{"1":"0","2":"male","3":"47.0","4":"15.000","5":"yes","6":"4","7":"17","8":"6","9":"5","_rn_":"1799"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"no","6":"2","7":"17","8":"5","9":"4","_rn_":"1803"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"14","8":"5","9":"5","_rn_":"1806"},{"1":"0","2":"female","3":"22.0","4":"4.000","5":"no","6":"2","7":"14","8":"3","9":"3","_rn_":"1807"},{"1":"0","2":"male","3":"37.0","4":"7.000","5":"yes","6":"2","7":"20","8":"6","9":"5","_rn_":"1808"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"no","6":"4","7":"12","8":"4","9":"3","_rn_":"1814"},{"1":"0","2":"male","3":"42.0","4":"10.000","5":"yes","6":"4","7":"18","8":"6","9":"4","_rn_":"1815"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"3","7":"14","8":"1","9":"5","_rn_":"1818"},{"1":"0","2":"female","3":"22.0","4":"4.000","5":"yes","6":"2","7":"14","8":"1","9":"3","_rn_":"1827"},{"1":"0","2":"female","3":"57.0","4":"15.000","5":"no","6":"4","7":"20","8":"6","9":"5","_rn_":"1834"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"14","8":"4","9":"3","_rn_":"1835"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"3","7":"18","8":"5","9":"5","_rn_":"1843"},{"1":"0","2":"female","3":"17.5","4":"10.000","5":"no","6":"4","7":"14","8":"4","9":"5","_rn_":"1846"},{"1":"0","2":"male","3":"22.0","4":"4.000","5":"yes","6":"4","7":"16","8":"5","9":"5","_rn_":"1850"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"2","7":"16","8":"1","9":"4","_rn_":"1851"},{"1":"0","2":"female","3":"37.0","4":"15.000","5":"yes","6":"2","7":"14","8":"5","9":"1","_rn_":"1854"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"5","7":"14","8":"1","9":"4","_rn_":"1859"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"yes","6":"2","7":"20","8":"5","9":"4","_rn_":"1861"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"4","7":"14","8":"5","9":"5","_rn_":"1866"},{"1":"0","2":"male","3":"22.0","4":"0.125","5":"no","6":"1","7":"16","8":"3","9":"5","_rn_":"1873"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"14","8":"1","9":"4","_rn_":"1875"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"5","7":"16","8":"5","9":"3","_rn_":"1885"},{"1":"0","2":"male","3":"32.0","4":"10.000","5":"yes","6":"4","7":"18","8":"5","9":"4","_rn_":"1892"},{"1":"0","2":"female","3":"32.0","4":"15.000","5":"yes","6":"2","7":"14","8":"3","9":"4","_rn_":"1895"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"3","7":"17","8":"5","9":"5","_rn_":"1896"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"4","7":"17","8":"4","9":"4","_rn_":"1897"},{"1":"0","2":"female","3":"52.0","4":"15.000","5":"yes","6":"5","7":"14","8":"1","9":"5","_rn_":"1899"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"2","7":"12","8":"1","9":"2","_rn_":"1904"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"3","7":"12","8":"1","9":"4","_rn_":"1905"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"2","7":"14","8":"1","9":"4","_rn_":"1908"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"4","7":"14","8":"5","9":"4","_rn_":"1916"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"yes","6":"4","7":"14","8":"3","9":"3","_rn_":"1918"},{"1":"0","2":"male","3":"27.0","4":"7.000","5":"yes","6":"2","7":"20","8":"6","9":"2","_rn_":"1920"},{"1":"0","2":"female","3":"42.0","4":"15.000","5":"yes","6":"3","7":"12","8":"3","9":"3","_rn_":"1930"},{"1":"0","2":"male","3":"27.0","4":"4.000","5":"yes","6":"3","7":"16","8":"3","9":"5","_rn_":"1940"},{"1":"0","2":"female","3":"27.0","4":"7.000","5":"yes","6":"3","7":"14","8":"1","9":"4","_rn_":"1947"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"14","8":"4","9":"5","_rn_":"1949"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"4","7":"14","8":"1","9":"4","_rn_":"1951"},{"1":"0","2":"female","3":"22.0","4":"4.000","5":"no","6":"4","7":"14","8":"5","9":"5","_rn_":"1952"},{"1":"0","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"16","8":"4","9":"5","_rn_":"1960"},{"1":"0","2":"male","3":"47.0","4":"15.000","5":"no","6":"4","7":"14","8":"5","9":"4","_rn_":"9001"},{"1":"0","2":"male","3":"37.0","4":"10.000","5":"yes","6":"2","7":"18","8":"6","9":"2","_rn_":"9012"},{"1":"0","2":"male","3":"37.0","4":"15.000","5":"yes","6":"3","7":"17","8":"5","9":"4","_rn_":"9023"},{"1":"0","2":"female","3":"27.0","4":"4.000","5":"yes","6":"2","7":"16","8":"1","9":"4","_rn_":"9029"},{"1":"3","2":"male","3":"27.0","4":"1.500","5":"no","6":"3","7":"18","8":"4","9":"4","_rn_":"6"},{"1":"3","2":"female","3":"27.0","4":"4.000","5":"yes","6":"3","7":"17","8":"1","9":"5","_rn_":"12"},{"1":"7","2":"male","3":"37.0","4":"15.000","5":"yes","6":"5","7":"18","8":"6","9":"2","_rn_":"43"},{"1":"12","2":"female","3":"32.0","4":"10.000","5":"yes","6":"3","7":"17","8":"5","9":"2","_rn_":"53"},{"1":"1","2":"male","3":"22.0","4":"0.125","5":"no","6":"4","7":"16","8":"5","9":"5","_rn_":"67"},{"1":"1","2":"female","3":"22.0","4":"1.500","5":"yes","6":"2","7":"14","8":"1","9":"5","_rn_":"79"},{"1":"12","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"14","8":"5","9":"2","_rn_":"122"},{"1":"7","2":"female","3":"22.0","4":"1.500","5":"no","6":"2","7":"14","8":"3","9":"4","_rn_":"126"},{"1":"2","2":"male","3":"37.0","4":"15.000","5":"yes","6":"2","7":"18","8":"6","9":"4","_rn_":"133"},{"1":"3","2":"female","3":"32.0","4":"15.000","5":"yes","6":"4","7":"12","8":"3","9":"2","_rn_":"138"},{"1":"1","2":"female","3":"37.0","4":"15.000","5":"yes","6":"4","7":"14","8":"4","9":"2","_rn_":"154"},{"1":"7","2":"female","3":"42.0","4":"15.000","5":"yes","6":"3","7":"17","8":"1","9":"4","_rn_":"159"},{"1":"12","2":"female","3":"42.0","4":"15.000","5":"yes","6":"5","7":"9","8":"4","9":"1","_rn_":"174"},{"1":"12","2":"male","3":"37.0","4":"10.000","5":"yes","6":"2","7":"20","8":"6","9":"2","_rn_":"176"},{"1":"12","2":"female","3":"32.0","4":"15.000","5":"yes","6":"3","7":"14","8":"1","9":"2","_rn_":"181"},{"1":"3","2":"male","3":"27.0","4":"4.000","5":"no","6":"1","7":"18","8":"6","9":"5","_rn_":"182"},{"1":"7","2":"male","3":"37.0","4":"10.000","5":"yes","6":"2","7":"18","8":"7","9":"3","_rn_":"186"},{"1":"7","2":"female","3":"27.0","4":"4.000","5":"no","6":"3","7":"17","8":"5","9":"5","_rn_":"189"},{"1":"1","2":"male","3":"42.0","4":"15.000","5":"yes","6":"4","7":"16","8":"5","9":"5","_rn_":"204"},{"1":"1","2":"female","3":"47.0","4":"15.000","5":"yes","6":"5","7":"14","8":"4","9":"5","_rn_":"215"},{"1":"7","2":"female","3":"27.0","4":"4.000","5":"yes","6":"3","7":"18","8":"5","9":"4","_rn_":"232"},{"1":"1","2":"female","3":"27.0","4":"7.000","5":"yes","6":"5","7":"14","8":"1","9":"4","_rn_":"233"},{"1":"12","2":"male","3":"27.0","4":"1.500","5":"yes","6":"3","7":"17","8":"5","9":"4","_rn_":"252"},{"1":"12","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"14","8":"6","9":"2","_rn_":"253"},{"1":"3","2":"female","3":"42.0","4":"15.000","5":"yes","6":"4","7":"16","8":"5","9":"4","_rn_":"274"},{"1":"7","2":"female","3":"27.0","4":"10.000","5":"yes","6":"4","7":"12","8":"7","9":"3","_rn_":"275"},{"1":"1","2":"male","3":"27.0","4":"1.500","5":"no","6":"2","7":"18","8":"5","9":"2","_rn_":"287"},{"1":"1","2":"male","3":"32.0","4":"4.000","5":"no","6":"4","7":"20","8":"6","9":"4","_rn_":"288"},{"1":"1","2":"female","3":"27.0","4":"7.000","5":"yes","6":"3","7":"14","8":"1","9":"3","_rn_":"325"},{"1":"3","2":"female","3":"32.0","4":"10.000","5":"yes","6":"4","7":"14","8":"1","9":"4","_rn_":"328"},{"1":"3","2":"male","3":"27.0","4":"4.000","5":"yes","6":"2","7":"18","8":"7","9":"2","_rn_":"344"},{"1":"1","2":"female","3":"17.5","4":"0.750","5":"no","6":"5","7":"14","8":"4","9":"5","_rn_":"353"},{"1":"1","2":"female","3":"32.0","4":"10.000","5":"yes","6":"4","7":"18","8":"1","9":"5","_rn_":"354"},{"1":"7","2":"female","3":"32.0","4":"7.000","5":"yes","6":"2","7":"17","8":"6","9":"4","_rn_":"367"},{"1":"7","2":"male","3":"37.0","4":"15.000","5":"yes","6":"2","7":"20","8":"6","9":"4","_rn_":"369"},{"1":"7","2":"female","3":"37.0","4":"10.000","5":"no","6":"1","7":"20","8":"5","9":"3","_rn_":"390"},{"1":"12","2":"female","3":"32.0","4":"10.000","5":"yes","6":"2","7":"16","8":"5","9":"5","_rn_":"392"},{"1":"7","2":"male","3":"52.0","4":"15.000","5":"yes","6":"2","7":"20","8":"6","9":"4","_rn_":"423"},{"1":"7","2":"female","3":"42.0","4":"15.000","5":"yes","6":"1","7":"12","8":"1","9":"3","_rn_":"432"},{"1":"1","2":"male","3":"52.0","4":"15.000","5":"yes","6":"2","7":"20","8":"6","9":"3","_rn_":"436"},{"1":"2","2":"male","3":"37.0","4":"15.000","5":"yes","6":"3","7":"18","8":"6","9":"5","_rn_":"483"},{"1":"12","2":"female","3":"22.0","4":"4.000","5":"no","6":"3","7":"12","8":"3","9":"4","_rn_":"513"},{"1":"12","2":"male","3":"27.0","4":"7.000","5":"yes","6":"1","7":"18","8":"6","9":"2","_rn_":"516"},{"1":"1","2":"male","3":"27.0","4":"4.000","5":"yes","6":"3","7":"18","8":"5","9":"5","_rn_":"518"},{"1":"12","2":"male","3":"47.0","4":"15.000","5":"yes","6":"4","7":"17","8":"6","9":"5","_rn_":"520"},{"1":"12","2":"female","3":"42.0","4":"15.000","5":"yes","6":"4","7":"12","8":"1","9":"1","_rn_":"526"},{"1":"7","2":"male","3":"27.0","4":"4.000","5":"no","6":"3","7":"14","8":"3","9":"4","_rn_":"528"},{"1":"7","2":"female","3":"32.0","4":"7.000","5":"yes","6":"4","7":"18","8":"4","9":"5","_rn_":"553"},{"1":"1","2":"male","3":"32.0","4":"0.417","5":"yes","6":"3","7":"12","8":"3","9":"4","_rn_":"576"},{"1":"3","2":"male","3":"47.0","4":"15.000","5":"yes","6":"5","7":"16","8":"5","9":"4","_rn_":"611"},{"1":"12","2":"male","3":"37.0","4":"15.000","5":"yes","6":"2","7":"20","8":"5","9":"4","_rn_":"625"},{"1":"7","2":"male","3":"22.0","4":"4.000","5":"yes","6":"2","7":"17","8":"6","9":"4","_rn_":"635"},{"1":"1","2":"male","3":"27.0","4":"4.000","5":"no","6":"2","7":"14","8":"4","9":"5","_rn_":"646"},{"1":"7","2":"female","3":"52.0","4":"15.000","5":"yes","6":"5","7":"16","8":"1","9":"3","_rn_":"657"},{"1":"1","2":"male","3":"27.0","4":"4.000","5":"no","6":"3","7":"14","8":"3","9":"3","_rn_":"659"},{"1":"1","2":"female","3":"27.0","4":"10.000","5":"yes","6":"4","7":"16","8":"1","9":"4","_rn_":"666"},{"1":"1","2":"male","3":"32.0","4":"7.000","5":"yes","6":"3","7":"14","8":"7","9":"4","_rn_":"679"},{"1":"7","2":"male","3":"32.0","4":"7.000","5":"yes","6":"2","7":"18","8":"4","9":"1","_rn_":"729"},{"1":"3","2":"male","3":"22.0","4":"1.500","5":"no","6":"1","7":"14","8":"3","9":"2","_rn_":"755"},{"1":"7","2":"male","3":"22.0","4":"4.000","5":"yes","6":"3","7":"18","8":"6","9":"4","_rn_":"758"},{"1":"7","2":"male","3":"42.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"4","_rn_":"770"},{"1":"2","2":"female","3":"57.0","4":"15.000","5":"yes","6":"1","7":"18","8":"5","9":"4","_rn_":"786"},{"1":"7","2":"female","3":"32.0","4":"4.000","5":"yes","6":"3","7":"18","8":"5","9":"2","_rn_":"797"},{"1":"1","2":"male","3":"27.0","4":"4.000","5":"yes","6":"1","7":"16","8":"4","9":"4","_rn_":"811"},{"1":"7","2":"male","3":"32.0","4":"7.000","5":"yes","6":"4","7":"16","8":"1","9":"4","_rn_":"834"},{"1":"2","2":"male","3":"57.0","4":"15.000","5":"yes","6":"1","7":"17","8":"4","9":"4","_rn_":"858"},{"1":"7","2":"female","3":"42.0","4":"15.000","5":"yes","6":"4","7":"14","8":"5","9":"2","_rn_":"885"},{"1":"7","2":"male","3":"37.0","4":"10.000","5":"yes","6":"1","7":"18","8":"5","9":"3","_rn_":"893"},{"1":"3","2":"male","3":"42.0","4":"15.000","5":"yes","6":"3","7":"17","8":"6","9":"1","_rn_":"927"},{"1":"1","2":"female","3":"52.0","4":"15.000","5":"yes","6":"3","7":"14","8":"4","9":"4","_rn_":"928"},{"1":"2","2":"female","3":"27.0","4":"7.000","5":"yes","6":"3","7":"17","8":"5","9":"3","_rn_":"933"},{"1":"12","2":"male","3":"32.0","4":"7.000","5":"yes","6":"2","7":"12","8":"4","9":"2","_rn_":"951"},{"1":"1","2":"male","3":"22.0","4":"4.000","5":"no","6":"4","7":"14","8":"2","9":"5","_rn_":"968"},{"1":"3","2":"male","3":"27.0","4":"7.000","5":"yes","6":"3","7":"18","8":"6","9":"4","_rn_":"972"},{"1":"12","2":"female","3":"37.0","4":"15.000","5":"yes","6":"1","7":"18","8":"5","9":"5","_rn_":"975"},{"1":"7","2":"female","3":"32.0","4":"15.000","5":"yes","6":"3","7":"17","8":"1","9":"3","_rn_":"977"},{"1":"7","2":"female","3":"27.0","4":"7.000","5":"no","6":"2","7":"17","8":"5","9":"5","_rn_":"981"},{"1":"1","2":"female","3":"32.0","4":"7.000","5":"yes","6":"3","7":"17","8":"5","9":"3","_rn_":"986"},{"1":"1","2":"male","3":"32.0","4":"1.500","5":"yes","6":"2","7":"14","8":"2","9":"4","_rn_":"1002"},{"1":"12","2":"female","3":"42.0","4":"15.000","5":"yes","6":"4","7":"14","8":"1","9":"2","_rn_":"1007"},{"1":"7","2":"male","3":"32.0","4":"10.000","5":"yes","6":"3","7":"14","8":"5","9":"4","_rn_":"1011"},{"1":"7","2":"male","3":"37.0","4":"4.000","5":"yes","6":"1","7":"20","8":"6","9":"3","_rn_":"1035"},{"1":"1","2":"female","3":"27.0","4":"4.000","5":"yes","6":"2","7":"16","8":"5","9":"3","_rn_":"1050"},{"1":"12","2":"female","3":"42.0","4":"15.000","5":"yes","6":"3","7":"14","8":"4","9":"3","_rn_":"1056"},{"1":"1","2":"male","3":"27.0","4":"10.000","5":"yes","6":"5","7":"20","8":"6","9":"5","_rn_":"1057"},{"1":"12","2":"male","3":"37.0","4":"10.000","5":"yes","6":"2","7":"20","8":"6","9":"2","_rn_":"1075"},{"1":"12","2":"female","3":"27.0","4":"7.000","5":"yes","6":"1","7":"14","8":"3","9":"3","_rn_":"1080"},{"1":"3","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"12","8":"1","9":"2","_rn_":"1125"},{"1":"3","2":"male","3":"32.0","4":"10.000","5":"yes","6":"2","7":"14","8":"4","9":"4","_rn_":"1131"},{"1":"12","2":"female","3":"17.5","4":"0.750","5":"yes","6":"2","7":"12","8":"1","9":"3","_rn_":"1138"},{"1":"12","2":"female","3":"32.0","4":"15.000","5":"yes","6":"3","7":"18","8":"5","9":"4","_rn_":"1150"},{"1":"2","2":"female","3":"22.0","4":"7.000","5":"no","6":"4","7":"14","8":"4","9":"3","_rn_":"1163"},{"1":"1","2":"male","3":"32.0","4":"7.000","5":"yes","6":"4","7":"20","8":"6","9":"5","_rn_":"1169"},{"1":"7","2":"male","3":"27.0","4":"4.000","5":"yes","6":"2","7":"18","8":"6","9":"2","_rn_":"1198"},{"1":"1","2":"female","3":"22.0","4":"1.500","5":"yes","6":"5","7":"14","8":"5","9":"3","_rn_":"1204"},{"1":"12","2":"female","3":"32.0","4":"15.000","5":"no","6":"3","7":"17","8":"5","9":"1","_rn_":"1218"},{"1":"12","2":"female","3":"42.0","4":"15.000","5":"yes","6":"2","7":"12","8":"1","9":"2","_rn_":"1230"},{"1":"7","2":"male","3":"42.0","4":"15.000","5":"yes","6":"3","7":"20","8":"5","9":"4","_rn_":"1236"},{"1":"12","2":"male","3":"32.0","4":"10.000","5":"no","6":"2","7":"18","8":"4","9":"2","_rn_":"1247"},{"1":"12","2":"female","3":"32.0","4":"15.000","5":"yes","6":"3","7":"9","8":"1","9":"1","_rn_":"1259"},{"1":"7","2":"male","3":"57.0","4":"15.000","5":"yes","6":"5","7":"20","8":"4","9":"5","_rn_":"1294"},{"1":"12","2":"male","3":"47.0","4":"15.000","5":"yes","6":"4","7":"20","8":"6","9":"4","_rn_":"1353"},{"1":"2","2":"female","3":"42.0","4":"15.000","5":"yes","6":"2","7":"17","8":"6","9":"3","_rn_":"1370"},{"1":"12","2":"male","3":"37.0","4":"15.000","5":"yes","6":"3","7":"17","8":"6","9":"3","_rn_":"1427"},{"1":"12","2":"male","3":"37.0","4":"15.000","5":"yes","6":"5","7":"17","8":"5","9":"2","_rn_":"1445"},{"1":"7","2":"male","3":"27.0","4":"10.000","5":"yes","6":"2","7":"20","8":"6","9":"4","_rn_":"1460"},{"1":"2","2":"male","3":"37.0","4":"15.000","5":"yes","6":"2","7":"16","8":"5","9":"4","_rn_":"1480"},{"1":"12","2":"female","3":"32.0","4":"15.000","5":"yes","6":"1","7":"14","8":"5","9":"2","_rn_":"1505"},{"1":"7","2":"male","3":"32.0","4":"10.000","5":"yes","6":"3","7":"17","8":"6","9":"3","_rn_":"1543"},{"1":"2","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"18","8":"5","9":"1","_rn_":"1548"},{"1":"7","2":"female","3":"27.0","4":"1.500","5":"no","6":"2","7":"17","8":"5","9":"5","_rn_":"1550"},{"1":"3","2":"female","3":"47.0","4":"15.000","5":"yes","6":"2","7":"17","8":"5","9":"2","_rn_":"1561"},{"1":"12","2":"male","3":"37.0","4":"15.000","5":"yes","6":"2","7":"17","8":"5","9":"4","_rn_":"1564"},{"1":"12","2":"female","3":"27.0","4":"4.000","5":"no","6":"2","7":"14","8":"5","9":"5","_rn_":"1573"},{"1":"2","2":"female","3":"27.0","4":"10.000","5":"yes","6":"4","7":"14","8":"1","9":"5","_rn_":"1575"},{"1":"1","2":"female","3":"22.0","4":"4.000","5":"yes","6":"3","7":"16","8":"1","9":"3","_rn_":"1599"},{"1":"12","2":"male","3":"52.0","4":"7.000","5":"no","6":"4","7":"16","8":"5","9":"5","_rn_":"1622"},{"1":"2","2":"female","3":"27.0","4":"4.000","5":"yes","6":"1","7":"16","8":"3","9":"5","_rn_":"1629"},{"1":"7","2":"female","3":"37.0","4":"15.000","5":"yes","6":"2","7":"17","8":"6","9":"4","_rn_":"1664"},{"1":"2","2":"female","3":"27.0","4":"4.000","5":"no","6":"1","7":"17","8":"3","9":"1","_rn_":"1669"},{"1":"12","2":"female","3":"17.5","4":"0.750","5":"yes","6":"2","7":"12","8":"3","9":"5","_rn_":"1674"},{"1":"7","2":"female","3":"32.0","4":"15.000","5":"yes","6":"5","7":"18","8":"5","9":"4","_rn_":"1682"},{"1":"7","2":"female","3":"22.0","4":"4.000","5":"no","6":"1","7":"16","8":"3","9":"5","_rn_":"1685"},{"1":"2","2":"male","3":"32.0","4":"4.000","5":"yes","6":"4","7":"18","8":"6","9":"4","_rn_":"1697"},{"1":"1","2":"female","3":"22.0","4":"1.500","5":"yes","6":"3","7":"18","8":"5","9":"2","_rn_":"1716"},{"1":"3","2":"female","3":"42.0","4":"15.000","5":"yes","6":"2","7":"17","8":"5","9":"4","_rn_":"1730"},{"1":"1","2":"male","3":"32.0","4":"7.000","5":"yes","6":"4","7":"16","8":"4","9":"4","_rn_":"1731"},{"1":"12","2":"male","3":"37.0","4":"15.000","5":"no","6":"3","7":"14","8":"6","9":"2","_rn_":"1732"},{"1":"1","2":"male","3":"42.0","4":"15.000","5":"yes","6":"3","7":"16","8":"6","9":"3","_rn_":"1743"},{"1":"1","2":"male","3":"27.0","4":"4.000","5":"yes","6":"1","7":"18","8":"5","9":"4","_rn_":"1751"},{"1":"2","2":"male","3":"37.0","4":"15.000","5":"yes","6":"4","7":"20","8":"7","9":"3","_rn_":"1757"},{"1":"7","2":"male","3":"37.0","4":"15.000","5":"yes","6":"3","7":"20","8":"6","9":"4","_rn_":"1763"},{"1":"3","2":"male","3":"22.0","4":"1.500","5":"no","6":"2","7":"12","8":"3","9":"3","_rn_":"1766"},{"1":"3","2":"male","3":"32.0","4":"4.000","5":"yes","6":"3","7":"20","8":"6","9":"2","_rn_":"1772"},{"1":"2","2":"male","3":"32.0","4":"15.000","5":"yes","6":"5","7":"20","8":"6","9":"5","_rn_":"1776"},{"1":"12","2":"female","3":"52.0","4":"15.000","5":"yes","6":"1","7":"18","8":"5","9":"5","_rn_":"1782"},{"1":"12","2":"male","3":"47.0","4":"15.000","5":"no","6":"1","7":"18","8":"6","9":"5","_rn_":"1784"},{"1":"3","2":"female","3":"32.0","4":"15.000","5":"yes","6":"4","7":"16","8":"4","9":"4","_rn_":"1791"},{"1":"7","2":"female","3":"32.0","4":"15.000","5":"yes","6":"3","7":"14","8":"3","9":"2","_rn_":"1831"},{"1":"7","2":"female","3":"27.0","4":"7.000","5":"yes","6":"4","7":"16","8":"1","9":"2","_rn_":"1840"},{"1":"12","2":"male","3":"42.0","4":"15.000","5":"yes","6":"3","7":"18","8":"6","9":"2","_rn_":"1844"},{"1":"7","2":"female","3":"42.0","4":"15.000","5":"yes","6":"2","7":"14","8":"3","9":"2","_rn_":"1856"},{"1":"12","2":"male","3":"27.0","4":"7.000","5":"yes","6":"2","7":"17","8":"5","9":"4","_rn_":"1876"},{"1":"3","2":"male","3":"32.0","4":"10.000","5":"yes","6":"4","7":"14","8":"4","9":"3","_rn_":"1929"},{"1":"7","2":"male","3":"47.0","4":"15.000","5":"yes","6":"3","7":"16","8":"4","9":"2","_rn_":"1935"},{"1":"1","2":"male","3":"22.0","4":"1.500","5":"yes","6":"1","7":"12","8":"2","9":"5","_rn_":"1938"},{"1":"7","2":"female","3":"32.0","4":"10.000","5":"yes","6":"2","7":"18","8":"5","9":"4","_rn_":"1941"},{"1":"2","2":"male","3":"32.0","4":"10.000","5":"yes","6":"2","7":"17","8":"6","9":"5","_rn_":"1954"},{"1":"2","2":"male","3":"22.0","4":"7.000","5":"yes","6":"3","7":"18","8":"6","9":"2","_rn_":"1959"},{"1":"1","2":"female","3":"32.0","4":"15.000","5":"yes","6":"3","7":"14","8":"1","9":"5","_rn_":"9010"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Neste post, a an√°lise de dados ser√° feita considerando a vari√°vel <code>affairs</code> (Quantas vezes envolvido em caso extraconjugal no √∫ltimo ano (aparentemente em 1977)) e a base de dados conta com as vari√°veis g√™nero, idade, anos de casado, se tem crian√ßas, religiosidade, educa√ß√£o, ocupa√ß√£o e como avalia o casamento.</p>
<p>Informa√ß√µes detalhadas podem ser conferidas na tabela a seguir, retirada do artigo apresentado:</p>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/tab.png" /></p>
<p>Obs.: Essa tabela foi feita com o pacote <a href="https://plot.ly/r/"><code>plotly</code></a>, o c√≥digo pode ser conferido <a href="https://gist.github.com/gomesfellipe/4d1d17ca97ac6dadfabad6baef3c5539">aqui</a>.</p>
</div>
</div>
</div>
<div id="vis√£o-geral-dos-dados" class="section level1">
<h1>Vis√£o geral dos dados</h1>
<p>Entendendo as dimens√µes do conjunto de dados, nomes de vari√°veis, resumo geral, vari√°veis ausentes e tipos de dados de cada vari√°vel com a fun√ß√£o <code>ExpData()</code>, se o argumento Type = 1, visualiza√ß√£o dos dados (os nomes das colunas s√£o ‚ÄúDescri√ß√µes‚Äù, ‚ÄúObs.‚Äù), j√° se Type = 2, estrutura dos dados (os nomes das colunas s√£o ‚ÄúS.no‚Äù, ‚ÄúVarName‚Äù, ‚ÄúVarClass‚Äù, ‚ÄúVarType‚Äù)
:</p>
<pre class="r"><code># Visao geral dos dados - Type = 1
ExpData(data=Affairs, type=1) # O tipo 1 √© uma vis√£o geral dos dados</code></pre>
<pre><code>##                                           Descriptions    Value
## 1                                   Sample size (nrow)      601
## 2                              No. of variables (ncol)        9
## 3                    No. of numeric/interger variables        7
## 4                              No. of factor variables        2
## 5                                No. of text variables        0
## 6                             No. of logical variables        0
## 7                          No. of identifier variables        0
## 8                                No. of date variables        0
## 9             No. of zero variance variables (uniform)        0
## 10               %. of variables having complete cases 100% (9)
## 11   %. of variables having &gt;0% and &lt;50% missing cases   0% (0)
## 12 %. of variables having &gt;=50% and &lt;90% missing cases   0% (0)
## 13          %. of variables having &gt;=90% missing cases   0% (0)</code></pre>
<p>Conferindo o nome das vari√°veis e os tipos de cada uma:</p>
<pre class="r"><code># Estrutura dos dados - Type = 2
ExpData(data=Affairs, type=2) # O tipo 2 √© a estrutura dos dados</code></pre>
<pre><code>##   Index Variable_Name Variable_Type Per_of_Missing No_of_distinct_values
## 1     1       affairs       numeric              0                     6
## 2     2        gender        factor              0                     2
## 3     3           age       numeric              0                     9
## 4     4  yearsmarried       numeric              0                     8
## 5     5      children        factor              0                     2
## 6     6 religiousness       integer              0                     5
## 7     7     education       numeric              0                     7
## 8     8    occupation       integer              0                     7
## 9     9        rating       integer              0                     5</code></pre>
<p>Esta fun√ß√£o fornece vis√£o geral e estrutura dos quadros de dados.</p>
</div>
<div id="an√°lise-explorat√≥ria-dos-dados" class="section level1">
<h1>An√°lise explorat√≥ria dos dados</h1>
<p>As fun√ß√µes a seguir apresentam a sa√≠da EDA para 3 casos diferentes de an√°lise explorat√≥ria dos dados, s√£o elas:</p>
<ul>
<li><p>A vari√°vel de destino n√£o est√° definida</p></li>
<li><p>A vari√°vel alvo √© cont√≠nua</p></li>
<li><p>A vari√°vel de destino √© categ√≥rica</p></li>
</ul>
<p>Para fins ilustrativos, ser√° feita inicialmente uma an√°lise considerando que n√£o existe vari√°vel resposta, em seguida ser√° considerada a vari√°vel <code>affairs</code> como vari√°vel resposta e por fim, ser√° feita uma transforma√ß√£o nesta vari√°vel resposta num√©rica de forma que ela seja discretizada da seguinte maneira:</p>
<p><span class="math display">\[
1 \text{ se j√° houve caso extraconjugal} \\
0 \text{ se n√£o houve caso extraconjugal}
\]</span></p>
</div>
<div id="relat√≥rio-em-uma-linha" class="section level1">
<h1>Relat√≥rio em uma linha</h1>
<p>Caso o interesse seja apenas ter uma no√ß√£o geral dos dados de forma extremamente r√°pida, basta rodar a linha de c√≥digo abaixo:</p>
<pre><code>ExpReport(Affairs,op_file = &quot;teste.html&quot;)</code></pre>
<p>Antes de come√ßar a explanar cada um dos casos, achei que seria legal frisar que al√©m de tudo que ser√° apresentado, existe a op√ß√£o de se obter um relat√≥rio extenso sobre a an√°lise explorat√≥ria dos dados em apenas uma linha!</p>
<div id="exemplo-para-o-caso-1-a-vari√°vel-de-destino-n√£o-est√°-definida" class="section level2">
<h2>Exemplo para o caso 1: a vari√°vel de destino n√£o est√° definida</h2>
<p>Para ilustrar o primeiro caso, onde a vari√°vel destino n√£o √© definida, vamos supor que n√£o existe uma vari√°vel alvo na nossa base de dados e estamos interessados em simplesmente obter uma vis√£o geral enquanto pensamos em quais t√©cnicas estat√≠sticas ser√£o utilizadas para avaliar nosso dataset.</p>
<div id="resumo-das-vari√°veis-num√©ricas" class="section level3">
<h3>Resumo das vari√°veis num√©ricas</h3>
<p>Resumo de de todas as vari√°veis num√©ricas:</p>
<pre class="r"><code>ExpNumStat (Affairs, 
            by = &quot;A&quot;,       # Agrupar por A (estat√≠sticas resumidas por Todos), G (estat√≠sticas resumidas por grupo), GA (estat√≠sticas resumidas por grupo e Geral)
            gp = NULL,      # vari√°vel de destino, se houver, padr√£o NULL
            MesofShape = 2, # Medidas de formas (assimetria e curtose).
            Outlier = TRUE, # Calcular o limite inferior, o limite superior e o n√∫mero de outliers
            round = 2)      # Arredondar</code></pre>
<pre><code>##   Vname Group
## 1     1   All</code></pre>
<p>Podemos ver que n√£o existem vari√°veis negativas e a √∫nica vari√°vel que apresentou ‚Äúzero‚Äù foi a vari√°vel resposta. Nenhum registro como <code>Inf</code> ou como <code>NA</code> e al√©m das medidas descritivas tamb√©m podemos notar as medidas de <code>skweness</code> e <code>kurtosis</code>. Alguns coment√°rios sobre essas medidas:</p>
<p>Medidas de forma para dar uma avalia√ß√£o detalhada dos dados. Explica a quantidade e a dire√ß√£o do desvio.</p>
<ul>
<li><strong>Kurotsis</strong> explica o qu√£o alto e afiado √© o pico central (Achatamento).</li>
<li><strong>Skewness</strong> n√£o tem unidades: mas um n√∫mero, como um escore z (medida da assimetria)</li>
</ul>
<p>Onde:</p>
<p><a href="https://pt.wikipedia.org/wiki/Curtose"><strong>Kurtose</strong></a>:</p>
<p>A curtose √© uma medida de forma que caracteriza o achatamento da curva da fun√ß√£o de distribui√ß√£o de probabilidade, Assim:</p>
<ul>
<li>Se o valor da curtose for = 0 (ou 3, pela segunda defini√ß√£o), ent√£o tem o mesmo achatamento que a distribui√ß√£o normal. Chama-se a estas fun√ß√µes de mesoc√∫rticas</li>
<li>Se o valor √© &gt; 0 (ou &gt; 3), ent√£o a distribui√ß√£o em quest√£o √© mais alta (afunilada) e concentrada que a distribui√ß√£o normal. Diz-se que esta fun√ß√£o probabilidade √© leptoc√∫rtica, ou que a distribui√ß√£o tem caudas pesadas (o significado √© que √© relativamente f√°cil obter valores que n√£o se aproximam da m√©dia a v√°rios m√∫ltiplos do desvio padr√£o)</li>
<li>Se o valor √© &lt; 0 (ou &lt; 3), ent√£o a fun√ß√£o de distribui√ß√£o √© mais ‚Äúachatada‚Äù que a distribui√ß√£o normal. Chama-se-lhe platic√∫rtica</li>
</ul>
<p><a href="https://pt.wikipedia.org/wiki/Obliquidade"><strong>Skewness</strong></a>:</p>
<p>O Skewness mede a assimetria das caudas da distribui√ß√£o. Distribui√ß√µes assim√©tricas que tem uma cauda mais ‚Äúpesada‚Äù que a outra apresentam obliquidade. Distribui√ß√µes sim√©tricas tem obliquidade zero. Assim:</p>
<ul>
<li>Se v&gt;0, ent√£o a distribui√ß√£o tem uma cauda direita (valores acima da m√©dia) mais pesada</li>
<li>Se v&lt;0, ent√£o a distribui√ß√£o tem uma cauda esquerda (valores abaixo da m√©dia) mais pesada</li>
<li>Se v=0, ent√£o a distribui√ß√£o √© aproximadamente sim√©trica (na terceira pot√™ncia do desvio em rela√ß√£o √† m√©dia).</li>
</ul>
<div id="distribui√ß√µes-de-vari√°veis-num√©ricas" class="section level4">
<h4>Distribui√ß√µes de vari√°veis num√©ricas</h4>
<p>Representa√ß√£o gr√°fica de todos os recursos num√©ricos com <strong>gr√°fico de densidade</strong> (uni variada):</p>
<pre class="r"><code># Nota: Vari√°vel exclu√≠da (se o valor √∫nico da vari√°vel for menor ou igual a 10 [im = 10])

ExpNumViz(Affairs,
          Page=c(2,2), # padr√£o de sa√≠da. 
          sample=NULL) # sele√ß√£o aleat√≥ria de plots</code></pre>
<pre><code>## $`0`</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-9-1.png" width="672" /><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-9-2.png" width="672" /></p>
<p>Exibidos os gr√°ficos com as densidades das vari√°veis num√©ricas. Como podemos ver a maioria da amostra n√£o registrou caso extraconjugal, a maioria tem de 12 ou mais anos de casado. A m√©dia amostral da idade dos indiv√≠duos √© de aproximadamente 32 anos apresentando leve assimetria com cauda a direita. As demais vari√°veis podem ser conferidas visualmente.</p>
</div>
</div>
<div id="resumo-de-vari√°veis-categ√≥ricas" class="section level3">
<h3>Resumo de vari√°veis categ√≥ricas</h3>
<p>Essa fun√ß√£o selecionar√° automaticamente vari√°veis categ√≥ricas e gerar√° frequ√™ncia ou tabelas cruzadas com base nas entradas do usu√°rio. A sa√≠da inclui contagens, porcentagens, total de linhas e total de colunas.</p>
<p>Frequ√™ncia para todas as vari√°veis independentes categ√≥ricas:</p>
<pre class="r"><code>ExpCTable(Affairs,
          Target=NULL)</code></pre>
<pre><code>##         Variable  Valid Frequency Percent CumPercent
## 1         gender female       315   52.41      52.41
## 2         gender   male       286   47.59     100.00
## 3         gender  TOTAL       601      NA         NA
## 4       children     no       171   28.45      28.45
## 5       children    yes       430   71.55     100.00
## 6       children  TOTAL       601      NA         NA
## 7        affairs      0       451   75.04      75.04
## 8        affairs      1        34    5.66      80.70
## 9        affairs     12        38    6.32      87.02
## 10       affairs      2        17    2.83      89.85
## 11       affairs      3        19    3.16      93.01
## 12       affairs      7        42    6.99     100.00
## 13       affairs  TOTAL       601      NA         NA
## 14           age   17.5         6    1.00       1.00
## 15           age     22       117   19.47      20.47
## 16           age     27       153   25.46      45.93
## 17           age     32       115   19.13      65.06
## 18           age     37        88   14.64      79.70
## 19           age     42        56    9.32      89.02
## 20           age     47        23    3.83      92.85
## 21           age     52        21    3.49      96.34
## 22           age     57        22    3.66     100.00
## 23           age  TOTAL       601      NA         NA
## 24  yearsmarried  0.125        11    1.83       1.83
## 25  yearsmarried  0.417        10    1.66       3.49
## 26  yearsmarried   0.75        31    5.16       8.65
## 27  yearsmarried    1.5        88   14.64      23.29
## 28  yearsmarried     10        70   11.65      34.94
## 29  yearsmarried     15       204   33.94      68.88
## 30  yearsmarried      4       105   17.47      86.35
## 31  yearsmarried      7        82   13.64      99.99
## 32  yearsmarried  TOTAL       601      NA         NA
## 33 religiousness      1        48    7.99       7.99
## 34 religiousness      2       164   27.29      35.28
## 35 religiousness      3       129   21.46      56.74
## 36 religiousness      4       190   31.61      88.35
## 37 religiousness      5        70   11.65     100.00
## 38 religiousness  TOTAL       601      NA         NA
## 39     education     12        44    7.32       7.32
## 40     education     14       154   25.62      32.94
## 41     education     16       115   19.13      52.07
## 42     education     17        89   14.81      66.88
## 43     education     18       112   18.64      85.52
## 44     education     20        80   13.31      98.83
## 45     education      9         7    1.16      99.99
## 46     education  TOTAL       601      NA         NA
## 47    occupation      1       113   18.80      18.80
## 48    occupation      2        13    2.16      20.96
## 49    occupation      3        47    7.82      28.78
## 50    occupation      4        68   11.31      40.09
## 51    occupation      5       204   33.94      74.03
## 52    occupation      6       143   23.79      97.82
## 53    occupation      7        13    2.16      99.98
## 54    occupation  TOTAL       601      NA         NA
## 55        rating      1        16    2.66       2.66
## 56        rating      2        66   10.98      13.64
## 57        rating      3        93   15.47      29.11
## 58        rating      4       194   32.28      61.39
## 59        rating      5       232   38.60      99.99
## 60        rating  TOTAL       601      NA         NA</code></pre>
<p>Obs.: <code>NA</code> significa <code>Not Applicable</code></p>
</div>
<div id="distribui√ß√µes-de-vari√°veis-categ√≥ricas" class="section level3">
<h3>Distribui√ß√µes de vari√°veis categ√≥ricas</h3>
<p>Essa fun√ß√£o varre automaticamente cada vari√°vel e cria um gr√°fico de barras para vari√°veis categ√≥ricas.</p>
<p>Gr√°ficos de barra para todas as vari√°veis categ√≥ricas</p>
<pre class="r"><code>ExpCatViz(Affairs,
          fname=NULL, # Nome do arquivo de saida, default √© pdf
          clim=10,# categorias m√°ximas a incluir nos gr√°ficos de barras.
          margin=2,# √≠ndice, 1 para propor√ß√µes baseadas em linha e 2 para propor√ß√µes baseadas em colunas
          Page = c(2,1), # padrao de saida
          sample=4) # sele√ß√£o aleat√≥ria de plot</code></pre>
<pre><code>## $`0`</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-11-1.png" width="672" /><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
</div>
<div id="machine-lerning-usando-algor√≠timo-n√£o-supervisionado-de-agrupamento" class="section level3">
<h3>Machine Lerning usando algor√≠timo n√£o supervisionado de agrupamento</h3>
<p>Apenas para efeitos ilustrativos, como estamos supondo que n√£o temos a vari√°vel resposta vou remover a coluna <code>affairs</code> do data set e considerarei apenas as vari√°veis num√©ricas para fazer uma an√°lise multivariada com o algor√≠timo de machine learning <a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/kmeans.html"><code>kmeans</code></a>.</p>
<p>A fun√ß√£o <a href="https://github.com/gomesfellipe/functions/blob/master/plot_kmeans.R"><code>plot_kmeans()</code></a> pode ser encontrada em <a href="github.com/gomesfellipe">meu github</a> no <a href="https://github.com/gomesfellipe/functions">reposit√≥rio aberto de fun√ß√µes</a>.</p>
<p>Vejamos os resultados:</p>
<pre class="r"><code>plot_kmeans(Affairs[,-c(1)] %&gt;% select_if(is.numeric) , 2)</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Como podemos observar, foram detectados dois grupos no conjunto de dados. O ideal agora seria fazer uma AED desses clusters identificados e avaliar qual o comportamento dos grupos formados mas como essa vari√°vel foi omitida e a seguir discutiremos a avalia√ß√£o da base diante de da vari√°vel resposta, deixo essas an√°lises aos curiosos de plant√£o.</p>
<p>Mais informa√ß√µes sobre an√°lise multivariava podem ser encontrada no meu post sobre <a href="https://gomesfellipe.github.io/post/2018-01-01-analise-multivariada-em-r/an%C3%A1lise-multivariada-em-r/">An√°lise Multivariada com r</a> e tamb√©m em um <a href="https://www.kaggle.com/gomes555/an-lise-multivariada-pca-e-kmeans">kernel que escrevi para a plataforma kaggle</a>.</p>
<p>Al√©m disso disponibilizo uma aplica√ß√£o Shiny que criei a algum tempo para PCA (An√°lise de componentes Principais) e tarefa de machine learning com agrupamento <a href="https://gomesfellipe.shinyapps.io/appPCAkmeans/">nenste link</a>.</p>
</div>
</div>
<div id="exemplo-para-o-caso-2-a-vari√°vel-de-destino-√©-cont√≠nua" class="section level2">
<h2>Exemplo para o caso 2: A vari√°vel de destino √© cont√≠nua</h2>
<p>Agora vamos considerar que estamos diante de um desfecho onde a vari√°vel alvo √© cont√≠nua, para isso ser√° considerada a vari√°vel <code>affairs</code> como vari√°vel alvo.</p>
<div id="resumo-da-vari√°vel-dependente-cont√≠nua" class="section level3">
<h3>Resumo da vari√°vel dependente cont√≠nua</h3>
<p>Descri√ß√£o da vari√°vel affairs:</p>
<pre class="r"><code>summary(Affairs[,&quot;affairs&quot;])</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   0.000   0.000   0.000   1.456   0.000  12.000</code></pre>
</div>
<div id="resumo-das-vari√°veis-num√©ricas-1" class="section level3">
<h3>Resumo das vari√°veis num√©ricas</h3>
<p>Estat√≠sticas de resumo quando a vari√°vel dependente √© cont√≠nua Pre√ßo.</p>
<pre class="r"><code>ExpNumStat(Affairs,
           by=&quot;A&quot;, # Agrupar por A (estat√≠sticas resumidas por Todos), G (estat√≠sticas resumidas por grupo), GA (estat√≠sticas resumidas por grupo e Geral)
           Qnt=seq(0,1,0.1), # padr√£o NULL. Quantis especificados [c (0,25,0,75) encontrar√£o os percentis 25 e 75]
           MesofShape=1, # Medidas de formas (assimetria e curtose)
           Outlier=TRUE, # Calcular limite superior , inferior e numero de outliers
           round=2) # Arredondamento</code></pre>
<pre><code>##   Vname Group
## 1     1   All</code></pre>
<pre class="r"><code>#Se a vari√°vel de destino for cont√≠nua, as estat√≠sticas de resumo adicionar√£o a coluna de correla√ß√£o (Correla√ß√£o entre a vari√°vel de destino e todas as vari√°veis independentes)</code></pre>
<div id="distribui√ß√µes-de-vari√°veis-num√©ricas-1" class="section level4">
<h4>Distribui√ß√µes de vari√°veis num√©ricas</h4>
<p>Representa√ß√£o gr√°fica de todas as vari√°veis num√©ricas com gr√°ficos de dispers√£o (bivariada)</p>
<p>Gr√°fico de dispers√£o entre todas as vari√°veis num√©ricas e a vari√°vel de destino affairs. Esta trama ajuda a examinar qu√£o bem uma vari√°vel alvo est√° correlacionada com vari√°veis dependentes.</p>
<p>Vari√°vel dependente √© affairs (cont√≠nuo).</p>
<pre class="r"><code>ExpNumViz(Affairs,
            target=&quot;affairs&quot;, # Variavel alvo
            nlim=4, # a vari√°vel num√©rica com valor exclusivo √© maior que 4
            Page=c(2,2), # formato de saida
            sample=NULL) # selecionado aleatoriamente 8 gr√°ficos de dispers√£o</code></pre>
<pre><code>## $`0`</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-15-1.png" width="672" /><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-15-2.png" width="672" /></p>
</div>
</div>
<div id="resumo-de-vari√°veis-categ√≥ricas-1" class="section level3">
<h3>Resumo de vari√°veis categ√≥ricas</h3>
<p>Resumo de vari√°veis categ√≥ricas de acordo com a frequ√™ncia para todas as vari√°veis independentes categ√≥ricas por Affairs</p>
<pre class="r"><code>##bin=4, descretized 4 categories based on quantiles
ExpCTable(Affairs, Target=&quot;affairs&quot;)</code></pre>
<pre><code>##         VARIABLE CATEGORY affairs:(-0.012,4] affairs:(4,8] affairs:(8,12] TOTAL
## 1         gender   female                273            22             20   315
## 2         gender     male                248            20             18   286
## 3         gender    TOTAL                521            42             38   601
## 4       children       no                157             7              7   171
## 5       children      yes                364            35             31   430
## 6       children    TOTAL                521            42             38   601
## 7        affairs        0                451             0              0   451
## 8        affairs        1                 34             0              0    34
## 9        affairs       12                  0             0             38    38
## 10       affairs        2                 17             0              0    17
## 11       affairs        3                 19             0              0    19
## 12       affairs        7                  0            42              0    42
## 13       affairs    TOTAL                521            42             38   601
## 14           age     17.5                  4             0              2     6
## 15           age       22                112             4              1   117
## 16           age       27                138             9              6   153
## 17           age       32                 95            11              9   115
## 18           age       37                 71             8              9    88
## 19           age       42                 44             6              6    56
## 20           age       47                 19             1              3    23
## 21           age       52                 17             2              2    21
## 22           age       57                 21             1              0    22
## 23           age    TOTAL                521            42             38   601
## 24  yearsmarried    0.125                 11             0              0    11
## 25  yearsmarried    0.417                 10             0              0    10
## 26  yearsmarried     0.75                 29             0              2    31
## 27  yearsmarried      1.5                 85             2              1    88
## 28  yearsmarried       10                 57             8              5    70
## 29  yearsmarried       15                165            17             22   204
## 30  yearsmarried        4                 94             9              2   105
## 31  yearsmarried        7                 70             6              6    82
## 32  yearsmarried    TOTAL                521            42             38   601
## 33 religiousness        1                 37             5              6    48
## 34 religiousness        2                138            14             12   164
## 35 religiousness        3                105            13             11   129
## 36 religiousness        4                177             6              7   190
## 37 religiousness        5                 64             4              2    70
## 38 religiousness    TOTAL                521            42             38   601
## 39     education       12                 36             2              6    44
## 40     education       14                139             6              9   154
## 41     education       16                108             5              2   115
## 42     education       17                 72             9              8    89
## 43     education       18                 94            11              7   112
## 44     education       20                 67             9              4    80
## 45     education        9                  5             0              2     7
## 46     education    TOTAL                521            42             38   601
## 47    occupation        1                101             6              6   113
## 48    occupation        2                 13             0              0    13
## 49    occupation        3                 39             5              3    47
## 50    occupation        4                 60             4              4    68
## 51    occupation        5                177            12             15   204
## 52    occupation        6                120            13             10   143
## 53    occupation        7                 11             2              0    13
## 54    occupation    TOTAL                521            42             38   601
## 55        rating        1                 11             1              4    16
## 56        rating        2                 43             8             15    66
## 57        rating        3                 80             9              4    93
## 58        rating        4                169            18              7   194
## 59        rating        5                218             6              8   232
## 60        rating    TOTAL                521            42             38   601</code></pre>
<div id="distribui√ß√µes-de-vari√°veis-categ√≥ricas-1" class="section level4">
<h4>Distribui√ß√µes de vari√°veis categ√≥ricas</h4>
<p>Essa fun√ß√£o varre automaticamente cada vari√°vel e cria um gr√°fico de barras para vari√°veis categ√≥ricas.</p>
<p>Gr√°ficos de barra para todas as vari√°veis categ√≥ricas</p>
<pre class="r"><code>ExpCatViz(Affairs,
          target=&quot;affairs&quot;, # Variavel target
          fname=NULL, # Nome do arquivo de saida, default √© pdf
          clim=10,# categorias m√°ximas a incluir nos gr√°ficos de barras.
          margin=2,# √≠ndice, 1 para propor√ß√µes baseadas em linha e 2 para propor√ß√µes baseadas em colunas
          Page = c(2,1), # padrao de saida
          sample=4) # sele√ß√£o aleat√≥ria de plot</code></pre>
<pre><code>## $`0`</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-17-1.png" width="672" /><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-17-2.png" width="672" /></p>
</div>
</div>
<div id="avaliando-a-correla√ß√£o-entre-as-vari√°veis" class="section level3">
<h3>Avaliando a correla√ß√£o entre as vari√°veis</h3>
<pre class="r"><code>library(ggplot2)
library(dplyr)
library(GGally)
data(&quot;Affairs&quot;)
#Correla√ßoes cruzadas
Affairs%&gt;%
  select(age:rating,affairs)%&gt;%
ggpairs(lower = list(continuous = my_fn,combo=wrap(&quot;facethist&quot;, binwidth=1), 
                                       continuous=wrap(my_bin, binwidth=0.25)),aes(fill=affairs))+theme_bw()</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<pre class="r"><code>ggcorr(Affairs,label = T,nbreaks = 5,label_round = 4)</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
</div>
<div id="modelo-de-regress√£o-linear-usando-stepwiseaic" class="section level3">
<h3>Modelo de regress√£o linear usando stepwiseAIC</h3>
<p>Por fim, vamos ajustar um modelo de regress√£o linear para entender quais s√£o as vari√°veis significativas para explicar a varia√ß√£o da vari√°vel resposta e qual o efeito de cada uma dessas vari√°veis explicativas no nosso desfecho.</p>
<p>Com o R base √© poss√≠vel ajustar um modelo de regress√£o linear simples utilizando a fun√ß√£o <code>lm()</code> e em seguida usar a fun√ß√£o <code>step()</code> para utilizar t√©cnicas como <a href="https://en.wikipedia.org/wiki/Stepwise_regression">stepwise</a>, por√©m como quero utilizar tamb√©m a t√©cnica de <a href="https://pt.wikipedia.org/wiki/Valida%C3%A7%C3%A3o_cruzada">valida√ß√£o cruzada</a>. Para isso vou utilizar o pacote <a href="https://cran.r-project.org/web/packages/caret/caret.pdf"><code>caret</code></a>, muito famoso por facilitar o ajuste de modelos de machine learning (ou mesmo modelos estat√≠sticos tradicionais).</p>
<p>Al√©m disso estou usando as transforma√ß√µes <a href="https://www.rdocumentation.org/packages/caret/versions/6.0-79/topics/preProcess"><code>center()</code></a>, que subtrai a m√©dia dos dados e <a href="https://www.rdocumentation.org/packages/caret/versions/6.0-79/topics/preProcess"><code>scale()</code></a> divide pelo desvio padr√£o.</p>
<pre class="r"><code>data(&quot;Affairs&quot;)
library(caret)
set.seed(123)
index &lt;- sample(1:2,nrow(Affairs),replace=T,prob=c(0.8,0.2))
train = Affairs[index==1,] %&gt;%as.data.frame()
test = Affairs[index==2,] %&gt;%as.data.frame()

# Setando os par√¢metros para o controle do ajuste do modelo:
fitControl &lt;- trainControl(method = &quot;repeatedcv&quot;,         # 10fold cross validation
                     number = 10, repeats=5                         # do 5 repititi√ß√µes of cv
                     )

# Regress√£o Linear com Stepwise
set.seed(825)
lmFit &lt;- train(affairs ~ ., data = train,
                method = &quot;lmStepAIC&quot;, 
                trControl = fitControl,
                preProc = c(&quot;center&quot;, &quot;scale&quot;),trace=F)
summary(lmFit)</code></pre>
<pre><code>## 
## Call:
## lm(formula = .outcome ~ age + yearsmarried + religiousness + 
##     occupation + rating, data = dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.1452 -1.7819 -0.7601  0.2719 11.3518 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     1.4146     0.1401  10.096  &lt; 2e-16 ***
## age            -0.6890     0.2291  -3.007 0.002779 ** 
## yearsmarried    1.1058     0.2302   4.804 2.09e-06 ***
## religiousness  -0.5121     0.1455  -3.519 0.000475 ***
## occupation      0.3858     0.1445   2.669 0.007858 ** 
## rating         -0.7830     0.1470  -5.326 1.55e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.07 on 474 degrees of freedom
## Multiple R-squared:  0.144,  Adjusted R-squared:  0.135 
## F-statistic: 15.95 on 5 and 474 DF,  p-value: 1.542e-14</code></pre>
<p>Como podemos ver as vari√°veis Idade, Anos de casado, religiosidade, ocupa√ß√£o e como avaliam o pr√≥prio relacionamento se apresentaram significantes</p>
<p>Como o <span class="math inline">\(R^2=0,144\)</span>, conclui-se que <span class="math inline">\(14,4%\)</span> da varia√ß√£o da quantidade de vezes que foi envolvida em caso extraconjugal no √∫ltimo ano √© explicada pelo modelo ajustado.</p>
<p>Observando a coluna das estimativas, podemos notar o quanto varia a quantidade de vezes que foi envolvido em caso extraconjugal ao aumentar em 1 unidade cada uma das vari√°veis explicativas.</p>
<p>Al√©m disso o valor p obtido atrav√©s da estat√≠stica F foi menor do que <span class="math inline">\(\alpha = 0.05\)</span>, o que implica que pelo menos uma das vari√°veis explicativas tem rela√ß√£o significativa com a vari√°vel resposta.</p>
<p>Selecionando apenas as vari√°veis selecionadas com o ajuste do modelo:</p>
<pre class="r"><code>train=as.data.frame(train[,c(1,3,4,6,8,9)])
test=as.data.frame(test[,c(1,3,4,6,8,9)])</code></pre>
<div id="diagn√≥stico-do-modelo" class="section level4">
<h4>Diagn√≥stico do modelo</h4>
<p>Existem varias formas e t√©cnicas de se avaliar o ajuste de um modelo e como o foco deste post √© apresentar as utilidades do pacote <code>SmartEAD</code> irei fazer uma avalia√ß√£o muito breve sobre os res√≠duos, apresento mais algumas maneiras no post sobre <a href="https://gomesfellipe.github.io/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos/">pacotes do R para avaliar o ajuste de modelos</a>.</p>
<div id="avaliando-residuos" class="section level5">
<h5>Avaliando residuos</h5>
<pre class="r"><code>library(GGally)
# calculate all residuals prior to display
residuals &lt;- lapply(train[2:ncol(train)], function(x) {
  summary(lm(affairs ~ x, data = train))$residuals
})

# add a &#39;fake&#39; column
train$Residual &lt;- seq_len(nrow(train))

# calculate a consistent y range for all residuals
y_range &lt;- range(unlist(residuals))

# plot the data
ggduo(
  train,
  2:6, c(1,7),
  types = list(continuous = lm_or_resid)
)+ theme_bw()</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<pre class="r"><code>train=train%&gt;%
  select(-Residual)</code></pre>
<p>Neste gr√°fico √© poss√≠vel observar como se comportam os ajustes de modelos lineares de cada vari√°vel explicativa em rela√ß√£o √† vari√°vel resposta e al√©m disso na segunda linha √© poss√≠vel notar o comportamento dos res√≠duos no modelo.</p>
<p>Uma das suposi√ß√µes do ajuste de um modelo linear normal √© de que <span class="math inline">\(\epsilon \sim N(0,\sigma^2)\)</span> e visualmente parece que essa condi√ß√£o n√£o deve ser atendida, pois esperar√≠amos algo como uma ‚Äúnuvem‚Äù aleat√≥ria de pontos em torno de zero.</p>
</div>
<div id="residuos-e-medidas-de-influencia" class="section level5">
<h5>Residuos e medidas de influencia</h5>
<p>Al√©m da suposi√ß√£o da normalidade dos res√≠duos, existem ainda mais detalhes do comportamento desses erros, uma breve apresenta√ß√£o no gr√°fico a seguir:</p>
<pre class="r"><code>library(ggfortify)

autoplot(lmFit$finalModel, which = 1:6, data = train,
         colour = &#39;affairs&#39;, label.size = 3,
         ncol = 3)+theme_classic()</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>Pelo que parece no gr√°fico com t√≠tulo ‚ÄúNormal Q-Q‚Äù, as vari√°veis associadas √† vari√°vel resposta com valores acima de 6 se comportam de forma inesperadas quando comparadas com os quantis te√≥ricos.</p>
</div>
</div>
</div>
</div>
<div id="exemplo-para-o-caso-3-a-vari√°vel-de-destino-√©-categ√≥rica" class="section level2">
<h2>Exemplo para o caso 3: a vari√°vel de destino √© categ√≥rica</h2>
<p>Para finalizar a avalia√ß√£o da base de dados, a Vari√°vel alvo ser√° discretizado de tal forma:</p>
<p><span class="math display">\[
1 = \text{se affairs} &gt; 0\\
0 = c.c.
\]</span></p>
<p>Essa transforma√ß√£o ser√° utilizada apenas com fins ilustrativos do algor√≠timo de √°rvore de decis√µes, que est√° ficando muito comum na ci√™ncia de dados como uma tarefa supervisionada de machine learning.</p>
<pre class="r"><code>Affairs = Affairs %&gt;% 
  mutate(daffairs = ifelse(Affairs$affairs!=0,1,0)) %&gt;% 
  mutate(daffairs = as.factor(daffairs))%&gt;% 
  select(-affairs)
levels(Affairs$daffairs) = c(&quot;N√£o&quot;, &quot;Sim&quot;)</code></pre>
<div id="resumo-das-vari√°veis-num√©ricas-2" class="section level3">
<h3>Resumo das vari√°veis num√©ricas</h3>
<p>Resumo de todas as vari√°veis num√©ricas</p>
<pre class="r"><code>ExpNumStat(Affairs,
           by=&quot;A&quot;, # Agrupar por A (estat√≠sticas resumidas por Todos), G (estat√≠sticas resumidas por grupo), GA (estat√≠sticas resumidas por grupo e Geral)
           gp=&quot;daffairs&quot;, # Variavel alvo
           Qnt=seq(0,1,0.1), # padr√£o NULL. Quantis especificados [c (0,25,0,75) encontrar√£o os percentis 25 e 75]
           MesofShape=1, # Medidas de formas (assimetria e curtose)
           Outlier=TRUE, # Calcular limite superior , inferior e numero de outliers
           round=2) # Arredondamento</code></pre>
<pre><code>##   Vname Group
## 1     1   All</code></pre>
<div id="distribui√ß√µes-de-vari√°veis-num√©ricas-2" class="section level4">
<h4>Distribui√ß√µes de vari√°veis num√©ricas</h4>
<p>Box plots para todas as vari√°veis num√©ricas vs vari√°vel dependente categ√≥rica - Compara√ß√£o bivariada apenas com categorias</p>
<p>Boxplot para todos os atributos num√©ricos por cada categoria de affair</p>
<pre class="r"><code>ExpNumViz(Affairs, target=&quot;daffairs&quot;) # amostra de variaveis para o resumo</code></pre>
<pre><code>## [[1]]</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<pre><code>## 
## [[2]]</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-26-2.png" width="672" /></p>
<pre><code>## 
## [[3]]</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-26-3.png" width="672" /></p>
<pre><code>## 
## [[4]]</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-26-4.png" width="672" /></p>
<pre><code>## 
## [[5]]</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-26-5.png" width="672" /></p>
<pre><code>## 
## [[6]]</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-26-6.png" width="672" /></p>
</div>
</div>
<div id="resumo-das-vari√°veis-categ√≥ricas" class="section level3">
<h3>Resumo das vari√°veis categ√≥ricas</h3>
<p>Tabula√ß√£o cruzada com vari√°vel de destino com tabelas customizadas entre todas as vari√°veis independentes categ√≥ricas e a vari√°vel de destino <code>daffairs</code>:</p>
<pre class="r"><code>ExpCTable(Affairs,
          Target=&quot;daffairs&quot;, # variavel alvo
          margin=1, # 1 para proporcoes por linha, 2 para colunas
          clim=10, # maximo de categorias consideradas por frequencia/ custom table
          round=2, # arredondar
          per=F) # valores percentuais. Tabela padr√£o dar√° contagens.</code></pre>
<pre><code>##         VARIABLE CATEGORY daffairs:N√£o daffairs:Sim TOTAL
## 1         gender   female          243           72   315
## 2         gender     male          208           78   286
## 3         gender    TOTAL          451          150   601
## 4       children       no          144           27   171
## 5       children      yes          307          123   430
## 6       children    TOTAL          451          150   601
## 7            age     17.5            3            3     6
## 8            age       22          101           16   117
## 9            age       27          117           36   153
## 10           age       32           77           38   115
## 11           age       37           65           23    88
## 12           age       42           38           18    56
## 13           age       47           16            7    23
## 14           age       52           15            6    21
## 15           age       57           19            3    22
## 16           age    TOTAL          451          150   601
## 17  yearsmarried    0.125           10            1    11
## 18  yearsmarried    0.417            9            1    10
## 19  yearsmarried     0.75           28            3    31
## 20  yearsmarried      1.5           76           12    88
## 21  yearsmarried       10           49           21    70
## 22  yearsmarried       15          142           62   204
## 23  yearsmarried        4           78           27   105
## 24  yearsmarried        7           59           23    82
## 25  yearsmarried    TOTAL          451          150   601
## 26 religiousness        1           28           20    48
## 27 religiousness        2          123           41   164
## 28 religiousness        3           86           43   129
## 29 religiousness        4          157           33   190
## 30 religiousness        5           57           13    70
## 31 religiousness    TOTAL          451          150   601
## 32     education       12           31           13    44
## 33     education       14          119           35   154
## 34     education       16           95           20   115
## 35     education       17           62           27    89
## 36     education       18           79           33   112
## 37     education       20           60           20    80
## 38     education        9            5            2     7
## 39     education    TOTAL          451          150   601
## 40    occupation        1           90           23   113
## 41    occupation        2           10            3    13
## 42    occupation        3           32           15    47
## 43    occupation        4           47           21    68
## 44    occupation        5          160           44   204
## 45    occupation        6          104           39   143
## 46    occupation        7            8            5    13
## 47    occupation    TOTAL          451          150   601
## 48        rating        1            8            8    16
## 49        rating        2           33           33    66
## 50        rating        3           66           27    93
## 51        rating        4          146           48   194
## 52        rating        5          198           34   232
## 53        rating    TOTAL          451          150   601</code></pre>
<div id="distribui√ß√µes-de-vari√°veis-categ√≥ricas-2" class="section level4">
<h4>Distribui√ß√µes de vari√°veis categ√≥ricas</h4>
<p>Gr√°fico de barras empilhadas com barras verticais ou horizontais para todas as vari√°veis categ√≥ricas</p>
<pre class="r"><code>ExpCatViz(Affairs,
          target=&quot;daffairs&quot;,
          fname=NULL, # Nome do arquivo de saida, default √© pdf
          clim=10,# categorias m√°ximas a incluir nos gr√°ficos de barras.
          margin=2,# √≠ndice, 1 para propor√ß√µes baseadas em linha e 2 para propor√ß√µes baseadas em colunas
          Page = c(2,1), # padrao de saida
          sample=4) # sele√ß√£o aleat√≥ria de plot</code></pre>
<pre><code>## $`0`</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-28-1.png" width="672" /><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-28-2.png" width="672" /></p>
</div>
</div>
<div id="valor-da-informa√ß√£o" class="section level3">
<h3>Valor da informa√ß√£o</h3>
<p><code>IV</code> √© o peso da evid√™ncia e valores da informa√ß√£o, <span class="math inline">\(ln(odss) \times(pct0 - pct1)\)</span> onde <span class="math inline">\(pct1 =\frac{\text{&quot;boas observa√ß√µes&quot;}}{\text{&quot;total boas observa√ß√µes&quot;}}\)</span>; <span class="math inline">\(pct0 = \frac{&quot;\text{observa√ß√µes ruins&quot;} }{ \text{&quot;total de observa√ß√µes ruins&quot;}}\)</span> e $odds =  $</p>
<pre class="r"><code>ExpCatStat(Affairs %&gt;% mutate(daffairs = if_else(daffairs==&quot;N√£o&quot;, 0, 1)) ,
           Target=&quot;daffairs&quot;,
           result = &quot;IV&quot;) %&gt;% 
  select(-one_of(&quot;Target&quot;,&quot;Ref_1&quot;,&quot;Ref_0&quot;))</code></pre>
<pre><code>##           Variable  Class Out_1 Out_0 TOTAL Per_1 Per_0 Odds   WOE   IV
## 1         gender.1 female    72   243   315  0.48  0.54 0.79 -0.12 0.01
## 2         gender.2   male    78   208   286  0.52  0.46 1.27  0.12 0.01
## 3       children.1     no    27   144   171  0.18  0.32 0.47 -0.58 0.08
## 4       children.2    yes   123   307   430  0.82  0.68 2.14  0.19 0.03
## 5            age.1   17.5     3     3     6  0.02  0.01 3.05  0.69 0.01
## 6            age.2     22    16   101   117  0.11  0.22 0.41 -0.69 0.08
## 7            age.3     27    36   117   153  0.24  0.26 0.90 -0.08 0.00
## 8            age.4     32    38    77   115  0.25  0.17 1.65  0.39 0.03
## 9            age.5     37    23    65    88  0.15  0.14 1.08  0.07 0.00
## 10           age.6     42    18    38    56  0.12  0.08 1.48  0.41 0.02
## 11           age.7     47     7    16    23  0.05  0.04 1.33  0.22 0.00
## 12           age.8     52     6    15    21  0.04  0.03 1.21  0.29 0.00
## 13           age.9     57     3    19    22  0.02  0.04 0.46 -0.69 0.01
## 14  yearsmarried.1  0.125     1    10    11  0.01  0.02 0.30 -0.69 0.01
## 15  yearsmarried.2  0.417     1     9    10  0.01  0.02 0.33 -0.69 0.01
## 16  yearsmarried.3   0.75     3    28    31  0.02  0.06 0.31 -1.11 0.04
## 17  yearsmarried.4    1.5    12    76    88  0.08  0.17 0.43 -0.76 0.07
## 18  yearsmarried.5     10    21    49    70  0.14  0.11 1.34  0.24 0.01
## 19  yearsmarried.6     15    62   142   204  0.41  0.31 1.53  0.28 0.03
## 20  yearsmarried.7      4    27    78   105  0.18  0.17 1.05  0.06 0.00
## 21  yearsmarried.8      7    23    59    82  0.15  0.13 1.20  0.14 0.00
## 22 religiousness.1      1    20    28    48  0.13  0.06 2.32  0.77 0.05
## 23 religiousness.2      2    41   123   164  0.27  0.27 1.00  0.00 0.00
## 24 religiousness.3      3    43    86   129  0.29  0.19 1.71  0.43 0.04
## 25 religiousness.4      4    33   157   190  0.22  0.35 0.53 -0.46 0.06
## 26 religiousness.5      5    13    57    70  0.09  0.13 0.66 -0.37 0.01
## 27     education.1     12    13    31    44  0.09  0.07 1.29  0.25 0.00
## 28     education.2     14    35   119   154  0.23  0.26 0.85 -0.13 0.00
## 29     education.3     16    20    95   115  0.13  0.21 0.58 -0.48 0.04
## 30     education.4     17    27    62    89  0.18  0.14 1.38  0.25 0.01
## 31     education.5     18    33    79   112  0.22  0.18 1.33  0.20 0.01
## 32     education.6     20    20    60    80  0.13  0.13 1.00  0.00 0.00
## 33     education.7      9     2     5     7  0.01  0.01 1.21  0.00 0.00
## 34    occupation.1      1    23    90   113  0.15  0.20 0.73 -0.29 0.01
## 35    occupation.2      2     3    10    13  0.02  0.02 0.90  0.00 0.00
## 36    occupation.3      3    15    32    47  0.10  0.07 1.45  0.36 0.01
## 37    occupation.4      4    21    47    68  0.14  0.10 1.40  0.34 0.01
## 38    occupation.5      5    44   160   204  0.29  0.35 0.75 -0.19 0.01
## 39    occupation.6      6    39   104   143  0.26  0.23 1.17  0.12 0.00
## 40    occupation.7      7     5     8    13  0.03  0.02 1.91  0.41 0.00
## 41        rating.1      1     8     8    16  0.05  0.02 3.12  0.92 0.03
## 42        rating.2      2    33    33    66  0.22  0.07 3.57  1.14 0.17
## 43        rating.3      3    27    66    93  0.18  0.15 1.28  0.18 0.01
## 44        rating.4      4    48   146   194  0.32  0.32 0.98  0.00 0.00
## 45        rating.5      5    34   198   232  0.23  0.44 0.37 -0.65 0.14</code></pre>
</div>
<div id="testes-estat√≠sticos" class="section level3">
<h3>Testes estat√≠sticos</h3>
<p>Al√©m de toda a informa√ß√£o visual e das estat√≠sticas descritivas, ainda contamos com alguma fun√ß√£o que fornece estat√≠sticas resumidas para todas as colunas de caracteres ou categ√≥ricas no data frame</p>
<pre class="r"><code>ExpCatStat(Affairs %&gt;% mutate(daffairs = if_else(daffairs==&quot;N√£o&quot;, 0, 1)),
           Target=&quot;daffairs&quot;, # variavel alvo
           result = &quot;Stat&quot;) # resumo de estatisticas</code></pre>
<pre><code>##        Variable   Target Unique Chi-squared p-value df IV Value Cramers V
## 1        gender daffairs      2       1.334   0.248  1     0.02      0.05
## 2      children daffairs      2      10.055   0.002  1     0.11      0.13
## 3           age daffairs      9      17.771   0.023  8     0.15      0.17
## 4  yearsmarried daffairs      8      17.177   0.016  7     0.17      0.17
## 5 religiousness daffairs      5      19.354   0.001  4     0.16      0.18
## 6     education daffairs      7       7.057   0.316  6     0.06      0.11
## 7    occupation daffairs      7       6.718   0.348  6     0.04      0.11
## 8        rating daffairs      5      41.433   0.000  4     0.35      0.26
##   Degree of Association    Predictive Power
## 1             Very Weak      Not Predictive
## 2                  Weak Somewhat Predictive
## 3                  Weak Somewhat Predictive
## 4                  Weak Somewhat Predictive
## 5                  Weak Somewhat Predictive
## 6                  Weak      Not Predictive
## 7                  Weak      Not Predictive
## 8              Moderate   Highly Predictive</code></pre>
<p>Os crit√©rios usados para classifica√ß√£o de poder preditivo vari√°vel categ√≥rico s√£o</p>
<ul>
<li><p>Se o valor da informa√ß√£o for &lt;0,03, ent√£o, poder de previs√£o = ‚ÄúN√£o Preditivo‚Äù</p></li>
<li><p>Se o valor da informa√ß√£o √© de 0,3 a 0,1, ent√£o o poder preditivo = ‚Äúum pouco preditivo‚Äù</p></li>
<li><p>Se o valor da informa√ß√£o for de 0,1 a 0,3, ent√£o, poder preditivo = ‚ÄúMedium Predictive‚Äù</p></li>
<li><p>Se o valor da informa√ß√£o for&gt; 0.3, ent√£o, poder preditivo = ‚ÄúAltamente Preditivo‚Äù</p></li>
</ul>
<p>Nota para a vari√°vel <code>rating</code> que segundo essas regras, demonstrou alto poder preditivo.</p>
</div>
<div id="machine-learning-com-random-forest" class="section level3">
<h3>Machine Learning com Random Forest</h3>
<p>O algor√≠timo supervisionado de machine learning conhecido como <a href="https://www.stat.berkeley.edu/~breiman/RandomForests/">Random Forest</a> √© uma grande caixa preta. Apresenta resultados muito robustos pois combina o resultado de v√°rias √°rvores de decis√µes e pode ser facilmente aplicada com o pacote <code>caret</code>.</p>
<p><a href="https://topepo.github.io/caret/variable-importance.html">No livro do pacote caret</a> o algor√≠timo √© apresentado da seguinte maneira: ‚Äúsegundo o pacote do R: Para cada √°rvore, a precis√£o da previs√£o na parte fora do saco dos dados √© registrada. Ent√£o, o mesmo √© feito ap√≥s a permuta√ß√£o de cada vari√°vel preditora. A diferen√ßa entre as duas precis√µes √© calculada pela m√©dia de todas as √°rvores e normalizada pelo erro padr√£o. Para a regress√£o, o MSE √© calculado nos dados fora da bolsa para cada √°rvore e, em seguida, o mesmo √© computado ap√≥s a permuta√ß√£o de uma vari√°vel. As diferen√ßas s√£o calculadas e normalizadas pelo erro padr√£o. Se o erro padr√£o √© igual a 0 para uma vari√°vel, a divis√£o n√£o √© feita.‚Äù</p>
<p>N√£o entrarei em muitos detalhes sobre o algor√≠timo pois esta parte √© apenas um demonstrativo dos diferentes cen√°rios de an√°lise explorat√≥ria dos dados. Ser√£o comentadas apenas algumas m√©tricas utilizadas.</p>
<p>Ajuste com o algor√≠timo Random Forest:</p>
<pre class="r"><code>library(caret)
set.seed(1)
index &lt;- sample(1:2,nrow(Affairs),replace=T,prob=c(0.8,0.2))
train = Affairs[index==1,] %&gt;%as.data.frame()
test = Affairs[index==2,] %&gt;%as.data.frame()


# Setando os par√¢metros para o controle do ajuste do modelo:
fitControl &lt;- trainControl(method = &quot;repeatedcv&quot;,         # 10fold cross validation
                     number = 10
                     )

# Random Forest
set.seed(825)
antes = Sys.time()
rfFit &lt;- train(daffairs ~ ., data = train,
                method = &quot;rf&quot;, 
                trControl = fitControl,
                trace=F,
                preProc = c(&quot;center&quot;, &quot;scale&quot;))

antes - Sys.time() # Para saber quanto tempo durou o ajuste</code></pre>
<pre><code>## Time difference of -13.38876 secs</code></pre>
<p>Resultados do ajuste:</p>
<pre class="r"><code>rfFit</code></pre>
<pre><code>## Random Forest 
## 
## 484 samples
##   8 predictor
##   2 classes: &#39;N√£o&#39;, &#39;Sim&#39; 
## 
## Pre-processing: centered (8), scaled (8) 
## Resampling: Cross-Validated (10 fold, repeated 1 times) 
## Summary of sample sizes: 436, 436, 435, 435, 435, 436, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##   2     0.7539966  0.1369868
##   5     0.7292942  0.1727691
##   8     0.7231718  0.1613695
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 2.</code></pre>
<p><strong>Accurary e Kappa</strong></p>
<p>Essas s√£o as m√©tricas padr√£o usadas para avaliar algoritmos em conjuntos de dados de classifica√ß√£o bin√°ria.</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Accuracy_and_precision"><strong>Accuray</strong></a>: √© a porcentagem de classificar corretamente as inst√¢ncias fora de todas as inst√¢ncias. √â mais √∫til em uma classifica√ß√£o bin√°ria do que problemas de classifica√ß√£o de v√°rias classes, porque pode ser menos claro exatamente como a precis√£o √© dividida entre essas classes (por exemplo, voc√™ precisa ir mais fundo com uma matriz de confus√£o).</li>
<li><a href="https://en.wikipedia.org/wiki/Cohen%27s_kappa"><strong>Kappa ou Kappa de Cohen</strong></a> √© como a precis√£o da classifica√ß√£o, exceto que √© normalizado na linha de base da chance aleat√≥ria em seu conjunto de dados. √â uma medida mais √∫til para usar em problemas que t√™m um desequil√≠brio nas classes (por exemplo, divis√£o de 70 a 30 para as classes 0 e 1 e voc√™ pode atingir 70% de precis√£o prevendo que todas as inst√¢ncias s√£o para a classe 0).</li>
</ul>
<p>A seguir a ‚ÄúVariable Importance‚Äù de cada vari√°vel:</p>
<pre class="r"><code>rfImp = varImp(rfFit);rfImp</code></pre>
<pre><code>## rf variable importance
## 
##               Overall
## rating         100.00
## age             94.66
## religiousness   85.77
## education       78.99
## yearsmarried    67.41
## occupation      62.48
## gendermale       6.94
## childrenyes      0.00</code></pre>
<pre class="r"><code>plot(rfImp)</code></pre>
<p><img src="/post/2018-05-26-smarteademachinelearning/smarteademachinelearning_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>A fun√ß√£o dimensiona automaticamente as pontua√ß√µes de import√¢ncia entre 0 e 100, os escores de import√¢ncia da vari√°vel em Random Forest s√£o medidas agregadas. Eles apenas quantificam o impacto do preditor, n√£o o efeito espec√≠fico, para isso utilizamos o ajuste um modelo param√©trico onde conseguimos estimar termos estruturais.</p>
<p>√â claro que existem muitos adentos a serem feitos tanto na forma como os dados foram apresentados no ajuste do modelo linear e no Random Forest, mas como a finalidade do post continua sendo apresentar o pacote SmartEAD, encerrarei a avalia√ß√£o por aqui.</p>
<p>Caso algu√©m queira entender com mais detalhes a avalia√ß√£o de modelos de machine learning, talvez <a href="https://topepo.github.io/caret/measuring-performance.html">o livro do pacote caret</a> seja uma alternativa interessante para ter uma no√ß√£o geral.</p>
<blockquote>
<p><em>Todos os modelos est√£o errados, alguns s√£o √∫teis - George Box</em></p>
</blockquote>
<p>N√£o conseguimos nenhum modelo √∫til que quantificasse as incertezas nas modelagens deste post mas conseguimos executar praticamente todas as fun√ß√µes do pacote <code>SmartEAD</code> e foi muito √∫til para conhecer a base em poucas linhas, obrigado Dayanand Ubrangala, Kiran R. e Ravi Prasad Kondapalli!</p>
</div>
</div>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2018-05-26-smarteademachinelearning/smarteademachinelearning/">AED de forma r√°pida e um pouco de Machine Learning</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Analise Explorat√≥ria</category>
      <category>Data mining</category>
      <category>Estatistica</category>
      <category>Modelagem Estatistica</category>
      <category>Pr√°tica</category>
      <category>R</category>
      <category>Reports</category>
      <category>Machine Learning</category>
      <category>Analise Mutivariada</category>
      <category>Aprendizado Supervisionado</category>
      <category>Aprendizado N√£o Supervisionado</category>
      <category domain="tag">analise multivariada</category>
      <category domain="tag">Correlacoes</category>
      <category domain="tag">Data Mining</category>
      <category domain="tag">Estatistica</category>
      <category domain="tag">gomesfellipe</category>
      <category domain="tag">kmeans</category>
      <category domain="tag">modelagem</category>
      <category domain="tag">pca</category>
      <category domain="tag">R</category>
      <category domain="tag">RStudio</category>
    </item>
    <item>
      <title>Tabelas incriveis com R</title>
      <link>https://gomesfellipe.github.io/post/2018-01-12-tabelas-incriveis-com-r/tabelas-incriveis-com-r/</link>
      <pubDate>Fri, 12 Jan 2018 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2018-01-12-tabelas-incriveis-com-r/tabelas-incriveis-com-r/</guid>
      <description>Alguns pacotes que ser√£o bem √∫teis na hora de criar tabelas lindas e informativas!</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<script src="/rmarkdown-libs/kePrint/kePrint.js"></script>
<link href="/rmarkdown-libs/lightable/lightable.css" rel="stylesheet" />


<div id="import√¢cia-da-apresenta√ß√£o-dos-dados" class="section level1">
<h1>Import√¢cia da apresenta√ß√£o dos dados</h1>
<p>O trabalho do estat√≠stico vai muito al√©m do planejamento, sumariza√ß√£o e interpreta√ß√£o de observa√ß√µes para fornecer a melhor informa√ß√£o poss√≠vel a partir do dados dispon√≠veis. O processo de analises deve ser tratado na etapa final de todo projeto ou pesquisa que envolva apresenta√ß√£o dos resultados, n√£o √© atoa que j√° at√© existem √°reas dentro da ci√™ncia de dados focada nesta tarefa, recebendo o t√≠tulo de ‚ÄúData Artist‚Äù.</p>
<p>Al√©m da variedade de pacotes que auxiliam na apresenta√ß√£o das figuras geradas nas an√°lises(como j√° foi visto em alguns posts como estes <a href="https://gomesfellipe.github.io/post/2017-12-24-diagnostico-de-modelo/diagnostico-de-modelos/">para visualizar a qualidade do ajuste de modelos</a> ou <a href="https://gomesfellipe.github.io/post/2017-12-18-bayesiana-jags-mcmcplot/bayesiana-jags-mcmcplot/">este para valiar ajuste de modelos pela abordagem bayesiana</a>), tamb√©m contamos com alguns pacotes que possibilitam a apresenta√ß√£o de tabelas de maneira bastante satisfat√≥ria (de forma elegante e at√© interativa)</p>
<p>Seja escrevendo relat√≥rios em <span class="math inline">\(\LaTeX\)</span>, Rmarkdown ou at√© mesmo um aplicativo shiny , este posta tem a finalidade de trazer algumas alternativas para a boa apresenta√ß√£o dos resultados.</p>
<p>Como de costume vou apresentar alguns pacotes que ser√£o bem √∫teis na hora de criar aquelas tabelas lindas e informativas que qualquer cliente adora.</p>
</div>
<div id="pacote-dt" class="section level1">
<h1>Pacote <code>DT</code></h1>
<p>O pacote <code>DT</code> √© uma excelente op√ß√£o quando se trata de incluir tabelas de dados em relat√≥rios Rmarkdown, o pacote esta hospedado neste <a href="https://rstudio.github.io/DT/">link no github</a>, veja a seguir um simples exemplo de uso:</p>
<pre class="r"><code># install.packages(&quot;DT&quot;)  #caso ainda nao tenha o pacote instalado
DT::datatable(iris[1:20, c(5, 1:4)], rownames = FALSE)</code></pre>
<p><img src="/post/2018-01-12-tabelas-incriveis-com-r/tabelas-incriveis-com-r_files/img11.png" /></p>
<p>Com este <a href="https://cran.r-project.org/web/packages/DT/vignettes/DT.html">link do manual no CRAN</a> √© poss√≠vel entender melhor o funcionamento do pacote e conferir mais exemplos de uso.</p>
</div>
<div id="pacote-formattable" class="section level1">
<h1>Pacote <code>formattable</code></h1>
<p>Este pacote √© repleto de funcionalidades interessantes para a formata√ß√£o dos resultados dispostos em tabelas, tamb√©m est√° <a href="https://github.com/renkun-ken/formattable">hospedado no github</a>, podendo ser instalado pelo CRAN ou com os comandos:</p>
<pre class="r"><code># Instalando pelo github
# library(devtools)
# devtools::install_github(&quot;renkun-ken/formattable&quot;)
library(formattable)</code></pre>
<p>Com o pacote carregado vamos conferir algumas das funcionalidades b√°sicas:</p>
<pre class="r"><code>#Exemplo de formata√ß√£o para resultados de porcentagem:
percent(c(0.1, 0.02, 0.03, 0.12))</code></pre>
<pre><code>## [1] 10.00% 2.00%  3.00%  12.00%</code></pre>
<pre class="r"><code>#Exemplo de formata√ß√£o para resultados de na casa do milhar:
accounting(c(1000, 500, 200, -150, 0, 1200))</code></pre>
<pre><code>## [1] 1,000.00 500.00   200.00   (150.00) 0.00     1,200.00</code></pre>
<p>Vamos criar um <code>data.frame</code> para ilustrar algumas das funcionalidades do pacote:</p>
<pre class="r"><code>#criando um data.frame
df &lt;- data.frame(
  id = 1:10, 
  Nomes = c(&quot;Sofia&quot;, &quot;Kiara&quot;, &quot;Dunki&quot;, &quot;Edgar&quot;, &quot;Emilia&quot;,&quot;Gertrudes&quot;, &quot;Genovena&quot;, &quot;Champanhe&quot;, &quot;Amora&quot;, &quot;Penelope&quot;),
  Kilos = accounting(c(20000, 30000, 50000, 70000, 47000,80000,45000,35000,20000,25000), format = &quot;d&quot;),
  Crescimento = percent(c(0.1, 0.2, 0.5, 0.95, 0.97,0.45,0.62,0.57,0.37, 0.3), format = &quot;d&quot;),
  Suficiente = formattable(c(T, F, T, F, T,F,F,T,T,F), &quot;Sim&quot;, &quot;N√£o&quot;))</code></pre>
<p>Com esses resultados, vejamos um exemplo de tabela que pode ser criada para apresentar esses resultados com o pacote:</p>
<pre class="r"><code>formattable(df, list(
  id = color_tile(&quot;white&quot;, &quot;orange&quot;),
  Suficiente = formatter(&quot;span&quot;, style = x ~ ifelse(x == T, 
                                               style(color = &quot;green&quot;, font.weight = &quot;bold&quot;), NA)),
  area(col = c(Kilos)) ~ normalize_bar(&quot;lightgrey&quot;, 0.2),
  Crescimento = formatter(&quot;span&quot;,
                          style = x ~ style(color = ifelse(rank(-x) &lt;= 3, &quot;green&quot;, &quot;gray&quot;)),
                          x ~ sprintf(&quot;%.2f (rank: %02g)&quot;, x, rank(-x)))
))</code></pre>
<table class="table table-condensed">
<thead>
<tr>
<th style="text-align:right;">
id
</th>
<th style="text-align:right;">
Nomes
</th>
<th style="text-align:right;">
Kilos
</th>
<th style="text-align:right;">
Crescimento
</th>
<th style="text-align:right;">
Suficiente
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
<span style="display: block; padding: 0 4px; border-radius: 4px; background-color: #ffffff">1</span>
</td>
<td style="text-align:right;">
Sofia
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: lightgrey; width: 20.00%">20,000</span>
</td>
<td style="text-align:right;">
<span style="color: gray">0.10 (rank: 10)</span>
</td>
<td style="text-align:right;">
<span style="color: green; font-weight: bold">Sim</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: block; padding: 0 4px; border-radius: 4px; background-color: #fff5e2">2</span>
</td>
<td style="text-align:right;">
Kiara
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: lightgrey; width: 33.33%">30,000</span>
</td>
<td style="text-align:right;">
<span style="color: gray">0.20 (rank: 09)</span>
</td>
<td style="text-align:right;">
<span>N√£o</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: block; padding: 0 4px; border-radius: 4px; background-color: #ffebc6">3</span>
</td>
<td style="text-align:right;">
Dunki
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: lightgrey; width: 60.00%">50,000</span>
</td>
<td style="text-align:right;">
<span style="color: gray">0.50 (rank: 05)</span>
</td>
<td style="text-align:right;">
<span style="color: green; font-weight: bold">Sim</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: block; padding: 0 4px; border-radius: 4px; background-color: #ffe1aa">4</span>
</td>
<td style="text-align:right;">
Edgar
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: lightgrey; width: 86.67%">70,000</span>
</td>
<td style="text-align:right;">
<span style="color: green">0.95 (rank: 02)</span>
</td>
<td style="text-align:right;">
<span>N√£o</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: block; padding: 0 4px; border-radius: 4px; background-color: #ffd78d">5</span>
</td>
<td style="text-align:right;">
Emilia
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: lightgrey; width: 56.00%">47,000</span>
</td>
<td style="text-align:right;">
<span style="color: green">0.97 (rank: 01)</span>
</td>
<td style="text-align:right;">
<span style="color: green; font-weight: bold">Sim</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: block; padding: 0 4px; border-radius: 4px; background-color: #ffcd71">6</span>
</td>
<td style="text-align:right;">
Gertrudes
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: lightgrey; width: 100.00%">80,000</span>
</td>
<td style="text-align:right;">
<span style="color: gray">0.45 (rank: 06)</span>
</td>
<td style="text-align:right;">
<span>N√£o</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: block; padding: 0 4px; border-radius: 4px; background-color: #ffc355">7</span>
</td>
<td style="text-align:right;">
Genovena
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: lightgrey; width: 53.33%">45,000</span>
</td>
<td style="text-align:right;">
<span style="color: green">0.62 (rank: 03)</span>
</td>
<td style="text-align:right;">
<span>N√£o</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: block; padding: 0 4px; border-radius: 4px; background-color: #ffb938">8</span>
</td>
<td style="text-align:right;">
Champanhe
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: lightgrey; width: 40.00%">35,000</span>
</td>
<td style="text-align:right;">
<span style="color: gray">0.57 (rank: 04)</span>
</td>
<td style="text-align:right;">
<span style="color: green; font-weight: bold">Sim</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: block; padding: 0 4px; border-radius: 4px; background-color: #ffaf1c">9</span>
</td>
<td style="text-align:right;">
Amora
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: lightgrey; width: 20.00%">20,000</span>
</td>
<td style="text-align:right;">
<span style="color: gray">0.37 (rank: 07)</span>
</td>
<td style="text-align:right;">
<span style="color: green; font-weight: bold">Sim</span>
</td>
</tr>
<tr>
<td style="text-align:right;">
<span style="display: block; padding: 0 4px; border-radius: 4px; background-color: #ffa500">10</span>
</td>
<td style="text-align:right;">
Penelope
</td>
<td style="text-align:right;">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: lightgrey; width: 26.67%">25,000</span>
</td>
<td style="text-align:right;">
<span style="color: gray">0.30 (rank: 08)</span>
</td>
<td style="text-align:right;">
<span>N√£o</span>
</td>
</tr>
</tbody>
</table>
<p>Para entender melhor o funcionamento do pacote e conferir mais exemplo de uso confira o <a href="https://cran.r-project.org/web/packages/formattable/vignettes/introduction.html">manual de introdu√ß√£o ao pacote</a> e <a href="https://cran.r-project.org/web/packages/formattable/vignettes/formattable-data-frame.html">manual do pacote</a>, ambos dispon√≠veis no CRAN.</p>
</div>
<div id="o-pacote-knitr-e-kabbleextra" class="section level1">
<h1>O pacote <code>knitr</code> e <code>kabbleExtra</code></h1>
<p>O pacote <code>knitr</code> permite o uso da fun√ß√£o <code>kable()</code> que produz tabelas parecidas com as apresentadas com o pacote <code>DT</code>, por√©m tr√°s diversas outras funcionalidades que podem ser combinadas com as funcionalidades de outros pacotes, como o <code>kableExtra</code> e at√© mesmo o <code>formattable</code> apresentado acima.</p>
<p>Os exemplos aqui apresentados foram retirados (sem altera√ß√µes) do <a href="https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html#overview">manual do pacote no CRAN</a></p>
<pre class="r"><code>#Carregando pacotes
library(knitr)
library(kableExtra)
#Carregando pacote para ajudar na manipula√ß√£o dos dados:
library(dplyr)

mtcars[1:10, 1:2] %&gt;%
  mutate(
    car = row.names(.),
    # Voc√™ n√£o precisa de formato = &quot;html&quot; se voc√™ j√° definiu op√ß√µes (knitr.table.format)
    mpg = cell_spec(mpg, &quot;html&quot;, color = ifelse(mpg &gt; 20, &quot;red&quot;, &quot;blue&quot;)),
    cyl = cell_spec(cyl, &quot;html&quot;, color = &quot;white&quot;, align = &quot;c&quot;, angle = 45, 
                    background = factor(cyl, c(4, 6, 8), 
                                        c(&quot;#666666&quot;, &quot;#999999&quot;, &quot;#BBBBBB&quot;)))) %&gt;%
  select(car, mpg, cyl) %&gt;%
  kable(&quot;html&quot;, escape = F) %&gt;%
  kable_styling(&quot;striped&quot;, full_width = F)</code></pre>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
car
</th>
<th style="text-align:left;">
mpg
</th>
<th style="text-align:left;">
cyl
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Mazda RX4
</td>
<td style="text-align:left;">
<span style="     color: red !important;">21</span>
</td>
<td style="text-align:left;">
<span style="-webkit-transform: rotate(45deg); -moz-transform: rotate(45deg); -ms-transform: rotate(45deg); -o-transform: rotate(45deg); transform: rotate(45deg); display: inline-block; "><span style="     color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #999999 !important;text-align: c;">6</span></span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Mazda RX4 Wag
</td>
<td style="text-align:left;">
<span style="     color: red !important;">21</span>
</td>
<td style="text-align:left;">
<span style="-webkit-transform: rotate(45deg); -moz-transform: rotate(45deg); -ms-transform: rotate(45deg); -o-transform: rotate(45deg); transform: rotate(45deg); display: inline-block; "><span style="     color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #999999 !important;text-align: c;">6</span></span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Datsun 710
</td>
<td style="text-align:left;">
<span style="     color: red !important;">22.8</span>
</td>
<td style="text-align:left;">
<span style="-webkit-transform: rotate(45deg); -moz-transform: rotate(45deg); -ms-transform: rotate(45deg); -o-transform: rotate(45deg); transform: rotate(45deg); display: inline-block; "><span style="     color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #666666 !important;text-align: c;">4</span></span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Hornet 4 Drive
</td>
<td style="text-align:left;">
<span style="     color: red !important;">21.4</span>
</td>
<td style="text-align:left;">
<span style="-webkit-transform: rotate(45deg); -moz-transform: rotate(45deg); -ms-transform: rotate(45deg); -o-transform: rotate(45deg); transform: rotate(45deg); display: inline-block; "><span style="     color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #999999 !important;text-align: c;">6</span></span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Hornet Sportabout
</td>
<td style="text-align:left;">
<span style="     color: blue !important;">18.7</span>
</td>
<td style="text-align:left;">
<span style="-webkit-transform: rotate(45deg); -moz-transform: rotate(45deg); -ms-transform: rotate(45deg); -o-transform: rotate(45deg); transform: rotate(45deg); display: inline-block; "><span style="     color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #BBBBBB !important;text-align: c;">8</span></span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Valiant
</td>
<td style="text-align:left;">
<span style="     color: blue !important;">18.1</span>
</td>
<td style="text-align:left;">
<span style="-webkit-transform: rotate(45deg); -moz-transform: rotate(45deg); -ms-transform: rotate(45deg); -o-transform: rotate(45deg); transform: rotate(45deg); display: inline-block; "><span style="     color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #999999 !important;text-align: c;">6</span></span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Duster 360
</td>
<td style="text-align:left;">
<span style="     color: blue !important;">14.3</span>
</td>
<td style="text-align:left;">
<span style="-webkit-transform: rotate(45deg); -moz-transform: rotate(45deg); -ms-transform: rotate(45deg); -o-transform: rotate(45deg); transform: rotate(45deg); display: inline-block; "><span style="     color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #BBBBBB !important;text-align: c;">8</span></span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Merc 240D
</td>
<td style="text-align:left;">
<span style="     color: red !important;">24.4</span>
</td>
<td style="text-align:left;">
<span style="-webkit-transform: rotate(45deg); -moz-transform: rotate(45deg); -ms-transform: rotate(45deg); -o-transform: rotate(45deg); transform: rotate(45deg); display: inline-block; "><span style="     color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #666666 !important;text-align: c;">4</span></span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Merc 230
</td>
<td style="text-align:left;">
<span style="     color: red !important;">22.8</span>
</td>
<td style="text-align:left;">
<span style="-webkit-transform: rotate(45deg); -moz-transform: rotate(45deg); -ms-transform: rotate(45deg); -o-transform: rotate(45deg); transform: rotate(45deg); display: inline-block; "><span style="     color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #666666 !important;text-align: c;">4</span></span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Merc 280
</td>
<td style="text-align:left;">
<span style="     color: blue !important;">19.2</span>
</td>
<td style="text-align:left;">
<span style="-webkit-transform: rotate(45deg); -moz-transform: rotate(45deg); -ms-transform: rotate(45deg); -o-transform: rotate(45deg); transform: rotate(45deg); display: inline-block; "><span style="     color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: #999999 !important;text-align: c;">6</span></span>
</td>
</tr>
</tbody>
</table>
<pre class="r"><code>#Outro exemplo colorido legal:
iris[1:10, ] %&gt;%
  mutate_if(is.numeric, function(x) {
    cell_spec(x, &quot;html&quot;, bold = T, color = spec_color(x, end = 0.9),
              font_size = spec_font_size(x))
  }) %&gt;%
  mutate(Species = cell_spec(
    Species, &quot;html&quot;, color = &quot;white&quot;, bold = T,
    background = spec_color(1:10, end = 0.9, option = &quot;A&quot;, direction = -1)
  )) %&gt;%
  kable(&quot;html&quot;, escape = F, align = &quot;c&quot;) %&gt;%
  kable_styling(&quot;striped&quot;, full_width = F)</code></pre>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:center;">
Sepal.Length
</th>
<th style="text-align:center;">
Sepal.Width
</th>
<th style="text-align:center;">
Petal.Length
</th>
<th style="text-align:center;">
Petal.Width
</th>
<th style="text-align:center;">
Species
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(40, 174, 128, 1) !important;font-size: 14px;">5.1</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(31, 154, 138, 1) !important;font-size: 13px;">3.5</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(62, 75, 138, 1) !important;font-size: 10px;">1.4</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(53, 96, 141, 1) !important;font-size: 11px;">0.2</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: rgba(254, 206, 145, 1) !important;">setosa</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(37, 131, 142, 1) !important;font-size: 12px;">4.9</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(72, 34, 116, 1) !important;font-size: 9px;">3</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(62, 75, 138, 1) !important;font-size: 10px;">1.4</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(53, 96, 141, 1) !important;font-size: 11px;">0.2</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: rgba(254, 160, 109, 1) !important;">setosa</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(57, 87, 140, 1) !important;font-size: 10px;">4.7</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(56, 88, 140, 1) !important;font-size: 10px;">3.2</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(68, 1, 84, 1) !important;font-size: 8px;">1.3</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(53, 96, 141, 1) !important;font-size: 11px;">0.2</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: rgba(246, 110, 92, 1) !important;">setosa</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(67, 62, 133, 1) !important;font-size: 10px;">4.6</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(67, 62, 133, 1) !important;font-size: 10px;">3.1</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(37, 131, 142, 1) !important;font-size: 12px;">1.5</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(53, 96, 141, 1) !important;font-size: 11px;">0.2</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: rgba(222, 73, 104, 1) !important;">setosa</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(31, 154, 138, 1) !important;font-size: 13px;">5</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(41, 175, 127, 1) !important;font-size: 14px;">3.6</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(62, 75, 138, 1) !important;font-size: 10px;">1.4</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(53, 96, 141, 1) !important;font-size: 11px;">0.2</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: rgba(183, 55, 121, 1) !important;">setosa</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(187, 223, 39, 1) !important;font-size: 16px;">5.4</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(187, 223, 39, 1) !important;font-size: 16px;">3.9</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(187, 223, 39, 1) !important;font-size: 16px;">1.7</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(187, 223, 39, 1) !important;font-size: 16px;">0.4</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: rgba(140, 41, 129, 1) !important;">setosa</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(67, 62, 133, 1) !important;font-size: 10px;">4.6</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(37, 131, 142, 1) !important;font-size: 12px;">3.4</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(62, 75, 138, 1) !important;font-size: 10px;">1.4</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(34, 168, 132, 1) !important;font-size: 13px;">0.3</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: rgba(100, 26, 128, 1) !important;">setosa</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(31, 154, 138, 1) !important;font-size: 13px;">5</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(37, 131, 142, 1) !important;font-size: 12px;">3.4</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(37, 131, 142, 1) !important;font-size: 12px;">1.5</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(53, 96, 141, 1) !important;font-size: 11px;">0.2</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: rgba(60, 15, 112, 1) !important;">setosa</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(68, 1, 84, 1) !important;font-size: 8px;">4.4</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(68, 1, 84, 1) !important;font-size: 8px;">2.9</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(62, 75, 138, 1) !important;font-size: 10px;">1.4</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(53, 96, 141, 1) !important;font-size: 11px;">0.2</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: rgba(20, 14, 54, 1) !important;">setosa</span>
</td>
</tr>
<tr>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(37, 131, 142, 1) !important;font-size: 12px;">4.9</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(67, 62, 133, 1) !important;font-size: 10px;">3.1</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(37, 131, 142, 1) !important;font-size: 12px;">1.5</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: rgba(68, 1, 84, 1) !important;font-size: 8px;">0.1</span>
</td>
<td style="text-align:center;">
<span style=" font-weight: bold;    color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: rgba(0, 0, 4, 1) !important;">setosa</span>
</td>
</tr>
</tbody>
</table>
<p>Mais um exemplo, dessa vez integrando com o pacote <code>formattable</code>:</p>
<pre class="r"><code>#Integrando com formattable
suppressMessages(library(formattable))
mtcars[1:5, 1:4] %&gt;%
  mutate(
    car = row.names(.),
    mpg = color_tile(&quot;white&quot;, &quot;orange&quot;)(mpg),
    cyl = cell_spec(cyl, &quot;html&quot;, angle = (1:5)*60, 
                    background = &quot;red&quot;, color = &quot;white&quot;, align = &quot;center&quot;),
    disp = ifelse(disp &gt; 200,
                  cell_spec(disp, &quot;html&quot;, color = &quot;red&quot;, bold = T),
                  cell_spec(disp, &quot;html&quot;, color = &quot;green&quot;, italic = T)),
    hp = color_bar(&quot;lightgreen&quot;)(hp)
  ) %&gt;%
  select(car, everything()) %&gt;%
  kable(&quot;html&quot;, escape = F) %&gt;%
  kable_styling(&quot;hover&quot;, full_width = F) %&gt;%
  column_spec(5, width = &quot;3cm&quot;) %&gt;%
  add_header_above(c(&quot; &quot;, &quot;Hello&quot; = 2, &quot;World&quot; = 2))</code></pre>
<table class="table table-hover" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="empty-cells: hide;border-bottom:hidden;" colspan="1">
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Hello
</div>
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
World
</div>
</th>
</tr>
<tr>
<th style="text-align:left;">
car
</th>
<th style="text-align:left;">
mpg
</th>
<th style="text-align:left;">
cyl
</th>
<th style="text-align:left;">
disp
</th>
<th style="text-align:left;">
hp
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Mazda RX4
</td>
<td style="text-align:left;">
<span style="display: block; padding: 0 4px; border-radius: 4px; background-color: #ffcc6f">21.0</span>
</td>
<td style="text-align:left;">
<span style="-webkit-transform: rotate(60deg); -moz-transform: rotate(60deg); -ms-transform: rotate(60deg); -o-transform: rotate(60deg); transform: rotate(60deg); display: inline-block; "><span style="     color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: red !important;text-align: center;">6</span></span>
</td>
<td style="text-align:left;">
<span style="  font-style: italic;   color: green !important;">160</span>
</td>
<td style="text-align:left;width: 3cm; ">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: lightgreen; width: 62.86%">110</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Mazda RX4 Wag
</td>
<td style="text-align:left;">
<span style="display: block; padding: 0 4px; border-radius: 4px; background-color: #ffcc6f">21.0</span>
</td>
<td style="text-align:left;">
<span style="-webkit-transform: rotate(120deg); -moz-transform: rotate(120deg); -ms-transform: rotate(120deg); -o-transform: rotate(120deg); transform: rotate(120deg); display: inline-block; "><span style="     color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: red !important;text-align: center;">6</span></span>
</td>
<td style="text-align:left;">
<span style="  font-style: italic;   color: green !important;">160</span>
</td>
<td style="text-align:left;width: 3cm; ">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: lightgreen; width: 62.86%">110</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Datsun 710
</td>
<td style="text-align:left;">
<span style="display: block; padding: 0 4px; border-radius: 4px; background-color: #ffa500">22.8</span>
</td>
<td style="text-align:left;">
<span style="-webkit-transform: rotate(180deg); -moz-transform: rotate(180deg); -ms-transform: rotate(180deg); -o-transform: rotate(180deg); transform: rotate(180deg); display: inline-block; "><span style="     color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: red !important;text-align: center;">4</span></span>
</td>
<td style="text-align:left;">
<span style="  font-style: italic;   color: green !important;">108</span>
</td>
<td style="text-align:left;width: 3cm; ">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: lightgreen; width: 53.14%">93</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Hornet 4 Drive
</td>
<td style="text-align:left;">
<span style="display: block; padding: 0 4px; border-radius: 4px; background-color: #ffc357">21.4</span>
</td>
<td style="text-align:left;">
<span style="-webkit-transform: rotate(240deg); -moz-transform: rotate(240deg); -ms-transform: rotate(240deg); -o-transform: rotate(240deg); transform: rotate(240deg); display: inline-block; "><span style="     color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: red !important;text-align: center;">6</span></span>
</td>
<td style="text-align:left;">
<span style=" font-weight: bold;    color: red !important;">258</span>
</td>
<td style="text-align:left;width: 3cm; ">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: lightgreen; width: 62.86%">110</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Hornet Sportabout
</td>
<td style="text-align:left;">
<span style="display: block; padding: 0 4px; border-radius: 4px; background-color: #ffffff">18.7</span>
</td>
<td style="text-align:left;">
<span style="-webkit-transform: rotate(300deg); -moz-transform: rotate(300deg); -ms-transform: rotate(300deg); -o-transform: rotate(300deg); transform: rotate(300deg); display: inline-block; "><span style="     color: white !important;border-radius: 4px; padding-right: 4px; padding-left: 4px; background-color: red !important;text-align: center;">8</span></span>
</td>
<td style="text-align:left;">
<span style=" font-weight: bold;    color: red !important;">360</span>
</td>
<td style="text-align:left;width: 3cm; ">
<span style="display: inline-block; direction: rtl; unicode-bidi: plaintext; border-radius: 4px; padding-right: 2px; background-color: lightgreen; width: 100.00%">175</span>
</td>
</tr>
</tbody>
</table>
</div>
<div id="o-pacote-sparklike" class="section level1">
<h1>O pacote <code>sparklike</code></h1>
<p>O sparklike √© um pacote √≥timo para enriquecer as apresta√ß√µes de forma que possibilita incluir ‚Äúmini gr√°ficos‚Äù como boxplots, gr√°fico de linhas ou barras diretamente nas tabelas, como se fosse uma coluna do <code>data.frame</code>!</p>
<p>Seu funcionamento √© bem simples e poderoso, apresento aqui alguns exemplo de uso, caso queira conferir mais detalhes, pode conferir o <a href="https://cran.r-project.org/web/packages/sparkline/vignettes/intro_sparkline.html">manual do pacote no CRAN</a> ou <a href="https://omnipotent.net/jquery.sparkline/#s-about">esta p√°gina</a>.</p>
<pre class="r"><code># library(devtools)
# install_github(&#39;htmlwidgets/sparkline&#39;)

#Carregando o pacote:
library(htmlwidgets)
library(sparkline)

#Exemplos de uso:
x = rnorm(20)
sparkline(x)
sparkline(x, type = &#39;bar&#39;)
sparkline(x, type = &#39;box&#39;)</code></pre>
<p><img src="/post/2018-01-12-tabelas-incriveis-com-r/tabelas-incriveis-com-r_files/img22.png" /></p>
<p>Exemplo de tabela para rmarkdown, a partir dessa sequ√™ncia de c√≥digos markdown e R:</p>
<pre class="r"><code>#Seja:
set.seed(1234)
x = rnorm(10)
y = rnorm(10)

#Ao digitar isso:

| Var.  | Sparkline         | Boxplot                       | Bar                          
|-------|-------------------|-------------------------------|------------------------------
| x     | `r sparkline(x)`  | `r sparkline(x, type =&#39;box&#39;)` |`r sparkline(x, type = &#39;bar&#39;)`
| y     | `r sparkline(y)`  | `r sparkline(y, type =&#39;box&#39;)` |`r sparkline(y, type = &#39;bar&#39;)`</code></pre>
<p>Exibe isso:</p>
<p><img src="/post/2018-01-12-tabelas-incriveis-com-r/tabelas-incriveis-com-r_files/img33.png" /></p>
</div>
<div id="o-pacote-rhandsontable" class="section level1">
<h1>O pacote <code>rhandsontable</code></h1>
<p>Mais um pacote repleto de funcionalidades que permitem a implementa√ß√£o de tabelas elegantes para a apresenta√ß√£o de projetos e pesquisas. Por se tratar de um <a href="www.htmlwidgets.org">htmlwidgets</a>, este pacote em especial √© uma boa op√ß√£o quando deseja-se apresentar tabela sem documentos no formato html ou com aplicativos shiny por exemplo.</p>
<p>Primeiramente apresentarei aqui primeiramente um exemplo com tabela de correla√ß√µes utilizando formata√ß√£o condicional:</p>
<pre class="r"><code>#Carregando o pacote:
library(rhandsontable)

#Tabela para correla√ß√µes
rhandsontable(cor(iris[,-5]), readOnly = TRUE, width = 750, height = 300) %&gt;%
  hot_cols(renderer = &quot;
           function (instance, td, row, col, prop, value, cellProperties) {
           Handsontable.renderers.TextRenderer.apply(this, arguments);
           if (row == col) {
           td.style.background = &#39;lightgrey&#39;;
           } else if (col &gt; row) {
           td.style.background = &#39;grey&#39;;
           td.style.color = &#39;grey&#39;;
           } else if (value &lt; -0.75) {
           td.style.background = &#39;pink&#39;;
           } else if (value &gt; 0.75) {
           td.style.background = &#39;lightgreen&#39;;
           }
           }&quot;)</code></pre>
<p><img src="/post/2018-01-12-tabelas-incriveis-com-r/tabelas-incriveis-com-r_files/img44.png" /></p>
<p>Como este pacote possui muitas funcionalidades, apresentarei mais tr√™s exemplos baseados nas instru√ß√µes do pacote e caso queira entender melhor o funcionamento e obter mais exemplos, consultar o <a href="https://cran.r-project.org/web/packages/rhandsontable/vignettes/intro_rhandsontable.html">manual no CRAN</a></p>
<pre class="r"><code>#Tabela com mini gr√°ficos
#criando um data.frame
df &lt;- data.frame(
  id = 1:10, 
  Nomes = c(&quot;Sofia&quot;, &quot;Kiara&quot;, &quot;Dunki&quot;, &quot;Edgar&quot;, &quot;Aline&quot;,&quot;Gertrudes&quot;, &quot;Genovena&quot;, &quot;Champanhe&quot;, &quot;P√©rola&quot;, &quot;Penelope&quot;),
  Kilos = accounting(c(20000, 30000, 50000, 70000, 47000,80000,45000,35000,20000,25000), format = &quot;d&quot;),
  Crescimento = percent(c(0.1, 0.2, 0.5, 0.95, 0.97,0.45,0.62,0.57,0.37, 0.3), format = &quot;d&quot;),
  Suficiente = c(T, F, T, F, T,F,F,T,T,F))

#E os gr√°ficos de barra:
df$chart = c(sapply(1:5,
                    function(x) jsonlite::toJSON(list(values=rnorm(10,10,10),
                                                      options = list(type = &quot;bar&quot;)))),
             sapply(1:5,
                    function(x) jsonlite::toJSON(list(values=rnorm(10,10,10),
                                                      options = list(type = &quot;line&quot;)))))
rhandsontable(df, rowHeaders = NULL, width = 550, height = 300) %&gt;%
  hot_col(&quot;chart&quot;, renderer = htmlwidgets::JS(&quot;renderSparkline&quot;))</code></pre>
<p><img src="/post/2018-01-12-tabelas-incriveis-com-r/tabelas-incriveis-com-r_files/img55.png" /></p>
<p>Tamb√©m podemos incluir coment√°rios em c√©lulas espec√≠ficas da tabela utilizando este pacote, veja:</p>
<p>(Para ver os coment√°rios basta passar o mouse sobre a c√©lula com a marca√ß√£o vermelha na borda)</p>
<pre class="r"><code>#Incluindo comentarios:
comments = matrix(ncol = ncol(df), nrow = nrow(df))
comments[1, 1] = &quot;Exemplo de coment√°rio&quot;
comments[2, 2] = &quot;Outro exemplo de comentario&quot;

rhandsontable(df, comments = comments, width = 550, height = 300)%&gt;%
  hot_col(&quot;chart&quot;, renderer = htmlwidgets::JS(&quot;renderSparkline&quot;))</code></pre>
<p><img src="/post/2018-01-12-tabelas-incriveis-com-r/tabelas-incriveis-com-r_files/img66.png" /></p>
<p>Caso a tabela dos dados seja muito grande, tamb√©m podemos utilizar o pacote para gerar a tabela com a barra de rolar</p>
<pre class="r"><code>#Tabela com barra de rolar para grande base de dados
rhandsontable(mtcars, rowHeaderWidth = 200, width = 700, height = 550)</code></pre>
</div>
<div id="relat√≥rios-muito-mais-bonitos" class="section level1">
<h1>Relat√≥rios muito mais bonitos</h1>
<p>Com essas lindas tabelas seus relat√≥rios ser√£o irresist√≠veis, at√© quem n√£o entende de estat√≠stica vai passar a gostar depois de ver tanta beleza com n√∫meros! Espero que tenha gostado do conte√∫do, caso queira acrescentar ou reportar algo basta entrar em contato.</p>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2018-01-12-tabelas-incriveis-com-r/tabelas-incriveis-com-r/">Tabelas incriveis com R</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Estatistica</category>
      <category>Analise Mutivariada</category>
      <category>Reports</category>
      <category>Pr√°tica</category>
      <category>Analise Explorat√≥ria</category>
      <category domain="tag">Estatistica</category>
      <category domain="tag">Data Mining</category>
      <category domain="tag">gomesfellipe</category>
      <category domain="tag">Pr√°tica</category>
      <category domain="tag">analise multivariada</category>
      <category domain="tag">R Markdown</category>
      <category domain="tag">Tabelas</category>
    </item>
    <item>
      <title>Manipulando dados com dplyr</title>
      <link>https://gomesfellipe.github.io/post/2017-12-07-manipulando-dados-com-dplyr/manipulando-dados-com-dplyr/</link>
      <pubDate>Thu, 07 Dec 2017 00:00:00 +0000</pubDate>
      <author>gomes.fellipe1@gmail.com (Fellipe Carvalho Gomes)</author>
      <guid>https://gomesfellipe.github.io/post/2017-12-07-manipulando-dados-com-dplyr/manipulando-dados-com-dplyr/</guid>
      <description>Manipular bases de dados nunca foi t√£o simples</description>
      <content:encoded>&lt;![CDATA[
        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="o-pacote-dplyr" class="section level1">
<h1>o pacote <code>dplyr</code>:</h1>
<p>A an√°lise explorat√≥ria dos dados √© uma tarefa de bastante relev√¢ncia para entendermos a natureza dos dados e o tempo de an√°lise gastro √© muito precioso. √â necess√°ria bastante curiosidade e criatividade para fazer uma boa an√°lise explorat√≥ria dos dados pois √© dif√≠cil receber aqueles dados bonitinhos igual aos nativos do banco de dados do <strong>R</strong>.</p>
<p>Existem diversos pacotes para as mais variadas necessidades dos cientistas de dados (ou qualquer pessoa que precise fazer alguma an√°lise ou programa√ß√£o estat√≠stica) dispon√≠veis no <a href="https://cran.r-project.org/">CRAN</a> e hoje quero registrar aqui algumas das funcionalidades do pacote <a href="https://cran.r-project.org/package=dplyr">dplyr</a> que s√£o muito √∫teis.</p>
<p>√â um dos pacotes mais poderosos e populares do R, desenvolvido por Hadley Wickham, faz a explora√ß√£o de dados e permite a manipula√ß√£o de dados de forma f√°cil e r√°pida no R.</p>
<p>Segundo sua descri√ß√£o no <a href="https://cran.r-project.org/">CRAN</a>, o <a href="https://cran.r-project.org/package=dplyr">dplyr</a> √© definido como uma ferramenta r√°pida e consistente para trabalhar com data.frames como objetos, tanto na mem√≥ria quanto fora da mem√≥ria. Vamos conferir ent√£o o que de t√£o especial tem nesse pacote.</p>
<div id="operador-pipe" class="section level2">
<h2>Operador pipe: %&gt;%</h2>
<p>O operador <code>%&gt;%</code> √© uma op√ß√£o incrivelmente √∫til para a manipula√ß√£o dos dados, funcionando com uma l√≥gica diferente da nativa do <strong>R</strong>, que executa fun√ß√µes no formato <code>f(x)</code>, o pipe permite que fa√ßamos opera√ß√µes no formato <code>x %&gt;% f()</code> que basicamente funciona da maneira como raciocinamos: ‚ÄúPega esse objeto e executa isso, depois isso, depois isso‚Ä¶‚Äù
Realiza m√∫ltiplas a√ß√µes sem guardar os passos intermedi√°rios.</p>
<!-- Vejamos um resumo dos dados: -->
<p>Vamos come√ßar carregando o pacote:</p>
<pre class="r"><code># Carregando o pacote dplyr
suppressMessages(library(dplyr))</code></pre>
</div>
<div id="selecionando-n-linhas-aleatorias-fun√ß√£o-sample_n" class="section level2">
<h2>Selecionando n linhas aleatorias: fun√ß√£o <code>sample_n()</code></h2>
<pre class="r"><code># Selecionando 5 linhas aleatoriamente
df%&gt;%
  sample_n(5)</code></pre>
</div>
<div id="removendo-linhas-duplicadas-fun√ß√£o-distinct" class="section level2">
<h2>Removendo linhas duplicadas: fun√ß√£o <code>distinct()</code></h2>
<div id="baseado-em-todas-as-vari√°veis" class="section level3">
<h3>Baseado em todas as vari√°veis</h3>
<pre class="r"><code># excluindo linhas iguais
df%&gt;%
  distinct()</code></pre>
</div>
<div id="baseado-em-uma-vari√°vel" class="section level3">
<h3>Baseado em uma vari√°vel</h3>
<pre class="r"><code># excluindo linhas que possuem Datas iguais
df%&gt;%
  distinct(Datas)</code></pre>
</div>
<div id="baseado-em-mais-de-uma-vari√°vel" class="section level3">
<h3>Baseado em mais de uma vari√°vel</h3>
<pre class="r"><code># excluindo linhas que possuem ano e consumo iguais
df%&gt;%
  distinct(ano, consumo)</code></pre>
</div>
</div>
<div id="selecionando-colunas-vari√°veis-fun√ß√£o-select" class="section level2">
<h2>Selecionando colunas (vari√°veis): fun√ß√£o <code>select()</code></h2>
<pre class="r"><code># Selecionando a variavel ano e  todas as vari√°veis de sucesso at√© tonalidade na df
df%&gt;%
  select(ano, sucesso:tonalidade)

# Selecionando todas as variaveis com exce√ß√£o de ano e id
df%&gt;%
  select(-c(ano,id))


# Selecionando todas as variaveis cujo nome inicia com e
df%&gt;%
  select(starts_with(&quot;a&quot;))</code></pre>
<div id="podem-ser-√∫teis-tamb√©m-ends_with-e-contains." class="section level4">
<h4>Podem ser √∫teis tamb√©m <code>ends_with()</code> e <code>contains()</code>.</h4>
</div>
</div>
<div id="reordenando-as-as-colunas-das-vari√°veis-fun√ß√£o-select" class="section level2">
<h2>Reordenando as as colunas das vari√°veis: fun√ß√£o <code>select()</code></h2>
<pre class="r"><code># reorganiza o data frame, iniciando com a vari√°vel Datas e depois as demais
df%&gt;%
  select(Datas,everything())</code></pre>
</div>
<div id="renomeando-vari√°veis-fun√ß√£o-rename" class="section level2">
<h2>Renomeando vari√°veis: fun√ß√£o <code>rename()</code></h2>
<pre class="r"><code># Renomeando a vari√°vel Datas para micro
df%&gt;%
  rename(Dia = Datas)</code></pre>
</div>
<div id="selecionando-um-subconjunto-de-linhas-que-satisfazem-uma-ou-mais-condi√ß√µes-fun√ß√£o-filter" class="section level2">
<h2>Selecionando um subconjunto de linhas que satisfazem uma ou mais condi√ß√µes: fun√ß√£o <code>filter()</code></h2>
<pre class="r"><code># Selecionando somente os indiv√≠duos do sexo masculino
df%&gt;%
  filter( sexo == &quot;Masculino&quot;)


# Selecionando somente os indiv√≠duos do sexo masculino e branco
df%&gt;%
  filter( sexo == &quot;Masculino&quot; &amp; tonalidade == &quot;Branco&quot;)


# Selecionando somente os indiv√≠duos com consumo de 1 a 3 anos e 4 a 7 anos
df%&gt;%
  filter( consumo %in% c(&quot;1 a 3&quot;,&quot;4 a 7&quot;))


# Selecionando indiv√≠duos que ou s√£o homens solteiros ou s√£o mulheres casadas
df%&gt;%
  filter( (estado_civil==&quot;Solteiro&quot; &amp; sexo ==&quot;Masculino&quot;) | (estado_civil==&quot;Casado&quot; &amp; sexo ==&quot;Feminino&quot;) )</code></pre>
</div>
<div id="ordenando-seus-data-frames-fun√ß√£o-arrange" class="section level2">
<h2>Ordenando seus data frames: fun√ß√£o <code>arrange()</code></h2>
<pre class="r"><code># Ordenando os dados pela vari√°vel ano de forma crescente
df%&gt;%
  arrange( ano )

# Ordenando os dados pela vari√°vel ano e consumo
df%&gt;%
  arrange( ano ,consumo)

# Ordenando os dados pela vari√°vel ano de forma decrescente
df%&gt;%
  arrange( desc(ano) )</code></pre>
</div>
<div id="criando-uma-nova-vari√°vel-fun√ß√£o-mutate-e-transmute" class="section level2">
<h2>Criando uma nova vari√°vel: fun√ß√£o <code>mutate()</code> e <code>transmute()</code></h2>
<pre class="r"><code># Criando a vari√°vel ano ao quadrado
df%&gt;%
  mutate( ano2 = ano**2 )

# Criando a vari√°vel Dia e a vari√°vel Mes
df%&gt;%
  mutate( Dia = substring(Datas,1,2), Mes = substring(mensalidade,1,1) )

# Se voc√™ quiser somente manter as vari√°veis criadas
df18 = transmute(mensalidade = substring(Datas,1,2), Mes = substring(mensalidade,1,1) )</code></pre>
</div>
<div id="resumindo-vari√°veis-fun√ß√£o-summarize" class="section level2">
<h2>Resumindo vari√°veis: fun√ß√£o <code>summarize()</code></h2>
<pre class="r"><code># Calculando a media e a mediana da vari√°vel ano
df%&gt;%
  summarise(  media.ano = mean(ano), mediana.ano = median(ano))</code></pre>
</div>
<div id="resumindo-vari√°veis-por-grupo-fun√ß√£o-group_by-e-summarize" class="section level2">
<h2>Resumindo vari√°veis por grupo: fun√ß√£o <code>group_by()</code> e <code>summarize()</code></h2>
<pre class="r"><code># Calculando a media da vari√°vel ano para as combina√ß√µes entre sexo, consumo e estado civil e a frequencia de indiv√≠duos em cada combina√ß√£o
df%&gt;%
  group_by(sexo, consumo, estado_civil)%&gt;%
  summarise(media.ano = mean(ano),frequencia=n())


# Calculando a media da vari√°vel ano para as combina√ß√µes entre ano legal, consumo e estado civil e a frequencia de indiv√≠duos em cada combina√ß√£o
df%&gt;%
  group_by(id, consumo, estado_civil)%&gt;%
  summarise(media.ano = mean(ano),frequencia=n())</code></pre>
</div>
<div id="operador-pipe-1" class="section level2">
<h2>Operador pipe: %&gt;%</h2>
<p>Portanto, o operador <code>%&gt;%</code> realiza m√∫ltiplas a√ß√µes sem guardar os passos intermedi√°rios. Mais alguns exemplos:</p>
<pre class="r"><code># Selecionando as vari√°veis ano e id
df %&gt;%
  select(ano,id)


df %&gt;% select(-estado_civil) %&gt;%
  filter(sexo==&quot;Masculino&quot;) %&gt;%
  group_by(tonalidade,consumo) %&gt;%
  summarise(maximo=max(ano),media=mean(ano))</code></pre>
</div>
<div id="aplicando-fun√ß√µes-em-linhas" class="section level2">
<h2>Aplicando fun√ß√µes em linhas</h2>
<pre class="r"><code>df%&gt;%
  mutate(ano2 = ano**2 )%&gt;%
  rowwise() %&gt;% mutate(Max= max(ano:ano2)) %&gt;%
  select(ano,ano2,Max)</code></pre>
</div>
</div>
<div id="fazendo-a-uni√£o-de-banco-de-dados-distintos" class="section level1">
<h1>Fazendo a uni√£o de banco de dados distintos</h1>
<div id="combinando-duas-bases-de-dados" class="section level2">
<h2>Combinando duas bases de dados</h2>
<p>O pacote dplyr possui um conjunto de fun√ß√µes que nos auxiliam a combinar dos data frames do nosso interesse.</p>
<div id="inner_join" class="section level3">
<h3>inner_join</h3>
<pre class="r"><code># Fun√ß√£o inner_join: Combina as duas bases incluindo todas as vari√°veis de ambas as bases e todas as linhas comuns as duas bases
inner_join(df1,df2,by=&quot;ID&quot;)

inner_join(df1,df3,by=c(&quot;ID&quot;=&quot;Identificacao&quot;))</code></pre>
</div>
<div id="left_join" class="section level3">
<h3>left_join</h3>
<pre class="r"><code># Fun√ß√£o left_join: Combina as duas bases incluindo todas as vari√°veis de ambas as bases e todas as linhas da base a esquerda
left_join(df1,df2,by=&quot;ID&quot;)</code></pre>
</div>
<div id="right_join" class="section level3">
<h3>right_join</h3>
<pre class="r"><code># Fun√ß√£o right_join: Combina as duas bases incluindo todas as vari√°veis de ambas as bases e todas as linhas da base a direita
right_join(df1,df2,by=&quot;ID&quot;)</code></pre>
</div>
<div id="full_join" class="section level3">
<h3>full_join</h3>
<pre class="r"><code># Fun√ß√£o full_join: Combina as duas bases incluindo todas as vari√°veis de ambas as bases e todas as linhas de ambas as bases
full_join(df1,df2,by=&quot;ID&quot;)</code></pre>
</div>
<div id="semi_join" class="section level3">
<h3>semi_join</h3>
<pre class="r"><code># Fun√ß√£o semi_join: Combina as duas bases incluindo as vari√°veis da basea a esquerda e todas as linhas comuns as duas bases
semi_join(df1,df2,by=&quot;ID&quot;)</code></pre>
</div>
<div id="anti_join" class="section level3">
<h3>anti_join</h3>
<pre class="r"><code># Fun√ß√£o anti_join: Combina as duas bases incluindo as vari√°veis da base a esquerda e todas as linhas que n√£o s√£o comuns as duas bases
anti_join(df1,df2,by=&quot;ID&quot;)</code></pre>
</div>
</div>
<div id="combinando-dados-verticalmente" class="section level2">
<h2>Combinando dados verticalmente</h2>
<div id="juntando-por-linhas-comuns-com-intersect" class="section level3">
<h3>Juntando por linhas comuns com intersect</h3>
<pre class="r"><code>#Criando uma base com as linhas comus as duas bases
intersect(d1,d2)</code></pre>
</div>
<div id="juntando-todas-as-linhas-com-union" class="section level3">
<h3>Juntando todas as linhas com union</h3>
<pre class="r"><code>#Criando uma base unindo todas as linhas das duas bases
union(d1,d3)</code></pre>
</div>
<div id="base-com-linhas-distintas-nas-duas-bases-com-setdiff" class="section level3">
<h3>Base com linhas distintas nas duas bases com setdiff</h3>
<pre class="r"><code>#Criando uma base com as linhas distintas nas duas bases
setdiff(d1,d3)</code></pre>
</div>
<div id="empilhando-duas-bases-uma-sobre-a-outra-com-rbind" class="section level3">
<h3>Empilhando duas bases uma sobre a outra com rbind</h3>
<pre class="r"><code>#Empilhando duas bases, uma em cima da outra
rbind(d1,d3)</code></pre>
</div>
<div id="empilhando-duas-bases-lado-a-lado-com-cbind" class="section level3">
<h3>Empilhando duas bases lado a lado com cbind</h3>
<pre class="r"><code>#Empilhando duas bases, uma ao lado da outra
cbind(d3,d4)</code></pre>
<p>√â realmente impressionante como este pacote pode impulsionar nossas habilidades na manipula√ß√£o de dados! Espero que a partir de hoje o <code>%&gt;%</code> n√£o seja mais visto coisa ‚Äúesquisita‚Äù</p>
</div>
</div>
</div>

        <p><strong>Leia o post completo em:</strong> <a href="https://gomesfellipe.github.io/post/2017-12-07-manipulando-dados-com-dplyr/manipulando-dados-com-dplyr/">Manipulando dados com dplyr</a></p>
        <p><em>Este post foi originalmente publicado em <a href="https://gomesfellipe.github.io/">Fellipe Gomes - Data Science Blog</a></em></p>
      ]]></content:encoded>
      <category>Estatistica</category>
      <category>R</category>
      <category>Teoria</category>
      <category>Tidyverse</category>
      <category>Data mining</category>
      <category domain="tag">Estatistica</category>
      <category domain="tag">gomesfellipe</category>
      <category domain="tag">R</category>
      <category domain="tag">Pr√°tica</category>
      <category domain="tag">Dplyr</category>
      <category domain="tag">Tidyverse</category>
      <category domain="tag">Data Mining</category>
    </item>
  </channel>
</rss>